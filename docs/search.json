[
  {
    "objectID": "voting_stacking.html",
    "href": "voting_stacking.html",
    "title": "13¬† Advanced Ensemble Learning",
    "section": "",
    "text": "13.1 Why Ensemble Diverse Models:\nEnsembling models can improve predictive performance by leveraging the diversity and collective wisdom of multiple models. Instead of relying on a single model, we train several individual models and combine their predictions to make a final decision.\nWe have already seen ensemble methods like bagging and boosting. These ensembles primarily reduce error by:\nIn this chapter, we‚Äôll go a step further and explore ensembles that combine different types of models. For example, we might ensemble a linear regression model, a random forest, and a gradient boosting model. The goal is to build stronger predictors by combining models that complement each other‚Äôs strengths and weaknesses.\nBias Reduction:\nVariance Reduction:",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Advanced Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "voting_stacking.html#why-ensemble-diverse-models",
    "href": "voting_stacking.html#why-ensemble-diverse-models",
    "title": "13¬† Advanced Ensemble Learning",
    "section": "",
    "text": "Different models often exhibit distinct biases. For example, a linear regression model might underfit complex patterns, while a Random Forest might overfit noisy data. Combining their predictions can mitigate these biases, leading to a more generalized model.\nExample: If a linear model overpredicts and a boosting model underpredicts for the same instance, averaging their predictions can cancel out the biases.\n\n\n\nAs seen with Random Forests, averaging predictions from multiple models reduces variance, especially when the models are uncorrelated (recall the variance reduction formula for bagging).\nKey Requirement: For effective variance reduction, the models should have low correlation in their predictions.\n\n\n13.1.1 Mathematical Justification for Ensembles\nWe can mathematically illustrate how ensembling improves prediction accuracy using the case of regression.\nLet the predictors be denoted by ( X ), and the response by ( Y ). Assume we have ( m ) individual models $( f_1, f_2, , f_m $). The ensemble predictor is the average:\n\\[\n\\hat{f}_{ensemble}(X) = \\frac{1}{m} \\sum_{i=1}^{m} f_i(X)\n\\]\nThe expected mean squared error (MSE) of the ensemble model is:\n\\[\nE(MSE_{Ensemble}) = E\\left[\\left( \\frac{1}{m} \\sum_{i = 1}^{m} f_i(X) - Y \\right)^2 \\right]\n\\]\nThis expands to:\n\\[\nE(MSE_{Ensemble}) = \\frac{1}{m^2} \\sum_{i = 1}^{m} E\\left[(f_i(X) - Y)^2 \\right] + \\frac{1}{m^2} \\sum_{i \\ne j} E\\left[(f_i(X) - Y)(f_j(X) - Y) \\right]\n\\]\n\\[\n= \\frac{1}{m} \\left( \\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i}) \\right) + \\frac{1}{m^2} \\sum_{i \\ne j} E\\left[(f_i(X) - Y)(f_j(X) - Y) \\right]\n\\]\nIf the individual models $( f_1, , f_m $) are unbiased, the cross terms become covariances:\n\\[\nE(MSE_{Ensemble}) = \\frac{1}{m} \\left( \\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i}) \\right) + \\frac{1}{m^2} \\sum_{i \\ne j} Cov(f_i(X), f_j(X))\n\\]\nIf the models are uncorrelated, the covariance terms vanish:\n\\[\nE(MSE_{Ensemble}) = \\frac{1}{m} \\left( \\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i}) \\right)\n\\]\n\nüîç Conclusion: When the individual models are both unbiased and uncorrelated, the expected MSE of the ensemble is strictly lower than the average MSE of the individual models. This provides a strong theoretical justification for ensembling diverse models to improve prediction accuracy.\nIn practice, the ensemble‚Äôs performance tends to improve unless a single model is significantly more accurate than the others. For example, if one model has near-zero MSE while others perform poorly, ensembling may actually hurt performance. Therefore, the benefit of ensembling depends not only on diversity but also on the relative quality of the individual models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Advanced Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "voting_stacking.html#combining-model-predictions-two-common-approaches",
    "href": "voting_stacking.html#combining-model-predictions-two-common-approaches",
    "title": "13¬† Advanced Ensemble Learning",
    "section": "13.2 Combining Model Predictions: Two Common Approaches",
    "text": "13.2 Combining Model Predictions: Two Common Approaches\nThere are two widely used methods for combining model predictions in ensemble learning:\n\nVoting: Combines the predictions of multiple models directly. In classification, this could be majority voting; in regression, it‚Äôs often simple averaging. Voting is intuitive and works well when the base models are reasonably strong and diverse.\nStacking: Trains a new model (called a meta-learner) to learn how to best combine the predictions of the base models. Stacking can capture more complex relationships among the models and often yields higher accuracy, especially when base models differ significantly in structure or behavior.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Advanced Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "voting_stacking.html#exploring-stacking-and-voting-in-regression",
    "href": "voting_stacking.html#exploring-stacking-and-voting-in-regression",
    "title": "13¬† Advanced Ensemble Learning",
    "section": "13.3 Exploring Stacking and Voting in Regression",
    "text": "13.3 Exploring Stacking and Voting in Regression\nBuilding on the previous chapters where we consistently used the car dataset for regression tasks, we will now explore the application of stacking and voting ensemble methods using the same dataset.\nLet‚Äôs begin by importing the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import cross_val_score,train_test_split, GridSearchCV, ParameterGrid, \\\nStratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import root_mean_squared_error, mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, roc_auc_score, f1_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.ensemble import VotingRegressor, VotingClassifier, StackingRegressor, \\\nStackingClassifier, GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor, \\\nBaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.linear_model import LinearRegression,LogisticRegression, Ridge, ElasticNetCV\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools as it\nimport time as time\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nLoad the dataset\n\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\ncar.head()\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\nvw\nBeetle\n2014\nManual\n55457\nDiesel\n30\n65.3266\n1.6\n7490\n\n\n1\nvauxhall\nGTC\n2017\nManual\n15630\nPetrol\n145\n47.2049\n1.4\n10998\n\n\n2\nmerc\nG Class\n2012\nAutomatic\n43000\nDiesel\n570\n25.1172\n3.0\n44990\n\n\n3\naudi\nRS5\n2019\nAutomatic\n10\nPetrol\n145\n30.5593\n2.9\n51990\n\n\n4\nmerc\nX-CLASS\n2018\nAutomatic\n14000\nDiesel\n240\n35.7168\n2.3\n28990\n\n\n\n\n\n\n\nData preprocessing\n\nX = car.drop(columns=['price'])\ny = car['price']\n\n# extract the categorical columns and put them in a list\ncategorical_features = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nWhile some of the models we will ensemble‚Äîsuch as tree-based models‚Äîdo not require feature scaling or one-hot encoding, we‚Äôll apply a unified preprocessing pipeline for all models to ensure consistency and compatibility. This approach simplifies the workflow and avoids errors when combining models with different preprocessing requirements.\n\n# Create preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n    ])\n\nFor quick prototyping, we started with default model settings to evaluate whether ensembling improves performance. We also set a fixed random state to ensure reproducibility. Below is a list of the models you have learned so far.\n\n# Define models to evaluate\nregressor_models = {\n    'Baseline Linear Regression': LinearRegression(),\n    'Baseline KNN Regressor': KNeighborsRegressor(),\n    'Baseline Decision Tree': DecisionTreeRegressor(random_state=42),\n    'Baseline Random Forest': RandomForestRegressor( random_state=42),\n    'Baseline XGBoost': xgb.XGBRegressor( random_state=42),\n    'Baseline LightGBM': lgb.LGBMRegressor(random_state=42, verbose=0),\n    'Baseline CatBoost': cb.CatBoostRegressor(random_state=42, verbose=0)\n}\n\nWe will first build each model using its default settings to establish baseline performance before applying ensembling techniques.\n\n# store the results\nreg_results = {}\n\n# Loop through models\nfor name, model in regressor_models.items():\n    # Create a pipeline with preprocessing and the model\n    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('model', model)])\n    \n    # Fit the model\n    pipeline.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = pipeline.predict(X_test)\n    \n    # Calculate RMSE\n    rmse = root_mean_squared_error(y_test, y_pred)\n    \n    # Store the results\n    reg_results[name] = rmse\n\n# Convert results to DataFrame for better visualization\nreg_results_df = pd.DataFrame.from_dict(reg_results, orient='index', columns=['RMSE'])\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True)\nreg_results_df.reset_index(inplace=True)\nreg_results_df.columns = ['Model', 'RMSE']\n# Print the results\nreg_results_df\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nBaseline CatBoost\n3296.493137\n\n\n1\nBaseline XGBoost\n3397.155518\n\n\n2\nBaseline Random Forest\n3660.145970\n\n\n3\nBaseline LightGBM\n3729.955778\n\n\n4\nBaseline KNN Regressor\n4062.839680\n\n\n5\nBaseline Decision Tree\n5015.812547\n\n\n6\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nEvidently, CatBoost outperforms the other models using its default settings, achieving the lowest RMSE. This aligns with what you‚Äôve learned‚ÄîCatBoost typically requires less hyperparameter tuning compared to XGBoost and LightGBM.\n\n13.3.1 Voting Regressor\nIn this section, Next, we will build an ensemble model, starting with voting regressor, which assigns equal weight to each base model. In this approach, all predictions are treated equally when averaged to produce the final combined prediction.\n\n\n\n\n\nThe idea behind voting is similar to bagging in that it combines predictions from multiple models by averaging their predictions. However, unlike bagging which typically uses homogeneous models (e.g., multiple decision trees), voting allows for combining heterogeneous models‚Äîdifferent types of algorithms‚Äîto leverage their individual strengths.\nBelow is how you can ensemble the same models using VotingRegressor with the same preprocessor in a pipeline\n\n# Define base regressors (same as before)\nbase_regressor_list = [\n    ('lr', LinearRegression()),\n    ('knn', KNeighborsRegressor()),\n    ('dt', DecisionTreeRegressor(random_state=42)),\n    ('rf', RandomForestRegressor(random_state=42)),\n    ('xgb', xgb.XGBRegressor(random_state=42)),\n    ('lgb', lgb.LGBMRegressor(random_state=42, verbose=0)),\n    ('cat', cb.CatBoostRegressor(random_state=42, verbose=0))\n]\n\n# Create a VotingRegressor with equal weights\nvoting_regressor = VotingRegressor(estimators=base_regressor_list)\n\n# Create a pipeline with preprocessing and voting ensemble\nvoting_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('ensemble', voting_regressor)\n])\n\n# Fit the ensemble model (no need to fit individual models beforehand!)\nvoting_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('ensemble',\n                 VotingRegressor(estimators=[('lr', LinearRegression()),\n                                             ('knn', KNeighborsRegressor()),\n                                             ('dt',\n                                              DecisionTreeRegressor...\n                                                           max_cat_threshold=None,\n                                                           max_cat_to_onehot=None,\n                                                           max_delta_step=None,\n                                                           max_depth=None,\n                                                           max_leaves=None,\n                                                           min_child_weight=None,\n                                                           missing=nan,\n                                                           monotone_constraints=None,\n                                                           multi_strategy=None,\n                                                           n_estimators=None,\n                                                           n_jobs=None,\n                                                           num_parallel_tree=None, ...)),\n                                             ('lgb',\n                                              LGBMRegressor(random_state=42,\n                                                            verbose=0)),\n                                             ('cat',\n                                              &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)]))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('ensemble',\n                 VotingRegressor(estimators=[('lr', LinearRegression()),\n                                             ('knn', KNeighborsRegressor()),\n                                             ('dt',\n                                              DecisionTreeRegressor...\n                                                           max_cat_threshold=None,\n                                                           max_cat_to_onehot=None,\n                                                           max_delta_step=None,\n                                                           max_depth=None,\n                                                           max_leaves=None,\n                                                           min_child_weight=None,\n                                                           missing=nan,\n                                                           monotone_constraints=None,\n                                                           multi_strategy=None,\n                                                           n_estimators=None,\n                                                           n_jobs=None,\n                                                           num_parallel_tree=None, ...)),\n                                             ('lgb',\n                                              LGBMRegressor(random_state=42,\n                                                            verbose=0)),\n                                             ('cat',\n                                              &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)]))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['year', 'mileage', 'tax', 'mpg',\n                                  'engineSize']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) num['year', 'mileage', 'tax', 'mpg', 'engineSize'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') ensemble: VotingRegressor?Documentation for ensemble: VotingRegressorVotingRegressor(estimators=[('lr', LinearRegression()),\n                            ('knn', KNeighborsRegressor()),\n                            ('dt', DecisionTreeRegressor(random_state=42)),\n                            ('rf', RandomForestRegressor(random_state=42)),\n                            ('xgb',\n                             XGBRegressor(base_score=None, booster=None,\n                                          callbacks=None,\n                                          colsample_bylevel=None,\n                                          colsample_bynode=None,\n                                          colsample_bytree=None, device=None,\n                                          early_stopping_rounds=None,\n                                          enab...\n                                          max_cat_threshold=None,\n                                          max_cat_to_onehot=None,\n                                          max_delta_step=None, max_depth=None,\n                                          max_leaves=None,\n                                          min_child_weight=None, missing=nan,\n                                          monotone_constraints=None,\n                                          multi_strategy=None,\n                                          n_estimators=None, n_jobs=None,\n                                          num_parallel_tree=None, ...)),\n                            ('lgb', LGBMRegressor(random_state=42, verbose=0)),\n                            ('cat',\n                             &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)]) lrLinearRegression?Documentation for LinearRegressionLinearRegression() knnKNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor() dtDecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(random_state=42) rfRandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=42) xgbXGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...) lgbLGBMRegressorLGBMRegressor(random_state=42, verbose=0) catCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt; \n\n\nNote: You do not need to fit the models individually before including them in the VotingRegressor. Doing so would result in unnecessary computation and waste time, as VotingRegressor.fit() will handle training for all models internally.\n\n# Predict and evaluate\ny_pred_vote = voting_pipeline.predict(X_test)\nrmse_vote = root_mean_squared_error(y_test, y_pred_vote)\n\n# Add ensemble result to the results_df\nreg_results_df.loc[len(reg_results_df.index)] = ['Voting Regressor', rmse_vote]\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n\n# Show updated results\nreg_results_df\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nBaseline CatBoost\n3296.493137\n\n\n1\nVoting Regressor\n3302.202159\n\n\n2\nBaseline XGBoost\n3397.155518\n\n\n3\nBaseline Random Forest\n3660.145970\n\n\n4\nBaseline LightGBM\n3729.955778\n\n\n5\nBaseline KNN Regressor\n4062.839680\n\n\n6\nBaseline Decision Tree\n5015.812547\n\n\n7\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nIt‚Äôs not surprising that CatBoost outperformed the Voting Ensemble, as a well-optimized gradient boosting model can often outshine an ensemble‚Äîespecially when the ensemble‚Äôs constituent models lack diversity, are poorly tuned, or fail to leverage the dataset‚Äôs characteristics.\n\n13.3.1.1 Strategies to Improve Voting Ensemble Performance\nTo boost the effectiveness of a voting ensemble, consider the following enhancements:\n\nIncrease Model Diversity\nIncorporate a wider range of model types (e.g., SVMs, neural networks, or k-NN) alongside tree-based models. Diverse models are more likely to capture different patterns in the data and produce uncorrelated errors ‚Äî a key factor in ensemble success, as discussed earlier in this chapter.\nTune Base Models Individually\nOptimize each base model using hyperparameter tuning techniques such as Optuna. Well-tuned individual models provide stronger building blocks for the ensemble, improving the final averaged prediction.\nUse Weighted Voting\nInstead of assigning equal importance to each model, assign weights based on their individual performance (e.g., lower RMSE ‚Üí higher weight). This helps emphasize the contribution of stronger models like CatBoost or XGBoost.\n(Note: In a more advanced setup, stacking takes this further by learning the best combination strategy using a meta-model.)\n\n\n\n\n13.3.2 Stacking Regressor\nStacking is a more sophisticated ensembling technique that learns how to best combine multiple base models using a separate meta-model (also called the final_estimator).\n\n\n\n\n\nHere‚Äôs how the process works:\n\nCross-validated predictions for base models\nThe training data is split into K folds (typically using cross-validation). For each fold:\n\nThe base models are trained on the remaining K‚Äì1 folds.\nPredictions are made on the held-out fold.\n\nOut-of-fold predictions become new features\nThis process generates out-of-fold predictions for each training point from each base model (i.e., predictions made on data not seen during training). These predictions are used as features for the next stage.\nTraining the meta-model (final_estimator)\nThe meta-model is trained on these out-of-fold predictions as input features and the original target variable as the response. It learns how to combine the base model outputs to make a better overall prediction.\n\nPlease see the stacking implementation below\nTraining Set\n‚îÇ\n‚îú‚îÄ‚îÄ Cross-Validation Process (k folds)\n‚îÇ ‚îú‚îÄ‚îÄ Fold 1: Train C‚ÇÅ-C‚ÇÑ on folds 2-k ‚Üí Predict on Fold 1 ‚Üí P‚ÇÅ-P‚ÇÑ for Fold 1\n‚îÇ ‚îú‚îÄ‚îÄ Fold 2: Train C‚ÇÅ-C‚ÇÑ on folds 1,3-k ‚Üí Predict on Fold 2 ‚Üí P‚ÇÅ-P‚ÇÑ for Fold 2\n‚îÇ ‚îî‚îÄ‚îÄ ‚Ä¶ (repeat for all k folds)\n‚îÇ\n‚îú‚îÄ‚îÄ Aggregated Meta-Features: P‚ÇÅ-P‚ÇÑ for entire training set\n‚îÇ\n‚îî‚îÄ‚îÄ Meta-Classifier ‚Üí Final Prediction\n\nThe goal of stacking is to leverage the strengths of each individual model while minimizing their weaknesses, often resulting in improved accuracy over any single model.\n\n\n13.3.2.1 Metamodel: Linear regression\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n# Define a meta-model\nmeta_lr = LinearRegression()\n\n# Create the stacking regressor\nstacking_model = StackingRegressor(\n    estimators=base_regressor_list,\n    final_estimator=meta_lr,\n    cv=KFold(n_splits=5, shuffle=True, random_state=42), # ensures all base models use the same 5-fold CV\n    n_jobs=-1,  # Use all available cores\n    passthrough=False  # Set to True if you want to include original features in meta-model\n)\n\n# Wrap with pipeline (using your preprocessor)\nstacking_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('stacking', stacking_model)\n])\n\n# Fit the stacking model\nstacking_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n                                   estimators=[('lr', LinearRegression()),\n                                               ('knn...\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('lgb',\n                                                LGBMRegressor(random_state=42,\n                                                              verbose=0)),\n                                               ('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                                   final_estimator=LinearRegression(),\n                                   n_jobs=-1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n                                   estimators=[('lr', LinearRegression()),\n                                               ('knn...\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('lgb',\n                                                LGBMRegressor(random_state=42,\n                                                              verbose=0)),\n                                               ('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                                   final_estimator=LinearRegression(),\n                                   n_jobs=-1))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['year', 'mileage', 'tax', 'mpg',\n                                  'engineSize']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) num['year', 'mileage', 'tax', 'mpg', 'engineSize'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') stacking: StackingRegressor?Documentation for stacking: StackingRegressorStackingRegressor(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n                  estimators=[('lr', LinearRegression()),\n                              ('knn', KNeighborsRegressor()),\n                              ('dt', DecisionTreeRegressor(random_state=42)),\n                              ('rf', RandomForestRegressor(random_state=42)),\n                              ('xgb',\n                               XGBRegressor(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_byt...\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None, ...)),\n                              ('lgb',\n                               LGBMRegressor(random_state=42, verbose=0)),\n                              ('cat',\n                               &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                  final_estimator=LinearRegression(), n_jobs=-1) lrLinearRegression?Documentation for LinearRegressionLinearRegression() knnKNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor() dtDecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(random_state=42) rfRandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=42) xgbXGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...) lgbLGBMRegressorLGBMRegressor(random_state=42, verbose=0) catCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt; final_estimatorLinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# Predict and evaluate\ny_pred_stack = stacking_pipeline.predict(X_test)\nrmse_stack = root_mean_squared_error(y_test, y_pred_stack)\n\nprint(f\"Stacking Regressor RMSE: {rmse_stack:.2f}\")\n\nStacking Regressor RMSE: 3190.11\n\n\n\n# Append the Stacking Regressor result to results_df\nreg_results_df.loc[len(reg_results_df.index)] = ['Lr Stacking Regressor', rmse_stack]\n\n# Sort by RMSE in ascending order and reset index\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n\n# Display updated results\nreg_results_df\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nLr Stacking Regressor\n3190.112423\n\n\n1\nBaseline CatBoost\n3296.493137\n\n\n2\nVoting Regressor\n3302.202159\n\n\n3\nBaseline XGBoost\n3397.155518\n\n\n4\nBaseline Random Forest\n3660.145970\n\n\n5\nBaseline LightGBM\n3729.955778\n\n\n6\nBaseline KNN Regressor\n4062.839680\n\n\n7\nBaseline Decision Tree\n5015.812547\n\n\n8\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nFrom the results, the stacking Regressor not only outperforms the Voting Regressor, but also surpasses the best individual model‚ÄîCatBoost‚Äîwhen using default settings.\nWhile voting assigns equal weights to each base model, stacking uses a meta-model (in this case, linear regression) to learn an optimal weighted combination of predictions. This allows it to assign different importance to each base model based on their contributions to overall performance.\nNext, let‚Äôs examine the coefficients learned by the stacking model to understand how it weighted each base model‚Äôs prediction.\n\n# Access the trained meta-model inside the stacking pipeline\nmeta_model = stacking_pipeline.named_steps['stacking'].final_estimator_\n\n# Get model names in the same order as the coefficients\nmodel_names = [name for name, _ in base_regressor_list]\n\n# Extract coefficients\ncoefs = meta_model.coef_\n\n# Create a DataFrame to display model weights\nimport pandas as pd\ncoef_df = pd.DataFrame({'Base Model': model_names, 'Meta-Model Coefficient': coefs})\n\n# Sort by weight (optional)\ncoef_df = coef_df.sort_values(by='Meta-Model Coefficient', ascending=False).reset_index(drop=True)\n\n# Show the weights\ncoef_df\n\n\n\n\n\n\n\n\nBase Model\nMeta-Model Coefficient\n\n\n\n\n0\ncat\n0.763462\n\n\n1\nknn\n0.140259\n\n\n2\nxgb\n0.084874\n\n\n3\nlr\n0.043660\n\n\n4\ndt\n0.037379\n\n\n5\nrf\n0.005741\n\n\n6\nlgb\n-0.056054\n\n\n\n\n\n\n\nNote the above coefficients of the meta-model. The model gives the highest weight to the catboost model, and the rf model, and the lowest weight to the lgb model.\nAlso, note that the coefficients need not sum to one.\n\n\n13.3.2.2 Why did LightGBM get the lowest (even negative) coefficient?\n\nStacking is not based on model performance alone\n\nThe meta-model (LinearRegression) doesn‚Äôt assign weights based on RMSE directly.\nInstead, it learns how to combine the model predictions to best fit the training data (specifically, the out-of-fold predictions from each base model).\nSo, even if LightGBM performs decently on its own, its predictions may be redundant or highly correlated with stronger models (e.g., CatBoost or XGBoost).\n\nLightGBM and XGBoost are often similar\n\nBoth are gradient boosting methods ‚Äî if they make very similar predictions, the meta-model may favor just one of them (in this case, XGBoost slightly more).\nIncluding both may introduce multicollinearity, and the linear model tries to suppress redundancy by assigning a near-zero or negative weight.\n\nLinear regression allows negative weights\n\nUnlike voting (which only uses positive weights), a linear model may assign a negative coefficient if it slightly improves the overall fit.\nThis doesn‚Äôt mean the model is ‚Äúbad,‚Äù but rather that its prediction direction may not help much in the presence of other models.\n\n\nTo further improve the RMSE, we will refine the ensemble by removing weaker or highly correlated base models. Specifically, we will retain only the top four models, selected based on the magnitude of their coefficients in the linear regression meta-model.\n\n# Define top 4 base models\ntop4_regressor_list = [\n    ('cat', cb.CatBoostRegressor(random_state=42, verbose=0)),\n    ('rf', RandomForestRegressor(random_state=42)),\n    ('xgb', xgb.XGBRegressor(random_state=42)),\n    ('knn', KNeighborsRegressor())\n]\n\n# Meta-model\ntop4_meta_lr = LinearRegression()\n\n# Build the stacking ensemble\nstacking_top4 = StackingRegressor(\n    estimators=top4_regressor_list,\n    final_estimator=top4_meta_lr,\n    cv=5\n)\n\n# Create pipeline with preprocessing\nstacking_top4_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('stacking', stacking_top4)\n])\n\n# Fit the pipeline\nstacking_top4_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F68DB1D0&gt;),\n                                               ('rf',\n                                                Ra...\n                                                             interaction_constraints=None,\n                                                             learning_rate=None,\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('knn', KNeighborsRegressor())],\n                                   final_estimator=LinearRegression()))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F68DB1D0&gt;),\n                                               ('rf',\n                                                Ra...\n                                                             interaction_constraints=None,\n                                                             learning_rate=None,\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('knn', KNeighborsRegressor())],\n                                   final_estimator=LinearRegression()))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['year', 'mileage', 'tax', 'mpg',\n                                  'engineSize']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) num['year', 'mileage', 'tax', 'mpg', 'engineSize'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') stacking: StackingRegressor?Documentation for stacking: StackingRegressorStackingRegressor(cv=5,\n                  estimators=[('cat',\n                               &lt;catboost.core.CatBoostRegressor object at 0x000001F1F68DB1D0&gt;),\n                              ('rf', RandomForestRegressor(random_state=42)),\n                              ('xgb',\n                               XGBRegressor(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None, device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_m...\n                                            interaction_constraints=None,\n                                            learning_rate=None, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None, ...)),\n                              ('knn', KNeighborsRegressor())],\n                  final_estimator=LinearRegression()) catCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001F1F68DB1D0&gt; rfRandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=42) xgbXGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...) knnKNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor() final_estimatorLinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# Predict and evaluate\ny_pred_top4 = stacking_top4_pipeline.predict(X_test)\nrmse_top4 = root_mean_squared_error(y_test, y_pred_top4)\n\n# Add result to results_df\nreg_results_df.loc[len(reg_results_df.index)] = ['Lr Stacking Top 4 Regressor', rmse_top4]\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n\n# Show updated results\nreg_results_df\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nLr Stacking Top 4 Regressor\n3135.705167\n\n\n1\nLr Stacking Regressor\n3190.112423\n\n\n2\nBaseline CatBoost\n3296.493137\n\n\n3\nVoting Regressor\n3302.202159\n\n\n4\nBaseline XGBoost\n3397.155518\n\n\n5\nBaseline Random Forest\n3660.145970\n\n\n6\nBaseline LightGBM\n3729.955778\n\n\n7\nBaseline KNN Regressor\n4062.839680\n\n\n8\nBaseline Decision Tree\n5015.812547\n\n\n9\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nThe metamodel accuracy improves further, when strong models are ensembled.\n\n# Access the trained meta-model inside the stacking pipeline\nmeta_model_top4 = stacking_top4_pipeline.named_steps['stacking'].final_estimator_\n\n# Get the names of the base models\ntop_model_names = [name for name, _ in top4_regressor_list]\n\n# Extract coefficients\ntop4_coefs = meta_model_top4.coef_\n\n# Create a DataFrame to display the weights\ntop4_coef_df = pd.DataFrame({\n    'Base Model': top_model_names,\n    'Meta-Model Coefficient': top4_coefs\n}).sort_values(by='Meta-Model Coefficient', ascending=False).reset_index(drop=True)\n\n# Show the result\ntop4_coef_df\n\n\n\n\n\n\n\n\nBase Model\nMeta-Model Coefficient\n\n\n\n\n0\ncat\n0.602477\n\n\n1\nrf\n0.156181\n\n\n2\nknn\n0.131000\n\n\n3\nxgb\n0.122639\n\n\n\n\n\n\n\n\n\n13.3.2.3 Choosing the Meta-Model in Stacking\nIn stacking, the meta-model (also called the final_estimator) is responsible for learning how to combine the predictions of the base models. It takes the base models‚Äô predictions as input features and learns how to best map them to the target.\nWhile LinearRegression is a popular default choice due to its simplicity, speed, and interpretability, you are not limited to it. Any regression model can be used as the meta-model, depending on your goals:\n\nUse Ridge or Lasso if regularization is needed (e.g., to handle multicollinearity).\nUse a tree-based model (e.g., RandomForestRegressor, XGBRegressor) if you suspect nonlinear interactions between base model predictions.\nUse SVR, MLPRegressor, or KNeighborsRegressor for flexible, non-parametric alternatives (though often more sensitive to tuning and data scale).\n\nThe choice of meta-model can significantly affect the performance of your stacked ensemble.\n\n\n13.3.2.4 General Guidelines for Choosing a Meta-Model in Stacking\nWhen selecting a meta-model (final estimator) for a stacking ensemble, consider the following:\n\nIf your base models are highly correlated\nUse a regularized linear model like Ridge or Lasso. These models help reduce overfitting by shrinking or zeroing out redundant coefficients.\nIf you suspect nonlinear interactions between base model predictions\nUse a flexible, non-linear model like RandomForestRegressor, XGBRegressor, or MLPRegressor as the meta-model. These can better capture complex relationships among the base model outputs.\nIf interpretability is not a priority\nConsider using more powerful learners (e.g., tree ensembles or neural networks) for the meta-model to potentially boost performance, especially on complex datasets.\n\nThe meta-model should complement your base models and match the complexity of the prediction task.\nIn this car dataset, CatBoost, XGBoost, LightGBM, and Random Forest are all tree-based models, and they are likely generating similar predictions. This can lead to:\n\nMulticollinearity among the base models\n\nUnstable or suboptimal coefficients in the meta-model\n\n\n\n13.3.2.5 Metamodel: Ridge regression\nTo address this, we‚Äôll try using a regularized linear model like Ridge regression as the meta-model. Ridge can better handle redundant information by shrinking correlated coefficients, which helps reduce overfitting and improve generalization.\n\n# Replace meta-model\nstacking_ridge_model = StackingRegressor(\n    estimators=base_regressor_list,\n    final_estimator=Ridge(),\n    cv=5\n)\n\n# Create pipeline with preprocessing\nstacking_ridge_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('stacking', stacking_ridge_model)\n])\n# Fit the pipeline\nstacking_ridge_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('lr', LinearRegression()),\n                                               ('knn', KNeighborsRegressor()),\n                                               ('dt',\n                                                DecisionTreeRe...\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('lgb',\n                                                LGBMRegressor(random_state=42,\n                                                              verbose=0)),\n                                               ('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                                   final_estimator=Ridge()))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('lr', LinearRegression()),\n                                               ('knn', KNeighborsRegressor()),\n                                               ('dt',\n                                                DecisionTreeRe...\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('lgb',\n                                                LGBMRegressor(random_state=42,\n                                                              verbose=0)),\n                                               ('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                                   final_estimator=Ridge()))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['year', 'mileage', 'tax', 'mpg',\n                                  'engineSize']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) num['year', 'mileage', 'tax', 'mpg', 'engineSize'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') stacking: StackingRegressor?Documentation for stacking: StackingRegressorStackingRegressor(cv=5,\n                  estimators=[('lr', LinearRegression()),\n                              ('knn', KNeighborsRegressor()),\n                              ('dt', DecisionTreeRegressor(random_state=42)),\n                              ('rf', RandomForestRegressor(random_state=42)),\n                              ('xgb',\n                               XGBRegressor(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None, device=None,\n                                            early_stopping_rounds=No...\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None, ...)),\n                              ('lgb',\n                               LGBMRegressor(random_state=42, verbose=0)),\n                              ('cat',\n                               &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                  final_estimator=Ridge()) lrLinearRegression?Documentation for LinearRegressionLinearRegression() knnKNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor() dtDecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(random_state=42) rfRandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=42) xgbXGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...) lgbLGBMRegressorLGBMRegressor(random_state=42, verbose=0) catCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt; final_estimatorRidge?Documentation for RidgeRidge() \n\n\n\n# Predict and evaluate\ny_pred_ridge = stacking_ridge_pipeline.predict(X_test)\nrmse_ridge = root_mean_squared_error(y_test, y_pred_ridge)\n# Add result to results_df\nreg_results_df.loc[len(reg_results_df.index)] = ['Stacking with Ridge', rmse_ridge]\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n# Show updated results\nreg_results_df\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nLr Stacking Top 4 Regressor\n3135.705167\n\n\n1\nStacking with Ridge\n3147.701164\n\n\n2\nLr Stacking Regressor\n3190.112423\n\n\n3\nBaseline CatBoost\n3296.493137\n\n\n4\nVoting Regressor\n3302.202159\n\n\n5\nBaseline XGBoost\n3397.155518\n\n\n6\nBaseline Random Forest\n3660.145970\n\n\n7\nBaseline LightGBM\n3729.955778\n\n\n8\nBaseline KNN Regressor\n4062.839680\n\n\n9\nBaseline Decision Tree\n5015.812547\n\n\n10\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nInterpretation of Results:\nFrom the table, we observe that the ‚ÄúLr Stacking Top 4 Regressor‚Äù achieved the lowest RMSE, indicating the best performance among all models evaluated. This model stacked the top 4 individually optimized regressors (CatBoost, XGBoost, Random Forest, and KNN) using a simple linear regression as the meta-model.\nInterestingly, the ‚ÄúStacking with Ridge‚Äù model‚Äîanother stacking model using Ridge Regression as the meta-model‚Äîperformed slightly worse than the simple linear stacking. This could be due to the Ridge model introducing regularization that slightly underweighted some strong base learners, leading to a minor decrease in overall performance.\nAdditional observations:\n\n‚ÄúVoting Regressor‚Äù performed better than most baseline models but was still outperformed by stacking approaches. This is expected, as voting ensembles assign equal weights to base learners, whereas stacking learns optimal weights through a meta-model.\nAll baseline tree-based models (CatBoost, XGBoost, Random Forest, LightGBM) performed reasonably well, with CatBoost being the best among them even without tuning.\nThe baseline linear regression performed the worst due to its limited capability to capture non-linear relationship in the data\n\n\n\n\n13.3.3 Should You Tune Base Models Before Stacking or Voting?\nTuning base models is not strictly required, but it‚Äôs strongly recommended for building effective ensembles ‚Äî especially when using stacking.\n\n13.3.3.1 Benefits of Tuning Base Models\n\nImproved accuracy: Better-tuned models contribute more meaningful predictions.\nReduced noise: Untuned or weak models can hurt ensemble performance, especially in voting, where all models contribute equally.\nStronger stacking: The meta-model in stacking learns how to combine predictions ‚Äî and works best when those predictions are strong and diverse.\n\n\n\n13.3.3.2 Summary\n\n\n\n\n\n\n\n\nEnsemble Method\nShould You Tune Base Models?\nWhy?\n\n\n\n\nVoting\nOptional but helpful\nWeak models can dilute strong ones.\n\n\nStacking\nStrongly recommended\nMeta-model depends on meaningful base predictions.\n\n\n\nFor quick prototyping, so far we have used default model settings to assess whether ensembling could improve performance. Now that we have identified the top 4 models, we will leverage their optimized versions for stacking. These models were fine-tuned in previous chapters using tools like Optuna, RandomizedSearchCV, or BayesSearchCV to efficiently search for the best hyperparameters.\n\n# tuned catboost model from section 11.3.6\ncatboost_tuned = cb.CatBoostRegressor(\n    iterations=1021,\n    learning_rate=0.0536,\n    depth=7,\n    l2_leaf_reg=2.56,\n    random_seed=42,\n    min_data_in_leaf=28,\n    bagging_temperature=0.034,\n    random_strength=2.2,\n    verbose=0\n)\n# tuned random forest model from section 7.4.4\nrf_tuned = RandomForestRegressor(\n    n_estimators=60,\n    max_depth=28,\n    max_features=0.58,\n    max_samples=1.0,\n    random_state=42\n)\n\n#tuned knn model from section 2.4.1\nknn_tuned = KNeighborsRegressor(\n    n_neighbors=8,\n    weights='distance',\n    metric='manhattan',\n    p=2\n)\n\n# tuned xgboost model from section  10.5.10\nxgb_tuned = xgb.XGBRegressor(\n    n_estimators=193,\n    learning_rate=0.15,\n    max_depth=7,\n    min_child_weight=1,\n    gamma=0,\n    subsample=0.78,\n    reg_lambda=9.33,\n    reg_alpha=5.0,\n    colsample_bytree=0.635,\n    random_state=42\n)\n\n\n# Define models to evaluate\ntuned_regressor_models = {\n    'Optmized KNN Regressor': knn_tuned,\n    'Optmized Random Forest': rf_tuned,\n    'Optmized XGBoost': xgb_tuned,\n    'Optmized CatBoost': catboost_tuned\n}\n\n\n# Initialize results dictionary\ntuned_top4_results = {}\n\n# Loop through tuned regressor models\nfor name, model in tuned_regressor_models.items():\n    # Create a pipeline with preprocessing and the model\n    tuned_pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n    \n    # Fit the model\n    tuned_pipeline.fit(X_train, y_train)\n    \n    # Make predictions\n    y_tuned_pred = tuned_pipeline.predict(X_test)\n    \n    # Calculate RMSE (correct target used)\n    tuned_rmse = root_mean_squared_error(y_test, y_tuned_pred)\n    \n    # Store the result with a consistent label\n    tuned_top4_results[name] = tuned_rmse\n\n# Append new results to reg_results_df\nfor name, rmse in tuned_top4_results.items():\n    reg_results_df.loc[len(reg_results_df)] = [name, rmse]\n\n# Sort and reset index\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n\n# Show updated results\nreg_results_df\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nOptmized XGBoost\n3105.835205\n\n\n1\nLr Stacking Top 4 Regressor\n3135.705167\n\n\n2\nStacking with Ridge\n3147.701164\n\n\n3\nOptmized CatBoost\n3175.161327\n\n\n4\nLr Stacking Regressor\n3190.112423\n\n\n5\nOptmized Random Forest\n3279.094977\n\n\n6\nBaseline CatBoost\n3296.493137\n\n\n7\nVoting Regressor\n3302.202159\n\n\n8\nBaseline XGBoost\n3397.155518\n\n\n9\nBaseline Random Forest\n3660.145970\n\n\n10\nOptmized KNN Regressor\n3680.370319\n\n\n11\nBaseline LightGBM\n3729.955778\n\n\n12\nBaseline KNN Regressor\n4062.839680\n\n\n13\nBaseline Decision Tree\n5015.812547\n\n\n14\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\n\n\n13.3.3.3 Metamodel: Linear Regression on Tuned Top 4 Models\n\n# define top 4 optimized base models\ntuned_top4_regressor_list = [\n    ('cat', catboost_tuned),\n    ('rf', rf_tuned),\n    ('knn', knn_tuned),\n    ('xgb', xgb_tuned)\n]\n\n\n# Build the stacking ensemble\nstacking_tuned_top4 = StackingRegressor(\n    estimators=tuned_top4_regressor_list,\n    final_estimator=LinearRegression(),\n    cv=5\n)\n# Create pipeline with preprocessing\nstacking_tuned_top4_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('stacking', stacking_tuned_top4)\n])\n\n# Fit the pipeline\nstacking_tuned_top4_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F65C5A60&gt;),\n                                               ('rf',\n                                                Ra...\n                                                             importance_type=None,\n                                                             interaction_constraints=None,\n                                                             learning_rate=0.15,\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=7,\n                                                             max_leaves=None,\n                                                             min_child_weight=1,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=193,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...))],\n                                   final_estimator=LinearRegression()))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F65C5A60&gt;),\n                                               ('rf',\n                                                Ra...\n                                                             importance_type=None,\n                                                             interaction_constraints=None,\n                                                             learning_rate=0.15,\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=7,\n                                                             max_leaves=None,\n                                                             min_child_weight=1,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=193,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...))],\n                                   final_estimator=LinearRegression()))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['year', 'mileage', 'tax', 'mpg',\n                                  'engineSize']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) num['year', 'mileage', 'tax', 'mpg', 'engineSize'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') stacking: StackingRegressor?Documentation for stacking: StackingRegressorStackingRegressor(cv=5,\n                  estimators=[('cat',\n                               &lt;catboost.core.CatBoostRegressor object at 0x000001F1F65C5A60&gt;),\n                              ('rf',\n                               RandomForestRegressor(max_depth=28,\n                                                     max_features=0.58,\n                                                     max_samples=1.0,\n                                                     n_estimators=60,\n                                                     random_state=42)),\n                              ('knn',\n                               KNeighborsRegressor(metric='manhattan',\n                                                   n_neighbors=8,\n                                                   weights='distance')),\n                              ('xgb',\n                               XGBRegressor(base_score=None, booster=None,\n                                            callback...\n                                            grow_policy=None,\n                                            importance_type=None,\n                                            interaction_constraints=None,\n                                            learning_rate=0.15, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=7,\n                                            max_leaves=None, min_child_weight=1,\n                                            missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=193, n_jobs=None,\n                                            num_parallel_tree=None, ...))],\n                  final_estimator=LinearRegression()) catCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001F1F65C5A60&gt; rfRandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(max_depth=28, max_features=0.58, max_samples=1.0,\n                      n_estimators=60, random_state=42) knnKNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor(metric='manhattan', n_neighbors=8, weights='distance') xgbXGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.635, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=0, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=0.15, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=7,\n             max_leaves=None, min_child_weight=1, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=193,\n             n_jobs=None, num_parallel_tree=None, ...) final_estimatorLinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# Predict and evaluate\ny_pred_top4_tuned = stacking_tuned_top4_pipeline.predict(X_test)\nrmse_top4_tuned = root_mean_squared_error(y_test, y_pred_top4_tuned)\n\n# Add result to results_df\nreg_results_df.loc[len(reg_results_df.index)] = ['Stacking Optimized Top 4 Models', rmse_top4_tuned]\n\n# Sort and reset index\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n\n# Display results\nreg_results_df\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nStacking Optimized Top 4 Models\n2971.389848\n\n\n1\nOptmized XGBoost\n3105.835205\n\n\n2\nLr Stacking Top 4 Regressor\n3135.705167\n\n\n3\nStacking with Ridge\n3147.701164\n\n\n4\nOptmized CatBoost\n3175.161327\n\n\n5\nLr Stacking Regressor\n3190.112423\n\n\n6\nOptmized Random Forest\n3279.094977\n\n\n7\nBaseline CatBoost\n3296.493137\n\n\n8\nVoting Regressor\n3302.202159\n\n\n9\nBaseline XGBoost\n3397.155518\n\n\n10\nBaseline Random Forest\n3660.145970\n\n\n11\nOptmized KNN Regressor\n3680.370319\n\n\n12\nBaseline LightGBM\n3729.955778\n\n\n13\nBaseline KNN Regressor\n4062.839680\n\n\n14\nBaseline Decision Tree\n5015.812547\n\n\n15\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nKey Takeaway\n\nBy using the tuned regressors, the performance of our stacking model improved further, achieving the lowest RMSE overall.\nOptimized regularized boosting tree models‚Äîsuch as XGBoost and CatBoost‚Äîdemonstrated strong predictive performance even without stacking.\nThese optimized models outperformed several stacking models that used default base regressors.\nThis highlights the importance of proper hyperparameter tuning, especially for powerful individual learners.\n\nOverall, stacking with well-tuned base models and a simple meta-model (even without regularization) offers a significant boost in predictive accuracy, outperforming both individual models and simple averaging ensembles.\n\n\n\n13.3.4 Ensembling Models Based on Different Feature Sets\nEnsemble learning benefits significantly from model diversity‚Äîit helps reduce overfitting and improves the robustness of predictions. By combining models that learn from different perspectives, an ensemble has the potential to outperform any single model.\nSo far, we‚Äôve shown how to ensemble diverse models by varying algorithms. Another powerful approach is to introduce diversity through different feature subsets, allowing each model to focus on specific aspects of the data. This idea is similar to how methods like bagging and boosting inject diversity through data sampling or random feature selection (max_features, colsample_bytree, etc.).\nAn alternative way to introduce such diversity is to train strong base models on different subsets of features. These subsets can be selected using techniques like: - Polynomial features - Tree-based feature importance - Stepwise feature selection\nBy assigning tailored feature sets to different models, tuning each individually, and then combining them in a stacked ensemble, we can potentially achieve even lower RMSE.\nBuilding such a system involves: 1. Selecting a meaningful feature subset for each base model, 2. Tuning each model on its selected features, and 3. Ensembling them using a meta-learner.\nThis more advanced form of stacking is a powerful technique‚ÄîI‚Äôll leave its full exploration to you!",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Advanced Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "voting_stacking.html#exploring-stacking-and-voting-in-classification",
    "href": "voting_stacking.html#exploring-stacking-and-voting-in-classification",
    "title": "13¬† Advanced Ensemble Learning",
    "section": "13.4 Exploring Stacking and Voting in Classification",
    "text": "13.4 Exploring Stacking and Voting in Classification\n\n13.4.1 Voting Classifier\nThere are two main types of voting in classification ensembles:\nHard Voting:\n\nEach base model votes for a class label.\nThe final prediction is the majority class across models.\nSimple and interpretable.\n\nSoft Voting: - Each base model predicts class probabilities. - The probabilities are averaged, and the final prediction is the class with the highest average probability. - Usually performs better than hard voting (especially with well-calibrated models).\nThe figure below illustrates the difference between hard and soft voting.\n\n\n\n\n\nWe‚Äôll build an ensemble of models to predict whether a person has heart disease, focusing on improving classification accuracy.\n\nheart_data = pd.read_csv('./Datasets/Heart.csv')\nprint(heart_data.shape)\nheart_data.head()\n\n(303, 14)\n\n\n\n\n\n\n\n\n\nAge\nSex\nChestPain\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nThal\nAHD\n\n\n\n\n0\n63\n1\ntypical\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\nfixed\nNo\n\n\n1\n67\n1\nasymptomatic\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\nnormal\nYes\n\n\n2\n67\n1\nasymptomatic\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\nreversable\nYes\n\n\n3\n37\n1\nnonanginal\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\nnormal\nNo\n\n\n4\n41\n0\nnontypical\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\nnormal\nNo\n\n\n\n\n\n\n\n\n# Define target and features\nclf_X = heart_data.drop(columns='AHD')\nclf_y = heart_data['AHD'].map({'Yes': 1, 'No': 0})  # Convert target to binary 0/1\n\n# Train-test split\nclf_X_train, clf_X_test, clf_y_train, clf_y_test = train_test_split(\n    clf_X, clf_y, stratify=clf_y, test_size=0.2, random_state=42\n)\n\n\n# Identify column types\nclf_numeric_cols = ['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak']\nclf_categorical_cols = ['Sex', 'ChestPain', 'Fbs', 'RestECG', 'ExAng', 'Slope', 'Ca', 'Thal']\n\n# Preprocessing pipelines\nclf_numeric_transformer = StandardScaler()\nclf_categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\nclf_preprocessor = ColumnTransformer([\n    ('num', clf_numeric_transformer, clf_numeric_cols),\n    ('cat', clf_categorical_transformer, clf_categorical_cols)\n])\n\n\n# Define all 8 base classifiers\nbase_clf_models = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n    'Decision Tree': DecisionTreeClassifier(random_state=42),\n    'KNN': KNeighborsClassifier(),\n    'Random Forest': RandomForestClassifier(random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n    'XGBoost': xgb.XGBClassifier(eval_metric='logloss', random_state=42),\n    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1),\n    'CatBoost': cb.CatBoostClassifier(random_state=42, verbose=0)\n}\n\n# Store results for both base and ensemble models\nclf_results = []\n\nfor name, model in base_clf_models.items():\n    # Create pipeline\n    base_clf_pipeline = Pipeline([\n        ('preprocessor', clf_preprocessor),\n        ('classifier', model)\n    ])\n    \n    # Train\n    base_clf_pipeline.fit(clf_X_train, clf_y_train)\n    \n    # Predict\n    base_y_pred = base_clf_pipeline.predict(clf_X_test)\n    base_y_proba = base_clf_pipeline.predict_proba(clf_X_test)[:, 1]\n    \n    # Compute metrics\n    base_acc = accuracy_score(clf_y_test, base_y_pred)\n    base_roc = roc_auc_score(clf_y_test, base_y_proba)\n    base_f1 = f1_score(clf_y_test, base_y_pred)\n    \n    clf_results.append({\n        'Model': name,\n        'Accuracy': base_acc,\n        'ROC AUC': base_roc,\n        'F1 Score': base_f1\n    })\n\n# Create DataFrame\nclf_results_df = pd.DataFrame(clf_results).sort_values(by='ROC AUC', ascending=False).reset_index(drop=True)\nclf_results_df\n\n\n\n\n\n\n\n\nModel\nAccuracy\nROC AUC\nF1 Score\n\n\n\n\n0\nLightGBM\n0.901639\n0.965368\n0.896552\n\n\n1\nLogistic Regression\n0.885246\n0.958874\n0.881356\n\n\n2\nKNN\n0.868852\n0.946970\n0.862069\n\n\n3\nCatBoost\n0.885246\n0.945887\n0.885246\n\n\n4\nGradient Boosting\n0.868852\n0.943723\n0.866667\n\n\n5\nRandom Forest\n0.868852\n0.937229\n0.866667\n\n\n6\nXGBoost\n0.836066\n0.920996\n0.838710\n\n\n7\nDecision Tree\n0.655738\n0.660173\n0.655738\n\n\n\n\n\n\n\nDefine base classifiers (as tuples) for ensemble models like VotingClassifier or StackingClassifier\n\n# Define all classifiers (as tuples for VotingClassifier)\nclf_base_learners = [\n    ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n    ('dt', DecisionTreeClassifier(random_state=42)),\n    ('knn', KNeighborsClassifier()),\n    ('rf', RandomForestClassifier(random_state=42)),\n    ('gb', GradientBoostingClassifier(random_state=42)),\n    ('xgb', xgb.XGBClassifier(eval_metric='logloss', random_state=42)),\n    ('lgb', lgb.LGBMClassifier(random_state=42)),\n    ('cat', cb.CatBoostClassifier(random_state=42, verbose=0))\n]\n\n\n13.4.1.1 Hard Voting Ensemble\n\n# Create a hard voting classifier (voting='hard')\nens_clf_hard_voting = VotingClassifier(\n    estimators=clf_base_learners,\n    voting='hard'  # Use predicted class labels for majority rule voting\n)\n\n# Pipeline with preprocessing\nens_clf_hard_voting_pipeline = Pipeline([\n    ('preprocessor', clf_preprocessor),\n    ('voting', ens_clf_hard_voting)\n])\n\n# Fit model\nens_clf_hard_voting_pipeline.fit(clf_X_train, clf_y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['Age', 'RestBP', 'Chol',\n                                                   'MaxHR', 'Oldpeak']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Sex', 'ChestPain', 'Fbs',\n                                                   'RestECG', 'ExAng', 'Slope',\n                                                   'Ca', 'Thal'])])),\n                ('voting',\n                 VotingClassifier(estimators=[('lr',\n                                               LogisticRegression(max_iter=1000,\n                                                                  random_state=42)),\n                                              ('dt',\n                                               Dec...\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                              ('lgb',\n                                               LGBMClassifier(random_state=42)),\n                                              ('cat',\n                                               &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)]))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['Age', 'RestBP', 'Chol',\n                                                   'MaxHR', 'Oldpeak']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Sex', 'ChestPain', 'Fbs',\n                                                   'RestECG', 'ExAng', 'Slope',\n                                                   'Ca', 'Thal'])])),\n                ('voting',\n                 VotingClassifier(estimators=[('lr',\n                                               LogisticRegression(max_iter=1000,\n                                                                  random_state=42)),\n                                              ('dt',\n                                               Dec...\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                              ('lgb',\n                                               LGBMClassifier(random_state=42)),\n                                              ('cat',\n                                               &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)]))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['Sex', 'ChestPain', 'Fbs', 'RestECG', 'ExAng',\n                                  'Slope', 'Ca', 'Thal'])]) num['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['Sex', 'ChestPain', 'Fbs', 'RestECG', 'ExAng', 'Slope', 'Ca', 'Thal'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') voting: VotingClassifier?Documentation for voting: VotingClassifierVotingClassifier(estimators=[('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('dt', DecisionTreeClassifier(random_state=42)),\n                             ('knn', KNeighborsClassifier()),\n                             ('rf', RandomForestClassifier(random_state=42)),\n                             ('gb',\n                              GradientBoostingClassifier(random_state=42)),\n                             ('xgb',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,...\n                                            learning_rate=None, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None, ...)),\n                             ('lgb', LGBMClassifier(random_state=42)),\n                             ('cat',\n                              &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)]) lrLogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) dtDecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(random_state=42) knnKNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier() rfRandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(random_state=42) gbGradientBoostingClassifier?Documentation for GradientBoostingClassifierGradientBoostingClassifier(random_state=42) xgbXGBClassifier?Documentation for XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, ...) lgbLGBMClassifierLGBMClassifier(random_state=42) catCatBoostClassifier&lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt; \n\n\n\n# Predict\nens_y_pred_hard = ens_clf_hard_voting_pipeline.predict(clf_X_test)\n\n# Compute metrics\nens_acc_hard = accuracy_score(clf_y_test, ens_y_pred_hard)\nens_roc_hard = roc_auc_score(clf_y_test, ens_y_pred_hard)\nens_f1_hard = f1_score(clf_y_test, ens_y_pred_hard)\n\n# Store results\nclf_results.append({\n    'Model': 'Hard Voting Classifier',\n    'Accuracy': ens_acc_hard,\n    'ROC AUC': ens_roc_hard,\n    'F1 Score': ens_f1_hard\n})\n\n# Create or update results DataFrame\nclf_results_df = pd.DataFrame(clf_results).sort_values(by='ROC AUC', ascending=False).reset_index(drop=True)\nclf_results_df\n\n\n\n\n\n\n\n\nModel\nAccuracy\nROC AUC\nF1 Score\n\n\n\n\n0\nLightGBM\n0.901639\n0.965368\n0.896552\n\n\n1\nLogistic Regression\n0.885246\n0.958874\n0.881356\n\n\n2\nKNN\n0.868852\n0.946970\n0.862069\n\n\n3\nCatBoost\n0.885246\n0.945887\n0.885246\n\n\n4\nGradient Boosting\n0.868852\n0.943723\n0.866667\n\n\n5\nRandom Forest\n0.868852\n0.937229\n0.866667\n\n\n6\nXGBoost\n0.836066\n0.920996\n0.838710\n\n\n7\nHard Voting Classifier\n0.901639\n0.906385\n0.900000\n\n\n8\nDecision Tree\n0.655738\n0.660173\n0.655738\n\n\n\n\n\n\n\n\n\n13.4.1.2 Soft Voting Ensemble\n\n## Soft Voting Ensemble\n\n# Define soft voting classifier\nens_clf_soft_voting = VotingClassifier(\n    estimators=clf_base_learners,\n    voting='soft'  # Use predicted probabilities\n)\n\n# Create pipeline with preprocessing\nens_clf_soft_voting_pipeline = Pipeline([\n    ('preprocessor', clf_preprocessor),\n    ('voting', ens_clf_soft_voting)\n])\n\n# Fit model\nens_clf_soft_voting_pipeline.fit(clf_X_train, clf_y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['Age', 'RestBP', 'Chol',\n                                                   'MaxHR', 'Oldpeak']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Sex', 'ChestPain', 'Fbs',\n                                                   'RestECG', 'ExAng', 'Slope',\n                                                   'Ca', 'Thal'])])),\n                ('voting',\n                 VotingClassifier(estimators=[('lr',\n                                               LogisticRegression(max_iter=1000,\n                                                                  random_state=42)),\n                                              ('dt',\n                                               Dec...\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                              ('lgb',\n                                               LGBMClassifier(random_state=42)),\n                                              ('cat',\n                                               &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)],\n                                  voting='soft'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['Age', 'RestBP', 'Chol',\n                                                   'MaxHR', 'Oldpeak']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Sex', 'ChestPain', 'Fbs',\n                                                   'RestECG', 'ExAng', 'Slope',\n                                                   'Ca', 'Thal'])])),\n                ('voting',\n                 VotingClassifier(estimators=[('lr',\n                                               LogisticRegression(max_iter=1000,\n                                                                  random_state=42)),\n                                              ('dt',\n                                               Dec...\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                              ('lgb',\n                                               LGBMClassifier(random_state=42)),\n                                              ('cat',\n                                               &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)],\n                                  voting='soft'))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['Sex', 'ChestPain', 'Fbs', 'RestECG', 'ExAng',\n                                  'Slope', 'Ca', 'Thal'])]) num['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['Sex', 'ChestPain', 'Fbs', 'RestECG', 'ExAng', 'Slope', 'Ca', 'Thal'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') voting: VotingClassifier?Documentation for voting: VotingClassifierVotingClassifier(estimators=[('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('dt', DecisionTreeClassifier(random_state=42)),\n                             ('knn', KNeighborsClassifier()),\n                             ('rf', RandomForestClassifier(random_state=42)),\n                             ('gb',\n                              GradientBoostingClassifier(random_state=42)),\n                             ('xgb',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,...\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None, ...)),\n                             ('lgb', LGBMClassifier(random_state=42)),\n                             ('cat',\n                              &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)],\n                 voting='soft') lrLogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) dtDecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(random_state=42) knnKNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier() rfRandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(random_state=42) gbGradientBoostingClassifier?Documentation for GradientBoostingClassifierGradientBoostingClassifier(random_state=42) xgbXGBClassifier?Documentation for XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, ...) lgbLGBMClassifierLGBMClassifier(random_state=42) catCatBoostClassifier&lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt; \n\n\n\n# Predict\nens_y_pred_soft = ens_clf_soft_voting_pipeline.predict(clf_X_test)\n\n# Compute metrics\nens_acc_soft = accuracy_score(clf_y_test, ens_y_pred_soft)\nens_roc_soft = roc_auc_score(clf_y_test, ens_y_pred_soft)\nens_f1_soft = f1_score(clf_y_test, ens_y_pred_soft)\n\n# Store results\nclf_results.append({\n    'Model': 'Soft Voting Classifier',\n    'Accuracy': ens_acc_soft,\n    'ROC AUC': ens_roc_soft,\n    'F1 Score': ens_f1_soft\n})\n\n# Update results DataFrame\nclf_results_df = pd.DataFrame(clf_results).sort_values(by='ROC AUC', ascending=False).reset_index(drop=True)\nclf_results_df\n\n\n\n\n\n\n\n\nModel\nAccuracy\nROC AUC\nF1 Score\n\n\n\n\n0\nLightGBM\n0.901639\n0.965368\n0.896552\n\n\n1\nLogistic Regression\n0.885246\n0.958874\n0.881356\n\n\n2\nKNN\n0.868852\n0.946970\n0.862069\n\n\n3\nCatBoost\n0.885246\n0.945887\n0.885246\n\n\n4\nGradient Boosting\n0.868852\n0.943723\n0.866667\n\n\n5\nRandom Forest\n0.868852\n0.937229\n0.866667\n\n\n6\nXGBoost\n0.836066\n0.920996\n0.838710\n\n\n7\nHard Voting Classifier\n0.901639\n0.906385\n0.900000\n\n\n8\nSoft Voting Classifier\n0.901639\n0.906385\n0.900000\n\n\n9\nDecision Tree\n0.655738\n0.660173\n0.655738\n\n\n\n\n\n\n\n\n\n\n13.4.2 Stacking Classifier\nConceptually, the idea is similar to that of Stacking regressor.\n\n# Create a meta-model\nens_meta_clf = LogisticRegression(max_iter=1000, random_state=42)\n\n# Define the stacking classifier\nens_clf_stacking = StackingClassifier(\n    estimators=clf_base_learners,\n    final_estimator=ens_meta_clf,\n    cv=5\n)\n\n# Create pipeline with preprocessing\nens_clf_stacking_pipeline = Pipeline([\n    ('preprocessor', clf_preprocessor),\n    ('stacking', ens_clf_stacking)\n])\n\n# Fit the pipeline\nens_clf_stacking_pipeline.fit(clf_X_train, clf_y_train)\n\n# Predict\nens_y_pred_stack = ens_clf_stacking_pipeline.predict(clf_X_test)\n\n# Compute metrics\nens_acc_stack = accuracy_score(clf_y_test, ens_y_pred_stack)\nens_roc_stack = roc_auc_score(clf_y_test, ens_y_pred_stack)\nens_f1_stack = f1_score(clf_y_test, ens_y_pred_stack)\n\n# Store results\nclf_results.append({\n    'Model': 'Stacking Classifier',\n    'Accuracy': ens_acc_stack,\n    'ROC AUC': ens_roc_stack,\n    'F1 Score': ens_f1_stack\n})\n\n# Update results DataFrame\nclf_results_df = pd.DataFrame(clf_results).sort_values(by='ROC AUC', ascending=False).reset_index(drop=True)\nclf_results_df\n\n\n\n\n\n\n\n\nModel\nAccuracy\nROC AUC\nF1 Score\n\n\n\n\n0\nLightGBM\n0.901639\n0.965368\n0.896552\n\n\n1\nLogistic Regression\n0.885246\n0.958874\n0.881356\n\n\n2\nKNN\n0.868852\n0.946970\n0.862069\n\n\n3\nCatBoost\n0.885246\n0.945887\n0.885246\n\n\n4\nGradient Boosting\n0.868852\n0.943723\n0.866667\n\n\n5\nRandom Forest\n0.868852\n0.937229\n0.866667\n\n\n6\nXGBoost\n0.836066\n0.920996\n0.838710\n\n\n7\nHard Voting Classifier\n0.901639\n0.906385\n0.900000\n\n\n8\nSoft Voting Classifier\n0.901639\n0.906385\n0.900000\n\n\n9\nStacking Classifier\n0.901639\n0.906385\n0.900000\n\n\n10\nDecision Tree\n0.655738\n0.660173\n0.655738\n\n\n\n\n\n\n\nLet‚Äôs print out the coefficients of the logistic regression meta-model\n\n# Access the trained meta-model inside the stacking pipeline\nens_meta_clf = ens_clf_stacking_pipeline.named_steps['stacking'].final_estimator_\n\n# Get model names in the same order as the coefficients\nmeta_model_names = [name for name, _ in clf_base_learners]\n\n# Extract coefficients\nmeta_model_coefs = ens_meta_clf.coef_[0]  # For binary classification\n\n# Create a DataFrame to display model weights\nmeta_coef_df = pd.DataFrame({\n    'Base Model': meta_model_names,\n    'Meta-Model Coefficient': meta_model_coefs\n})\n\n# Sort by coefficient value (optional)\nmeta_coef_df = meta_coef_df.sort_values(by='Meta-Model Coefficient', ascending=False).reset_index(drop=True)\n\n# Show the coefficients\nmeta_coef_df\n\n\n\n\n\n\n\n\nBase Model\nMeta-Model Coefficient\n\n\n\n\n0\nlr\n2.296773\n\n\n1\nrf\n1.251281\n\n\n2\ncat\n0.636938\n\n\n3\nknn\n0.471488\n\n\n4\ndt\n0.446707\n\n\n5\nxgb\n0.338406\n\n\n6\nlgb\n0.168090\n\n\n7\ngb\n-0.317501\n\n\n\n\n\n\n\n\n\n13.4.3 Interpretation of Ensemble Results\nInterestingly, the Hard Voting, Soft Voting, and Stacking Classifier all achieved identical performance across accuracy, ROC_AUC, and F1_score. This outcome can be explained by the following factors:\n\nHighly correlated base models: All three ensemble methods use the same set of strong classifiers (e.g., LightGBM, CatBoost, Logistic Regression), which already produce very similar predictions. As a result, the ensembles also produce very similar outputs.\nLimited benefit from ensembling: Since the individual base models perform well and are well-aligned, combining them using different ensemble strategies yields nearly identical predictions.\nMeta-model in stacking mimics soft voting: The stacking classifier uses a Logistic Regression meta-model, which may learn weights close to equal when base predictions are similar‚Äîeffectively behaving like soft voting.\nNo diversity in feature views or training subsets: All base models were trained on the same feature set and data. Adding diversity (e.g., by using different subsets of features or training data) could help stacking outperform voting-based ensembles.\n\nWhile ensemble methods are designed to improve performance by combining diverse models, in this case, the base learners were already strong and aligned, limiting the potential gain from ensembling.\n\n\n13.4.4 Summary: Stacking and Voting Ensembles\nIn this chapter, we explored ensemble learning methods‚ÄîVoting and Stacking‚Äîand applied them to both regression and classification tasks. We incorporated all the models you‚Äôve learned throughout this sequence and compared their performance against individual base models using a variety of evaluation metrics, including RMSE, Accuracy, ROC AUC, and F1 Score.\nKey takeaways include:\n\nVoting combines predictions from multiple models either by majority rule (hard voting) or by averaging probabilities (soft voting). It‚Äôs simple, intuitive, and often improves robustness.\nStacking uses a meta-model to learn the best way to combine base model predictions. It has the potential to outperform voting when base models offer complementary strengths.\nIn practice, the performance gains from ensembling depend heavily on the diversity and strength of the base models. When base models are highly correlated or already strong, ensembling may yield limited additional benefit.\n\nThis chapter also highlighted the importance of:\n\nSelecting diverse and well-tuned base learners,\nUnderstanding the behavior of meta-models in stacking,\nEvaluating ensemble strategies based on the specific task and dataset.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Advanced Ensemble Learning</span>"
    ]
  }
]