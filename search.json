[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science III with python (Class notes)",
    "section": "",
    "text": "Preface\nThese are class notes for the course STAT303-3. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThe course notes are currently being written, and will continue to being developed as the course progresses (just like the class notes last quarter). Please report any typos / mistakes / inconsistencies / issues with the class notes / class presentations in your comments here. Thank you!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html",
    "href": "L1_Scikit-learn.html",
    "title": "1  Introduction to scikit-learn",
    "section": "",
    "text": "1.1 Splitting data into train and test\nLet us create train and test datasets for developing a model to predict if a person has diabetes.\n# Creating training and test data\n    # 80-20 split, which is usual - 70-30 split is also fine, 90-10 is fine if the dataset is large\n    # random_state to set a random seed for the splitting - reproducible results\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the complete dataset.\n#Proportion of 0s and 1s in the complete data\ny.value_counts()/y.shape\n\n0    0.651042\n1    0.348958\nName: Outcome, dtype: float64\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the train dataset.\n#Proportion of 0s and 1s in train data\ny_train.value_counts()/y_train.shape\n\n0    0.644951\n1    0.355049\nName: Outcome, dtype: float64\n#Proportion of 0s and 1s in test data\ny_test.value_counts()/y_test.shape\n\n0    0.675325\n1    0.324675\nName: Outcome, dtype: float64\nWe observe that the proportion of 0s and 1s in the train and test dataset are slightly different from that in the complete data. In order for these datasets to be more representative of the population, they should have a proportion of 0s and 1s similar to that in the complete dataset. This is especially critical in case of imbalanced datasets, where one class is represented by a significantly smaller number of instances than the other(s).\nWhen training a classification model on an imbalanced dataset, the model might not learn enough about the minority class, which can lead to poor generalization performance on new data. This happens because the model is biased towards the majority class, and it might even predict all instances as belonging to the majority class.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "href": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "title": "1  Introduction to scikit-learn",
    "section": "",
    "text": "1.1.1 Stratified splitting\nWe will use the argument stratify to obtain a proportion of 0s and 1s in the train and test datasets that is similar to the proportion in the complete `data.\n\n#Stratified train-test split\nX_train_stratified, X_test_stratified, y_train_stratified,\\\ny_test_stratified = train_test_split(X, y, test_size = 0.2, random_state = 45, stratify=y)\n\n\n#Proportion of 0s and 1s in train data with stratified split\ny_train_stratified.value_counts()/y_train.shape\n\n0    0.651466\n1    0.348534\nName: Outcome, dtype: float64\n\n\n\n#Proportion of 0s and 1s in test data with stratified split\ny_test_stratified.value_counts()/y_test.shape\n\n0    0.649351\n1    0.350649\nName: Outcome, dtype: float64\n\n\nThe proportion of the classes in the stratified split mimics the proportion in the complete dataset more closely.\nBy using stratified splitting, we ensure that both the train and test data sets have the same proportion of instances from each class, which means that the model will see enough instances from the minority class during training. This, in turn, helps the model learn to distinguish between the classes better, leading to better performance on new data.\nThus, stratified splitting helps to ensure that the model sees enough instances from each class during training, which can improve the model’s ability to generalize to new data, particularly in cases where one class is underrepresented in the dataset.\nLet us develop a logistic regression model for predicting if a person has diabetes.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#scaling-data",
    "href": "L1_Scikit-learn.html#scaling-data",
    "title": "1  Introduction to scikit-learn",
    "section": "1.2 Scaling data",
    "text": "1.2 Scaling data\nIn certain models, it may be important to scale data for various reasons. In a logistic regression model, scaling can help with model convergence. Scikit-learn uses a method known as gradient-descent (not in scope of the syllabus of this course) to obtain a solution. In case the predictors have different orders of magnitude, the algorithm may fail to converge. In such cases, it is useful to standardize the predictors so that all of them are at the same scale.\n\n# With linear/logistic regression in scikit-learn, especially when the predictors have different orders \n# of magn., scaling is necessary. This is to enable the training algo. which we did not cover. (Gradient Descent)\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test) # Do NOT refit the scaler with the test data, just transform it.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#fitting-a-model",
    "href": "L1_Scikit-learn.html#fitting-a-model",
    "title": "1  Introduction to scikit-learn",
    "section": "1.3 Fitting a model",
    "text": "1.3 Fitting a model\nLet us fit a logistic regression model for predicting if a person has diabetes. Let us try fitting a model with the un-scaled data.\n\n# Create a model object - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train, y_train)\n\nC:\\Users\\akl0407\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\nNote that the model with the un-scaled predictors fails to converge. Check out the data X_train to see that this may be probably due to the predictors have different orders of magnitude. For example, the predictor DiabetesPedigreeFunction has values in [0.078, 2.42], while the predictor Insulin has values in [0, 800].\nLet us fit the model to the scaled data.\n\n# Create a model - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train_scaled, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\nThe model converges to a solution with the scaled data!\nThe coefficients of the model can be returned with the coef_ attribute of the LogisticRegression() object. However, the output is not as well formatted as in the case of the statsmodels library since sklearn is developed primarily for the purpose of prediction, and not inference.\n\n# Use coef_ to return the coefficients - only log reg inference you can do with sklearn\nprint(logreg.coef_) \n\n[[ 0.32572891  1.20110566 -0.32046591  0.06849882 -0.21727131  0.72619528\n   0.40088897  0.29698818]]",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#computing-performance-metrics",
    "href": "L1_Scikit-learn.html#computing-performance-metrics",
    "title": "1  Introduction to scikit-learn",
    "section": "1.4 Computing performance metrics",
    "text": "1.4 Computing performance metrics\n\n1.4.1 Accuracy\nLet us test the model prediction accuracy on the test data. We’ll demonstrate two different functions that can be used to compute model accuracy - accuracy_score(), and score().\nThe accuracy_score() function from the metrics module of the sklearn library is general, and can be used for any classification model. We’ll use it along with the predict() method of the LogisticRegression() object, which returns the predicted class based on a threshold probability of 0.5.\n\n# Get the predicted classes first\ny_pred = logreg.predict(X_test_scaled)\n\n# Use the predicted and true classes for accuracy\nprint(accuracy_score(y_pred, y_test)*100) \n\n73.37662337662337\n\n\nThe score() method of the LogisticRegression() object can be used to compute the accuracy only for a logistic regression model. Note that for a LinearRegression() object, the score() method will return the model \\(R\\)-squared.\n\n# Use .score with test predictors and response to get the accuracy\n# Implements the same thing under the hood\nprint(logreg.score(X_test_scaled, y_test)*100)  \n\n73.37662337662337\n\n\n\n\n1.4.2 ROC-AUC\nThe roc_curve() and auc() functions from the metrics module of the sklearn library can be used to compute the ROC-AUC, or the area under the ROC curve. Note that for computing ROC-AUC, we need the predicted probability, instead of the predicted class. Thus, we’ll use the predict_proba() method of the LogisticRegression() object, which returns the predicted probability for the observation to belong to each of the classes, instead of using the predict() method, which returns the predicted class based on threshold probability of 0.5.\n\n#Computing the predicted probability for the observation to belong to the positive class (y=1);\n#The 2nd column in the output of predict_proba() consists of the probability of the observation to \n#belong to the positive class (y=1)\ny_pred_prob = logreg.predict_proba(X_test_scaled)[:,1] \n\n#Using the predicted probability computed above to find ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test, y_pred_prob)\nprint(auc(fpr, tpr))# AUC of ROC\n\n0.7923076923076922\n\n\n\n\n1.4.3 Confusion matrix & precision-recall\nThe confusion_matrix(), precision_score(), and recall_score() functions from the metrics module of the sklearn library can be used to compute the confusion matrix, precision, and recall respectively.\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\n\n\n\n\n\n\n\n\nprint(\"Precision: \", precision_score(y_test, y_pred))\nprint(\"Recall: \", recall_score(y_test, y_pred))\n\nPrecision:  0.6046511627906976\nRecall:  0.52\n\n\nLet us compute the performance metrics if we develop the model using stratified splitting.\n\n# Developing the model with stratified splitting\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n#Computing the accuracy\ny_pred_stratified = logreg.predict(X_test_stratified_scaled)\nprint(\"Accuracy: \",accuracy_score(y_pred_stratified, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_stratified))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_stratified))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_stratified), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  78.57142857142857\nROC-AUC:  0.8505555555555556\nPrecision:  0.7692307692307693\nRecall:  0.5555555555555556\n\n\n\n\n\n\n\n\n\nThe model with the stratified train-test split has a better performance as compared to the other model on all the performance metrics!",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#tuning-the-model-hyperparameters",
    "href": "L1_Scikit-learn.html#tuning-the-model-hyperparameters",
    "title": "1  Introduction to scikit-learn",
    "section": "1.5 Tuning the model hyperparameters",
    "text": "1.5 Tuning the model hyperparameters\nA hyperparameter (among others) that can be trained in a logistic regression model is the regularization parameter.\nWe may also wish to tune the decision threshold probability. Note that the decision threshold probability is not considered a hyperparameter of the model. Hyperparameters are model parameters that are set prior to training and cannot be directly adjusted by the model during training. Examples of hyperparameters in a logistic regression model include the regularization parameter, and the type of shrinkage penalty - lasso / ridge. These hyperparameters are typically optimized through a separate tuning process, such as cross-validation or grid search, before training the final model.\nThe performance metrics can be computed using a desired value of the threshold probability. Let us compute the performance metrics for a desired threshold probability of 0.3.\n\n# Performance metrics computation for a desired threshold probability of 0.3\ndesired_threshold = 0.3\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob &gt; desired_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  75.32467532467533\nROC-AUC:  0.8505555555555556\nPrecision:  0.6111111111111112\nRecall:  0.8148148148148148\n\n\n\n\n\n\n\n\n\n\n1.5.1 Tuning decision threshold probability\nSuppose we wish to find the optimal decision threshold probability to maximize accuracy. Note that we cannot use the test dataset to optimize model hyperparameters, as that may lead to overfitting on the test data. We’ll use \\(K\\)-fold cross validation on train data to find the optimal decision threshold probability.\nWe’ll use the cross_val_predict() function from the model_selection module of sklearn to compute the \\(K\\)-fold cross validated predicted probabilities. Note that this function simplifies the task of manually creating the \\(K\\)-folds, training the model \\(K\\)-times, and computing the predicted probabilities on each of the \\(K\\)-folds. Thereafter, the predicted probabilities will be used to find the optimal threshold probability that maximizes the classification accuracy.\n\nhyperparam_vals = np.arange(0,1.01,0.01)\naccuracy_iter = []\n\npredicted_probability = cross_val_predict(LogisticRegression(), X_train_stratified_scaled, \n                                              y_train_stratified, cv = 5, method = 'predict_proba')\n\nfor threshold_prob in hyperparam_vals:\n    predicted_class = predicted_probability[:,1] &gt; threshold_prob\n    predicted_class = predicted_class.astype(int)\n\n    #Computing the accuracy\n    accuracy = accuracy_score(predicted_class, y_train_stratified)*100\n    accuracy_iter.append(accuracy)\n\nLet us visualize the accuracy with change in decision threshold probability.\n\n# Accuracy vs decision threshold probability\nsns.scatterplot(x = hyperparam_vals, y = accuracy_iter)\nplt.xlabel('Decision threshold probability')\nplt.ylabel('Average 5-fold CV accuracy');\n\n\n\n\n\n\n\n\nThe optimal decision threshold probability is the one that maximizes the \\(K\\)-fold cross validation accuracy.\n\n# Optimal decision threshold probability\nhyperparam_vals[accuracy_iter.index(max(accuracy_iter))]\n\n0.46\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.46\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob &gt; desired_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8505555555555556\nPrecision:  0.7804878048780488\nRecall:  0.5925925925925926\n\n\n\n\n\n\n\n\n\nModel performance on test data has improved with the optimal decision threshold probability.\n\n\n1.5.2 Tuning the regularization parameter\nThe LogisticRegression() method has a default L2 regularization penalty, which means ridge regression.C is \\(1/\\lambda\\), where \\(\\lambda\\) is the hyperparameter that is multiplied with the ridge penalty. C is 1 by default.\n\naccuracy_iter = []\nhyperparam_vals = 10**np.linspace(-3.5, 1)\n\nfor c_val in hyperparam_vals: # For each possible C value in your grid\n    logreg_model = LogisticRegression(C=c_val) # Create a model with the C value\n    \n    accuracy_iter.append(cross_val_score(logreg_model, X_train_stratified_scaled, y_train_stratified,\n                                      scoring='accuracy', cv=5)) # Find the cv results\n\n\nplt.plot(hyperparam_vals, np.mean(np.array(accuracy_iter), axis=1))\nplt.xlabel('C')\nplt.ylabel('Average 5-fold CV accuracy')\nplt.xscale('log')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Optimal value of the regularization parameter 'C'\noptimal_C = hyperparam_vals[np.argmax(np.array(accuracy_iter).mean(axis=1))]\noptimal_C\n\n0.11787686347935879\n\n\n\n# Developing the model with stratified splitting and optimal 'C'\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg = LogisticRegression(C = optimal_C)\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n#Computing the accuracy\ny_pred_stratified = logreg.predict(X_test_stratified_scaled)\nprint(\"Accuracy: \",accuracy_score(y_pred_stratified, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_stratified))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_stratified))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_stratified), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  78.57142857142857\nROC-AUC:  0.8516666666666666\nPrecision:  0.7837837837837838\nRecall:  0.5370370370370371\n\n\n\n\n\n\n\n\n\n\n\n1.5.3 Tuning the decision threshold probability and the regularization parameter simultaneously\n\nthreshold_hyperparam_vals = np.arange(0,1.01,0.01)\nC_hyperparam_vals = 10**np.linspace(-3.5, 1)\naccuracy_iter = pd.DataFrame({'threshold':[], 'C':[], 'accuracy':[]})\niter_number = 0\n\nfor c_val in C_hyperparam_vals:\n    predicted_probability = cross_val_predict(LogisticRegression(C = c_val), X_train_stratified_scaled, \n                                                  y_train_stratified, cv = 5, method = 'predict_proba')\n\n    for threshold_prob in threshold_hyperparam_vals:\n        predicted_class = predicted_probability[:,1] &gt; threshold_prob\n        predicted_class = predicted_class.astype(int)\n\n        #Computing the accuracy\n        accuracy = accuracy_score(predicted_class, y_train_stratified)*100\n        accuracy_iter.loc[iter_number, 'threshold'] = threshold_prob\n        accuracy_iter.loc[iter_number, 'C'] = c_val\n        accuracy_iter.loc[iter_number, 'accuracy'] = accuracy\n        iter_number = iter_number + 1\n\n\n# Parameters for highest accuracy\noptimal_C = accuracy_iter.sort_values(by = 'accuracy', ascending = False).iloc[0,:]['C']\noptimal_threshold = accuracy_iter.sort_values(by = 'accuracy', ascending = False).iloc[0, :]['threshold']\n\n#Optimal decision threshold probability\nprint(\"Optimal decision threshold = \", optimal_threshold)\n\n#Optimal C\nprint(\"Optimal C = \", optimal_C)\n\nOptimal decision threshold =  0.46\nOptimal C =  4.291934260128778\n\n\n\n# Developing the model with stratified splitting, optimal decision threshold probability, and optimal 'C'\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg = LogisticRegression(C = optimal_C)\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n# Performance metrics computation for the optimal threshold probability\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob &gt; optimal_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8509259259259259\nPrecision:  0.7804878048780488\nRecall:  0.5925925925925926\n\n\n\n\n\n\n\n\n\nLater in the course, we’ll see the sklearn function GridSearchCV, which is used to optimize several model hyperparameters simultaneously with \\(K\\)-fold cross validation, while avoiding for loops.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "Bias_variance_code.html",
    "href": "Bias_variance_code.html",
    "title": "2  Bias-variance tradeoff",
    "section": "",
    "text": "2.1 Simple model (Less flexible)\nLet us consider a linear regression model as the less-flexible (or relatively simple) model.\nWe will first simulate the test dataset for which we will compute the bias and variance.\nnp.random.seed(101)\n\n# Simulating predictor values of test data\nxtest = np.random.uniform(-15, 10, 200)\n\n# Assuming the true mean response is square of the predictor value\nfxtest = xtest**2\n\n# Simulating test response by adding noise to the true mean response\nytest = fxtest + np.random.normal(0, 10, 200)\n\n# We will find bias and variance using a linear regression model for prediction\nmodel = LinearRegression()\n# Visualizing the data and the true mean response\nsns.scatterplot(x = xtest, y = ytest)\nsns.lineplot(x = xtest, y = fxtest, color = 'grey', linewidth = 2)\n\n# Initializing objects to store predictions and mean squared error\n# of 100 models developed on 100 distinct training datasets samples\npred_test = []; mse_test = []\n\n# Iterating over each of the 100 models\nfor i in range(100):\n    np.random.seed(i)\n    \n    # Simulating the ith training data\n    x = np.random.uniform(-15, 10, 200)\n    fx = x**2\n    y = fx + np.random.normal(0, 10, 200)\n    \n    # Fitting the ith model on the ith training data\n    model.fit(x.reshape(-1,1), y)\n    \n    # Plotting the ith model\n    sns.lineplot(x = x, y = model.predict(x.reshape(-1,1)))\n    \n    # Storing the predictions of the ith model on test data\n    pred_test.append(model.predict(xtest.reshape(-1,1)))\n    \n    # Storing the mean squared error of the ith model on test data\n    mse_test.append(mean_squared_error(model.predict(xtest.reshape(-1,1)), ytest))\nThe above plots show that the 100 models seem to have low variance, but high bias. Note that the bias is low only around a couple of points (x = -10 & x = 5).\nLet us compute the average squared bias over all the test data points.\nmean_pred = np.array(pred_test).mean(axis = 0)\nsq_bias = ((mean_pred - fxtest)**2).mean()\nsq_bias\n\n2042.104126728109\nLet us compute the average variance over all the test data points.\nmean_var = np.array(pred_test).var(axis = 0).mean()\nmean_var\n\n28.37397844429763\nLet us compute the mean squared error over all the test data points.\nnp.array(mse_test).mean()\n\n2201.957555529835\nNote that the mean squared error should be the same as the sum of squared bias, variance, and irreducible error.\nThe sum of squared bias, model variance, and irreducible error is:\nsq_bias + mean_var + 100\n\n2170.4781051724067\nNote that this is approximately, but not exactly, the same as the mean squared error computed above as we are developing a finite number of models, and making predictions on a finite number of test data points.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "Bias_variance_code.html#complex-model-more-flexible",
    "href": "Bias_variance_code.html#complex-model-more-flexible",
    "title": "2  Bias-variance tradeoff",
    "section": "2.2 Complex model (more flexible)",
    "text": "2.2 Complex model (more flexible)\nLet us consider a decion tree as the more flexible model.\n\nnp.random.seed(101)\nxtest = np.random.uniform(-15, 10, 200)\nfxtest = xtest**2\nytest = fxtest + np.random.normal(0, 10, 200)\nmodel = DecisionTreeRegressor()\n\n\nsns.scatterplot(x = xtest, y = ytest)\nsns.lineplot(x = xtest, y = fxtest, color = 'grey', linewidth = 2)\npred_test = []; mse_test = []\nfor i in range(100):\n    np.random.seed(i)\n    x = np.random.uniform(-15, 10, 200)\n    fx = x**2\n    y = fx + np.random.normal(0, 10, 200)\n    model.fit(x.reshape(-1,1), y)\n    sns.lineplot(x = x, y = model.predict(x.reshape(-1,1)))\n    pred_test.append(model.predict(xtest.reshape(-1,1)))\n    mse_test.append(mean_squared_error(model.predict(xtest.reshape(-1,1)), ytest))\n\n\n\n\n\n\n\n\nThe above plots show that the 100 models seem to have high variance, but low bias.\nLet us compute the average squared bias over all the test data points.\n\nmean_pred = np.array(pred_test).mean(axis = 0)\nsq_bias = ((mean_pred - fxtest)**2).mean()\nsq_bias\n\n1.3117561629333938\n\n\nLet us compute the average model variance over all the test data points.\n\nmean_var = np.array(pred_test).var(axis = 0).mean()\nmean_var\n\n102.5226748977198\n\n\nLet us compute the average mean squared error over all the test data points.\n\nnp.array(mse_test).mean()\n\n225.92027460924726\n\n\nNote that the above error is approximately the same as the sum of the squared bias, model variance and the irreducible error.\nNote that the relatively more flexible model has a higher variance, but lower bias as compared to the less flexible linear model. This will typically be the case, but may not be true in all scenarios. We will discuss one such scenario later.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "KNN.html",
    "href": "KNN.html",
    "title": "3  KNN",
    "section": "",
    "text": "3.1 KNN for regression\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\npredictors = ['mpg', 'engineSize', 'year', 'mileage']\n\nX_train = train[predictors]\ny_train = train['price']\n\nX_test = test[predictors]\ny_test = test['price']\nLet us scale data as we are using KNN.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "KNN.html#knn-for-regression",
    "href": "KNN.html#knn-for-regression",
    "title": "3  KNN",
    "section": "",
    "text": "3.1.1 Scaling data\n\n# Scale\nsc = StandardScaler()\n\nsc.fit(X_train)\nX_train_scaled = sc.transform(X_train)\nX_test_scaled = sc.transform(X_test)\n\nLet fit the model and compute the RMSE on test data. If the number of neighbors is not specified, the default value is taken.\n\n\n3.1.2 Fitting and validating model\n\nknn_model = KNeighborsRegressor() \n\nknn_model.fit(X_train_scaled, (y_train))\n\ny_pred = knn_model.predict(X_test_scaled)\ny_pred_train = knn_model.predict(X_train_scaled)\n\nmean_squared_error(y_test, (y_pred), squared=False)\n\n6329.691192885354\n\n\n\nknn_model2 = KNeighborsRegressor(n_neighbors = 5, weights='distance') # Default weights is uniform\n\nknn_model2.fit(X_train_scaled, y_train)\n\ny_pred = knn_model2.predict(X_test_scaled)\n\nmean_squared_error(y_test, y_pred, squared=False)\n\n6063.327598353961\n\n\nThe model seems to fit better than all the linear models in STAT303-2.\n\n\n3.1.3 Hyperparameter tuning\nWe will use cross-validation to find the optimal value of the hyperparameter n_neighbors.\n\nKs = np.arange(1,601)\n\ncv_scores = []\n\nfor K in Ks:\n    model = KNeighborsRegressor(n_neighbors = K, weights='distance')\n    score = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring = 'neg_root_mean_squared_error')\n    cv_scores.append(score)\n\n\nnp.array(cv_scores).shape\n# Each row is a K\n\n(600, 5)\n\n\n\ncv_scores_array = np.array(cv_scores)\n\navg_cv_scores = -cv_scores_array.mean(axis=1)\n\n\nsns.lineplot(x = range(600), y = avg_cv_scores);\nplt.xlabel('K')\nplt.ylabel('5-fold Cross-validated RMSE');\n\n\n\n\n\n\n\n\n\navg_cv_scores.min() # Best CV score\n \nKs[avg_cv_scores.argmin()] # Best hyperparam value\n\n366\n\n\nThe optimal hyperparameter value is 366. Does it seem to be too high?\n\nbest_model = KNeighborsRegressor(n_neighbors = Ks[avg_cv_scores.argmin()], weights='distance')\n\nbest_model.fit(X_train_scaled, y_train)\n\ny_pred = best_model.predict(X_test_scaled)\n\nmean_squared_error(y_test, y_pred, squared=False)\n\n7724.452068618346\n\n\nThe test error with the optimal hyperparameter value based on cross-validation is much higher than that based on the default value of the hyperparameter. Why is that?\nSometimes this may happen by chance due to the specific observations in the \\(k\\) folds. One option is to shuffle the dataset before splitting into folds.\nThe function KFold() can be used to shuffle the data before splitting it into folds.\n\n3.1.3.1 KFold()\n\nkcv = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\nNow, let us again try to find the opimal \\(K\\) for KNN, using the new folds, based on shuffled data.\n\nKs = np.arange(1,601)\n\ncv_scores = []\n\nfor K in Ks:\n    model = KNeighborsRegressor(n_neighbors = K, weights='distance')\n    score = cross_val_score(model, X_train_scaled, y_train, cv = kcv, scoring = 'neg_root_mean_squared_error')\n    cv_scores.append(score)\n\n\ncv_scores_array = np.array(cv_scores)\navg_cv_scores = -cv_scores_array.mean(axis=1)\nsns.lineplot(x = range(600), y = avg_cv_scores);\nplt.xlabel('K')\nplt.ylabel('5-fold Cross-validated RMSE');\n\n\n\n\n\n\n\n\nThe optimal K is:\n\nKs[avg_cv_scores.argmin()]\n\n10\n\n\nRMSE on test data with this optimal value of \\(K\\) is:\n\nknn_model2 = KNeighborsRegressor(n_neighbors = 10, weights='distance') # Default weights is uniform\nknn_model2.fit(X_train_scaled, y_train)\ny_pred = knn_model2.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n6043.889393238132\n\n\nIn order to avoid these errors due the specific observations in the \\(k\\) folds, it will be better to repeat the \\(k\\)-fold cross-validation multiple times, where the data is shuffled after each \\(k\\)-fold cross-validation, so that the cross-validation takes place on new folds for each repetition.\nThe function RepeatedKFold() repeats \\(k\\)-fold cross validation multiple times (10 times by default). Let us use it to have a more robust optimal value of the number of neighbors \\(K\\).\n\n\n3.1.3.2 RepeatedKFold()\n\nkcv = RepeatedKFold(n_splits = 5, random_state = 1)\n\n\nKs = np.arange(1,601)\n\ncv_scores = []\n\nfor K in Ks:\n    model = KNeighborsRegressor(n_neighbors = K, weights='distance')\n    score = cross_val_score(model, X_train_scaled, y_train, cv = kcv, scoring = 'neg_root_mean_squared_error')\n    cv_scores.append(score)\n\n\ncv_scores_array = np.array(cv_scores)\navg_cv_scores = -cv_scores_array.mean(axis=1)\nsns.lineplot(x = range(600), y = avg_cv_scores);\nplt.xlabel('K')\nplt.ylabel('5-fold Cross-validated RMSE');\n\n\n\n\n\n\n\n\nThe optimal K is:\n\nKs[avg_cv_scores.argmin()]\n\n9\n\n\nRMSE on test data with this optimal value of \\(K\\) is:\n\nknn_model2 = KNeighborsRegressor(n_neighbors = 9, weights='distance') # Default weights is uniform\nknn_model2.fit(X_train_scaled, y_train)\ny_pred = knn_model2.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n6051.157910333279\n\n\n\n\n\n3.1.4 KNN hyperparameters\nThe model hyperparameters can be obtained using the get_params() method. Note that there are other hyperparameters to tune in addition to number of neighbors. However, the number of neighbours may be the most influential hyperparameter in most cases.\n\nbest_model.get_params()\n\n{'algorithm': 'auto',\n 'leaf_size': 30,\n 'metric': 'minkowski',\n 'metric_params': None,\n 'n_jobs': None,\n 'n_neighbors': 366,\n 'p': 2,\n 'weights': 'distance'}\n\n\nThe distances and the indices of the nearest K observations to each test observation can be obtained using the kneighbors() method.\n\nbest_model.kneighbors(X_test_scaled, return_distance=True)\n\n# Each row is a test obs\n# The cols are the indices of the K Nearest Neighbors (in the training data) to the test obs\n\n(array([[1.92799060e-02, 1.31899013e-01, 1.89662146e-01, ...,\n         8.38960707e-01, 8.39293053e-01, 8.39947823e-01],\n        [7.07215830e-02, 1.99916181e-01, 2.85592939e-01, ...,\n         1.15445056e+00, 1.15450848e+00, 1.15512897e+00],\n        [1.32608205e-03, 1.43558347e-02, 1.80622215e-02, ...,\n         5.16758453e-01, 5.17378567e-01, 5.17852312e-01],\n        ...,\n        [1.29209535e-02, 1.59187173e-02, 3.67038947e-02, ...,\n         8.48811744e-01, 8.51235616e-01, 8.55044146e-01],\n        [1.84971803e-02, 1.67471541e-01, 1.69374312e-01, ...,\n         7.76743422e-01, 7.76943691e-01, 7.77760930e-01],\n        [4.63762129e-01, 5.88639393e-01, 7.54718535e-01, ...,\n         3.16994824e+00, 3.17126663e+00, 3.17294300e+00]]),\n array([[1639, 1647, 4119, ..., 3175, 2818, 4638],\n        [ 367, 1655, 1638, ..., 2010, 3600,  268],\n        [ 393, 4679, 3176, ..., 4663,  357,  293],\n        ...,\n        [3116, 3736, 3108, ..., 3841, 2668, 2666],\n        [4864, 3540, 4852, ..., 3596, 3605, 4271],\n        [ 435,  729, 4897, ..., 4112, 2401, 2460]], dtype=int64))",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "KNN.html#knn-for-classification",
    "href": "KNN.html#knn-for-classification",
    "title": "3  KNN",
    "section": "3.2 KNN for classification",
    "text": "3.2 KNN for classification\nKNN model for classification can developed and tuned in a similar manner using the sklearn function KNeighborsClassifier()\n\nFor classification, KNeighborsClassifier\nExact same inputs\n\nOne detail: Not common to use even numbers for K in classification because of majority voting\nKs = np.arange(1,41,2) –&gt; To get the odd numbers",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html",
    "href": "Hyperparameter tuning.html",
    "title": "4  Hyperparameter tuning",
    "section": "",
    "text": "4.1 GridSearchCV\nThe function is used to compute the cross-validated score (MSE, RMSE, accuracy, etc.) over a grid of hyperparameter values. This helps avoid nested for() loops if multiple hyperparameter values need to be tuned.\n# GridSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor() # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n    # the keys should be EXACTLY the same as the names of the model inputs\n    # the values should be an array or list of hyperparam values you want to try out\n    \n# 30 K values x 2 weight settings x 3 metric settings = 180 different combinations in this grid\ngrid = {'n_neighbors': np.arange(5, 151, 5), 'weights':['uniform', 'distance'], \n        'metric': ['manhattan', 'euclidean', 'chebyshev']}\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, use it if you \n# have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within the function\ngcv = GridSearchCV(model, grid, cv = kfold, scoring = 'neg_root_mean_squared_error', n_jobs = -1, verbose = 10)\n\n# Fit the models, and cross-validate\ngcv.fit(X_train_scaled, y_train)\n\nFitting 5 folds for each of 180 candidates, totalling 900 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(), n_jobs=-1,\n             param_grid={'metric': ['manhattan', 'euclidean', 'chebyshev'],\n                         'n_neighbors': array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n        70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125, 130,\n       135, 140, 145, 150]),\n                         'weights': ['uniform', 'distance']},\n             scoring='neg_root_mean_squared_error', verbose=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(), n_jobs=-1,\n             param_grid={'metric': ['manhattan', 'euclidean', 'chebyshev'],\n                         'n_neighbors': array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n        70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125, 130,\n       135, 140, 145, 150]),\n                         'weights': ['uniform', 'distance']},\n             scoring='neg_root_mean_squared_error', verbose=10)estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()\nThe optimal estimator based on cross-validation is:\ngcv.best_estimator_\n\nKNeighborsRegressor(metric='manhattan', n_neighbors=10, weights='distance')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor(metric='manhattan', n_neighbors=10, weights='distance')\nThe optimal hyperparameter values (based on those considered in the grid search) are:\ngcv.best_params_\n\n{'metric': 'manhattan', 'n_neighbors': 10, 'weights': 'distance'}\nThe cross-validated root mean squared error for the optimal hyperparameter values is:\n-gcv.best_score_\n\n5740.928686723918\nThe RMSE on test data for the optimal hyperparameter values is:\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n5747.466851437544\nNote that the error is further reduced as compared to the case when we tuned only one hyperparameter in the previous chatper. We must tune all the hyperparameters that can effect prediction accuracy, in order to get the most accurate model.\nThe results for each cross-validation are stored in the cv_results_ attribute.\npd.DataFrame(gcv.cv_results_).head()\n\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_metric\nparam_n_neighbors\nparam_weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.011169\n0.005060\n0.011768\n0.001716\nmanhattan\n5\nuniform\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6781.316742\n-5997.969637\n-6726.786770\n-6488.191029\n-6168.502006\n-6432.553237\n306.558600\n19\n\n\n1\n0.009175\n0.001934\n0.009973\n0.000631\nmanhattan\n5\ndistance\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6449.449369\n-5502.975790\n-6306.888303\n-5780.902979\n-5365.980081\n-5881.239304\n429.577113\n3\n\n\n2\n0.008976\n0.001092\n0.012168\n0.001323\nmanhattan\n10\nuniform\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6668.299079\n-6116.693116\n-6387.505084\n-6564.727623\n-6219.094608\n-6391.263902\n205.856097\n16\n\n\n3\n0.007979\n0.000001\n0.011970\n0.000892\nmanhattan\n10\ndistance\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6331.374493\n-5326.304310\n-5787.179591\n-5809.777811\n-5450.007229\n-5740.928687\n349.872624\n1\n\n\n4\n0.006781\n0.000748\n0.012367\n0.001017\nmanhattan\n15\nuniform\n{'metric': 'manhattan', 'n_neighbors': 15, 'we...\n-6871.063499\n-6412.214411\n-6544.343677\n-7008.348770\n-6488.345118\n-6664.863095\n232.385843\n33\nThese results can be useful to see if other hyperparameter values are almost equally good.\nFor example, the next two best optimal values of the hyperparameter correspond to neighbors being 15 and 5 respectively. As the test error has a high variance, the best hyperparameter values need not necessarily be actually optimal.\npd.DataFrame(gcv.cv_results_).sort_values(by = 'rank_test_score').head()\n\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_metric\nparam_n_neighbors\nparam_weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n3\n0.007979\n0.000001\n0.011970\n0.000892\nmanhattan\n10\ndistance\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6331.374493\n-5326.304310\n-5787.179591\n-5809.777811\n-5450.007229\n-5740.928687\n349.872624\n1\n\n\n5\n0.009374\n0.004829\n0.013564\n0.001850\nmanhattan\n15\ndistance\n{'metric': 'manhattan', 'n_neighbors': 15, 'we...\n-6384.403268\n-5427.978762\n-5742.606651\n-6041.135255\n-5563.240077\n-5831.872803\n344.192700\n2\n\n\n1\n0.009175\n0.001934\n0.009973\n0.000631\nmanhattan\n5\ndistance\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6449.449369\n-5502.975790\n-6306.888303\n-5780.902979\n-5365.980081\n-5881.239304\n429.577113\n3\n\n\n7\n0.007977\n0.001092\n0.017553\n0.002054\nmanhattan\n20\ndistance\n{'metric': 'manhattan', 'n_neighbors': 20, 'we...\n-6527.825519\n-5534.609170\n-5860.837805\n-6100.919269\n-5679.403544\n-5940.719061\n349.270714\n4\n\n\n9\n0.007777\n0.000748\n0.019349\n0.003374\nmanhattan\n25\ndistance\n{'metric': 'manhattan', 'n_neighbors': 25, 'we...\n-6620.272336\n-5620.462675\n-5976.406911\n-6181.847891\n-5786.081991\n-6037.014361\n346.791650\n5\nLet us compute the RMSE on test data based on the 2nd and 3rd best hyperparameter values.\nmodel = KNeighborsRegressor(n_neighbors=15, metric='manhattan', weights='distance').fit(X_train_scaled, y_train)\nmean_squared_error(model.predict(X_test_scaled), y_test, squared = False)\n\n5800.418957612656\nmodel = KNeighborsRegressor(n_neighbors=5, metric='manhattan', weights='distance').fit(X_train_scaled, y_train)\nmean_squared_error(model.predict(X_test_scaled), y_test, squared = False)\n\n5722.4859230146685\nWe can see that the RMSE corresponding to the 3rd best hyperparameter value is the least. Due to variance in test errors, it may be a good idea to consider the set of top few best hyperparameter values, instead of just considering the best one.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#randomizedsearchcv",
    "href": "Hyperparameter tuning.html#randomizedsearchcv",
    "title": "4  Hyperparameter tuning",
    "section": "4.2 RandomizedSearchCV()",
    "text": "4.2 RandomizedSearchCV()\nIn case of many possible values of hyperparameters, it may be comptaionally very expensive to use GridSearchCV(). In such cases, RandomizedSearchCV() can be used to compute the cross-validated score on a randomly selected subset of hyperparameter values from the specified grid. The number of values can be fixed by the user, as per the available budget.\n\n# RandomizedSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor() # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n    # the keys should be EXACTLY the same as the names of the model inputs\n    # the values should be an array or list of hyperparam values, or distribution of hyperparameter values\n    \n    \ngrid = {'n_neighbors': range(1, 500), 'weights':['uniform', 'distance'], \n        'metric': ['minkowski'], 'p': uniform(loc=1, scale=10)} #We can specify a distribution \n                                                                #for continuous hyperparameter values\n\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, use it if you \n# have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within the function\ngcv = RandomizedSearchCV(model, param_distributions = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1, verbose = 10)\n\n# Fit the models, and cross-validate\ngcv.fit(X_train_scaled, y_train)\n\nFitting 5 folds for each of 180 candidates, totalling 900 fits\n\n\nRandomizedSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n                   estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n                   param_distributions={'metric': ['minkowski'],\n                                        'n_neighbors': range(1, 500),\n                                        'p': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000226D6E70700&gt;,\n                                        'weights': ['uniform', 'distance']},\n                   random_state=10, scoring='neg_root_mean_squared_error',\n                   verbose=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n                   estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n                   param_distributions={'metric': ['minkowski'],\n                                        'n_neighbors': range(1, 500),\n                                        'p': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000226D6E70700&gt;,\n                                        'weights': ['uniform', 'distance']},\n                   random_state=10, scoring='neg_root_mean_squared_error',\n                   verbose=10)estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()\n\n\n\ngcv.best_params_\n\n{'metric': 'minkowski',\n 'n_neighbors': 3,\n 'p': 1.252639454318171,\n 'weights': 'uniform'}\n\n\n\ngcv.best_score_\n\n-6239.171627183809\n\n\n\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n6176.533397589911\n\n\nNote that in this example, RandomizedSearchCV() helps search for optimal values of the hyperparameter \\(p\\) over a continuous domain space. In this dataset, \\(p = 1\\) seems to be the optimal value. However, if the optimal value was somewhere in the middle of a larger continuous domain space (instead of the boundary of the domain space), and there were several other hyperparameters, some of which were not influencing the response (effect sparsity), RandomizedSearchCV() is likely to be more effective in estimating the optimal value of the continuous hyperparameter.\nThe advantages of RandomizedSearchCV() over GridSearchCV() are:\n\nRandomizedSearchCV() fixes the computational cost in case of large number of hyperparameters / large number of levels of individual hyperparameters. If there are \\(n\\) hyper parameters, each with 3 levels, the number of all possible hyperparameter values will be \\(3^n\\). The computational cost increase exponentially with increase in number of hyperparameters.\nIn case of a hyperparameter having continuous values, the distribution of the hyperparameter can be specified in RandomizedSearchCV().\nIn case of effect sparsity of hyperparameters, i.e., if only a few hyperparameters significantly effect prediction accuracy, RandomizedSearchCV() is likely to consider more unique values of the influential hyperparameters as compared to GridSearchCV(), and is thus likely to provide more optimal hyperparameter values as compared to GridSearchCV(). The figure below shows effect sparsity where there are 2 hyperparameters, but only one of them is associated with the cross-validated score, Here, it is more likely that the optimal cross-validated score will be obtained by RandomizedSearchCV(), as it is evaluating the model on 9 unique values of the relevant hyperparameter, instead of just 3.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#bayessearchcv",
    "href": "Hyperparameter tuning.html#bayessearchcv",
    "title": "4  Hyperparameter tuning",
    "section": "4.3 BayesSearchCV()",
    "text": "4.3 BayesSearchCV()\nUnlike the grid search and random search, which treat hyperparameter sets independently, the Bayesian optimization is an informed search method, meaning that it learns from previous iterations. The number of trials in this approach is determined by the user.\n\nThe function begins by computing the cross-validated score by randomly selecting a few hyperparameter values from the specified disttribution of hyperparameter values.\nBased on the data of hyperparameter values tested (predictors), and the cross-validated score (the response), a Gaussian process model is developed to estimate the cross-validated score & the uncertainty in the estimate in the entire space of the hyperparameter values\nA criterion that “explores” uncertain regions of the space of hyperparameter values (where it is difficult to predict cross-validated score), and “exploits” promising regions of the space are of hyperparameter values (where the cross-validated score is predicted to minimize) is used to suggest the next hyperparameter value that will potentially minimize the cross-validated score\nCross-validated score is computed at the suggested hyperparameter value, the Gaussian process model is updated, and the previous step is repeated, until a certain number of iterations specified by the user.\n\nTo summarize, instead of blindly testing the model for the specified hyperparameter values (as in GridSearchCV()), or randomly testing the model on certain hyperparameter values (as in RandomizedSearchCV()), BayesSearchCV() smartly tests the model for those hyperparameter values that are likely to reduce the cross-validated score. The algorithm becomes “smarter” as it “learns” more with increasing iterations.\nHere is a nice blog, if you wish to understand more about the Bayesian optimization procedure.\n\n# BayesSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor(metric = 'minkowski') # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n# the keys should be EXACTLY the same as the names of the model inputs\n# the values should be the distribution of hyperparameter values. Lists and NumPy arrays can\n# also be used\n    \ngrid = {'n_neighbors': Integer(1, 500), 'weights': Categorical(['uniform', 'distance']), \n       'p': Real(1, 10, prior = 'uniform')} \n\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, \n# use it if you have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within \n# the function\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n\n# Fit the models, and cross-validate\n\n# Sometimes the Gaussian process model predicting the cross-validated score suggests a \n# \"promising point\" (i.e., set of hyperparameter values) for cross-validation that it has \n# already suggested earlier. In such  a case a warning is raised, and the objective \n# function (i.e., the cross-validation score) is computed at a randomly selected point \n# (as in RandomizedSearchCV()). This feature helps the algorithm explore other regions of\n# the hyperparameter space, rather than only searching in the promising regions. Thus, it \n# balances exploration (of the hyperparameter space) with exploitation (of the promising \n# regions of the hyperparameter space)\n\nwarnings.filterwarnings(\"ignore\")\ngcv.fit(X_train_scaled, y_train)\nwarnings.resetwarnings()\n\nThe optimal hyperparameter values (based on Bayesian search) on the provided distribution of hyperparameter values are:\n\ngcv.best_params_\n\nOrderedDict([('n_neighbors', 9),\n             ('p', 1.0008321732366932),\n             ('weights', 'distance')])\n\n\nThe cross-validated root mean squared error for the optimal hyperparameter values is:\n\n-gcv.best_score_\n\n5756.172382596493\n\n\nThe RMSE on test data for the optimal hyperparameter values is:\n\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n5740.432278861367\n\n\n\n4.3.1 Diagonosis of cross-validated score optimization\nBelow are the partial dependence plots of the objective function (i.e., the cross-validated score). The cross-validated score predictions are based on the most recently updated model (i.e., the updated Gaussian Process model at the end of n_iter iterations specified by the user) that predicts the cross-validated score.\nCheck the plot_objective() documentation to interpret the plots.\n\nplot_objective(gcv.optimizer_results_[0],\n                   dimensions=[\"n_neighbors\", \"p\", \"weights\"], size = 3)\nplt.show();\n\n\n\n\n\n\n\n\nThe frequence of individual hyperparameter values considered can also be visualized as below.\n\nfig, ax = plt.subplots(1, 3, figsize = (10, 3))\nplt.subplots_adjust(wspace=0.4)\nplot_histogram(gcv.optimizer_results_[0], 0, ax = ax[0])\nplot_histogram(gcv.optimizer_results_[0], 1, ax = ax[1])\nplot_histogram(gcv.optimizer_results_[0], 2, ax = ax[2])\nplt.show()\n\n\n\n\n\n\n\n\nBelow is the plot showing the minimum cross-validated score computed obtained until ‘n’ hyperparameter values are considered for cross-validation.\n\nplot_convergence(gcv.optimizer_results_)\nplt.show()\n\n\n\n\n\n\n\n\nNote that the cross-validated error is close to the optmial value in the 53rd iteration itself.\nThe cross-validated error at the 53rd iteration is:\n\ngcv.optimizer_results_[0]['func_vals'][53]\n\n5831.87280274334\n\n\nThe hyperparameter values at the 53rd iterations are:\n\ngcv.optimizer_results_[0]['x_iters'][53]\n\n[15, 1.0, 'distance']\n\n\nNote that this is the 2nd most optimal hyperparameter value based on GridSearchCV().\nBelow is the plot showing the cross-validated score computed at each of the 180 hyperparameter values considered for cross-validation. The plot shows that the algorithm seems to explore new regions of the domain space, instead of just exploting the promising ones. There is a balance between exploration and exploitation for finding the optimal hyperparameter values that minimize the objective function (i.e., the function that models the cross-validated score).\n\nsns.lineplot(x = range(1, 181), y = gcv.optimizer_results_[0]['func_vals'])\nplt.xlabel('Iteration')\nplt.ylabel('Cross-validated score')\nplt.show();\n\n\n\n\n\n\n\n\nThe advantages of BayesSearchCV() over GridSearchCV() and RandomizedSearchCV() are:\n\nThe Bayesian Optimization approach gives the benefit that we can give a much larger range of possible values, since over time we identify and exploit the most promising regions and discard the not so promising ones. Plain grid-search would burn computational resources to explore all regions of the domain space with the same granularity, even the not promising ones. Since we search much more effectively in Bayesian search, we can search over a larger domain space.\nBayesSearch CV may help us identify the optimal hyperparameter value in fewer iterations if the Gaussian process model estimating the cross-validated score is relatively accurate. However, this is not certain. Grid and random search are completely uninformed by past evaluations, and as a result, often spend a significant amount of time evaluating “bad” hyperparameters.\nBayesSearch CV is more reliable in cases of a large search space, where random selection may miss sampling values from optimal regions of the search space.\n\nThe disadvantages of BayesSearchCV() over GridSearchCV() and RandomizedSearchCV() are:\n\nBayesSearchCV() has a cost of learning from past data, i.e., updating the model that predicts the cross-validated score after every iteration of evaluating the cross-validated score on a new hyperparameter value. This cost will continue to increase as more and more data is collected. There is no such cost in GridSearchCV() and RandomizedSearchCV() as there is no learning. This implies that each iteration of BayesSearchCV() will take a longer time than each iteration of GridSearchCV() / RandomizedSearchCV(). Thus, even if BayesSearchCV() finds the optimal hyperparameter value in fewer iterations, it may take more time than GridSearchCV() / RandomizedSearchCV() for the same.\nThe success of BayesSearchCV() depends on the predictions and associated uncertainty estimated by the Gaussian process (GP) model that predicts the cross-validated score. The GP model, although works well in general, may not be suitable for certain datasets, or may take a relatively large number of iterations to learn for certain datasets.\n\n\n\n4.3.2 Live monitoring of cross-validated score\nNote that it will be useful monitor the cross-validated score while the Bayesian Search CV code is running, and stop the code as soon as the desired accuracy is reached, or the optimal cross-validated score doesn’t seem to improve. The fit() method of the BayesSeaerchCV() object has a callback argument that can be used as follows:\n\nmodel = KNeighborsRegressor(metric = 'minkowski') # No inputs defined inside the model\ngrid = {'n_neighbors': Integer(1, 500), 'weights': Categorical(['uniform', 'distance']), \n       'p': Real(1, 10, prior = 'uniform')} \n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n\n\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\n\n\ngcv.fit(X_train_scaled, y_train, callback = monitor)\n\n['n_neighbors', 'p', 'weights'] = [9, 1.0008321732366932, 'distance'] 5756.172382596493\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'n_neighbors': Integer(low=1, high=500, prior='uniform', transform='normalize'),\n                             'p': Real(low=1, high=10, prior='uniform', transform='normalize'),\n                             'weights': Categorical(categories=('uniform', 'distance'), prior=None)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'n_neighbors': Integer(low=1, high=500, prior='uniform', transform='normalize'),\n                             'p': Real(low=1, high=10, prior='uniform', transform='normalize'),\n                             'weights': Categorical(categories=('uniform', 'distance'), prior=None)})estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#cross_validate",
    "href": "Hyperparameter tuning.html#cross_validate",
    "title": "4  Hyperparameter tuning",
    "section": "4.4 cross_validate()",
    "text": "4.4 cross_validate()\nWe have used cross_val_score() and cross_val_predict() so far.\nWhen can we use one over the other?\nThe function cross_validate() is similar to cross_val_score() except that it has the option to return multiple cross-validated metrics, instead of a single one.\nConsider the heart disease classification problem, where the response is target (whether the person has a heart disease or not).\n\ndata = pd.read_csv('Datasets/heart_disease_classification.csv')\ndata.head()\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\n\nLet us pre-process the data.\n\n# First, separate the response and the predictors\ny = data['target']\nX = data.drop('target', axis=1)\n\n\n# Separate the data (X,y) into training and test\n\n# Inputs:\n    # data\n    # train-test ratio\n    # random_state for reproducible code\n    \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20, stratify=y) # 80%-20% split\n\n# stratify=y makes sure the class 0 to class 1 ratio in the training and test sets are kept the same as the entire dataset.\n\n\nmodel = KNeighborsClassifier() \nsc = StandardScaler()\nsc.fit(X_train)\nX_train_scaled = sc.transform(X_train)\nX_test_scaled = sc.transform(X_test)\n\nSuppose we want to take recall above a certain threshold with the highest precision possible. cross_validate() computes the cross-validated score for multiple metrics - rest is the same as cross_val_score().\n\nKs = np.arange(10,200,10)\n\nscores = []\n\nfor K in Ks:\n    model = KNeighborsClassifier(n_neighbors=K) # Keeping distance uniform\n    scores.append(cross_validate(model, X_train_scaled, y_train, cv=5, scoring = ['accuracy','recall', 'precision']))\n\n\nscores\n\n# The output is now a list of dicts - easy to convert to a df\n\ndf_scores = pd.DataFrame(scores) # We need to handle test_recall and test_precision cols\n\ndf_scores['CV_recall'] = df_scores['test_recall'].apply(np.mean)\ndf_scores['CV_precision'] = df_scores['test_precision'].apply(np.mean)\ndf_scores['CV_accuracy'] = df_scores['test_accuracy'].apply(np.mean)\n\ndf_scores.index = Ks # We can set K values as indices for convenience\n\n\n#df_scores\n# What happens as K increases?\n    # Recall increases (not monotonically)\n    # Precision decreases (not monotonically)\n# Why?\n    # Check the class distribution in the data - more obs with class 1\n    # As K gets higher, the majority class overrules (visualized in the slides)\n    # More 1s means less FNs - higher recall\n    # More 1s means more FPs - lower precision\n# Would this be the case for any dataset?\n    # NO!! Depends on what the majority class is!\n\nSuppose we wish to have the maximum possible precision for at least 95% recall.\nThe optimal ‘K’ will be:\n\ndf_scores.loc[df_scores['CV_recall'] &gt; 0.95, 'CV_precision'].idxmax()\n\n120\n\n\nThe cross-validated precision, recall and accuracy for the optimal ‘K’ are:\n\ndf_scores.loc[120, ['CV_recall', 'CV_precision', 'CV_accuracy']]\n\nCV_recall       0.954701\nCV_precision    0.734607\nCV_accuracy     0.785374\nName: 120, dtype: object\n\n\n\nsns.lineplot(x = df_scores.index, y = df_scores.CV_precision, color = 'blue', label = 'precision')\nsns.lineplot(x = df_scores.index, y = df_scores.CV_recall, color = 'red', label = 'recall')\nsns.lineplot(x = df_scores.index, y = df_scores.CV_accuracy, color = 'green', label = 'accuracy')\nplt.ylabel('Metric')\nplt.xlabel('K')\nplt.show()",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Lec3_RegressionTrees.html",
    "href": "Lec3_RegressionTrees.html",
    "title": "5  Regression trees",
    "section": "",
    "text": "5.1 Building a regression tree\nDevelop a regression tree to predict car price based on mileage\nX = train['mileage']\ny = train['price']\n#Defining the object to build a regression tree\nmodel = DecisionTreeRegressor(random_state=1, max_depth=3) \n\n#Fitting the regression tree to the data\nmodel.fit(X.values.reshape(-1,1), y)\n\nDecisionTreeRegressor(max_depth=3, random_state=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=3, random_state=1)\n#Visualizing the regression tree\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('car_price_tree.png')\nImage(graph.create_png())\n#prediction on test data\npred=model.predict(test[['mileage']].values)\n#RMSE on test data\nnp.sqrt(mean_squared_error(test.price, pred))\n\n13764.798425410803\n#Visualizing the model fit\nXtest = np.linspace(min(X), max(X), 100)\npred_test = model.predict(Xtest.reshape(-1,1))\nsns.scatterplot(x = 'mileage', y = 'price', data = train, color = 'orange')\nsns.lineplot(x = Xtest, y = pred_test, color = 'blue');\nAll cars falling within the same terminal node have the same predicted price, which is seen as flat line segments in the above model curve.\nDevelop a regression tree to predict car price based on mileage, mpg, engineSize and year\nX = train[['mileage','mpg','year','engineSize']]\nmodel = DecisionTreeRegressor(random_state=1, max_depth=3) \nmodel.fit(X, y)\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage','mpg','year','engineSize'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('car_price_tree.png')\nImage(graph.create_png())\nThe model can also be visualized in the text format as below.\nprint(export_text(model))\n\n|--- feature_3 &lt;= 2.75\n|   |--- feature_2 &lt;= 2018.50\n|   |   |--- feature_3 &lt;= 1.75\n|   |   |   |--- value: [9912.24]\n|   |   |--- feature_3 &gt;  1.75\n|   |   |   |--- value: [16599.03]\n|   |--- feature_2 &gt;  2018.50\n|   |   |--- feature_3 &lt;= 1.90\n|   |   |   |--- value: [19363.81]\n|   |   |--- feature_3 &gt;  1.90\n|   |   |   |--- value: [31919.42]\n|--- feature_3 &gt;  2.75\n|   |--- feature_2 &lt;= 2017.50\n|   |   |--- feature_0 &lt;= 53289.00\n|   |   |   |--- value: [31004.63]\n|   |   |--- feature_0 &gt;  53289.00\n|   |   |   |--- value: [15255.91]\n|   |--- feature_2 &gt;  2017.50\n|   |   |--- feature_1 &lt;= 21.79\n|   |   |   |--- value: [122080.00]\n|   |   |--- feature_1 &gt;  21.79\n|   |   |   |--- value: [49350.79]",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "Lec3_RegressionTrees.html#optimizing-parameters-to-improve-the-regression-tree",
    "href": "Lec3_RegressionTrees.html#optimizing-parameters-to-improve-the-regression-tree",
    "title": "5  Regression trees",
    "section": "5.2 Optimizing parameters to improve the regression tree",
    "text": "5.2 Optimizing parameters to improve the regression tree\nLet us find the optimal depth of the tree and the number of terminal nodes (leaves) by cross validation.\n\n5.2.1 Range of hyperparameter values\nFirst, we’ll find the minimum and maximum possible values of the depth and leaves, and then find the optimal value in that range.\n\nmodel = DecisionTreeRegressor(random_state=1) \nmodel.fit(X, y)\n\nprint(\"Maximum tree depth =\", model.get_depth())\n\nprint(\"Maximum leaves =\", model.get_n_leaves())\n\nMaximum tree depth = 29\nMaximum leaves = 4845\n\n\n\n\n5.2.2 Cross validation: Coarse grid\nWe’ll use the sklearn function GridSearchCV to find the optimal hyperparameter values over a grid of possible values. By default, GridSearchCV returns the optimal hyperparameter values based on the coefficient of determination \\(R^2\\). However, the scoring argument of the function can be used to find the optimal parameters based on several different criteria as mentioned in the scoring-parameter documentation.\n\n#Finding cross-validation error for trees \nparameters = {'max_depth':range(2,30, 3),'max_leaf_nodes':range(2,4900, 100)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv)\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \n\nFitting 5 folds for each of 490 candidates, totalling 2450 fits\n0.8433100904754441 {'max_depth': 11, 'max_leaf_nodes': 302}\n\n\nLet us find the optimal hyperparameters based on root mean squared error (RMSE), instead of \\(R^2\\). Let us compute \\(R^2\\) as well during cross validation, as we can compute multiple performance metrics using the scoring argument. However, when computing multiple performance metrics, we will need to specify the performance metric used to find the optimal hyperparameters with the refit argument.\n\n#Finding cross-validation error for trees \nparameters = {'max_depth':range(2,30, 3),'max_leaf_nodes':range(2,4900, 100)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv,\n                    scoring=['neg_root_mean_squared_error', 'r2'], refit = 'neg_root_mean_squared_error')\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \n\nFitting 5 folds for each of 490 candidates, totalling 2450 fits\n-6475.329183576911 {'max_depth': 11, 'max_leaf_nodes': 302}\n\n\nNote that as the GridSearchCV function maximizes the performance metric to find the optimal hyperparameters, we are maximizing the negative root mean squared error (neg_root_mean_squared_error), and the function returns the optimal negative mean squared error.\nLet us visualize the mean squared error based on the hyperparameter values. We’ll use the cross validation results stored in the cv_results_ attribute of the GridSearchCV fit() object.\n\n#Detailed results of k-fold cross validation\ncv_results = pd.DataFrame(model.cv_results_)\ncv_results.head()\n\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_max_depth\nparam_max_leaf_nodes\nparams\nsplit0_test_neg_root_mean_squared_error\nsplit1_test_neg_root_mean_squared_error\nsplit2_test_neg_root_mean_squared_error\n...\nstd_test_neg_root_mean_squared_error\nrank_test_neg_root_mean_squared_error\nsplit0_test_r2\nsplit1_test_r2\nsplit2_test_r2\nsplit3_test_r2\nsplit4_test_r2\nmean_test_r2\nstd_test_r2\nrank_test_r2\n\n\n\n\n0\n0.010178\n7.531409e-04\n0.003791\n0.000415\n2\n2\n{'max_depth': 2, 'max_leaf_nodes': 2}\n-13729.979521\n-13508.807583\n-12792.941600\n...\n390.725290\n481\n0.314894\n0.329197\n0.394282\n0.361007\n0.382504\n0.356377\n0.030333\n481\n\n\n1\n0.009574\n1.758238e-03\n0.003782\n0.000396\n2\n102\n{'max_depth': 2, 'max_leaf_nodes': 102}\n-10586.885662\n-11230.674720\n-10682.195189\n...\n321.837965\n433\n0.592662\n0.536369\n0.577671\n0.568705\n0.612407\n0.577563\n0.025368\n433\n\n\n2\n0.009774\n7.458305e-04\n0.003590\n0.000488\n2\n202\n{'max_depth': 2, 'max_leaf_nodes': 202}\n-10586.885662\n-11230.674720\n-10682.195189\n...\n321.837965\n433\n0.592662\n0.536369\n0.577671\n0.568705\n0.612407\n0.577563\n0.025368\n433\n\n\n3\n0.009568\n4.953541e-04\n0.003391\n0.000489\n2\n302\n{'max_depth': 2, 'max_leaf_nodes': 302}\n-10586.885662\n-11230.674720\n-10682.195189\n...\n321.837965\n433\n0.592662\n0.536369\n0.577671\n0.568705\n0.612407\n0.577563\n0.025368\n433\n\n\n4\n0.008976\n6.843901e-07\n0.003192\n0.000399\n2\n402\n{'max_depth': 2, 'max_leaf_nodes': 402}\n-10586.885662\n-11230.674720\n-10682.195189\n...\n321.837965\n433\n0.592662\n0.536369\n0.577671\n0.568705\n0.612407\n0.577563\n0.025368\n433\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n\nfig, axes = plt.subplots(1,2,figsize=(14,5))\nplt.subplots_adjust(wspace=0.2)\naxes[0].plot(cv_results.param_max_depth, (-cv_results.mean_test_neg_root_mean_squared_error), 'o')\naxes[0].set_ylim([6200, 7500])\naxes[0].set_xlabel('Depth')\naxes[0].set_ylabel('K-fold RMSE')\naxes[1].plot(cv_results.param_max_leaf_nodes, (-cv_results.mean_test_neg_root_mean_squared_error), 'o')\naxes[1].set_ylim([6200, 7500])\naxes[1].set_xlabel('Leaves')\naxes[1].set_ylabel('K-fold RMSE');\n\n\n\n\n\n\n\n\nWe observe that for a depth of around 8-14, and number of leaves within 1000, we get the lowest \\(K\\)-fold RMSE. So, we should do a finer search in that region to obtain more precise hyperparameter values.\n\n\n5.2.3 Cross validation: Finer grid\n\n#Finding cross-validation error for trees\nstart_time = tm.time()\nparameters = {'max_depth':range(8,15),'max_leaf_nodes':range(2,1000)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv,\n                    scoring = 'neg_root_mean_squared_error')\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \nprint(\"Time taken =\", round((tm.time() - start_time)/60), \"minutes\")\n\nFitting 5 folds for each of 6986 candidates, totalling 34930 fits\n-6414.468922119372 {'max_depth': 10, 'max_leaf_nodes': 262}\nTime taken = 2 minutes\n\n\nFrom the above cross-validation, the optimal hyperparameter values are max_depth = 10 and max_leaf_nodes = 262. Note that the cross-validation score with finer grid is only slightly lower than the course grid. However, depending on the dataset, the finer grid may lead to more benefit.\n\n#Developing the tree based on optimal hyperparameters found by cross-validation\nmodel = DecisionTreeRegressor(random_state=1, max_depth=10,max_leaf_nodes=262) \nmodel.fit(X, y)\n\nDecisionTreeRegressor(max_depth=10, max_leaf_nodes=262, random_state=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=10, max_leaf_nodes=262, random_state=1)\n\n\n\n#RMSE on test data\nXtest = test[['mileage','mpg','year','engineSize']]\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n6921.0404660552895\n\n\nThe RMSE for the decision tree is lower than that of linear regression models with these four predictors. This may be probably due to car price having a highly non-linear association with the predictors.\nNote that we may also use RandomizedSearchCV() or BayesSearchCV() to optimze the hyperparameters.\nPredictor importance: The importance of a predictor is computed as the (normalized) total reduction of the criterion (SSE in case of regression trees) brought by that predictor.\nWarning: impurity-based feature importances can be misleading for high cardinality features (many unique values) Source: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor.feature_importances_\nWhy?\nBecause high cardinality predictors will tend to overfit. When the predictors have high cardinality, it means they form little groups (in the leaf nodes) and then the model “learns” the individuals, instead of “learning” the general trend. The higher the cardinality of the predictor, the more prone is the model to overfitting.\n\nmodel.feature_importances_\n\narray([0.04490344, 0.15882336, 0.29739951, 0.49887369])\n\n\nEngine size is the most important predictor, followed by year, which is followed by mpg, and mileage is the least important predictor.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "Lec3_RegressionTrees.html#cost-complexity-pruning",
    "href": "Lec3_RegressionTrees.html#cost-complexity-pruning",
    "title": "5  Regression trees",
    "section": "5.3 Cost complexity pruning",
    "text": "5.3 Cost complexity pruning\nWhile optimizing parameters above, we optimized them within a range that we thought was reasonable. While doing so, we restricted ourselves to considering only a subset of the unpruned tree. Thus, we could have missed out on finding the optimal tree (or the best model).\nWith cost complexity pruning, we first develop an unpruned tree without any restrictions. Then, using cross validation, we find the optimal value of the tuning parameter \\(\\alpha\\). All the non-terminal nodes for which \\(\\alpha_{eff}\\) is smaller that the optimal \\(\\alpha\\) will be pruned. You will need to check out the link below to understand this better.\nCheck out a detailed explanation of how cost complexity pruning is implemented in sklearn at: https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning\nHere are some informative visualizations that will help you understand what is happening in cost complexity pruning: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n\nmodel = DecisionTreeRegressor(random_state = 1)#model without any restrictions\npath= model.cost_complexity_pruning_path(X,y)# Compute the pruning path during Minimal Cost-Complexity Pruning.\n\n\nalphas=path['ccp_alphas']\n\n\nlen(alphas)\n\n4126\n\n\n\nstart_time = tm.time()\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\ntree = GridSearchCV(DecisionTreeRegressor(random_state=1), param_grid = {'ccp_alpha':alphas}, \n                     scoring = 'neg_mean_squared_error',n_jobs=-1,verbose=1,cv=cv)\ntree.fit(X, y)\nprint (tree.best_score_, tree.best_params_)\nprint(\"Time taken =\",round((tm.time()-start_time)/60), \"minutes\")\n\nFitting 5 folds for each of 4126 candidates, totalling 20630 fits\n-44150619.209031895 {'ccp_alpha': 143722.94076639024}\nTime taken = 2 minutes\n\n\nThe code took 2 minutes to run on a dataset of about 5000 observations and 4 predictors.\n\nmodel = DecisionTreeRegressor(ccp_alpha=143722.94076639024,random_state=1)\nmodel.fit(X, y)\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(test.price, pred))\n\n7306.592294294368\n\n\nThe RMSE for the decision tree with cost complexity pruning is lower than that of linear regression models and spline regression models (including MARS), with these four predictors. However, it is higher than the one obtained with tuning tree parameters using grid search (shown previously). Cost complexity pruning considers a completely unpruned tree unlike the ‘grid search’ method of searching over a grid of hyperparameters such as max_depth and max_leaf_nodes, and thus may seem to be more comprehensive than the ‘grid search’ approach. However, both the approaches may consider trees that are not considered by the other approach, and thus either one may provide a more accurate model. Depending on the grid of parameters chosen for cross validation, the grid search method may be more or less comprehensive than cost complexity pruning.\n\ngridcv_results = pd.DataFrame(tree.cv_results_)\ncv_error = -gridcv_results['mean_test_score']\n\n\n#Visualizing the 5-fold cross validation error vs alpha\nplt.plot(alphas,cv_error)\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('K-fold MSE');\n\n\n\n\n\n\n\n\n\n#Zooming in the above visualization to see the alpha where the 5-fold cross validation error is minimizing\nplt.plot(alphas[0:4093],cv_error[0:4093])\nplt.xlabel('alpha')\nplt.ylabel('K-fold MSE');\n\n\n\n\n\n\n\n\n\n5.3.1 Depth vs alpha; Node counts vs alpha\n\nstime = time.time()\ntrees=[]\nfor i in alphas:\n    tree = DecisionTreeRegressor(ccp_alpha=i,random_state=1)\n    tree.fit(X, train['price'])\n    trees.append(tree)\nprint(time.time()-stime)\n\n268.10325384140015\n\n\nThis code takes 4.5 minutes to run\n\nnode_counts = [clf.tree_.node_count for clf in trees]\ndepth = [clf.tree_.max_depth for clf in trees]\n\n\nfig, ax = plt.subplots(1, 2,figsize=(10,6))\nax[0].plot(alphas[0:4093], node_counts[0:4093], marker=\"o\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(alphas[0:4093], depth[0:4093], marker=\"o\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\n#fig.tight_layout()\n\nText(0.5, 1.0, 'Depth vs alpha')\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Train and test accuracies (R-squared) vs alpha\n\ntrain_scores = [clf.score(X, y) for clf in trees]\ntest_scores = [clf.score(Xtest, test.price) for clf in trees]\n\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(alphas[0:4093], train_scores[0:4093], marker=\"o\", label=\"train\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax.plot(alphas[0:4093], test_scores[0:4093], marker=\"o\", label=\"test\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax.legend()\nplt.show()",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "Lec4_ClassificationTree.html",
    "href": "Lec4_ClassificationTree.html",
    "title": "6  Classification trees",
    "section": "",
    "text": "6.1 Building a classification tree\nDevelop a classification tree to predict if a person has diabetes.\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n#Defining the object to build a classification tree\nmodel = DecisionTreeClassifier(random_state=1, max_depth=3) \n\n#Fitting the regression tree to the data\nmodel.fit(X, y)\n\nDecisionTreeClassifier(max_depth=3, random_state=1)\n#Visualizing the regression tree\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =X.columns,precision=2)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n#graph.write_png('car_price_tree.png')\nImage(graph.create_png())\n# Performance metrics computation \n\n#Computing the accuracy\ny_pred = model.predict(Xtest)\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\ny_pred_prob = model.predict_proba(Xtest)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  73.37662337662337\nROC-AUC:  0.8349197955226512\nPrecision:  0.7777777777777778\nRecall:  0.45901639344262296",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Lec4_ClassificationTree.html#optimizing-hyperparameters-to-optimize-performance",
    "href": "Lec4_ClassificationTree.html#optimizing-hyperparameters-to-optimize-performance",
    "title": "6  Classification trees",
    "section": "6.2 Optimizing hyperparameters to optimize performance",
    "text": "6.2 Optimizing hyperparameters to optimize performance\nIn case of diabetes, it is important to reduce FNR (False negative rate) or maximize recall. This is because if a person has diabetes, the consequences of predicting that they don’t have diabetes can be much worse than the other way round.\nLet us find the optimal depth of the tree and the number of terminal nods (leaves) that minimizes the FNR or maximizes recall.\nFind the maximum values of depth and number of leaves.\n\n#Defining the object to build a regression tree\nmodel = DecisionTreeClassifier(random_state=1) \n\n#Fitting the regression tree to the data\nmodel.fit(X, y)\n\nDecisionTreeClassifier(random_state=1)\n\n\n\n# Maximum number of leaves\nmodel.get_n_leaves()\n\n118\n\n\n\n# Maximum depth\nmodel.get_depth()\n\n14\n\n\n\n#Defining parameters and the range of values over which to optimize\nparam_grid = {    \n    'max_depth': range(2,14),\n    'max_leaf_nodes': range(2,118),\n    'max_features': range(1, 9)\n}\n\n\n#Grid search to optimize parameter values\n\nstart_time = time.time()\nskf = StratifiedKFold(n_splits=5)#The folds are made by preserving the percentage of samples for each class.\n\n#Minimizing FNR is equivalent to maximizing recall\ngrid_search = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, scoring=['precision','recall'], \n                           refit=\"recall\", cv=skf, n_jobs=-1, verbose = True)\ngrid_search.fit(X, y)\n\n# make the predictions\ny_pred = grid_search.predict(Xtest)\n\nprint('Train accuracy : %.3f'%grid_search.best_estimator_.score(X, y))\nprint('Test accuracy : %.3f'%grid_search.best_estimator_.score(Xtest, ytest))\nprint('Best recall Through Grid Search : %.3f'%grid_search.best_score_)\n\nprint('Best params for recall')\nprint(grid_search.best_params_)\n\nprint(\"Time taken =\", round((time.time() - start_time)), \"seconds\")\n\nFitting 5 folds for each of 11136 candidates, totalling 55680 fits\nTrain accuracy : 0.785\nTest accuracy : 0.675\nBest recall Through Grid Search : 0.658\nBest params for recall\n{'max_depth': 4, 'max_features': 2, 'max_leaf_nodes': 8}\nTime taken = 70 seconds",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Lec4_ClassificationTree.html#optimizing-the-decision-threshold-probability",
    "href": "Lec4_ClassificationTree.html#optimizing-the-decision-threshold-probability",
    "title": "6  Classification trees",
    "section": "6.3 Optimizing the decision threshold probability",
    "text": "6.3 Optimizing the decision threshold probability\nNote that decision threshold probability is not tuned with GridSearchCV because GridSearchCV is a technique used for hyperparameter tuning in machine learning models, and the decision threshold probability is not a hyperparameter of the model.\nThe decision threshold is set to 0.5 by default during hyperparameter tuning with GridSearchCV.\nGridSearchCV is used to tune hyperparameters that control the internal settings of a machine learning model, such as learning rate, regularization strength, and maximum tree depth, among others. These hyperparameters affect the model’s internal behavior and performance. On the other hand, the decision threshold is an external parameter that is used to interpret the model’s output and make predictions based on the predicted probabilities.\nTo tune the decision threshold, one typically needs to manually adjust it after the model has been trained and evaluated using a specific set of hyperparameter values. This can be done using methods, which involve evaluating the model’s performance at different decision threshold values and selecting the one that best meets the desired trade-off between false positives and false negatives based on the specific problem requirements.\nAs the recall will always be 100% for a decision threshold probability of zero, we’ll find a decision threshold probability that balances recall with another performance metric such as precision, false positive rate, accuracy, etc. Below are a couple of examples that show we can balance recall with (1) precision or (2) false positive rate.\n\n6.3.1 Balancing recall with precision\nWe can find a threshold probability that balances recall with precision.\n\nmodel = DecisionTreeClassifier(random_state=1, max_depth = 4, max_leaf_nodes=8, max_features=2).fit(X, y)\n\n# Note that we are using the cross-validated predicted probabilities, instead of directly using the \n# predicted probabilities on train data, as the model may be overfitting on the train data, and \n# may lead to misleading results\ncross_val_ypred = cross_val_predict(DecisionTreeClassifier(random_state=1, max_depth = 4, \n                                                        max_leaf_nodes=8, max_features=2), X, \n                                              y, cv = 5, method = 'predict_proba')\n\np, r, thresholds = precision_recall_curve(y, cross_val_ypred[:,1])\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\n\n# Thresholds with precision and recall\nnp.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\n\narray([[0.08196721, 0.33713355, 1.        ],\n       [0.09045226, 0.34982332, 0.95652174],\n       [0.09248555, 0.36641221, 0.92753623],\n       [0.0964467 , 0.39293139, 0.91304348],\n       [0.1       , 0.42105263, 0.88888889],\n       [0.10810811, 0.42298851, 0.88888889],\n       [0.10869565, 0.42857143, 0.88405797],\n       [0.12820513, 0.48378378, 0.8647343 ],\n       [0.14285714, 0.48219178, 0.85024155],\n       [0.18518519, 0.48618785, 0.85024155],\n       [0.2       , 0.48611111, 0.84541063],\n       [0.20512821, 0.48876404, 0.84057971],\n       [0.20833333, 0.49418605, 0.82125604],\n       [0.21276596, 0.49411765, 0.8115942 ],\n       [0.22916667, 0.50151976, 0.79710145],\n       [0.23684211, 0.51582278, 0.78743961],\n       [0.27777778, 0.52786885, 0.77777778],\n       [0.3015873 , 0.54794521, 0.77294686],\n       [0.36      , 0.56554307, 0.7294686 ],\n       [0.3697479 , 0.56692913, 0.69565217],\n       [0.37931034, 0.58974359, 0.66666667],\n       [0.54954955, 0.59130435, 0.65700483],\n       [0.55172414, 0.59798995, 0.57487923],\n       [0.55882353, 0.59893048, 0.5410628 ],\n       [0.58823529, 0.6091954 , 0.51207729],\n       [0.61904762, 0.6       , 0.47826087],\n       [0.62337662, 0.60431655, 0.4057971 ],\n       [0.63461538, 0.59130435, 0.32850242],\n       [0.69354839, 0.59803922, 0.29468599],\n       [0.69642857, 0.59493671, 0.22705314],\n       [0.70149254, 0.56338028, 0.19323671],\n       [0.71153846, 0.61403509, 0.16908213],\n       [0.75609756, 0.5952381 , 0.12077295],\n       [0.76363636, 0.55555556, 0.09661836],\n       [0.76470588, 0.59090909, 0.06280193],\n       [0.875     , 0.66666667, 0.03864734],\n       [0.94117647, 0.66666667, 0.02898551],\n       [1.        , 0.6       , 0.01449275]])\n\n\nSuppose, we wish to have at least 80% recall, with the highest possible precision. Then, based on the precision-recall curve (or the table above), we should have a decision threshold probability of 0.21.\nLet’s assess the model’s performance on test data with a threshold probability of 0.21.\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.21\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  72.72727272727273\nROC-AUC:  0.7544509078089194\nPrecision:  0.611764705882353\nRecall:  0.8524590163934426\n\n\n\n\n\n\n\n\n\n\n\n6.3.2 Balancing recall with false positive rate\nSuppose we wish to balance recall with false positive rate. We can optimize the model to maximize ROC-AUC, and then choose a point on the ROC-curve that balances recall with the false positive rate.\n\n# Defining parameters and the range of values over which to optimize\nparam_grid = {    \n    'max_depth': range(2,14),\n    'max_leaf_nodes': range(2,118),\n    'max_features': range(1, 9)\n}\n\n\n#Grid search to optimize parameter values\n\nstart_time = time.time()\nskf = StratifiedKFold(n_splits=5)#The folds are made by preserving the percentage of samples for each class.\n\n#Minimizing FNR is equivalent to maximizing recall\ngrid_search = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, scoring=['precision','recall',\n                            'roc_auc'], refit=\"roc_auc\", cv=skf, n_jobs=-1, verbose = True)\ngrid_search.fit(X, y)\n\n# make the predictions\ny_pred = grid_search.predict(Xtest)\n\nprint('Best params for recall')\nprint(grid_search.best_params_)\n\nprint(\"Time taken =\", round((time.time() - start_time)), \"seconds\")\n\nFitting 5 folds for each of 11136 candidates, totalling 55680 fits\nBest params for recall\n{'max_depth': 6, 'max_features': 2, 'max_leaf_nodes': 9}\nTime taken = 72 seconds\n\n\n\nmodel = DecisionTreeClassifier(random_state=1, max_depth = 6, max_leaf_nodes=9, max_features=2).fit(X, y)\n\n\ncross_val_ypred = cross_val_predict(DecisionTreeClassifier(random_state=1, max_depth = 6, \n                                                           max_leaf_nodes=9, max_features=2), X, \n                                              y, cv = 5, method = 'predict_proba')\n\nfpr, tpr, auc_thresholds = roc_curve(y, cross_val_ypred[:,1])\nprint(auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot(fpr, tpr, 'o', color = 'blue')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(y, cross_val_ypred[:,1])\nplot_roc_curve(fpr, tpr)\n\n0.7605075431162388\n\n\n\n\n\n\n\n\n\n\n# Thresholds with TPR and FPR\nall_thresholds = np.concatenate([auc_thresholds.reshape(-1,1), tpr.reshape(-1,1), fpr.reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,1]&gt;0.8,:]\n# As the values in 'recall_more_than_80' are arranged in increasing order of recall and decreasing threshold,\n# the first value will provide the maximum threshold probability for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the minimum possible FPR\nrecall_more_than_80[0]\n\narray([0.21276596, 0.80676329, 0.39066339])\n\n\nSuppose, we wish to have at least 80% recall, with the lowest possible precision. Then, based on the ROC-AUC curve, we should have a decision threshold probability of 0.21.\nLet’s assess the model’s performance on test data with a threshold probability of 0.21.\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.21\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  71.42857142857143\nROC-AUC:  0.7618543980257358\nPrecision:  0.6075949367088608\nRecall:  0.7868852459016393",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Lec4_ClassificationTree.html#cost-complexity-pruning",
    "href": "Lec4_ClassificationTree.html#cost-complexity-pruning",
    "title": "6  Classification trees",
    "section": "6.4 Cost complexity pruning",
    "text": "6.4 Cost complexity pruning\nJust as we did cost complexity pruning in a regression tree, we can do it to optimize the model for a classification tree.\n\nmodel = DecisionTreeClassifier(random_state = 1)#model without any restrictions\npath= model.cost_complexity_pruning_path(X,y)# Compute the pruning path during Minimal Cost-Complexity Pruning.\n\n\nalphas=path['ccp_alphas']\nlen(alphas)\n\n58\n\n\n\n#Grid search to optimize parameter values\n\nskf = StratifiedKFold(n_splits=5)\ngrid_search = GridSearchCV(DecisionTreeClassifier(random_state = 1), param_grid = {'ccp_alpha':alphas}, \n                                                  scoring=['precision','recall','accuracy'], \n                                                  refit=\"recall\", cv=skf, n_jobs=-1, verbose = True)\ngrid_search.fit(X, y)\n\n# make the predictions\ny_pred = grid_search.predict(Xtest)\n\nprint('Best params for recall')\nprint(grid_search.best_params_)\n\nFitting 5 folds for each of 58 candidates, totalling 290 fits\nBest params for recall\n{'ccp_alpha': 0.010561291712538737}\n\n\n\n# Model with the optimal value of 'ccp_alpha'\nmodel = DecisionTreeClassifier(ccp_alpha=0.01435396,random_state=1)\nmodel.fit(X, y)\n\nDecisionTreeClassifier(ccp_alpha=0.01435396, random_state=1)\n\n\nNow we can tune the decision threshold probability to balance recall with another performance metrics as shown earlier in Section 4.3.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Lec4_Bagging.html",
    "href": "Lec4_Bagging.html",
    "title": "7  Bagging",
    "section": "",
    "text": "7.1 Bagging regression trees\nBag regression trees to develop a model to predict car price using the predictors mileage,mpg,year,and engineSize.\n#Bagging the results of 10 decision trees to predict car price\nmodel = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=1,\n                        n_jobs=-1).fit(X, y)\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5752.0779571060875\nThe RMSE has reduced a lot by averaging the predictions of 10 trees. The RMSE for a single tree model with optimized parameters was around 7000.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "Lec4_Bagging.html#bagging-regression-trees",
    "href": "Lec4_Bagging.html#bagging-regression-trees",
    "title": "7  Bagging",
    "section": "",
    "text": "7.1.1 Model accuracy vs number of trees\nHow does the model accuracy vary with the number of trees?\nAs we increase the number of trees, it will tend to reduce the variance of individual trees leading to a more accurate prediction.\n\n#Finding model accuracy vs number of trees\nwarnings.filterwarnings(\"ignore\")\noob_rsquared={};test_rsquared={};oob_rmse={};test_rmse = {}\nfor i in np.linspace(10,400,40,dtype=int):\n    model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=i, random_state=1,\n                        n_jobs=-1,oob_score=True).fit(X, y)\n    oob_rsquared[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_rsquared[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n    oob_rmse[i]=np.sqrt(mean_squared_error(model.oob_prediction_,y))\n    test_rmse[i]=np.sqrt(mean_squared_error(model.predict(Xtest),ytest))\nwarnings.resetwarnings()\n    \n# The hidden warning is: \"Some inputs do not have OOB scores. This probably means too few \n# estimators were used to compute any reliable oob estimates.\" This warning will appear\n# in case of small number of estimators. In such a case, some observations may be use\n# by all the estimators, and their OOB score can't be computed\n\nAs we are bagging only 10 trees in the first iteration, some of the observations are selected in every bootstrapped sample, and thus they don’t have an out-of-bag error, which is producing the warning. For every observation to have an out-of-bag error, the number of trees must be sufficiently large.\nLet us visualize the out-of-bag (OOB) R-squared and R-squared on test data vs the number of trees.\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),label = 'Out of bag R-squared')\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),'o',color = 'blue')\nplt.plot(test_rsquared.keys(),test_rsquared.values(), label = 'Test data R-squared')\nplt.xlabel('Number of trees')\nplt.ylabel('Rsquared')\nplt.legend();\n\n\n\n\n\n\n\n\nThe out-of-bag R-squared initially increases, and then stabilizes after a certain number of trees (around 150 in this case). Note that increasing the number of trees further will not lead to overfitting. However, increasing the number of trees will increase the computations. Thus, we don’t need to develop more trees once the R-squared stabilizes.\n\n#Visualizing out-of-bag RMSE and test data RMSE\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rmse.keys(),oob_rmse.values(),label = 'Out of bag RMSE')\nplt.plot(oob_rmse.keys(),oob_rmse.values(),'o',color = 'blue')\nplt.plot(test_rmse.keys(),test_rmse.values(), label = 'Test data RMSE')\nplt.xlabel('Number of trees')\nplt.ylabel('RMSE')\nplt.legend()\n\n\n\n\n\n\n\n\nA similar trend can be seen by plotting out-of-bag RMSE and test RMSE. Note that RMSE is proportional to R-squared. We only need to visualize one of RMSE or R-squared to find the optimal number of trees.\n\n#Bagging with 150 trees\nmodel = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=150, random_state=1,\n                        oob_score=True,n_jobs=-1).fit(X, y)\n\n\n#OOB R-squared\nmodel.oob_score_\n\n0.897561533100511\n\n\n\n#RMSE on test data\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(test.price, pred))\n\n5673.756466489405\n\n\n\n\n7.1.2 Optimizing bagging hyperparameters using grid search\nMore parameters of a bagged regression tree model can be optimized using the typical approach of k-fold cross validation over a grid of parameter values.\nNote that we don’t need to tune the number of trees in bagging as we know that the higher the number of trees, the lower will be the expected MSE. So, we will tune all the hyperparameters for a fixed number of trees. Once we have obtained the optimal hyperparameter values, we’ll keep increasing the number of trees until the gains are neglible.\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'base_estimator': [DecisionTreeRegressor(random_state = 1),LinearRegression()],#Comparing bagging with a linear regression model as well\n          'n_estimators': [100],\n          'max_samples': [0.5,1.0],\n          'max_features': [0.5,1.0],\n          'bootstrap': [True, False],\n          'bootstrap_features': [True, False]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\nbagging_regressor_grid = GridSearchCV(BaggingRegressor(random_state=1, n_jobs=-1), \n                                      param_grid =params, cv=cv, n_jobs=-1, verbose=1)\nbagging_regressor_grid.fit(X, y)\n\nprint('Train R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X, y))\nprint('Test R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(Xtest, ytest))\nprint('Best R^2 Score Through Grid Search : %.3f'%bagging_regressor_grid.best_score_)\nprint('Best Parameters : ',bagging_regressor_grid.best_params_)\n\nFitting 5 folds for each of 32 candidates, totalling 160 fits\nTrain R^2 Score : 0.986\nTest R^2 Score : 0.882\nBest R^2 Score Through Grid Search : 0.892\nBest Parameters :  {'base_estimator': DecisionTreeRegressor(random_state=1), 'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 100}\n\n\nYou may use the object bagging_regressor_grid to directly make the prediction.\n\nnp.sqrt(mean_squared_error(test.price, bagging_regressor_grid.predict(Xtest)))\n\n5708.308794847089\n\n\nNote that once the model has been tuned and the optimal hyperparameters identified, we can keep increasing the number of trees until it ceases to benefit.\n\n#Model with optimal hyperparameters and increased number of trees\nmodel = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=500, random_state=1,\n                        oob_score=True,n_jobs=-1,bootstrap_features=False,bootstrap=True,\n                        max_features=1.0,max_samples=1.0).fit(X, y)\n\n\n#RMSE on test data\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5624.685464926517",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "Lec4_Bagging.html#bagging-for-classification",
    "href": "Lec4_Bagging.html#bagging-for-classification",
    "title": "7  Bagging",
    "section": "7.2 Bagging for classification",
    "text": "7.2 Bagging for classification\nBag classification tree models to predict if a person has diabetes.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n#Bagging the results of 10 decision trees to predict car price\nmodel = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=150, random_state=1,\n                        n_jobs=-1).fit(X, y)\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.23\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  76.62337662337663\nROC-AUC:  0.8766084963863917\nPrecision:  0.6404494382022472\nRecall:  0.9344262295081968\n\n\n\n\n\n\n\n\n\nAs a result of bagging, we obtain a model (with a threshold probabiltiy cutoff of 0.23) that has a better performance on test data in terms of almost all the metrics - accuracy, precision (comparable performance), recall, and ROC-AUC, as compared the single tree classification model (with a threshold probability cutoff of 0.23). Note that we have not yet tuned the model using GridSearchCv here, which is shown towards the end of this chapter.\n\n7.2.1 Model accuracy vs number of trees\n\n#Finding model accuracy vs number of trees\noob_accuracy={};test_accuracy={};oob_rmse={};test_rmse = {}\nfor i in np.linspace(10,400,40,dtype=int):\n    model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=i, random_state=1,\n                        n_jobs=-1,oob_score=True).fit(X, y)\n    oob_accuracy[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_accuracy[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:640: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\"Some inputs do not have OOB scores. \"\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:644: RuntimeWarning: invalid value encountered in true_divide\n  oob_decision_function = (predictions /\n\n\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),label = 'Out of bag accuracy')\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),'o',color = 'blue')\nplt.plot(test_accuracy.keys(),test_accuracy.values(), label = 'Test data accuracy')\nplt.xlabel('Number of trees')\nplt.ylabel('Rsquared')\nplt.legend()\n\n\n\n\n\n\n\n\n\n#ROC curve on training data\nypred = model.predict_proba(X)[:, 1]\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nprint(auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nplot_roc_curve(fpr, tpr)\n\n1.0\n\n\n\n\n\n\n\n\n\nNote that there is perfect separation in train data as ROC-AUC = 1. This shows that the model is probably overfitting. However, this also shows that, despite the reduced variance (as compared to a single tree), the bagged tree model is flexibly enough to perfectly separate the classes.\n\n#ROC curve on test data\nypred = model.predict_proba(Xtest)[:, 1]\nfpr, tpr, auc_thresholds = roc_curve(ytest, ypred)\nprint(\"ROC-AUC = \",auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(ytest, ypred)\nplot_roc_curve(fpr, tpr)\n\nROC-AUC =  0.8781949585757096\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Optimizing bagging hyperparameters using grid search\nMore parameters of a bagged classification tree model can be optimized using the typical approach of k-fold cross validation over a grid of parameter values.\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'base_estimator': [DecisionTreeClassifier(random_state = 1),LogisticRegression()],#Comparing bagging with a linear regression model as well\n          'n_estimators': [150,200,250],\n          'max_samples': [0.5,1.0],\n          'max_features': [0.5,1.0],\n          'bootstrap': [True, False],\n          'bootstrap_features': [True, False]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\nbagging_classifier_grid = GridSearchCV(BaggingClassifier(random_state=1, n_jobs=-1), \n                                      param_grid =params, cv=cv, n_jobs=-1, verbose=1,\n                                      scoring = ['precision', 'recall'], refit='recall')\nbagging_classifier_grid.fit(X, y)\n\nprint('Train accuracy : %.3f'%bagging_classifier_grid.best_estimator_.score(X, y))\nprint('Test accuracy : %.3f'%bagging_classifier_grid.best_estimator_.score(Xtest, ytest))\nprint('Best accuracy Through Grid Search : %.3f'%bagging_classifier_grid.best_score_)\nprint('Best Parameters : ',bagging_classifier_grid.best_params_)\n\nFitting 5 folds for each of 96 candidates, totalling 480 fits\nTrain accuracy : 1.000\nTest accuracy : 0.786\nBest accuracy Through Grid Search : 0.573\nBest Parameters :  {'base_estimator': DecisionTreeClassifier(random_state=1), 'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200}\n\n\n\n\n7.2.3 Tuning the decision threshold probability\nWe’ll find a decision threshold probability that balances recall with precision.\n\nmodel = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=1), n_estimators=200, \n                          random_state=1,max_features=1.0, oob_score=True,\n                        max_samples=1.0,n_jobs=-1,bootstrap=True,bootstrap_features=False).fit(X, y)\n\nAs the model is overfitting on the train data, it will not be a good idea to tune the decision threshold probability based on the precision-recall curve on train data, as shown in the figure below.\n\nypred = model.predict_proba(X)[:,1]\np, r, thresholds = precision_recall_curve(y, ypred)\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\nInstead, we should make the precision-recall curve using the out-of-bag predictions, as shown below. The method oob_decision_function_ provides the predicted probability.\n\nypred = model.oob_decision_function_[:,1]\np, r, thresholds = precision_recall_curve(y, ypred)\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\n\n# Thresholds with precision and recall\nall_thresholds = np.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,2]&gt;0.8,:]\n# As the values in 'recall_more_than_80' are arranged in decreasing order of recall and increasing threshold,\n# the last value will provide the maximum threshold probability for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the maximum possible precision\nrecall_more_than_80[recall_more_than_80.shape[0]-1]\n\narray([0.2804878 , 0.53205128, 0.80193237])\n\n\nSuppose, we wish to have at least 80% recall, with the highest possible precision. Then, based on the precision-recall curve, we should have a decision threshold probability of 0.28.\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.28\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.22077922077922\nROC-AUC:  0.8802221047065044\nPrecision:  0.6705882352941176\nRecall:  0.9344262295081968\n\n\n\n\n\n\n\n\n\nNote that this model has a better performance than the untuned bagged model earlier, and the single tree classification model, as expected.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html",
    "href": "Bagging (OOB vs K-fold cross-validation).html",
    "title": "8  Bagging (addendum)",
    "section": "",
    "text": "8.1 Tree without tuning\nmodel = DecisionTreeRegressor()\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n-np.mean(cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv = cv))\n\n7056.960817154941\nparam_grid = {'max_depth': Integer(2, 30)}\ngcv = BayesSearchCV(model, search_spaces = param_grid, cv = cv, n_iter = 40, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\ngcv.fit(X, y, callback = monitor)    \n\n['max_depth'] = [10] 6341.1481858990355\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=DecisionTreeRegressor(), n_iter=40, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'max_depth': Integer(low=2, high=30, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=DecisionTreeRegressor(), n_iter=40, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'max_depth': Integer(low=2, high=30, prior='uniform', transform='normalize')})estimator: DecisionTreeRegressorDecisionTreeRegressor()DecisionTreeRegressorDecisionTreeRegressor()",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#performance-of-tree-improves-with-tuning",
    "href": "Bagging (OOB vs K-fold cross-validation).html#performance-of-tree-improves-with-tuning",
    "title": "8  Bagging (addendum)",
    "section": "8.2 Performance of tree improves with tuning",
    "text": "8.2 Performance of tree improves with tuning\n\nmodel = DecisionTreeRegressor(max_depth=10)\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n-np.mean(cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv = cv))\n\n6442.494300778735",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#bagging-tuned-trees",
    "href": "Bagging (OOB vs K-fold cross-validation).html#bagging-tuned-trees",
    "title": "8  Bagging (addendum)",
    "section": "8.3 Bagging tuned trees",
    "text": "8.3 Bagging tuned trees\n\nmodel = BaggingRegressor(DecisionTreeRegressor(max_depth = 10), oob_score=True, n_estimators = 100).fit(X, y)\nmean_squared_error(model.oob_prediction_, y, squared = False)\n\n5354.357809020438",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#bagging-untuned-trees",
    "href": "Bagging (OOB vs K-fold cross-validation).html#bagging-untuned-trees",
    "title": "8  Bagging (addendum)",
    "section": "8.4 Bagging untuned trees",
    "text": "8.4 Bagging untuned trees\n\nmodel = BaggingRegressor(DecisionTreeRegressor(), oob_score=True, n_estimators = 100).fit(X, y)\nmean_squared_error(model.oob_prediction_, y, squared = False)\n\n5248.720845665685\n\n\nWhy is bagging tuned trees worse than bagging untuned trees?\nIn the tuned tree here, the reduction in variance by controlling maximum depth resulted in an increas in bias of indivudual trees. Bagging trees only reduces the variance, but not the bias of the indivudal trees. Thus, bagging high bias models will result in a high-bias model, while bagging high variance models may result in a low variance model if the models are not highly correlated.\nBagging tuned models may provide a better performance as compared to bagging untuned models if the reduction in variance of the individual models is high enough to overshadow the increase in bias, and increase in pairwise correlation of the individual models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#tuning-bagged-model---oob",
    "href": "Bagging (OOB vs K-fold cross-validation).html#tuning-bagged-model---oob",
    "title": "8  Bagging (addendum)",
    "section": "8.5 Tuning bagged model - OOB",
    "text": "8.5 Tuning bagged model - OOB\n\nparam_grid1 = {'max_samples': [0.25, 0.5, 0.75, 1.0],\n             'max_features': [2, 3, 4],\n             'bootstrap_features': [True, False]}\nparam_grid2 = {'max_samples': [0.25, 0.5, 0.75, 1.0],\n             'max_features': [1],\n              'bootstrap_features': [False]}\nparam_list1 = list(it.product(*[values for key, values in param_grid1.items()]))\nparam_list2 = list(it.product(*[values for key, values in param_grid2.items()]))\nparam_list = param_list1 + param_list2\n\n\noob_score_pr = []\nfor pr in param_list:\n    model = BaggingRegressor(DecisionTreeRegressor(), max_samples=pr[0], max_features=pr[1],\n                            bootstrap_features=pr[2], n_jobs = -1, oob_score=True, n_estimators = 50).fit(X, y)\n    oob_score_pr.append(mean_squared_error(model.oob_prediction_, y, squared=False))\n\nWhat is the benefit of OOB validation to tune hyperparameters in bagging?\nIt is much cheaper than \\(k\\)-fold cross-validation, as only \\(1/k\\) of the models are trained with OOB validation as compared to \\(k\\)-fold cross-validation. However, the cost of training individual models is lower in \\(k\\)-fold cross-validation as models are trained on a smaller dataset. Typically, OOB will be faster than \\(k\\)-fold cross-validation. The higher the value of \\(k\\), the more faster OOB validation will be as compared to \\(k\\)-fold cross-validation.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#tuning-without-k-fold-cross-validation",
    "href": "Bagging (OOB vs K-fold cross-validation).html#tuning-without-k-fold-cross-validation",
    "title": "8  Bagging (addendum)",
    "section": "8.6 Tuning without k-fold cross-validation",
    "text": "8.6 Tuning without k-fold cross-validation\nWhen hyperparameters can be tuned with OOB validation, what is the benefit of using k-fold cross-validation?\n\nHyperparameters cannot be tuned over continuous spaces with OOB validation.\nOOB score is not computed if samping is done without replacement (bootstrap = False). Thus, for tuning the bootstrap hyperparameter, \\(k\\)-fold cross-validation will need to be used.\n\n\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\n\n\nparam_grid = {'max_samples': Real(0.2, 1.0),\n             'max_features': Integer(1, 4),\n             'bootstrap_features': [True, False],\n              'bootstrap': [True, False]}\ngcv = BayesSearchCV(BaggingRegressor(DecisionTreeRegressor(), bootstrap=False), \n                    search_spaces = param_grid, cv = cv, n_jobs = -1,\n                  scoring='neg_root_mean_squared_error')\n\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ngcv.fit(X, y, callback=monitor)\n\n['bootstrap', 'bootstrap_features', 'max_features', 'max_samples'] = [True, False, 4, 0.8061354588503475] 5561.064432968422\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=BaggingRegressor(bootstrap=False,\n                                         estimator=DecisionTreeRegressor()),\n              n_jobs=-1, scoring='neg_root_mean_squared_error',\n              search_spaces={'bootstrap': [True, False],\n                             'bootstrap_features': [True, False],\n                             'max_features': Integer(low=1, high=4, prior='uniform', transform='normalize'),\n                             'max_samples': Real(low=0.2, high=1.0, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=BaggingRegressor(bootstrap=False,\n                                         estimator=DecisionTreeRegressor()),\n              n_jobs=-1, scoring='neg_root_mean_squared_error',\n              search_spaces={'bootstrap': [True, False],\n                             'bootstrap_features': [True, False],\n                             'max_features': Integer(low=1, high=4, prior='uniform', transform='normalize'),\n                             'max_samples': Real(low=0.2, high=1.0, prior='uniform', transform='normalize')})estimator: BaggingRegressorBaggingRegressor(bootstrap=False, estimator=DecisionTreeRegressor())estimator: DecisionTreeRegressorDecisionTreeRegressor()DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nplot_histogram(gcv.optimizer_results_[0],0)\n\n\n\n\n\n\n\n\n\nplot_objective(gcv.optimizer_results_[0])",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#warm-start",
    "href": "Bagging (OOB vs K-fold cross-validation).html#warm-start",
    "title": "8  Bagging (addendum)",
    "section": "8.7 warm start",
    "text": "8.7 warm start\nWhat is the purpose of warm_start?\nThe purpose of warm_start is to avoid developing trees from scratch, and incrementally add trees to monitor the validation error. However, note that OOB score is not computed with warm_start. Thus, a validation set approach will need to be adopted to tune number of trees.\nA cheaper approach to tune number of estimators is to just use trial and error, and stop increasing once the cross-validation error / OOB error / validation set error stabilizes.\n\nmodel = BaggingRegressor(DecisionTreeRegressor(), oob_score=False, n_estimators = 5,\n                        warm_start=True).fit(X, y)\nrmse = []\nfor i in range(10, 200, 10):\n    model.n_estimators = i\n    model.fit(X, y)\n    rmse.append(mean_squared_error(model.predict(Xtest), ytest, squared=False))\n    sns.lineplot(x = range(10, i+1, 10), y = rmse)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#bagging-knn",
    "href": "Bagging (OOB vs K-fold cross-validation).html#bagging-knn",
    "title": "8  Bagging (addendum)",
    "section": "8.8 Bagging KNN",
    "text": "8.8 Bagging KNN\nShould we bag a tuned KNN model or an untuned one?\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nmodel = KNeighborsRegressor(n_neighbors=9) # optimal neigbors\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n-np.mean(cross_val_score((model), X_scaled, y, cv = cv, \n                         scoring='neg_root_mean_squared_error', n_jobs = -1))\n\n6972.997277781689\n\n\n\nmodel = KNeighborsRegressor(n_neighbors=1)\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n-np.mean(cross_val_score(BaggingRegressor(model), X_scaled, y, cv = cv, \n                         scoring='neg_root_mean_squared_error', n_jobs = -1))\n\n6254.305462266355\n\n\n\nmodel = BaggingRegressor(DecisionTreeRegressor(), n_estimators=5, warm_start=True)\nmodel.fit(X, y)\nrmse = []\nfor i in range(10, 200,10):\n    model.n_estimators = i\n    model.fit(X, y)\n    rmse.append(mean_squared_error(model.predict(Xtest), ytest, squared=False))\n    sns.lineplot(x = range(10, i + 1, 10), y = rmse)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html",
    "href": "Lec6_RandomForest.html",
    "title": "9  Random Forest",
    "section": "",
    "text": "9.1 Random Forest for regression\nNow, let us visualize small trees with the random forest algorithm to see if a predictor dominates all the trees.\n#Averaging the results of 10 decision trees, while randomly considering sqrt(4)=2 predictors at each node\n#to split, to predict car price\nmodel = RandomForestRegressor(n_estimators=10, random_state=1,max_features=\"sqrt\",max_depth=3,\n                        n_jobs=-1).fit(X, y)\n#Change the index of model.estimators_[index] to visualize the 10 random forest trees, one at a time\ndot_data = StringIO()\nexport_graphviz(model.estimators_[4], out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage','mpg','year','engineSize'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n#graph.write_png('car_price_tree.png')\nImage(graph.create_png())\nAs two of the four predictors are randomly selected for splitting each node, engineSize no longer seems to dominate the trees. This will tend to reduce correlation among trees, thereby reducing the prediction variance, which in turn will tend to improve prediction accuracy.\n#Averaging the results of 10 decision trees, while randomly considering sqrt(4)=2 predictors at each node\n#to split, to predict car price\nmodel = RandomForestRegressor(n_estimators=10, random_state=1,max_features=\"sqrt\",\n                        n_jobs=-1).fit(X, y)\nmodel.feature_importances_\n\narray([0.16370584, 0.35425511, 0.18552673, 0.29651232])\nNote that the feature importance of engineSize is reduced in random forests (as compared to bagged trees), and it no longer dominates the trees.\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5856.022395768459\nThe RMSE is similar to that obtained by bagging. We will discuss the comparison later.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-for-regression",
    "href": "Lec6_RandomForest.html#random-forest-for-regression",
    "title": "9  Random Forest",
    "section": "",
    "text": "9.1.1 Model accuracy vs number of trees\nHow does the model accuracy vary with the number of trees?\nAs we increase the number of trees, it will tend to reduce the variance of individual trees leading to a more accurate prediction.\n\n#Finding model accuracy vs number of trees\nwarnings.filterwarnings(\"ignore\")\noob_rsquared={};test_rsquared={};oob_rmse={};test_rmse = {}\n\nfor i in np.linspace(10,400,40,dtype=int):\n    model = RandomForestRegressor(n_estimators=i, random_state=1,max_features=\"sqrt\",\n                        n_jobs=-1,oob_score=True).fit(X, y)\n    oob_rsquared[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_rsquared[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n    oob_rmse[i]=np.sqrt(mean_squared_error(model.oob_prediction_,y))\n    test_rmse[i]=np.sqrt(mean_squared_error(model.predict(Xtest),ytest))\n\nwarnings.resetwarnings()\n    \n# The hidden warning is: \"Some inputs do not have OOB scores. This probably means too few \n# estimators were used to compute any reliable oob estimates.\" This warning will appear\n# in case of small number of estimators. In such a case, some observations may be use\n# by all the estimators, and their OOB score can't be computed\n\nAs we are ensemble only 10 trees in the first iteration, some of the observations are selected in every bootstrapped sample, and thus they don’t have an out-of-bag error, which is producing the warning. For every observation to have an out-of-bag error, the number of trees must be sufficiently large.\nLet us visualize the out-of-bag (OOB) R-squared and R-squared on test data vs the number of trees.\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),label = 'Out of bag R-squared')\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),'o',color = 'blue')\nplt.plot(test_rsquared.keys(),test_rsquared.values(), label = 'Test data R-squared')\nplt.xlabel('Number of trees')\nplt.ylabel('Rsquared')\nplt.legend();\n\n\n\n\n\n\n\n\nThe out-of-bag \\(R\\)-squared initially increases, and then stabilizes after a certain number of trees (around 200 in this case). Note that increasing the number of trees further will not lead to overfitting. However, increasing the number of trees will increase the computations. Thus, the number of trees developed should be the number beyond which the \\(R\\)-squared stabilizes.\n\n#Visualizing out-of-bag RMSE and test data RMSE\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rmse.keys(),oob_rmse.values(),label = 'Out of bag RMSE')\nplt.plot(oob_rmse.keys(),oob_rmse.values(),'o',color = 'blue')\nplt.plot(test_rmse.keys(),test_rmse.values(), label = 'Test data RMSE')\nplt.xlabel('Number of trees')\nplt.ylabel('RMSE')\nplt.legend();\n\n\n\n\n\n\n\n\nA similar trend can be seen by plotting out-of-bag RMSE and test RMSE. Note that RMSE is proportional to R-squared. You only need to visualize one of RMSE or \\(R\\)-squared to find the optimal number of trees.\n\n#Bagging with 150 trees\nmodel = RandomForestRegressor(n_estimators=200, random_state=1,max_features=\"sqrt\",\n                        oob_score=True,n_jobs=-1).fit(X, y)\n\n\n#OOB R-squared\nmodel.oob_score_\n\n0.8998265006519903\n\n\n\n#RMSE on test data\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(test.price, pred))\n\n5647.195064555622\n\n\n\n\n9.1.2 Tuning random forest\nThe Random forest object has options to set parameters such as depth, leaves, minimum number of observations in a leaf etc., for individual trees. These parameters are useful to prune a decision tree model consisting of a single tree, in order to avoid overfitting due to high variance of an unpruned tree.\nPruning individual trees in random forests is not likely to add much value, since averaging a sufficient number of unpruned trees reduces the variance of the trees, which enhances prediction accuracy. Pruning individual trees is unlikely to further reduce the prediction variance.\nHere is a comment from page 596 of the The Elements of Statistical Learning that supports the above statement: Segal (2004) demonstrates small gains in performance by controlling the depths of the individual trees grown in random forests. Our experience is that using full-grown trees seldom costs much, and results in one less tuning parameter.\nBelow we attempt to optimize parameters that prune individual trees. However, as expected, it does not result in a substantial increase in prediction accuracy.\nAlso, note that we don’t need to tune the number of trees in random forest with GridSearchCV. As we know the prediction accuracy will keep increasing with number of trees, we can tune the other hyperparameters with a constant value for the number of trees.\n\nmodel.estimators_[0].get_n_leaves()\n\n3086\n\n\n\nmodel.estimators_[0].get_depth()\n\n29\n\n\nCoarse grid search\n\n#Optimizing with OOB score takes half the time as compared to cross validation. \n#The number of models developed with OOB score tuning is one-fifth of the number of models developed with\n#5-fold cross validation\nstart_time = time.time()\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'max_depth': [5, 10, 15, 20, 25, 30],\n          'max_leaf_nodes':[600, 1200, 1800, 2400, 3000],\n          'max_features': [1,2,3,4]}\n\nparam_list=list(it.product(*(params[Name] for Name in params)))\n\noob_score = [0]*len(param_list)\ni=0\nfor pr in param_list:\n    model = RandomForestRegressor(random_state=1,oob_score=True,verbose=False,\n                    n_estimators = 100, max_depth=pr[0],\n                    max_leaf_nodes=pr[1], max_features=pr[2], n_jobs=-1).fit(X,y)\n    oob_score[i] = mean_squared_error(model.oob_prediction_, y, squared=False)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"Best params = \", param_list[np.argmin(oob_score)])\nprint(\"Optimal OOB validation RMSE = \", np.min(oob_score))\n\ntime taken =  1.230358862876892  minutes\nBest params =  (15, 1800, 3)\nOptimal OOB validation RMSE =  5243.408784594606\n\n\nFiner grid search\nBased on the coarse grid search, hyperparameters will be tuned in a finer grid around the optimal hyperparamter values obtained.\n\n#Optimizing with OOB score takes half the time as compared to cross validation. \n#The number of models developed with OOB score tuning is one-fifth of the number of models developed with\n#5-fold cross validation\nstart_time = time.time()\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'max_depth': [12, 15, 18],\n          'max_leaf_nodes':[1600, 1800, 2000],\n          'max_features': [1,2,3,4]}\n\nparam_list=list(it.product(*(params[Name] for Name in params)))\n\noob_score = [0]*len(param_list)\ni=0\nfor pr in param_list:\n    model = RandomForestRegressor(random_state=1,oob_score=True,verbose=False,\n             n_estimators = 100, max_depth=pr[0], max_leaf_nodes=pr[1],\n                    max_features=pr[2], n_jobs=-1).fit(X,y)\n    oob_score[i] = mean_squared_error(model.oob_prediction_, y, squared=False)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"Best params = \", param_list[np.argmin(oob_score)])\nprint(\"Optimal OOB validation RMSE = \", np.min(oob_score))\n\ntime taken =  0.4222299337387085  minutes\nBest params =  (15, 1800, 3)\nBest score =  5243.408784594606\n\n\n\n#Model with optimal parameters\nmodel = RandomForestRegressor(n_estimators = 100, random_state=1, max_leaf_nodes = 1800, max_depth = 15,\n                        oob_score=True,n_jobs=-1, max_features=3).fit(X, y)\n\n\n#RMSE on test data\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5671.410705964455\n\n\nOptimizing depth and leaves of individual trees didn’t improve the prediction accuracy of the model. Important parameters to optimize in random forests will be the number of trees (n_estimators), and number of predictors considered at each split (max_features). However, sometimes individual pruning of trees may be useful. This may happen when the increase in bias in individual trees (when pruned) is lesser than the decrease in variance of the tree. However, if the pairwise correlation coefficient \\(\\rho\\) of the trees increases by a certain extent on pruning, pruning may again be not useful.\n\n#Tuning only n_estimators and max_features produces similar results\nstart_time = time.time()\nparams = {'max_features': [1,2,3,4]}\n\nparam_list=list(it.product(*(params[Name] for Name in params)))\n\noob_score = [0]*len(param_list)\ni=0\nfor pr in param_list:\n    model = RandomForestRegressor(random_state=1,oob_score=True,verbose=False,\n                      n_estimators = 100, max_features=pr[0], n_jobs=-1).fit(X,y)\n    oob_score[i] = mean_squared_error(model.oob_prediction_, y, squared=False)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"Best params = \", param_list[np.argmin(oob_score)])\nprint(\"Optimal OOB validation RMSE = \", np.min(oob_score))\n\ntime taken =  0.02856200933456421  minutes\nBest params =  (3,)\nBest score (R-squared) =  5252.291978670057\n\n\n\n#Model with optimal parameters\nmodel = RandomForestRegressor(n_estimators=100, random_state=1,\n                        n_jobs=-1, max_features=3).fit(X, y)\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5656.561522632323\n\n\nConsidering hyperparameters involving pruning, we observe a marginal decrease in the out-of-bag RMSE. Thus, other hyperparameters (such as max_features and max_samples) must be prioritized for tuning over hyperparameters involving pruning.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-for-classification",
    "href": "Lec6_RandomForest.html#random-forest-for-classification",
    "title": "9  Random Forest",
    "section": "9.2 Random forest for classification",
    "text": "9.2 Random forest for classification\nRandom forest model to predict if a person has diabetes.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n#Ensembling the results of 10 decision trees\nmodel = RandomForestClassifier(n_estimators=200, random_state=1,max_features=\"sqrt\",n_jobs=-1).fit(X, y)\n\n\n#Feature importance for Random forest\nnp.mean([tree.feature_importances_ for tree in model.estimators_],axis=0)\n\narray([0.08380406, 0.25403736, 0.09000104, 0.07151063, 0.07733353,\n       0.16976023, 0.12289303, 0.13066012])\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.23\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  72.72727272727273\nROC-AUC:  0.8744050766790058\nPrecision:  0.6021505376344086\nRecall:  0.9180327868852459\n\n\n\n\n\n\n\n\n\nThe model obtained above is similar to the one obtained by bagging. We’ll discuss the comparison later.\n\n9.2.1 Model accuracy vs number of trees\n\n#Finding model accuracy vs number of trees\noob_accuracy={};test_accuracy={};oob_precision={}; test_precision = {}\nfor i in np.linspace(50,500,45,dtype=int):\n    model = RandomForestClassifier(n_estimators=i, random_state=1,max_features=\"sqrt\",n_jobs=-1,oob_score=True).fit(X, y)\n    oob_accuracy[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_accuracy[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n    oob_pred = (model.oob_decision_function_[:,1]&gt;=0.5).astype(int)     \n    oob_precision[i] = precision_score(y, oob_pred)\n    test_pred = model.predict(Xtest)\n    test_precision[i] = precision_score(ytest, test_pred)\n\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),label = 'Out of bag accuracy')\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),'o',color = 'blue')\nplt.plot(test_accuracy.keys(),test_accuracy.values(), label = 'Test data accuracy')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Classification accuracy')\nplt.legend();\n\n\n\n\n\n\n\n\nWe can also plot other metrics of interest such as out-of-bag precision vs number of trees.\n\n#Precision vs number of trees\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_precision.keys(),oob_precision.values(),label = 'Out of bag precision')\nplt.plot(oob_precision.keys(),oob_precision.values(),'o',color = 'blue')\nplt.plot(test_precision.keys(),test_precision.values(), label = 'Test data precision')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Precision')\nplt.legend();\n\n\n\n\n\n\n\n\n\n\n9.2.2 Tuning random forest\nHere we tune the number of predictors to be considered at each node for the split to maximize recall.\n\nstart_time = time.time()\n\nparams = {'n_estimators': [500],\n          'max_features': range(1,9),\n         }\n\nparam_list=list(it.product(*(params[Name] for Name in list(params.keys()))))\noob_recall = [0]*len(param_list)\n\ni=0\nfor pr in param_list:\n    model = RandomForestClassifier(random_state=1,oob_score=True,verbose=False,n_estimators = pr[0],\n                                  max_features=pr[1], n_jobs=-1).fit(X,y)\n    \n    oob_pred = (model.oob_decision_function_[:,1]&gt;=0.5).astype(int)     \n    oob_recall[i] = recall_score(y, oob_pred)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"max recall = \", np.max(oob_recall))\nprint(\"params= \", param_list[np.argmax(oob_recall)])\n\ntime taken =  0.08032723267873128  minutes\nmax recall =  0.5990338164251208\nparams=  (500, 8)\n\n\n\nmodel = RandomForestClassifier(random_state=1,n_jobs=-1,max_features=8,n_estimators=500).fit(X, y)\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.23\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  76.62337662337663\nROC-AUC:  0.8787237793054822\nPrecision:  0.6404494382022472\nRecall:  0.9344262295081968\n\n\n\n\n\n\n\n\n\n\nmodel.feature_importances_\n\narray([0.069273  , 0.31211579, 0.08492953, 0.05225877, 0.06179047,\n       0.17732674, 0.12342981, 0.1188759 ])",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-vs-bagging",
    "href": "Lec6_RandomForest.html#random-forest-vs-bagging",
    "title": "9  Random Forest",
    "section": "9.3 Random forest vs Bagging",
    "text": "9.3 Random forest vs Bagging\nWe saw in the above examples that the performance of random forest was similar to that of bagged trees. This may happen in some cases including but not limited to:\n\nAll the predictors are more or less equally important, and the bagged trees are not highly correlated.\nOne of the predictors dominates the trees, resulting in highly correlated trees. However, each of the highly correlated trees have high prediction accuracy, leading to overall high prediction accuracy of the bagged trees despite the high correlation.\n\nWhen can random forests perform poorly: When the number of variables is large, but the fraction of relevant variables small, random forests are likely to perform poorly with small \\(m\\) (fraction of predictors considered for each split). At each split the chance can be small that the relevant variables will be selected. - Elements of Statistical Learning, page 596.\nHowever, in general, random forests are expected to decorrelate and improve the bagged trees.\nLet us consider a classification example.\n\ndata = pd.read_csv('Heart.csv')\ndata.dropna(inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\n\nAge\nSex\nChestPain\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nThal\nAHD\n\n\n\n\n0\n63\n1\ntypical\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\nfixed\nNo\n\n\n1\n67\n1\nasymptomatic\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\nnormal\nYes\n\n\n2\n67\n1\nasymptomatic\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\nreversable\nYes\n\n\n3\n37\n1\nnonanginal\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\nnormal\nNo\n\n\n4\n41\n0\nnontypical\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\nnormal\nNo\n\n\n\n\n\n\n\n\nIn the above dataset, we wish to predict if a person has acquired heart disease (AHD = ‘Yes’), based on their symptoms.\n\n#Response variable\ny = pd.get_dummies(data['AHD'])['Yes']\n\n#Creating a dataframe for predictors with dummy variables replacing the categorical variables\nX = data.drop(columns = ['AHD','ChestPain','Thal'])\nX = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\nX.head()\n\n\n\n\n\n\n\n\n\nAge\nSex\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nasymptomatic\nnonanginal\nnontypical\ntypical\nfixed\nnormal\nreversable\n\n\n\n\n0\n63\n1\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\n0\n0\n0\n1\n1\n0\n0\n\n\n1\n67\n1\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\n1\n0\n0\n0\n0\n1\n0\n\n\n2\n67\n1\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\n1\n0\n0\n0\n0\n0\n1\n\n\n3\n37\n1\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\n0\n1\n0\n0\n0\n1\n0\n\n\n4\n41\n0\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\nX.shape\n\n(297, 18)\n\n\n\n#Creating train and test datasets\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#tuning-random-forest-2",
    "href": "Lec6_RandomForest.html#tuning-random-forest-2",
    "title": "9  Random Forest",
    "section": "Tuning random forest",
    "text": "Tuning random forest\n\n#Tuning the random forest parameters\nstart_time = time.time()\n\noob_score = {}\n\ni=0\nfor pr in range(1,19):\n    model = RandomForestClassifier(random_state=1,oob_score=True,verbose=False,n_estimators = 500,\n                                  max_features=pr, n_jobs=-1).fit(X,y)\n    oob_score[i] = model.oob_score_\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"max accuracy = \", np.max(list(oob_score.values())))\nprint(\"Best value of max_features= \", np.argmax(list(oob_score.values()))+1)\n\ntime taken =  0.21557459433873494  minutes\nmax accuracy =  0.8249158249158249\nBest value of max_features=  3\n\n\n\nsns.scatterplot(x = oob_score.keys(),y = oob_score.values())\nplt.xlabel('Max features')\nplt.ylabel('Classification accuracy')\n\nText(0, 0.5, 'Classification accuracy')\n\n\n\n\n\n\n\n\n\nNote that as the value of max_features is increasing, the accuracy is decreasing. This is probably due to the trees getting correlated as we consider more predictors for each split.\n\n#Finding model accuracy vs number of trees\noob_accuracy={};test_accuracy={};\noob_accuracy2={};test_accuacy2={};\n\nfor i in np.linspace(100,500,40,dtype=int):\n    #Bagging\n    model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=i, random_state=1,\n                        n_jobs=-1,oob_score=True).fit(Xtrain, ytrain)\n    oob_accuracy[i]=model.oob_score_  #Returns the out-of-bag classification accuracy of the model\n    test_accuracy[i]=model.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n    \n    #Random forest\n    model2 = RandomForestClassifier(n_estimators=i, random_state=1,max_features=3,\n                        n_jobs=-1,oob_score=True).fit(Xtrain, ytrain)\n    oob_accuracy2[i]=model2.oob_score_  #Returns the out-of-bag classification accuracy of the model\n    test_accuacy2[i]=model2.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n   \n\n\n#Feature importance for bagging\nnp.mean([tree.feature_importances_ for tree in model.estimators_],axis=0)\n\narray([0.04381883, 0.05913479, 0.08585651, 0.07165678, 0.00302965,\n       0.00903484, 0.05890448, 0.01223421, 0.072461  , 0.01337919,\n       0.17495662, 0.18224651, 0.00527156, 0.00953965, 0.00396654,\n       0.00163193, 0.09955286, 0.09332406])\n\n\nNote that no predictor is too important to consider. That’s why a small value of three for max_features is likely to decorrelate trees without compromising the quality of predictions.\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),label = 'Bagging OOB')\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),'o',color = 'blue')\nplt.plot(test_accuracy.keys(),test_accuracy.values(), label = 'Bagging test accuracy')\n\nplt.plot(oob_accuracy2.keys(),oob_accuracy2.values(),label = 'RF OOB')\nplt.plot(oob_accuracy2.keys(),oob_accuracy2.values(),'o',color = 'green')\nplt.plot(test_accuacy2.keys(),test_accuacy2.values(), label = 'RF test accuracy')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Classification accuracy')\nplt.legend(bbox_to_anchor=(0, -0.15, 1, 0), loc=2, ncol=2, mode=\"expand\", borderaxespad=0)\n\n\n\n\n\n\n\n\nIn the above example we observe that random forest does improve over bagged trees in terms of classification accuracy. Unlike the previous two examples, the optimal value of max_features for random forests is much smaller than the total number of available predictors, thereby making the random forest model much different than the bagged tree model.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html",
    "href": "Lec7_AdaBoost.html",
    "title": "10  Adaptive Boosting",
    "section": "",
    "text": "10.1 Hyperparameters\nThere are 3 important parameters to tune in AdaBoost:\nLet us visualize the accuracy of AdaBoost when we independently tweak each of the above parameters.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\nfrom sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\nrecall_score, precision_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold\nfrom sklearn.ensemble import BaggingRegressor,BaggingClassifier,AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nimport itertools as it\nimport time as time\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\nX = train[['mileage','mpg','year','engineSize']]\nXtest = test[['mileage','mpg','year','engineSize']]\ny = train['price']\nytest = test['price']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html#hyperparameters",
    "href": "Lec7_AdaBoost.html#hyperparameters",
    "title": "10  Adaptive Boosting",
    "section": "",
    "text": "Number of trees\nDepth of each tree\nLearning rate",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html#adaboost-for-regression",
    "href": "Lec7_AdaBoost.html#adaboost-for-regression",
    "title": "10  Adaptive Boosting",
    "section": "10.2 AdaBoost for regression",
    "text": "10.2 AdaBoost for regression\n\n10.2.1 Number of trees vs cross validation error\nAs the number of trees increases, the prediction bias will decrease, and the prediction variance will increase. Thus, there will be an optimal number of trees that minimizes the prediction error.\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [2, 5, 10, 50, 100, 500, 1000]\n    for n in n_trees:\n        models[str(n)] = AdaBoostRegressor(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;2 9190.253 (757.408)\n&gt;5 8583.629 (341.406)\n&gt;10 8814.328 (248.891)\n&gt;50 10763.138 (465.677)\n&gt;100 11217.783 (602.642)\n&gt;500 11336.088 (763.288)\n&gt;1000 11390.043 (752.446)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n10.2.2 Depth of tree vs cross validation error\nAs the depth of each weak learner (decision tree) increases, the complexity of the weak learner will increase. As the complexity increases, the prediction bias will decrease, while the prediction variance will increase. Thus, there will be an optimal depth for each weak learner that minimizes the prediction error.\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define base model\n        base = DecisionTreeRegressor(max_depth=i)\n        # define ensemble model\n        models[str(i)] = AdaBoostRegressor(base_estimator=base,n_estimators=50)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 12704.191 (661.913)\n&gt;2 10675.975 (382.400)\n&gt;3 10523.960 (557.974)\n&gt;4 9303.664 (500.022)\n&gt;5 7257.473 (385.578)\n&gt;6 6120.387 (371.625)\n&gt;7 5802.894 (428.146)\n&gt;8 5656.343 (521.073)\n&gt;9 5449.504 (471.809)\n&gt;10 5379.424 (452.370)\n&gt;11 5330.506 (428.361)\n&gt;12 5416.617 (580.948)\n&gt;13 5371.431 (495.273)\n&gt;14 5368.026 (417.437)\n&gt;15 5477.644 (538.878)\n&gt;16 5477.425 (468.785)\n&gt;17 5495.560 (520.657)\n&gt;18 5489.784 (462.329)\n&gt;19 5577.452 (564.083)\n&gt;20 5563.340 (479.502)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n10.2.3 Learning rate vs cross validation error\nThe optimal learning rate will depend on the number of trees, and vice-versa. If the learning rate is too low, it will take several trees to “learn” the response. If the learning rate is high, the response will be “learned” quickly (with fewer) trees. Learning too quickly will be prone to overfitting, while learning too slowly will be computationally expensive. Thus, there will be an optimal learning rate to minimize the prediction error.\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = AdaBoostRegressor(learning_rate=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.1 8347.3 (460.1)\n&gt;0.2 8478.3 (487.2)\n&gt;0.3 8717.4 (500.1)\n&gt;0.4 9091.3 (481.2)\n&gt;0.5 9374.3 (400.4)\n&gt;0.6 9712.9 (548.1)\n&gt;0.7 9866.8 (359.5)\n&gt;0.8 10443.6 (436.4)\n&gt;0.9 10526.6 (503.4)\n&gt;1.0 10821.8 (594.8)\n&gt;1.1 10688.5 (437.6)\n&gt;1.2 10840.2 (426.6)\n&gt;1.3 10892.1 (543.8)\n&gt;1.4 10932.6 (388.1)\n&gt;1.5 11316.6 (656.2)\n&gt;1.6 11098.3 (596.2)\n&gt;1.7 11037.3 (560.1)\n&gt;1.8 11236.2 (650.7)\n&gt;1.9 11770.4 (749.1)\n&gt;2.0 11404.9 (681.6)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n10.2.4 Tuning AdaBoost for regression\nAs the optimal value of the parameters depend on each other, we need to optimize them simultaneously.\n\nmodel = AdaBoostRegressor(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['base_estimator'] = [DecisionTreeRegressor(max_depth=3), DecisionTreeRegressor(max_depth=5), \n                          DecisionTreeRegressor(max_depth=10),DecisionTreeRegressor(max_depth=15)]\n# define the evaluation procedure\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='neg_mean_squared_error')\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nBest: -28598146.516266 using {'base_estimator': DecisionTreeRegressor(max_depth=10), 'learning_rate': 1.0, 'n_estimators': 50}\n\n\nNote that for tuning max_depth of the base estimator - decision tree, we specified 4 different base estimators with different depths. However, there is a more concise way to do that. We can specify the max_depth of the base estimator by adding a double underscore “__” between the base_estimator and the hyperparameter that we wish to tune (max_depth here), and then specify its potential values in the grid itself as shown below. However, we’ll then need to add DecisionTreeRegressor() as the base estimator within the AdaBoostRegressor() function.\n\nmodel = AdaBoostRegressor(random_state=1, base_estimator = DecisionTreeRegressor(random_state=1))\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['base_estimator__max_depth'] = [3, 5, 10, 15]\n# define the evaluation procedure\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='neg_mean_squared_error')\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nBest: -28598146.516266 using {'base_estimator__max_depth': 10, 'learning_rate': 1.0, 'n_estimators': 50}\n\n\n\n#Model based on the optimal hyperparameters\nmodel = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,learning_rate=1.0,\n                         random_state=1).fit(X,y)\n\n\n#RMSE of the optimized model on test data\npred1=model.predict(Xtest)\nprint(\"AdaBoost model RMSE = \", np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nAdaBoost model RMSE =  5693.165811600585\n\n\n\nmodel2 = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\npred2 = model2.predict(Xtest)\nprint(\"Random Forest model RMSE = \", np.sqrt(mean_squared_error(model2.predict(Xtest),ytest)))\n\nRandom Forest model RMSE =  5642.45839697972\n\n\n\n#Ensemble modeling\npred = 0.5*pred1+0.5*pred2\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5528.699297204213\n\n\nCombined, the random forest model and the Adaboost model do better than each of the individual models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html#adaboost-for-classification",
    "href": "Lec7_AdaBoost.html#adaboost-for-classification",
    "title": "10  Adaptive Boosting",
    "section": "10.3 AdaBoost for classification",
    "text": "10.3 AdaBoost for classification\nBelow is the AdaBoost implementation on a classification problem. The takeaways are the same as that of the regression problem above.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n10.3.1 Number of trees vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [10, 50, 100, 500, 1000, 5000]\n    for n in n_trees:\n        models[str(n)] = AdaBoostClassifier(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;10 0.718 (0.060)\n&gt;50 0.751 (0.051)\n&gt;100 0.748 (0.053)\n&gt;500 0.690 (0.045)\n&gt;1000 0.694 (0.048)\n&gt;5000 0.691 (0.044)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Depth of each tree vs cross validation accuracy\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define base model\n        base = DecisionTreeClassifier(max_depth=i)\n        # define ensemble model\n        models[str(i)] = AdaBoostClassifier(base_estimator=base)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 0.751 (0.051)\n&gt;2 0.699 (0.063)\n&gt;3 0.696 (0.062)\n&gt;4 0.707 (0.055)\n&gt;5 0.713 (0.021)\n&gt;6 0.710 (0.061)\n&gt;7 0.733 (0.057)\n&gt;8 0.738 (0.044)\n&gt;9 0.727 (0.053)\n&gt;10 0.738 (0.065)\n&gt;11 0.748 (0.048)\n&gt;12 0.699 (0.044)\n&gt;13 0.738 (0.047)\n&gt;14 0.697 (0.041)\n&gt;15 0.697 (0.052)\n&gt;16 0.692 (0.052)\n&gt;17 0.702 (0.056)\n&gt;18 0.702 (0.045)\n&gt;19 0.700 (0.040)\n&gt;20 0.696 (0.042)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n10.3.3 Learning rate vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = AdaBoostClassifier(learning_rate=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.1 0.749 (0.052)\n&gt;0.2 0.743 (0.050)\n&gt;0.3 0.731 (0.057)\n&gt;0.4 0.736 (0.053)\n&gt;0.5 0.733 (0.062)\n&gt;0.6 0.738 (0.058)\n&gt;0.7 0.741 (0.056)\n&gt;0.8 0.741 (0.049)\n&gt;0.9 0.736 (0.048)\n&gt;1.0 0.741 (0.035)\n&gt;1.1 0.734 (0.037)\n&gt;1.2 0.736 (0.038)\n&gt;1.3 0.731 (0.057)\n&gt;1.4 0.728 (0.041)\n&gt;1.5 0.730 (0.036)\n&gt;1.6 0.720 (0.038)\n&gt;1.7 0.707 (0.045)\n&gt;1.8 0.730 (0.024)\n&gt;1.9 0.712 (0.033)\n&gt;2.0 0.454 (0.191)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n10.3.4 Tuning AdaBoost Classifier hyperparameters\n\nmodel = AdaBoostClassifier(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['base_estimator'] = [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), \n                          DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=4)]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                          verbose = True)\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n#for mean, stdev, param in zip(means, stds, params):\n#    print(\"%f (%f) with: %r\" % (mean, stdev, param)\n\nFitting 5 folds for each of 100 candidates, totalling 500 fits\nBest: 0.763934 using {'base_estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 0.01, 'n_estimators': 200}\n\n\n\n\n10.3.5 Tuning the decision threshold probability\nWe’ll find a decision threshold probability that balances recall with precision.\n\n#Model based on the optimal parameters\nmodel = AdaBoostClassifier(random_state=1,base_estimator = DecisionTreeClassifier(max_depth=3),learning_rate=0.01,\n                          n_estimators=200).fit(X,y)\n\n# Note that we are using the cross-validated predicted probabilities, instead of directly using the \n# predicted probabilities on train data, as the model may be overfitting on the train data, and \n# may lead to misleading results\ncross_val_ypred = cross_val_predict(AdaBoostClassifier(random_state=1,base_estimator = DecisionTreeClassifier(max_depth=3),learning_rate=0.01,\n                          n_estimators=200), X, y, cv = 5, method = 'predict_proba')\n\np, r, thresholds = precision_recall_curve(y, cross_val_ypred[:,1])\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\n\n# Thresholds with precision and recall\nall_thresholds = np.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,2]&gt;0.8,:]\n# As the values in 'recall_more_than_80' are arranged in decreasing order of recall and increasing threshold,\n# the last value will provide the maximum threshold probability for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the maximum possible precision\nrecall_more_than_80[recall_more_than_80.shape[0]-1]\n\narray([0.33488762, 0.50920245, 0.80193237])\n\n\n\n#Optimal decision threshold probability\nthres = recall_more_than_80[recall_more_than_80.shape[0]-1][0]\nthres\n\n0.3348876199649718\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = thres\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8884188260179798\nPrecision:  0.6875\nRecall:  0.9016393442622951\n\n\n\n\n\n\n\n\n\nThe above model is similar to the one obtained with bagging / random forest. However, adaptive boosting may lead to better classification performance as compared to bagging / random forest.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html",
    "href": "Lec8_Gradient_Boosting.html",
    "title": "11  Gradient Boosting",
    "section": "",
    "text": "11.1 Hyperparameters\nThere are 4 important parameters to tune in Gradient boosting:\nLet us visualize the accuracy of Gradient boosting when we independently tweak each of the above parameters.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\nfrom sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\nrecall_score, precision_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nimport itertools as it\nimport time as time\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\nX = train[['mileage','mpg','year','engineSize']]\nXtest = test[['mileage','mpg','year','engineSize']]\ny = train['price']\nytest = test['price']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html#hyperparameters",
    "href": "Lec8_Gradient_Boosting.html#hyperparameters",
    "title": "11  Gradient Boosting",
    "section": "",
    "text": "Number of trees\nDepth of each tree\nLearning rate\nSubsample fraction",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html#gradient-boosting-for-regression",
    "href": "Lec8_Gradient_Boosting.html#gradient-boosting-for-regression",
    "title": "11  Gradient Boosting",
    "section": "11.2 Gradient boosting for regression",
    "text": "11.2 Gradient boosting for regression\n\n11.2.1 Number of trees vs cross validation error\nAs the number of trees increase, the prediction bias will decrease, and the prediction variance will increase. Thus, there will be an optimal number of trees that minimize the prediction error.\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [2, 5, 10, 50, 100, 500, 1000, 2000, 5000]\n    for n in n_trees:\n        models[str(n)] = GradientBoostingRegressor(n_estimators=n,random_state=1,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;2 14927.566 (179.475)\n&gt;5 12743.148 (189.408)\n&gt;10 10704.199 (226.234)\n&gt;50 6869.066 (278.885)\n&gt;100 6354.656 (270.097)\n&gt;500 5515.622 (424.516)\n&gt;1000 5515.251 (427.767)\n&gt;2000 5600.041 (389.687)\n&gt;5000 5854.168 (362.223)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n11.2.2 Depth of tree vs cross validation error\nAs the depth of each weak learner (decision tree) increases, the complexity of the weak learner will increase. As the complexity increases, the prediction bias will decrease, while the prediction variance will increase. Thus, there will be an optimal depth of each weak learner that minimizes the prediction error.\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define ensemble model\n        models[str(i)] = GradientBoostingRegressor(n_estimators=50,random_state=1,max_depth=i,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 9693.731 (810.090)\n&gt;2 7682.569 (489.841)\n&gt;3 6844.225 (536.792)\n&gt;4 5972.203 (538.693)\n&gt;5 5664.563 (497.882)\n&gt;6 5329.130 (404.330)\n&gt;7 5210.934 (461.038)\n&gt;8 5197.204 (494.957)\n&gt;9 5227.975 (478.789)\n&gt;10 5299.782 (446.509)\n&gt;11 5433.822 (451.673)\n&gt;12 5617.946 (509.797)\n&gt;13 5876.424 (542.981)\n&gt;14 6030.507 (560.447)\n&gt;15 6125.914 (643.852)\n&gt;16 6294.784 (672.646)\n&gt;17 6342.327 (677.050)\n&gt;18 6372.418 (791.068)\n&gt;19 6456.471 (741.693)\n&gt;20 6503.622 (759.193)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n11.2.3 Learning rate vs cross validation error\nThe optimal learning rate will depend on the number of trees, and vice-versa. If the learning rate is too low, it will take several trees to “learn” the response. If the learning rate is high, the response will be “learned” quickly (with fewer) trees. Learning too quickly will be prone to overfitting, while learning too slowly will be computationally expensive. Thus, there will be an optimal learning rate to minimize the prediction error.\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = GradientBoostingRegressor(learning_rate=i,random_state=1,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.1 6329.8 (450.7)\n&gt;0.2 5942.9 (454.8)\n&gt;0.3 5618.4 (490.8)\n&gt;0.4 5665.9 (577.3)\n&gt;0.5 5783.5 (561.7)\n&gt;0.6 5773.8 (500.3)\n&gt;0.7 5875.5 (565.7)\n&gt;0.8 5878.5 (540.5)\n&gt;0.9 6214.4 (594.3)\n&gt;1.0 5986.1 (601.5)\n&gt;1.1 6216.5 (395.3)\n&gt;1.2 6667.5 (657.2)\n&gt;1.3 6717.4 (594.4)\n&gt;1.4 7048.4 (531.7)\n&gt;1.5 7265.0 (742.0)\n&gt;1.6 7404.4 (868.2)\n&gt;1.7 7425.8 (606.3)\n&gt;1.8 8283.0 (1345.3)\n&gt;1.9 8872.2 (1137.9)\n&gt;2.0 17713.3 (865.3)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n11.2.4 Subsampling vs cross validation error\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for s in np.arange(0.25, 1.1, 0.25):\n        key = '%.2f' % s\n        models[key] = GradientBoostingRegressor(random_state=1,subsample=s,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.2f (%.2f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Subsample',fontsize=15)\n\n&gt;0.25 6219.59 (569.97)\n&gt;0.50 6178.28 (501.87)\n&gt;0.75 6141.96 (432.66)\n&gt;1.00 6329.79 (450.72)\n\n\nText(0.5, 0, 'Subsample')\n\n\n\n\n\n\n\n\n\n\n\n11.2.5 Tuning Gradient boosting for regression\nAs the optimal value of the parameters depend on each other, we need to optimize them simultaneously.\n\nstart_time = time.time()\nmodel = GradientBoostingRegressor(random_state=1,loss='huber')\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['max_depth'] = [3,5,8,10,12,15]\n\n# define the evaluation procedure\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='neg_mean_squared_error',\n                          verbose = True)\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (np.sqrt(-grid_result.best_score_), grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n#for mean, stdev, param in zip(means, stds, params):\n#    print(\"%f (%f) with: %r\" % (mean, stdev, param)\nprint(\"Time taken = \",(time.time()-start_time)/60,\" minutes\")\n\nBest: 5190.765919 using {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 100}\nTime taken =  46.925597019990285  minutes\n\n\nNote that the code takes 46 minutes to run. In case of a lot of hyperparameters, RandomizedSearchCV may be preferred to trade-off between optimality of the solution and computational cost.\n\n#Model based on the optimal parameters\nmodel = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n                         random_state=1,loss='huber').fit(X,y)\n\n\n#RMSE of the optimized model on test data\nprint(\"Gradient boost RMSE = \",np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nGradient boost RMSE =  5405.787029062213\n\n\n\n#Let us combine the Gradient boost model with other models\nmodel2 = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,learning_rate=1.0,\n                         random_state=1).fit(X,y)\nprint(\"AdaBoost RMSE = \",np.sqrt(mean_squared_error(model2.predict(Xtest),ytest)))\nmodel3 = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\nprint(\"Random Forest RMSE = \",np.sqrt(mean_squared_error(model3.predict(Xtest),ytest)))\n\nAdaBoost RMSE =  5693.165811600585\nRandom Forest RMSE =  5642.45839697972\n\n\n\n#Ensemble model\npred1=model.predict(Xtest)#Gradient boost\npred2=model2.predict(Xtest)#Adaboost\npred3=model3.predict(Xtest)#Random forest\npred = 0.34*pred1+0.33*pred2+0.33*pred3 #Higher weight to the better model\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5364.478227748279\n\n\n\n\n11.2.6 Ensemble modeling (for regression models)\n\n#Ensemble model\npred1=model.predict(Xtest)#Gradient boost\npred2=model2.predict(Xtest)#Adaboost\npred3=model3.predict(Xtest)#Random forest\npred = 0.6*pred1+0.2*pred2+0.2*pred3 #Higher weight to the better model\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5323.119083375402\n\n\nCombined, the random forest model, gradient boost and the Adaboost model do better than each of the individual models.\nNote that ideally we should do K-fold cross validation to figure out the optimal weights. We’ll learn about ensembling techniques later in the course.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html#gradient-boosting-for-classification",
    "href": "Lec8_Gradient_Boosting.html#gradient-boosting-for-classification",
    "title": "11  Gradient Boosting",
    "section": "11.3 Gradient boosting for classification",
    "text": "11.3 Gradient boosting for classification\nBelow is the Gradient boost implementation on a classification problem. The takeaways are the same as that of the regression problem above.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n11.3.1 Number of trees vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [10, 50, 100, 500, 1000, 5000]\n    for n in n_trees:\n        models[str(n)] = GradientBoostingClassifier(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;10 0.738 (0.031)\n&gt;50 0.748 (0.054)\n&gt;100 0.722 (0.075)\n&gt;500 0.707 (0.066)\n&gt;1000 0.712 (0.075)\n&gt;5000 0.697 (0.061)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n11.3.2 Depth of each tree vs cross validation accuracy\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define ensemble model\n        models[str(i)] = GradientBoostingClassifier(random_state=1,max_depth=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 0.746 (0.040)\n&gt;2 0.744 (0.046)\n&gt;3 0.722 (0.075)\n&gt;4 0.743 (0.049)\n&gt;5 0.738 (0.046)\n&gt;6 0.741 (0.047)\n&gt;7 0.735 (0.057)\n&gt;8 0.736 (0.051)\n&gt;9 0.728 (0.055)\n&gt;10 0.710 (0.050)\n&gt;11 0.697 (0.061)\n&gt;12 0.681 (0.056)\n&gt;13 0.709 (0.047)\n&gt;14 0.702 (0.048)\n&gt;15 0.705 (0.048)\n&gt;16 0.700 (0.042)\n&gt;17 0.699 (0.048)\n&gt;18 0.697 (0.050)\n&gt;19 0.696 (0.042)\n&gt;20 0.697 (0.048)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n11.3.3 Learning rate vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = GradientBoostingClassifier(learning_rate=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.1 0.747 (0.044)\n&gt;0.2 0.736 (0.028)\n&gt;0.3 0.726 (0.039)\n&gt;0.4 0.730 (0.034)\n&gt;0.5 0.726 (0.041)\n&gt;0.6 0.722 (0.043)\n&gt;0.7 0.717 (0.050)\n&gt;0.8 0.713 (0.033)\n&gt;0.9 0.694 (0.045)\n&gt;1.0 0.695 (0.032)\n&gt;1.1 0.718 (0.034)\n&gt;1.2 0.692 (0.045)\n&gt;1.3 0.708 (0.042)\n&gt;1.4 0.704 (0.050)\n&gt;1.5 0.702 (0.028)\n&gt;1.6 0.700 (0.050)\n&gt;1.7 0.694 (0.044)\n&gt;1.8 0.650 (0.075)\n&gt;1.9 0.551 (0.163)\n&gt;2.0 0.484 (0.123)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n11.3.4 Tuning Gradient boosting Classifier\n\nstart_time = time.time()\nmodel = GradientBoostingClassifier(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['max_depth'] = [1,2,3,4,5]\ngrid['subsample'] = [0.5,1.0]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, verbose = True, scoring = 'recall')\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nprint(\"Time taken = \", time.time() - start_time, \"seconds\")\n\nFitting 5 folds for each of 250 candidates, totalling 1250 fits\nBest: 0.701045 using {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}\nTime taken =  32.46394085884094\n\n\n\n#Model based on the optimal parameters\nmodel = GradientBoostingClassifier(random_state=1,max_depth=3,learning_rate=0.1,subsample=0.5,\n                          n_estimators=200).fit(X,y)\n\n# Note that we are using the cross-validated predicted probabilities, instead of directly using the \n# predicted probabilities on train data, as the model may be overfitting on the train data, and \n# may lead to misleading results\ncross_val_ypred = cross_val_predict(GradientBoostingClassifier(random_state=1,max_depth=3,\n                                                               learning_rate=0.1,subsample=0.5,\n                          n_estimators=200), X, y, cv = 5, method = 'predict_proba')\n\np, r, thresholds = precision_recall_curve(y, cross_val_ypred[:,1])\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\n\n# Thresholds with precision and recall\nall_thresholds = np.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,2]&gt;0.8,:]\n# As the values in 'recall_more_than_80' are arranged in decreasing order of recall and increasing threshold,\n# the last value will provide the maximum threshold probability for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the maximum possible precision\nrecall_more_than_80[recall_more_than_80.shape[0]-1]\n\narray([0.18497144, 0.53205128, 0.80193237])\n\n\n\n#Optimal decision threshold probability\nthres = recall_more_than_80[recall_more_than_80.shape[0]-1][0]\nthres\n\n0.18497143500912738\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = thres\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  77.92207792207793\nROC-AUC:  0.8704389212057112\nPrecision:  0.6626506024096386\nRecall:  0.9016393442622951\n\n\n\n\n\n\n\n\n\nThe model seems to be similar to the Adaboost model. However, gradient boosting algorithms with robust loss functions can perform better than Adaboost in the presence of outliers (in terms of response) in the data.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html",
    "href": "Lec9_XGBoost.html",
    "title": "12  XGBoost",
    "section": "",
    "text": "12.1 Hyperparameters\nThe following are some of the important hyperparameters to tune in XGBoost:\nHowever, there are other hyperparameters that can be tuned as well. Check out the list of all hyperparameters in the XGBoost documentation.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\nfrom sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\nrecall_score, precision_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.ensemble import VotingRegressor, VotingClassifier, StackingRegressor, StackingClassifier, GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.linear_model import LinearRegression,LogisticRegression, LassoCV, RidgeCV, ElasticNetCV\nfrom sklearn.neighbors import KNeighborsRegressor\nimport itertools as it\nimport time as time\nimport xgboost as xgb\nfrom pyearth import Earth\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\nX = train[['mileage','mpg','year','engineSize']]\nXtest = test[['mileage','mpg','year','engineSize']]\ny = train['price']\nytest = test['price']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html#hyperparameters",
    "href": "Lec9_XGBoost.html#hyperparameters",
    "title": "12  XGBoost",
    "section": "",
    "text": "Number of trees (n_estimators)\nDepth of each tree (max_depth)\nLearning rate (learning_rate)\nSampling observations / predictors (subsample for observations, colsample_bytree for predictors)\nRegularization parameters (reg_lambda & gamma)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html#xgboost-for-regression",
    "href": "Lec9_XGBoost.html#xgboost-for-regression",
    "title": "12  XGBoost",
    "section": "12.2 XGBoost for regression",
    "text": "12.2 XGBoost for regression\n\n12.2.1 Number of trees vs cross validation error\nAs the number of trees increase, the prediction bias will decrease, and the prediction variance will increase. Thus, there will be an optimal number of trees that minimize the prediction error.\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [5, 10, 50, 100, 500, 1000, 2000, 5000]\n    for n in n_trees:\n        models[str(n)] = xgb.XGBRegressor(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;5 7961.485 (192.906)\n&gt;10 5837.134 (217.986)\n&gt;50 5424.788 (263.890)\n&gt;100 5465.396 (237.938)\n&gt;500 5608.350 (235.903)\n&gt;1000 5635.159 (236.664)\n&gt;2000 5642.669 (236.192)\n&gt;5000 5643.411 (236.074)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n12.2.2 Depth of tree vs cross validation error\nAs the depth of each weak learner (decision tree) increases, the complexity of the weak learner will increase. As the complexity increases, the prediction bias will decrease, while the prediction variance will increase. Thus, there will be an optimal depth of each weak learner that minimizes the prediction error.\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define ensemble model\n        models[str(i)] = xgb.XGBRegressor(random_state=1,max_depth=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 7541.827 (545.951)\n&gt;2 6129.425 (393.357)\n&gt;3 5647.783 (454.318)\n&gt;4 5438.481 (453.726)\n&gt;5 5358.074 (379.431)\n&gt;6 5281.675 (383.848)\n&gt;7 5495.163 (459.356)\n&gt;8 5399.145 (380.437)\n&gt;9 5469.563 (384.004)\n&gt;10 5461.549 (416.630)\n&gt;11 5443.210 (432.863)\n&gt;12 5546.447 (412.097)\n&gt;13 5532.414 (369.131)\n&gt;14 5556.761 (362.746)\n&gt;15 5540.366 (452.612)\n&gt;16 5586.004 (451.199)\n&gt;17 5563.137 (464.344)\n&gt;18 5594.919 (480.221)\n&gt;19 5641.226 (451.713)\n&gt;20 5616.462 (417.405)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n12.2.3 Learning rate vs cross validation error\nThe optimal learning rate will depend on the number of trees, and vice-versa. If the learning rate is too low, it will take several trees to “learn” the response. If the learning rate is high, the response will be “learned” quickly (with fewer) trees. Learning too quickly will be prone to overfitting, while learning too slowly will be computationally expensive. Thus, there will be an optimal learning rate to minimize the prediction error.\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in [0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.8,1.0]:\n        key = '%.4f' % i\n        models[key] = xgb.XGBRegressor(learning_rate=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.0100 12223.8 (636.7)\n&gt;0.0500 5298.5 (383.5)\n&gt;0.1000 5236.3 (397.5)\n&gt;0.2000 5221.5 (347.5)\n&gt;0.3000 5281.7 (383.8)\n&gt;0.4000 5434.1 (364.6)\n&gt;0.5000 5537.0 (471.9)\n&gt;0.6000 5767.4 (478.5)\n&gt;0.8000 6132.7 (472.5)\n&gt;1.0000 6593.6 (408.9)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n12.2.4 Regularization (reg_lambda) vs cross validation error\nThe parameter reg_lambda penalizes the L2 norm of the leaf scores. For example, in case of classification, it will penalize the summation of the square of log odds of the predicted probability. This penalization will tend to reduce the log odds, thereby reducing the tendency to overfit. “Reducing the log odds” in layman terms will mean not being overly sure about the prediction.\nWithout regularization, the algorithm will be closer to the gradient boosting algorithm. Regularization may provide some additional boost to prediction accuracy by reducing over-fitting. In the example below, regularization with reg_lambda=1 turns out to be better than no regularization (reg_lambda=0)*. Of course, too much regularization may increase bias so much such that it leads to a decrease in prediction accuracy.\n\ndef get_models():\n    models = dict()\n    # explore 'reg_lambda' from 0.1 to 2 in 0.1 increments\n    for i in [0,0.5,1.0,1.5,2,10,100]:\n        key = '%.4f' % i\n        models[key] = xgb.XGBRegressor(reg_lambda=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('reg_lambda',fontsize=15)\n\n&gt;0.0000 5359.2 (317.0)\n&gt;0.5000 5382.7 (363.1)\n&gt;1.0000 5281.7 (383.8)\n&gt;1.5000 5348.0 (383.9)\n&gt;2.0000 5336.4 (426.6)\n&gt;10.0000 5410.9 (521.9)\n&gt;100.0000 5801.1 (563.7)\n\n\nText(0.5, 0, 'reg_lambda')\n\n\n\n\n\n\n\n\n\n\n\n12.2.5 Regularization (gamma) vs cross validation error\nThe parameter gamma penalizes the tree based on the number of leaves. This is similar to the parameter alpha of cost complexity pruning. As gamma increases, more leaves will be pruned. Note that the previous parameter reg_lambda penalizes the leaf score, but does not prune the tree.\nWithout regularization, the algorithm will be closer to the gradient boosting algorithm. Regularization may provide some additional boost to prediction accuracy by reducing over-fitting. However, in the example below, no regularization (in terms of gamma=0) turns out to be better than a non-zero regularization. (reg_lambda=0).\n\ndef get_models():\n    models = dict()\n    # explore gamma from 0.1 to 2 in 0.1 increments\n    for i in [0,10,1e2,1e3,1e4,1e5,1e6,1e7,1e8,1e9]:\n        key = '%.4f' % i\n        models[key] = xgb.XGBRegressor(gamma=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('gamma',fontsize=15)\n\n&gt;0.0000 5281.7 (383.8)\n&gt;10.0000 5281.7 (383.8)\n&gt;100.0000 5281.7 (383.8)\n&gt;1000.0000 5291.8 (381.8)\n&gt;10000.0000 5295.7 (370.2)\n&gt;100000.0000 5293.0 (402.5)\n&gt;1000000.0000 5322.2 (368.9)\n&gt;10000000.0000 5273.7 (409.8)\n&gt;100000000.0000 5362.1 (407.8)\n&gt;1000000000.0000 5932.3 (397.6)\n\n\nText(0.5, 0, 'gamma')\n\n\n\n\n\n\n\n\n\n\n\n12.2.6 Tuning XGboost regressor\nAlong with max_depth, learning_rate, and n_estimators, here we tune reg_lambda - the regularization parameter for penalizing the tree predictions.\n\n#K-fold cross validation to find optimal parameters for XGBoost\nstart_time = time.time()\nparam_grid = {'max_depth': [4,6,8],\n              'learning_rate': [0.01, 0.05, 0.1],\n               'reg_lambda':[0, 1, 10],\n                'n_estimators':[100, 500, 1000],\n                'gamma': [0, 10, 100],\n                'subsample': [0.5, 0.75, 1.0],\n                'colsample_bytree': [0.5, 0.75, 1.0]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = RandomizedSearchCV(estimator=xgb.XGBRegressor(random_state=1),                                                       \n                             param_distributions = param_grid, n_iter = 200,\n                             verbose = 1,\n                             n_jobs=-1,\n                             cv = cv)\noptimal_params.fit(X,y)\nprint(\"Optimal parameter values =\", optimal_params.best_params_)\nprint(\"Optimal cross validation R-squared = \",optimal_params.best_score_)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\nOptimal parameter values = {'subsample': 0.75, 'reg_lambda': 1, 'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.01, 'gamma': 100, 'colsample_bytree': 1.0}\nOptimal cross validation R-squared =  0.9002580404500382\nTime taken =  4  minutes\n\n\n\n#RMSE based on the optimal parameter values\nnp.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))\n\n5497.553788113875\n\n\n\n\n12.2.7 Early stopping with XGBoost\nIf we have a test dataset (or we can further split the train data into a smaller train and test data), we can use it with the early_stopping_rounds argument of XGBoost, where it will stop growing trees once the model accuracy fails to increase for a certain number of consecutive iterations, given as early_stopping_rounds.\n\nX_train_sub, X_test_sub, y_train_sub, y_test_sub = \\\ntrain_test_split(X, y, test_size = 0.2, random_state = 45)\n\n\nmodel = xgb.XGBRegressor(random_state = 1, max_depth = 8, learning_rate = 0.01,\n                        n_estimators = 20000,reg_lambda = 1, gamma = 100, subsample = 0.75, colsample_bytree = 1.0)\nmodel.fit(X_train_sub, y_train_sub, eval_set = ([(X_test_sub, y_test_sub)]), early_stopping_rounds = 250)\n\nThe results of the code are truncated to save space. A snapshot of the beginning and end of the results is below. The algorithm keeps adding trees to the model until the RMSE ceases to decrease for 10 consecutive iterations.\n\n\n\n\n\n\nprint(\"XGBoost RMSE = \",np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nXGBoost RMSE =  5508.787454011525\n\n\nLet us further reduce the learning rate to 0.001 and see if the accuracy increases further on the test data. We’ll use the early_stopping_rounds argument to stop growing trees once the accuracy fails to increase for 250 consecutive iterations.\n\nmodel = xgb.XGBRegressor(random_state = 1, max_depth = 8, learning_rate = 0.001,\n                        n_estimators = 20000,reg_lambda = 1, gamma = 100, subsample = 0.75, colsample_bytree = 1.0)\nmodel.fit(X_train_sub, y_train_sub, eval_set = ([(X_test_sub, y_test_sub)]), early_stopping_rounds = 250)\n\n\n\n\n\n\n\nprint(\"XGBoost RMSE = \",np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nXGBoost RMSE =  5483.518711988693\n\n\nNote that the accuracy on this test data has further increased with a lower learning rate.\n#Let us combine the XGBoost model with other tuned models from earlier chapters.\n\n#Tuned AdaBoost model from Section 7.2.4\nmodel_ada = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,learning_rate=1.0,\n                         random_state=1).fit(X,y)\nprint(\"AdaBoost RMSE = \", np.sqrt(mean_squared_error(model_ada.predict(Xtest),ytest)))\n\n#Tuned Random forest model from Section 6.1.2\nmodel_rf = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\nprint(\"Random Forest RMSE = \",np.sqrt(mean_squared_error(model_rf.predict(Xtest),ytest)))\n\n#Tuned gradient boosting model from Section 8.2.5\nmodel_gb = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n                         random_state=1,loss='huber').fit(X,y)\nprint(\"Gradient boost RMSE = \",np.sqrt(mean_squared_error(model_gb.predict(Xtest),ytest)))\n\nAdaBoost RMSE =  5693.165811600585\nRandom Forest RMSE =  5642.45839697972\nGradient boost RMSE =  5405.787029062213\n\n\n\n#Ensemble model\npred_xgb = model.predict(Xtest)    #XGBoost\npred_ada = model_ada.predict(Xtest)#AdaBoost\npred_rf = model_rf.predict(Xtest)  #Random Forest\npred_gb = model_gb.predict(Xtest)  #Gradient boost\npred = 0.25*pred_xgb + 0.25*pred_ada + 0.25*pred_rf + 0.25*pred_gb #Option 1 - All models are equally weighted\n#pred = 0.15*pred1+0.15*pred2+0.15*pred3+0.55*pred4 #Option 2 - Higher weight to the better model\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5352.145010078119\n\n\nCombined, the random forest model, gradient boost, XGBoost and the Adaboost model do better than each of the individual models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html#xgboost-for-classification",
    "href": "Lec9_XGBoost.html#xgboost-for-classification",
    "title": "12  XGBoost",
    "section": "12.3 XGBoost for classification",
    "text": "12.3 XGBoost for classification\n\ndata = pd.read_csv('./Datasets/Heart.csv')\ndata.dropna(inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\n\nAge\nSex\nChestPain\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nThal\nAHD\n\n\n\n\n0\n63\n1\ntypical\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\nfixed\nNo\n\n\n1\n67\n1\nasymptomatic\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\nnormal\nYes\n\n\n2\n67\n1\nasymptomatic\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\nreversable\nYes\n\n\n3\n37\n1\nnonanginal\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\nnormal\nNo\n\n\n4\n41\n0\nnontypical\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\nnormal\nNo\n\n\n\n\n\n\n\n\n\n#Response variable\ny = pd.get_dummies(data['AHD'])['Yes']\n\n#Creating a dataframe for predictors with dummy varibles replacing the categorical variables\nX = data.drop(columns = ['AHD','ChestPain','Thal'])\nX = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\nX.head()\n\n\n\n\n\n\n\n\n\nAge\nSex\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nasymptomatic\nnonanginal\nnontypical\ntypical\nfixed\nnormal\nreversable\n\n\n\n\n0\n63\n1\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\n0\n0\n0\n1\n1\n0\n0\n\n\n1\n67\n1\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\n1\n0\n0\n0\n0\n1\n0\n\n\n2\n67\n1\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\n1\n0\n0\n0\n0\n0\n1\n\n\n3\n37\n1\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\n0\n1\n0\n0\n0\n1\n0\n\n\n4\n41\n0\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\n#Creating train and test datasets\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)\n\nXGBoost has an additional parameter for classification: scale_pos_weight\nGradients are used as the basis for fitting subsequent trees added to boost or correct errors made by the existing state of the ensemble of decision trees.\nThe scale_pos_weight value is used to scale the gradient for the positive class.\nThis has the effect of scaling errors made by the model during training on the positive class and encourages the model to over-correct them. In turn, this can help the model achieve better performance when making predictions on the positive class. Pushed too far, it may result in the model overfitting the positive class at the cost of worse performance on the negative class or both classes.\nAs such, the scale_pos_weight can be used to train a class-weighted or cost-sensitive version of XGBoost for imbalanced classification.\nA sensible default value to set for the scale_pos_weight hyperparameter is the inverse of the class distribution. For example, for a dataset with a 1 to 100 ratio for examples in the minority to majority classes, the scale_pos_weight can be set to 100. This will give classification errors made by the model on the minority class (positive class) 100 times more impact, and in turn, 100 times more correction than errors made on the majority class.\nRef:https://machinelearningmastery.com/xgboost-for-imbalanced-classification/#:~:text=The%20scale_pos_weight%20value%20is%20used,model%20to%20over%2Dcorrect%20them.\n\nstart_time = time.time()\nparam_grid = {'n_estimators':[25,100,500],\n                'max_depth': [6,7,8],\n              'learning_rate': [0.01,0.1,0.2],\n               'gamma': [0.1,0.25,0.5],\n               'reg_lambda':[0,0.01,0.001],\n                'scale_pos_weight':[1.25,1.5,1.75]#Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances).\n             }\n\ncv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = GridSearchCV(estimator=xgb.XGBClassifier(objective = 'binary:logistic',random_state=1,\n                                                         use_label_encoder=False),\n                             param_grid = param_grid,\n                             scoring = 'accuracy',\n                             verbose = 1,\n                             n_jobs=-1,\n                             cv = cv)\noptimal_params.fit(Xtrain,ytrain)\nprint(optimal_params.best_params_,optimal_params.best_score_)\nprint(\"Time taken = \", (time.time()-start_time)/60, \" minutes\")\n\nFitting 5 folds for each of 729 candidates, totalling 3645 fits\n[22:00:02] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n{'gamma': 0.25, 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 25, 'reg_lambda': 0.01, 'scale_pos_weight': 1.5} 0.872183908045977\n\n\n\ncv_results=pd.DataFrame(optimal_params.cv_results_)\ncv_results.sort_values(by = 'mean_test_score',ascending=False)[0:5]\n\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_gamma\nparam_learning_rate\nparam_max_depth\nparam_n_estimators\nparam_reg_lambda\nparam_scale_pos_weight\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n409\n0.111135\n0.017064\n0.005629\n0.000737\n0.25\n0.2\n6\n25\n0.01\n1.5\n{'gamma': 0.25, 'learning_rate': 0.2, 'max_dep...\n0.866667\n0.766667\n0.9\n0.931034\n0.896552\n0.872184\n0.05656\n1\n\n\n226\n0.215781\n0.007873\n0.005534\n0.001615\n0.1\n0.2\n8\n100\n0\n1.5\n{'gamma': 0.1, 'learning_rate': 0.2, 'max_dept...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n290\n1.391273\n0.107808\n0.007723\n0.006286\n0.25\n0.01\n7\n500\n0\n1.75\n{'gamma': 0.25, 'learning_rate': 0.01, 'max_de...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n266\n1.247463\n0.053597\n0.006830\n0.002728\n0.25\n0.01\n6\n500\n0.01\n1.75\n{'gamma': 0.25, 'learning_rate': 0.01, 'max_de...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n269\n1.394361\n0.087307\n0.005530\n0.001718\n0.25\n0.01\n6\n500\n0.001\n1.75\n{'gamma': 0.25, 'learning_rate': 0.01, 'max_de...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n\n\n\n\n\n\n\n#Function to compute confusion matrix and prediction accuracy on test/train data\ndef confusion_matrix_data(data,actual_values,model,cutoff=0.5):\n#Predict the values using the Logit model\n    pred_values = model.predict_proba(data)[:,1]\n# Specify the bins\n    bins=np.array([0,cutoff,1])\n#Confusion matrix\n    cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n    cm_df = pd.DataFrame(cm)\n    cm_df.columns = ['Predicted 0','Predicted 1']\n    cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n# Calculate the accuracy\n    accuracy = 100*(cm[0,0]+cm[1,1])/cm.sum()\n    fnr = 100*(cm[1,0])/(cm[1,0]+cm[1,1])\n    precision = 100*(cm[1,1])/(cm[0,1]+cm[1,1])\n    fpr = 100*(cm[0,1])/(cm[0,0]+cm[0,1])\n    tpr = 100*(cm[1,1])/(cm[1,0]+cm[1,1])\n    print(\"Accuracy = \", accuracy)\n    print(\"Precision = \", precision)\n    print(\"FNR = \", fnr)\n    print(\"FPR = \", fpr)\n    print(\"TPR or Recall = \", tpr)\n    print(\"Confusion matrix = \\n\", cm_df)\n    return (\" \")\n\n\nmodel4 = xgb.XGBClassifier(objective = 'binary:logistic',random_state=1,gamma=0.25,learning_rate = 0.01,max_depth=6,\n                              n_estimators = 500,reg_lambda = 0.01,scale_pos_weight=1.75)\nmodel4.fit(Xtrain,ytrain)\nmodel4.score(Xtest,ytest)\n\n0.7718120805369127\n\n\n\n#Computing the accuracy\ny_pred = model4.predict(Xtest)\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\ny_pred_prob = model4.predict_proba(Xtest)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  77.18120805369128\nROC-AUC:  0.8815070986530761\nPrecision:  0.726027397260274\nRecall:  0.7910447761194029\n\n\n\n\n\n\n\n\n\nIf we increase the value of scale_pos_weight, the model will focus on classifying positives more correctly. This will increase the recall (true positive rate) since the focus is on identifying all positives. However, this will lead to identifying positives aggressively, and observations ‘similar’ to observations of the positive class will also be predicted as positive resulting in an increase in false positives and a decrease in precision. See the trend below as we increase the value of scale_pos_weight.\n\n12.3.1 Precision & recall vs scale_pos_weight\n\ndef get_models():\n    models = dict()\n    # explore 'scale_pos_weight' from 0.1 to 2 in 0.1 increments\n    for i in [0,1,10,1e2,1e3,1e4,1e5,1e6,1e7,1e8,1e9]:\n        key = '%.0f' % i\n        models[key] = xgb.XGBClassifier(objective = 'binary:logistic',scale_pos_weight=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores_recall = cross_val_score(model, X, y, scoring='recall', cv=cv, n_jobs=-1)\n    scores_precision = cross_val_score(model, X, y, scoring='precision', cv=cv, n_jobs=-1)\n    return list([scores_recall,scores_precision])\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults_recall, results_precision, names = list(), list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    scores_recall = scores[0]\n    scores_precision = scores[1]\n    # store the results\n    results_recall.append(scores_recall)\n    results_precision.append(scores_precision)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.2f (%.2f)' % (name, np.mean(scores_recall), np.std(scores_recall)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nsns.set(font_scale = 1.5)\npdata = pd.DataFrame(results_precision)\npdata.columns = list(['p1','p2','p3','p4','p5'])\npdata['metric'] = 'precision'\nrdata = pd.DataFrame(results_recall)\nrdata.columns = list(['p1','p2','p3','p4','p5'])\nrdata['metric'] = 'recall'\npr_data = pd.concat([pdata,rdata])\npr_data.reset_index(drop=False,inplace= True)\n#sns.boxplot(x=\"day\", y=\"total_bill\", hue=\"time\",pr_data=tips, linewidth=2.5)\npr_data_melt=pr_data.melt(id_vars = ['index','metric'])\npr_data_melt['index']=pr_data_melt['index']-1\npr_data_melt['index'] = pr_data_melt['index'].astype('str')\npr_data_melt.replace(to_replace='-1',value =  '-inf',inplace=True)\nsns.boxplot(x='index', y=\"value\", hue=\"metric\", data=pr_data_melt, linewidth=2.5)\nplt.xlabel('$log_{10}$(scale_pos_weight)',fontsize=15)\nplt.ylabel('Precision / Recall ',fontsize=15)\nplt.legend(loc=\"lower right\", frameon=True, fontsize=15)\n\n&gt;0 0.00 (0.00)\n&gt;1 0.77 (0.13)\n&gt;10 0.81 (0.09)\n&gt;100 0.85 (0.11)\n&gt;1000 0.85 (0.10)\n&gt;10000 0.90 (0.06)\n&gt;100000 0.90 (0.08)\n&gt;1000000 0.90 (0.06)\n&gt;10000000 0.91 (0.10)\n&gt;100000000 0.96 (0.03)\n&gt;1000000000 1.00 (0.00)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#instructions",
    "href": "Assignment1.html#instructions",
    "title": "Assignment 1",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Thursday, 11th April 2024 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (2 points). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#bias-variance-trade-off-for-regression-32-points",
    "href": "Assignment1.html#bias-variance-trade-off-for-regression-32-points",
    "title": "Assignment 1",
    "section": "1) Bias-Variance Trade-off for Regression (32 points)",
    "text": "1) Bias-Variance Trade-off for Regression (32 points)\nThe main goal of this question is to understand and visualize the bias-variance trade-off in a regression model by performing repetitive simulations.\nThe conceptual clarity about bias and variance will help with the main logic behind creating many models that will come up later in the course.\n\na)\nFirst, you need to implement the underlying function of the population you want to sample data from. Assume that the function is the Bukin function. Implement it as a user-defined function and run it with the test cases below to make sure it is implemented correctly. (3 points)\nNote: It would be more useful to have only one input to the function. You can treat the input as an array of two elements.\n\nprint(Bukin(np.array([1,2]))) # The output should be 141.177\nprint(Bukin(np.array([6,-4]))) # The output should be 208.966\nprint(Bukin(np.array([0,1]))) # The output should be 100.1\n\n\n\nb)\nUsing the following assumptions, sample a test dataset with 100 observations from the underlying function. Remember how the test dataset is supposed to be sampled for bias-variance calculations. No loops are allowed for this question - .apply should be very useful and actually simpler to use. (4 points)\nAssumptions:\n\nThe first predictor, \\(x_1\\), comes from a uniform distribution between -15 and -5. (\\(U[-15, -5]\\))\nThe second predictor, \\(x_2\\), comes from a uniform distribution between -3 and 3. (\\(U[-3, 3]\\))\nUse np.random.seed(100) for reproducibility.\n\n\n\nc)\nCreate an empty DataFrame with columns named degree, bias_sq and var. This will be useful to store the analysis results in this question. (1 point)\n\n\nd)\nSample 100 training datasets to calculate the bias and the variance of a Linear Regression model that predicts data coming from the underlying Bukin function. You need to repeat this process with polynomial transformations from degree 1 (which is the original predictors) to degree 7. For each degree, store the degree, bias-squared and variance values in the DataFrame. (15 points)\nNote:\n\nFor a linear regression model, bias refers to squared bias\nAssume that the noise in the population is a zero-mean Gaussian with a standard deviation of 10. (\\(N(0,10)\\))\nKeep the training data size the same as the test data size.\nYou need both the interactions and the higher-order transformations in your polynomial predictors.\nFor \\(i^{th}\\) training dataset, you can consider using np.random.seed(i) for reproducibility.\n\n\n\ne)\nUsing the results stored in the DataFrame, plot the (1) expected mean squared error, (2) expected squared bias, (3) expected variance, and (4) the expected sum of squared bias, variance and noise variance (i.e., summation of 2, 3, and noise variance), against the degree of the predictors in the model. (5 points)\nMake sure you add a legend to label the four lineplots. (1 point)\n\n\nf)\nWhat is the degree of the optimal model? (1 point) What are the squared bias, variance and mean squared error for that degree? (2 points)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#low-bias-low-variance-model-via-regularization-25-points",
    "href": "Assignment1.html#low-bias-low-variance-model-via-regularization-25-points",
    "title": "Assignment 1",
    "section": "2) Low-Bias-Low-Variance Model via Regularization (25 points)",
    "text": "2) Low-Bias-Low-Variance Model via Regularization (25 points)\nThe main goal of this question is to further reduce the total error by regularization - in other words, to implement the low-bias-low-variance model for the underlying function and the data coming from it.\n\na)\nFirst of all, explain why it is not guaranteed for the optimal model (with the optimal degree) in Question 1 to be the low-bias-low-variance model. (2 points) Why would regularization be necessary to achieve that model? (2 points)\n\n\nb)\nBefore repeating the process in Question 1, you should see from the figure in 1e and the results in 1f that there is no point in trying some degrees again with regularization. Find out these degrees and explain why you should not use them for this question, considering how regularization affects the bias and the variance of a model. (3 points)\n\n\nc)\nRepeat 1c and 1d with Ridge regularization. Exclude the degrees you found in 2b and also degree 7. Use Leave-One-Out (LOO) cross-validation (CV) to tune the model hyperparameter and use neg_root_mean_squared_error as the scoring metric. (7 points)\nConsider hyperparamter values in the range [1, 100].\n\n\nd)\nRepeat part 1e with Ridge regularization, using the results from 2c. (2 points)\n\n\ne)\nWhat is the degree of the optimal Ridge Regression model? (1 point) What are the bias-squared, variance and total error values for that degree? (1 point) How do they compare to the Linear Regression model results? (2 points)\n\n\nf)\nIs the regularization successful in reducing the total error of the regression model? (2 points) Explain the results in 2e in terms of how bias and variance change with regularization. (3 points)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#bias-variance-trade-off-for-classification-38-points",
    "href": "Assignment1.html#bias-variance-trade-off-for-classification-38-points",
    "title": "Assignment 1",
    "section": "3) Bias-Variance Trade-off for Classification (38 points)",
    "text": "3) Bias-Variance Trade-off for Classification (38 points)\nNow, it is time to understand and visualize the bias-variance trade-off in a classification model. As we covered in class, the error calculations for classification are different than regression, so it is necessary to understand the bias-variance analysis for classification as well.\nFirst of all, you need to visualize the underlying boundary between the classes in the population. Run the given code that implements the following:\n\n2000 test observations are sampled from a population with two predictors.\nEach predictor is uniformly distributed between -15 and 15. (\\(U[-15, 15]\\))\nThe underlying boundary between the classes is a circle with radius 10.\nThe noise in the population is a 30% chance that the observation is misclassified.\n\n\n# Number of observations\nn = 2000\n\nnp.random.seed(111)\n\n# Test predictors\nx1 = np.random.uniform(-15, 15, n)\nx2 = np.random.uniform(-15, 15, n)\nX_test = pd.DataFrame({'x1': x1, 'x2': x2})\n\n# Underlying boundary\nboundary = (x1**2) + (x2**2)\n\n# Test response (no noise!)\ny_test_wo_noise = (boundary &lt; 100).astype(int)\n\n# Test response with noise (for comparison)\nnoise_prob = 0.3\nnum_noisy_obs = int(noise_prob*n)\n\ny_test_w_noise = y_test_wo_noise.copy()\nnoise_indices = np.random.choice(range(len(y_test_w_noise)), num_noisy_obs, replace = False)\ny_test_w_noise[noise_indices] = 1 - y_test_wo_noise[noise_indices]\n\n\nsns.scatterplot(x = x1, y = x2, hue=y_test_wo_noise)\nplt.title('Sample without the noise')\nplt.show()\n\n\n\n\n\n\n\nsns.scatterplot(x = x1, y = x2, hue=y_test_w_noise)\nplt.title('Sample with the noise')\nplt.show()\n\n\n\n\n\n\n\na)\nCreate an empty DataFrame with columns named K, bias, var and noise. This will be useful to store the analysis results in this question. (1 point)\n\n\nb)\nSample 100 training datasets to calculate the bias and the variance of a K-Nearest Neighbors (KNN) Classifier that predicts data coming from the population with the circular underlying boundary. You need to repeat this process with a K value from 10 to 150, with a stepsize of 10. For each K, store the following values in the DataFrame:\n\nK,\nbias,\nvariance,\nexpected loss computed directly using the true response and predictions,\nexpected loss computed as (expected Bias) + (\\(c_2\\) expected variance) + (\\(c_1\\) expected noise)\n\n(20 points)\nNote:\n\nKeep the training data size the same as the test data size.\nThe given code should help you both with sampling the training data and adding noise to the training responses.\nFor \\(i^{th}\\) training dataset, you can consider using np.random.seed(i) for reproducibility.\nTo check the progress of the code while running, a simple but efficient method is to add a print(K) line in the loop.\n\n\n\nc)\nUsing the results stored in the DataFrame, plot the bias and the variance against the K value on one figure, and the expected loss (computed directly) & expected loss computed as (expected Bias) + (\\(c_2\\)expected variance) + (\\(c_1\\)expected noise) against the K value on a separate figure. (5 points) Make sure you add a legend to label the lineplots in the first figure. (1 point)\n\n\nd)\nWhat is the K of the optimal model? (1 point) What are the bias, variance and expected loss (computed either way) for that K? (2 points)\n\n\ne)\nIn part c, you should see the variance leveling off after a certain K value. Explain why this is the case, considering the effect of the K value on a KNN model. (2 points)\n\n\nf)\nLastly, visualize the decision boundary of a KNN Classifier with high-bias-low-variance (option 1) and low-bias-high-variance (option 2), using data from the same population.\n\nFor each option, pick a K value (1 and 90 would be good numbers.) You are expected to know which number belongs to which option.\nSample a training dataset. (Use np.random.seed(1).)\nUsing the training dataset, train a KNN model with the K value you picked.\nThe rest of the code is given below for you.\n\nNote that you need to produce two figures. (2x2 = 4 points) Put titles on the figures to describe which figure is which option. (2 points)\n\n# Develop and save the model as the 'model' object before using the code\nxx, yy = np.meshgrid(np.linspace(-15, 15, 100), np.linspace(-15, 15, 100))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nsns.scatterplot(x = x1, y = x2, hue=y_train, legend=False);\nplt.contour(xx, yy, Z, levels=[0.5], linewidths=2)\n\nplt.title('____-bias-____-variance Model')",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment 2 questions.html",
    "href": "Assignment 2 questions.html",
    "title": "Assignment 2 (Section 21)",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Assignment 2 (Section 21)</span>"
    ]
  },
  {
    "objectID": "Assignment 2 questions.html#instructions",
    "href": "Assignment 2 questions.html#instructions",
    "title": "Assignment 2 (Section 21)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Monday, 22nd April 2024 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (2 points). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Assignment 2 (Section 21)</span>"
    ]
  },
  {
    "objectID": "Assignment 2 questions.html#tuning-a-knn-classifier-with-sklearn-tools-40-points",
    "href": "Assignment 2 questions.html#tuning-a-knn-classifier-with-sklearn-tools-40-points",
    "title": "Assignment 2 (Section 21)",
    "section": "1) Tuning a KNN Classifier with Sklearn Tools (40 points)",
    "text": "1) Tuning a KNN Classifier with Sklearn Tools (40 points)\nIn this question, you will use classification_data.csv. Each row is a loan and the each column represents some financial information as follows:\n\nhi_int_prncp_pd: Indicates if a high percentage of the repayments went to interest rather than principal. This is the classification response.\nout_prncp_inv: Remaining outstanding principal for portion of total amount funded by investors\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\nint_rate: Interest Rate on the loan\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60.\nmort_acc: The number of mortgage accounts\napplication_type_Individual: 1 if the loan is an individual application or a joint application with two co-borrowers\ntot_cur_bal: Total current balance of all accounts\npub_rec: Number of derogatory public records\n\nAs indicated above, hi_int_prncp_pd is the response and all the remaining columns are predictors. You will tune and train a K-Nearest Neighbors (KNN) classifier throughout this question.\n\n1a)\nRead the dataset. Create the predictor and the response variables.\nCreate the training and the test data with the following specifications: - The split should be 75%-25%. - You need to ensure that the class ratio is preserved in the training and the test datasets. i.e. the data is stratified. - Use random_state=45.\nPrint the class ratios of the entire dataset, the training set and the test set to check if the ratio is kept the same.\n(1 point)\n\n\n1b)\nScale the datasets. The data is ready for modeling at this point.\nBefore creating and tuning a model, you need to create a sklearn cross-validation object to ensure the most accurate representation of the data among all the folds.\nUse the following specifications for your cross-validation settings: - Make sure the data is stratified in all the folds (Use StratifiedKFold()). - Use 5 folds. - Shuffle the data for more randomness. - Use random_state=14.\n(1 point)\nNote that you need to use these settings for the rest of this question (Q1) for consistency.\nCross-validate a KNN Classifier with the following specifications: - Use every odd K value between 1 and 50. (including 1) - Fix the weights at “uniform”, which is default. - Use the cv object you created in part 1(c). - Use accuracy as metric.\n(4 points)\nPrint the best average cross-validation accuracy and the K value that corresponds to it. (2 points)\n\n\n1c)\nUsing the optimal K value you found in part 1(b), find the threshold that maximizes the cross-validation accuracy with the following specifications:\n\nUse all the possible threshold values with a stepsize of 0.01.\nUse the cross-validation settings you created in part f.\nUse accuracy as metric, which is default.\n\n(4 points)\nPrint the best cross-validation accuracy (1 point) and the threshold value that corresponds to it. (1 points)\n\n\n1d)\nIs the method we used in parts 1(b) and 1(c) guaranteed to find the best K & threshold combination, i.e. tune the classifier to its best values? (1 point) Why or why not? (1 point)\n\n\n1e)\nUse the tuned classifier and threshold to find the test accuracy. (2 points) .\nHow does it compare to the cross-validation accuracy, i.e. is the model generalizing well? (1 point)\n\n\n1f)\nNow, you need to tune K and the threshold at the same time. Use the following specifications: - Use every odd K value between 1 and 50. (including 1) - Fix the weights at “uniform”. - Use all the possible threshold values with a stepsize of 0.01. - Use accuracy as metric.\n(5 points)\nPrint the best cross-validation accuracy, and the K and threshold values that correspond to it. (1 point)\n\n\n1g)\nHow does the best cross-validation accuracy in part 1(f) compare to parts 1(b) and 1(c)? (1 point) Did the K and threshold value change? (1 point) Explain why or why not. (2 points)\n\n\n1h)\nUse the tuned classifier and threshold from part 1(f) to find the test accuracy. (1 point)\n\n\n1i)\nCompare the methods you used in parts 1(b) & 1(c) with the method you used in part 1(f) in terms of computational power. How many K & threshold pairs did you try in both? (2 points) Combining your answer with the answer in part 1(i), explain the main trade-off while tuning a model. (2 points)\n\n\n1j)\nCross-validate a KNN classifier with the following specifications: - Use every odd K value between 1 and 50. (including 1) - Fix the weights at “uniform” - Use accuracy, precision and recall as three metrics at the same time.\nFind the K value that maximizes recall while having a precision above 75%. (3 points) Print the average cross-validation results of that K value. (1 point)\nWhich metric (among precision, recall, and accuracy) seems to be the least sensitive to the value of ‘K’. Why? (3 points)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Assignment 2 (Section 21)</span>"
    ]
  },
  {
    "objectID": "Assignment 2 questions.html#tuning-a-knn-regressor-with-sklearn-tools-55-points",
    "href": "Assignment 2 questions.html#tuning-a-knn-regressor-with-sklearn-tools-55-points",
    "title": "Assignment 2 (Section 21)",
    "section": "2) Tuning a KNN Regressor with Sklearn Tools (55 points)",
    "text": "2) Tuning a KNN Regressor with Sklearn Tools (55 points)\nIn this question, you will use bank_loan_train_data.csv to tune (the model hyperparameters) and train the model. Each row is a loan and the each column represents some financial information as follows:\n\nmoney_made_inv: Indicates the amount of money made by the bank on the loan. This is the regression response.\nout_prncp_inv: Remaining outstanding principal for portion of total amount funded by investors\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\nint_rate: Interest Rate on the loan\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60\nmort_acc: The number of mortgage accounts\napplication_type_Individual: 1 if the loan is an individual application or a joint application with two co-borrowers\ntot_cur_bal: Total current balance of all accounts\npub_rec: Number of derogatory public records\n\nAs indicated above, money_made_inv is the response and all the remaining columns are predictors. You will tune and train a K-Nearest Neighbors (KNN) regressor throughout this question.\n\n2a)\nFind the optimal hyperparameter values and the corresponging optimal cross-validated RMSE. The hyperparameters that you must consider are\n\nNumber of nearest neighbors,\nWeight of the neighbor, and\nthe power p of the Minkowski distance.\n\nFor the weights hyperparameter, in addition to uniform and distance, consider 3 custom weights as well. The custom weights to consider are weight inversely proportional to distance squared, weight inversely proportional to distance cube, and weight inversely proportional to distance raised to the power of 4. Mathematically, these weights can be written as:\nweight∝1weight \\propto 1,\nweight∝1distanceweight \\propto \\frac{1}{distance},\nweight∝1distance2weight \\propto \\frac{1}{distance^2}\nweight∝1distance3weight \\propto \\frac{1}{distance^3}\nweight∝1distance4weight \\propto \\frac{1}{distance^4}\nShow all the 3 search approaches - grid search, random search, and Bayes search. As this is a simple problem, all the 3 approaches should yield the same result.\nFor Bayes search, show the implementation of real-time monitoring of cross-validation error.\nNone of the cross-validation approaches should take more than a minute as this is a simlpe problem.\nHint:\nCreate three different user-defined functions. The functions should take one input, named distance and return 1/(1e-10+distance**n), where n is 2, 3, and 4, respectively. Note that the 1e-10 is to avoid computational overflow.\nName your functions, dist_power_n, where n is 2, 3, and 4, respectively. You can use these function names as the weights input to a KNN model.\n(15 points)\n\n\n2b)\nBased on the optimal model in 2(a), find the RMSE on test data (bank_loan_test_data.csv). It must be less than $1400.\nNote: You will achieve the test RMSE if you tuned the hyperparameters well in 2(a). If you did not, redo 2(a). You are not allowed to use test data for tuning the hyperparameter values.\n(2 points)\n\n\n2c)\nKNN performance may deteriorate significantly if irrelevant predictors are included. We’ll add variable selection as well in the cross-validation procedure along with tuning of the hyperparameters for those variables.\nUse a variable selection method to consider the best ‘r’ predictors, optimize the hyperparameters specified in 2(a), and compute the cross-validation error for those ‘r’ predictors. Note that ‘r’ will vary from 1 to 7, thus you will need to do 7 cross-validations - one for each ‘r’.\nReport the optimal value of ‘r’, the ‘r’ predictors, the optimal hyperparameter values, and the optimal cross-validated RMSE.\nYou are free to use any search method.\nHint: You may use Lasso to consider the best ‘r’ predictors as that is the only variable selection you have learned so far.\n(20 points)\n\n\n2d)\nFind the RMSE on test data based on the optimal model in 2(c). Your test RMSE must be less than $800.\nNote: You will achieve the test RMSE if you tuned the hyperparameters well in 2(c). If you did not, redo 2(c). You are not allowed to use test data for tuning the hyperparameter values.\n(2 points)\n\n\n2e)\nHow did you decide the range of hyperparameter values to consider in this question? Discuss for p and n_neighbors.\n(4 points)\n\n\n2f)\nIs it possible to futher improve the results if we also optimize the metric hyperparameter along with the hyperparameters specified in 2(a)? Why or why not?\n(4 points)\n\n\n2g)\nWhat is the benefit of using the RepeatedKFold() function over the KFold() function of the model_selection module of the sklearn library? Explain in terms of bias-variance of test error. Did you observe any benefit of using RepeatedKFold() over KFold() in Q2? Why or why not?\n(4 + 4 points)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Assignment 2 (Section 21)</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html",
    "href": "Assignment 3.html",
    "title": "Assignment 3 (Sections 21 & 22)",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment 3 (Sections 21 & 22)</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#instructions",
    "href": "Assignment 3.html#instructions",
    "title": "Assignment 3 (Sections 21 & 22)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Wednesday, 8th May 2024 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (2 pts). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file. If your issue doesn’t seem genuine, you will lose points.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment 3 (Sections 21 & 22)</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#regression-problem---miami-housing",
    "href": "Assignment 3.html#regression-problem---miami-housing",
    "title": "Assignment 3 (Sections 21 & 22)",
    "section": "1) Regression Problem - Miami housing",
    "text": "1) Regression Problem - Miami housing\n\n1a) Data preparation\nRead the data miami-housing.csv. Check the description of the variables here. Split the data into 60% train and 40% test. Use random_state = 45. The response is SALE_PRC, and the rest of the columns are predictors, except PARCELNO. Print the shape of the predictors dataframe of the train data.\n(2 points)\n\n\n1b) Decision tree\nDevelop a decision tree model to predict SALE_PRC based on all the predictors. Use random_state = 45. Use the default hyperparameter values. What is the MAE (mean absolute error) on test data, and the cross-validated MAE?\n(3 points)\n\n\n1c) Tuning decision tree\nTune the hyperparameters of the decision tree model developed in the previous question, and compute the MAE on test data. You must tune the hyperparameters in the following manner:\nThe cross-validated MAE obtained must be less than $68,000. You must show the optimal values of the hyperparameters obtained, and the find the test MAE with the tuned model.\nHint:\n\nBayesSearchCV() may take less than a minute with max_depth and max_features\nYou are free to decide which hyperparameters to tune.\n\n(9 points)\n\n\n1d) Bagging decision trees\nBag decision trees, and compute the out-of-bag MAE. Use enough number of trees, such that the MAE stabilizes. Other than n_estimators, use default values of hyperparameters.\nThe out-of-bag cross-validated MAE must be less than $48,000.\n(4 points)\n\n\n1e) Bagging without bootstrapping\nBag decision trees without bootstrapping, i.e., put bootstrap = False while bagging the trees, and compute the cross-valdiated MAE. Why is the MAE obtained much higher than that in the previous question, but lower than that obtained in 1(b)?\n(1 point for code, 3 + 3 points for reasoning)\n\n\n1f) Bagging without bootstrapping samples, but bootstrapping features\nBag decision trees without bootstrapping samplse, but bootstrapping features, i.e., put bootstrap = False, and bootstrap_features = True while bagging the trees, and compute the cross-validated MAE. Why is the MAE obtained much lower than that in the previous question?\n(1 point for code, 3 points for reasoning)\n\n\n1g) Tuning bagged tree model\n\n1g)i) Approaches\nThere are two approaches for tuning a bagged tree model:\n\nOut of bag prediction\nKK-fold cross validation using GridSearchCV.\n\nWhat is the advantage of each approach over the other, i.e., what is the advantage of the out-of-bag approach over KK-fold cross validation, and what is the advantage of KK-fold cross validation over the out-of-bag approach?\n(3 + 3 points)\n\n\n1g)ii) Tuning the hyperparameters\nTune the hyperparameters of the bagged tree model developed in 1(d). You may use either of the approaches mentioned in the previous question. Show the optimal values of the hyperparameters obtained. Compute the MAE on test data with the tuned model. Your cross-validated MAE must be less than the cross-validate MAE ontained in the previous question.\nIt is up to you to pick the hyperparameters and their values in the grid.\nHint:\nGridSearchCV() may work better than BayesSearchCV() in this case. Why?\n(9 points)\n\n\n\n1h) Random forest\n\n1h)(i) Tuning random forest\nTune a random forest model to predict SALE_PRC, and compute the MAE on test data. The cross-validated MAE must be less than $46,000.\nIt is up to you to pick the hyperparameters and their values in the grid.\nHint: OOB approach will take less than a minute.\n(9 points)\n\n\n1h)(ii) Feature importance\nArrange and print the predictors in decreasing order of importance.\n(4 points)\n\n\n1h)(iii) Feature selection\nDrop the least important predictor, and find the cross-validated MAE of the tuned model again. You may need to adjust the max_features hyperparameter to account for the dropped predictor. Did the cross-validate MAE reduce?\n(4 points)\n\n\n1h)(iv) Random forest vs bagging: max_features\nNote that the max_features hyperparameter is there both in the RandomForestRegressor() function and the BaggingRegressor() function. Does it have the same meaning in both the functions? If not, then what is the difference?\nHint: Check scikit-learn documentation\n(1 + 3 points)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment 3 (Sections 21 & 22)</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#classification---term-deposit",
    "href": "Assignment 3.html#classification---term-deposit",
    "title": "Assignment 3 (Sections 21 & 22)",
    "section": "2) Classification - Term deposit",
    "text": "2) Classification - Term deposit\nThe data for this question is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls, where bank clients were called to subscribe for a term deposit.\nThere is a train data - train.csv, which you will use to develop a model. There is a test data - test.csv, which you will use to test your model. Each dataset has the following attributes about the clients called in the marketing campaign:\n\nage: Age of the client\neducation: Education level of the client\nday: Day of the month the call is made\nmonth: Month of the call\ny: did the client subscribe to a term deposit?\nduration: Call duration, in seconds. This attribute highly affects the output target (e.g., if duration=0 then y=‘no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for inference purposes and should be discarded if the intention is to have a realistic predictive model.\n\n(Raw data source: Source. Do not use the raw data source for this assignment. It is just for reference.)\n\n2a) Data preparation\nConvert all the categorical predictors in the data to dummy variables. Note that month and education are categorical variables.\n(2 points)\n\n\n2b) Random forest\nDevelop and tune a random forest model to predict the probability of a client subscribing to a term deposit based on age, education, day and month. The model must have:\n\nMinimum overall classification accuracy of 75% among the classification accuracies on train.csv, and test.csv.\nMinimum recall of 60% among the recall on train.csv, and test.csv.\n\nPrint the accuracy and recall for both the datasets - train.csv, and test.csv.\nNote that:\n\nYou cannot use duration as a predictor. The predictor is not useful for prediction because its value is determined after the marketing call ends. However, after the call ends, we already know whether the client responded positively or negatively.\nYou are free to choose any value of threshold probability for classifying observations. However, you must use the same threshold on both the datasets.\nUse cross-validation on train data to optimize the model hyperparameters.\nUsing the optimal model hyperparameters obtained in (iii), develop the decision tree model. Plot the cross-validated accuracy and recall against decision threshold probability. Tune the decision threshold probability based on the plot, or the data underlying the plot to achieve the required trade-off between recall and accuracy.\nEvaluate the accuracy and recall of the developed model with the tuned decision threshold probability on both the datasets. Note that the test dataset must only be used to evaluate performance metrics, and not optimize any hyperparameters or decision threshold probability.\n\n(22 points - 8 points for tuning the hyperparameters, 5 points for making the plot, 5 points for tuning the decision threshold probability based on the plot, and 4 points for printing the accuracy & recall on both the datasets)\nHint:\n\nRestrict the search for max_depth to a maximum of 25, and max_leaf_nodes to a maximum of 45. Without this restriction, you may get a better recall for threshold probability = 0.5, but are likely to get a worse trade-off between recall and accuracy. Tune max_features, max_depth, and max_leaf_nodes with OOB cross-validation.\nUse oob_decision_function_ for OOB cross-validated probabilities.\n\nIt is up to you to pick the hyperparameters and their values in the grid.",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment 3 (Sections 21 & 22)</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#predictor-transformations-in-trees",
    "href": "Assignment 3.html#predictor-transformations-in-trees",
    "title": "Assignment 3 (Sections 21 & 22)",
    "section": "3) Predictor transformations in trees",
    "text": "3) Predictor transformations in trees\nCan a non-linear monotonic transformation of predictors (such as log(), sqrt() etc.) be useful in improving the accuracy of decision tree models?\n(4 points for answer)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment 3 (Sections 21 & 22)</span>"
    ]
  },
  {
    "objectID": "Stratified splitting.html",
    "href": "Stratified splitting.html",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "",
    "text": "A.1 Stratified splitting with respect to response\nQ: When splitting data into train and test for developing and assessing a classification model, it is recommended to stratify the split with respect to the response. Why?\nA: The main advantage of stratified splitting is that it can help ensure that the training and testing sets have similar distributions of the target variable, which can lead to more accurate and reliable model performance estimates.\nIn many real-world datasets, the target variable may be imbalanced, meaning that one class is more prevalent than the other(s). For example, in a medical dataset, the majority of patients may not have a particular disease, while only a small fraction may have the disease. If a random split is used to divide the dataset into training and testing sets, there is a risk that the testing set may not have enough samples from the minority class, which can lead to biased model performance estimates.\nStratified splitting addresses this issue by ensuring that both the training and testing sets have similar proportions of the target variable. This can lead to more accurate model performance estimates, especially for imbalanced datasets, by ensuring that the testing set contains enough samples from each class to make reliable predictions.\nAnother advantage of stratified splitting is that it can help ensure that the model is not overfitting to a particular class. If a random split is used and one class is overrepresented in the training set, the model may learn to predict that class well but perform poorly on the other class(es). Stratified splitting can help ensure that the model is exposed to a representative sample of all classes during training, which can improve its generalization performance on new, unseen data.\nIn summary, the advantages of stratified splitting are that it can lead to more accurate and reliable model performance estimates, especially for imbalanced datasets, and can help prevent overfitting to a particular class.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Stratified splitting (classification problem)</span>"
    ]
  },
  {
    "objectID": "Stratified splitting.html#stratified-splitting-with-respect-to-response-and-categorical-predictors",
    "href": "Stratified splitting.html#stratified-splitting-with-respect-to-response-and-categorical-predictors",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "A.2 Stratified splitting with respect to response and categorical predictors",
    "text": "A.2 Stratified splitting with respect to response and categorical predictors\nQ: Will it be better to stratify the split with respect to the response as well as categorical predictors, instead of only the response? In that case, the train and test datasets will be even more representative of the complete data.\nA: It is not recommended to stratify with respect to both the response and categorical predictors simultaneously, while splitting a dataset into train and test, because doing so may result in the test data being very similar to train data, thereby defeating the purpose of assessing the model on unseen data. This kind of a stratified splitting will tend to make the relationships between the response and predictors in train data also appear in test data, which will result in the performance on test data being very similar to that in train data. Thus, in this case, the ability of the model to generalize to new, unseen data won’t be assessed by test data.\nTherefore, it is generally recommended to only stratify the response variable when splitting the data for model training, and to use random sampling for the predictor variables. This helps to ensure that the model is able to capture the underlying relationships between the predictor variables and the response variable, while still being able to generalize well to new, unseen data.\nIn the extreme scenario, when there are no continuous predictors, and there are enough observations for stratification with respect to the response and the categorical predictors, the train and test datasets may turn out to be exactly the same. Example 1 below illustrates this scenario.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Stratified splitting (classification problem)</span>"
    ]
  },
  {
    "objectID": "Stratified splitting.html#example-1",
    "href": "Stratified splitting.html#example-1",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "A.3 Example 1",
    "text": "A.3 Example 1\nThe example below shows that the train and test data can be exactly the same if we stratify the split with respect to response and the categorical predictors.\n\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom itertools import product\nsns.set(font_scale=1.35)\n\nLet us simulate a dataset with 8 observations, two categorical predictors x1 and x2 and the the binary response y.\n\n#Setting a seed for reproducible results\nnp.random.seed(9)\n\n# 8 observations\nn = 8\n\n#Simulating the categorical predictors\nx1 = pd.Series(np.random.randint(0,2,n), name = 'x1')\nx2 = pd.Series(np.random.randint(0,2,n), name = 'x2')\n\n#Simulating the response\npr = (x1==1)*0.7+(x2==0)*0.3# + (x3*0.1&gt;0.1)*0.1\ny = pd.Series(1*(np.random.uniform(size = n) &lt; pr), name = 'y')\n\n#Defining the predictor object 'X'\nX = pd.concat([x1, x2], axis = 1)\n\n#Stratified splitting with respect to the response and predictors to create 50% train and test datasets\nX_train_stratified, X_test_stratified, y_train_stratified,\\\ny_test_stratified = train_test_split(X, y, test_size = 0.5, random_state = 45, stratify=data[['x1', 'x2', 'y']])\n\n#Train and test data resulting from the above stratified splitting\ndata_train = pd.concat([X_train_stratified, y_train_stratified], axis = 1)\ndata_test = pd.concat([X_test_stratified, y_test_stratified], axis = 1)\n\nLet us check the train and test datasets created with stratified splitting with respect to both the predictors and the response.\n\ndata_train\n\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n2\n0\n0\n1\n\n\n7\n0\n1\n0\n\n\n3\n1\n0\n1\n\n\n1\n0\n1\n0\n\n\n\n\n\n\n\n\n\ndata_test\n\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n4\n0\n1\n0\n\n\n6\n1\n0\n1\n\n\n0\n0\n1\n0\n\n\n5\n0\n0\n1\n\n\n\n\n\n\n\n\nNote that the train and test datasets are exactly the same! Stratified splitting tends to have the same proportion of observations corresponding to each strata in both the train and test datasets, where each strata is a unique combination of values of x1, x2, and y. This will tend to make the train and test datasets quite similar!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Stratified splitting (classification problem)</span>"
    ]
  },
  {
    "objectID": "Stratified splitting.html#example-2-simulation-results",
    "href": "Stratified splitting.html#example-2-simulation-results",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "A.4 Example 2: Simulation results",
    "text": "A.4 Example 2: Simulation results\nThe example below shows that train and test set performance will tend to be quite similar if we stratify the datasets with respect to the predictors and the response.\nWe’ll simulate a dataset consisting of 1000 observations, 2 categorical predictors x1 and x2, a continuous predictor x3, and a binary response y.\n\n#Setting a seed for reproducible results\nnp.random.seed(99)\n\n# 1000 Observations\nn = 1000\n\n#Simulating categorical predictors x1 and x2\nx1 = pd.Series(np.random.randint(0,2,n), name = 'x1')\nx2 = pd.Series(np.random.randint(0,2,n), name = 'x2')\n\n#Simulating continuous predictor x3\nx3 = pd.Series(np.random.normal(0,1,n), name = 'x3')\n\n#Simulating the response\npr = (x1==1)*0.7+(x2==0)*0.3 + (x3*0.1&gt;0.1)*0.1\ny = pd.Series(1*(np.random.uniform(size = n) &lt; pr), name = 'y')\n\n#Defining the predictor object 'X'\nX = pd.concat([x1, x2, x3], axis = 1)\n\nWe’ll comparing model performance metrics when the data is split into train and test by performing stratified splitting\n\nOnly with respect to the response\nWith respect to the response and categorical predictors\n\nWe’ll perform 1000 simulations, where the data is split using a different seed in each simulation.\n\n#Creating an empty dataframe to store simulation results of 1000 simulations\naccuracy_iter = pd.DataFrame(columns = {'train_y_stratified','test_y_stratified',\n                                        'train_y_CatPredictors_stratified','test_y_CatPredictors_stratified'})\n\n\n# Comparing model performance metrics when the data is split into train and test by performing stratified splitting\n# (1) only with respect to the response\n# (2) with respect to the response and categorical predictors\n\n# Stratified splitting is performed 1000 times and the results are compared\nfor i in np.arange(1,1000):\n \n    #--------Case 1-------------------#\n    # Stratified splitting with respect to response only to create train and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = i, stratify=y)\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Model accuracy on train and test data, with stratification only on response while splitting \n    # the complete data into train and test\n    accuracy_iter.loc[(i-1), 'train_y_stratified'] = model.score(X_train, y_train)\n    accuracy_iter.loc[(i-1), 'test_y_stratified'] = model.score(X_test, y_test)\n        \n    #--------Case 2-------------------#\n    # Stratified splitting with respect to response and categorical predictors to create train \n    # and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = i, \n                                                        stratify=pd.concat([x1, x2, y], axis = 1))\n    model.fit(X_train, y_train)\n\n    # Model accuracy on train and test data, with stratification on response and predictors while \n    # splitting the complete data into train and test\n    accuracy_iter.loc[(i-1), 'train_y_CatPredictors_stratified'] = model.score(X_train, y_train)\n    accuracy_iter.loc[(i-1), 'test_y_CatPredictors_stratified'] = model.score(X_test, y_test)\n    \n# Converting accuracy to numeric\naccuracy_iter = accuracy_iter.apply(lambda x:x.astype(float), axis = 1)\n\n\nDistribution of train and test accuracies\nThe table below shows the distribution of train and test accuracies when the data is split into train and test by performing stratified splitting:\n\nOnly with respect to the response (see train_y_stratified and test_y_stratified)\nWith respect to the response and categorical predictors (see train_y_CatPredictors_stratified and test_y_CatPredictors_stratified)\n\n\naccuracy_iter.describe()\n\n\n\n\n\n\n\n\n\ntrain_y_stratified\ntest_y_stratified\ntrain_y_CatPredictors_stratified\ntest_y_CatPredictors_stratified\n\n\n\n\ncount\n999.000000\n999.000000\n9.990000e+02\n9.990000e+02\n\n\nmean\n0.834962\n0.835150\n8.350000e-01\n8.350000e-01\n\n\nstd\n0.005833\n0.023333\n8.552999e-15\n8.552999e-15\n\n\nmin\n0.812500\n0.755000\n8.350000e-01\n8.350000e-01\n\n\n25%\n0.831250\n0.820000\n8.350000e-01\n8.350000e-01\n\n\n50%\n0.835000\n0.835000\n8.350000e-01\n8.350000e-01\n\n\n75%\n0.838750\n0.850000\n8.350000e-01\n8.350000e-01\n\n\nmax\n0.855000\n0.925000\n8.350000e-01\n8.350000e-01\n\n\n\n\n\n\n\n\nLet us visualize the distribution of these accuracies.\n\n\nA.4.1 Stratified splitting only with respect to the response\n\nsns.histplot(data=accuracy_iter, x=\"train_y_stratified\", color=\"red\", label=\"Train accuracy\", kde=True)\nsns.histplot(data=accuracy_iter, x=\"test_y_stratified\", color=\"skyblue\", label=\"Test accuracy\", kde=True);\nplt.legend()\nplt.xlabel('Accuracy')\n\nText(0.5, 0, 'Accuracy')\n\n\n\n\n\n\n\n\n\nNote the variability in train and test accuracies when the data is stratified only with respect to the response. The train accuracy varies between 81.2% and 85.5%, while the test accuracy varies between 75.5% and 92.5%.\n\n\nA.4.2 Stratified splitting with respect to the response and categorical predictors\n\nsns.histplot(data=accuracy_iter, x=\"train_y_CatPredictors_stratified\", color=\"red\", label=\"Train accuracy\", kde=True)\nsns.histplot(data=accuracy_iter, x=\"test_y_CatPredictors_stratified\", color=\"skyblue\", label=\"Test accuracy\", kde=True);\nplt.legend()\nplt.xlabel('Accuracy')\n\nText(0.5, 0, 'Accuracy')\n\n\n\n\n\n\n\n\n\nThe train and test accuracies are between 85% and 85.5% for all the simulations. As a results of stratifying the splitting with respect to both the response and the categorical predictors, the train and test datasets are almost the same because the datasets are engineered to be quite similar, thereby making the test dataset inappropriate for assessing accuracy on unseen data. Thus, it is recommended to stratify the splitting only with respect to the response.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Stratified splitting (classification problem)</span>"
    ]
  },
  {
    "objectID": "Parallel_processing_Bonus_Questions.html",
    "href": "Parallel_processing_Bonus_Questions.html",
    "title": "Appendix B — Parallel processing bonus Q",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, \\\ncross_validate, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, recall_score, mean_squared_error\nfrom scipy.stats import uniform\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nimport seaborn as sns\nfrom skopt.plots import plot_objective\nimport matplotlib.pyplot as plt\nimport warnings\nimport time as tm\n\n\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\npredictors = ['mpg', 'engineSize', 'year', 'mileage']\nX_train = train[predictors]\ny_train = train['price']\nX_test = test[predictors]\ny_test = test['price']\n\n# Scale\nsc = StandardScaler()\n\nsc.fit(X_train)\nX_train_scaled = sc.transform(X_train)\nX_test_scaled = sc.transform(X_test)\n\n\nCase 1: No parallelization\n\ntime_taken_case1 = []\nfor i in range(50):\n    start_time = tm.time()\n    Ks = range(1, 20)\n    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n    cross_val_error = []\n    for k in Ks:\n        cross_val_error.append(-cross_val_score(KNeighborsRegressor(n_neighbors=k), \n                          X_train_scaled, y_train, cv = kfold,\n                           scoring=\"neg_root_mean_squared_error\").mean())\n    time_taken_case1.append(tm.time() - start_time)\n\n\n\nCase 2: Parallelization in cross_val_score()\n\ntime_taken_case2 = []\nfor i in range(50):\n    start_time = tm.time()\n    Ks = range(1, 20)\n    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n    cross_val_error = []\n    for k in Ks:\n        cross_val_error.append(-cross_val_score(KNeighborsRegressor(n_neighbors=k), \n                         X_train_scaled, y_train, cv = kfold, n_jobs = -1,\n                                  scoring=\"neg_root_mean_squared_error\").mean())\n    time_taken_case2.append(tm.time() - start_time)\n\n\n\nCase 3: Parallelization in KNeighborsRegressor()\n\ntime_taken_case3 = []\nfor i in range(50):\n    start_time = tm.time()\n    Ks = range(1, 20)\n    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n    cross_val_error = []\n    for k in Ks:\n        cross_val_error.append(-cross_val_score(KNeighborsRegressor(n_neighbors=k, \n                n_jobs= -1), X_train_scaled, y_train, cv = kfold, \n                                  scoring=\"neg_root_mean_squared_error\").mean())\n    time_taken_case3.append(tm.time() - start_time)\n\n\n\nCase 4: Nested parallelization: Both cross_val_score() and KNeighborsRegressor()\n\ntime_taken_case4 = []\nfor i in range(50):\n    start_time = tm.time()\n    Ks = range(1, 20)\n    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n    cross_val_error = []\n    for k in Ks:\n        cross_val_error.append(-cross_val_score(KNeighborsRegressor(n_neighbors=k, \n                n_jobs= -1), X_train_scaled, y_train, cv = kfold, n_jobs = -1,\n                scoring=\"neg_root_mean_squared_error\").mean())\n    time_taken_case4.append(tm.time() - start_time)\n\n\nsns.boxplot([time_taken_case1, time_taken_case2, time_taken_case3, time_taken_case4])\nplt.xticks([0, 1, 2, 3], ['Case 1', 'Case 2', 'Case 3', 'Case 4']);\nplt.ylabel('Time');\n\n\n\n\n\n\n\n\nQ1\nCase 1 is without parallelization. Why is Case 3 with parallelization of KNeighborsRegressor() taking more time than case 1?\n\n\nQ2\nIf nested parallelization is worse than parallelization, why is case 4 with nested parallelization taking less time than case 3 with parallelization of KNeighborsRegressor()?\n\n\nQ3\nIf nested parallelization is worse than no parallelization, why is case 4 with nested parallelization taking less time than case 1 with no parallelization?\n\n\nQ4\nIf nested parallelization is the best scenario, why is case 4 with nested parallelization taking more time than case 2 with with parallelization in cross_val_score()?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Parallel processing bonus Q</span>"
    ]
  },
  {
    "objectID": "Miscellaneous questions.html",
    "href": "Miscellaneous questions.html",
    "title": "Appendix C — Miscellaneous questions",
    "section": "",
    "text": "Q1\nWhy is boosting inappropriate for linear Regression, but appropriate for decision trees?\nThe question has been well answered in the post. The intuitive explanation is that the weighted average of a sequence of linear regression models will also be a single linear regression model. However, if the weighted average of the sequence of linear regression models results in a linear regression model (say boosted_linear_regression) that is different from the linear regression model that is obtained by fitting directly to the data (say regular_linear_regression), then the boosted_linear_regression model will have a higher bias than the regular_linear_regression model as the regular_linear_regression model minimizes the sum of squared errors (SSE). Thus, the boosted_linear_regression model should be the same as the regular_linear_regression model for the optimal hyperparameter values of the boosting algorithm. Thus, all the hard-work of tuning the boosting model will at best lead to the linear regression model that can be obtained by fitting a linear regression model directly to the train data!\nHowever, a sequence of shallow regression trees will not lead to the same regression tree that can be developed directly. A sequence of shallow trees will continuously reduce bias with relative less increase in variance. A single decision tree is likely to have a relatively high variance, and thus boosting with shallow trees may provide a better performance. Boosting aims to reduce bias by controlling the increase in variance, while a single decision tree has almost zero bias at the cost of having a high variance.\nThe second response in the post provides a mathematical explanation, which is more convincing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Miscellaneous questions</span>"
    ]
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix D — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Datasets, assignment and project files</span>"
    ]
  }
]