<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Gradient Boosting – Data Science III with python (Class notes)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./XGBoost.html" rel="next">
<link href="./adaboost.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8da5b4427184b79ecddefad3d342027e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regression_tree_sp25.html">Tree based models</a></li><li class="breadcrumb-item"><a href="./Gradient_Boosting.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Gradient Boosting</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="https://statistics.northwestern.edu/" class="sidebar-logo-link">
      <img src="./NU_Stat_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science III with python (Class notes)</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bias &amp; Variance; KNN</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./KNN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">KNN</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Bias_variance_code.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bias-variance tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Hyperparameter tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Basic Hyperparameter tuning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Tree based models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression_tree_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Regression trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Classification _Tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bagging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bagging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./random_forest.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Random Forests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./adaboost.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Adaptive Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Gradient_Boosting.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Gradient Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./XGBoost.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">XGBoost</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LightGBM_CatBoost.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">LightGBM and CatBoost</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./smarter_hyper_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Smarter Hyperparameter Tuning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./voting_stacking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Advanced Ensemble Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Assignment1_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Assignment 1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Assignment2_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Assignment 2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Assignment3_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Assignment 3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Assignment4_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Assignment 4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Datasets, assignment and project files</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#gradient-boosting-intuition" id="toc-gradient-boosting-intuition" class="nav-link active" data-scroll-target="#gradient-boosting-intuition"><span class="header-section-number">9.1</span> Gradient Boosting Intuition</a></li>
  <li><a href="#how-gradient-boosting-works-regression-example" id="toc-how-gradient-boosting-works-regression-example" class="nav-link" data-scroll-target="#how-gradient-boosting-works-regression-example"><span class="header-section-number">9.2</span> How Gradient Boosting Works (Regression Example)</a></li>
  <li><a href="#gradient-boosting-in-scikit-learn" id="toc-gradient-boosting-in-scikit-learn" class="nav-link" data-scroll-target="#gradient-boosting-in-scikit-learn"><span class="header-section-number">9.3</span> Gradient Boosting in Scikit-Learn</a></li>
  <li><a href="#core-hyperparameters-categories" id="toc-core-hyperparameters-categories" class="nav-link" data-scroll-target="#core-hyperparameters-categories"><span class="header-section-number">9.4</span> Core Hyperparameters Categories</a></li>
  <li><a href="#hyperparameter-tuning" id="toc-hyperparameter-tuning" class="nav-link" data-scroll-target="#hyperparameter-tuning"><span class="header-section-number">9.5</span> Hyperparameter Tuning</a>
  <ul>
  <li><a href="#individual-hyperparameter-impact-analysis" id="toc-individual-hyperparameter-impact-analysis" class="nav-link" data-scroll-target="#individual-hyperparameter-impact-analysis"><span class="header-section-number">9.5.1</span> Individual Hyperparameter Impact Analysis</a>
  <ul class="collapse">
  <li><a href="#effect-of-number-of-trees-on-cross-validation-error" id="toc-effect-of-number-of-trees-on-cross-validation-error" class="nav-link" data-scroll-target="#effect-of-number-of-trees-on-cross-validation-error"><span class="header-section-number">9.5.1.1</span> Effect of Number of Trees on Cross-Validation Error</a></li>
  <li><a href="#early-stopping-in-gradient-boosting" id="toc-early-stopping-in-gradient-boosting" class="nav-link" data-scroll-target="#early-stopping-in-gradient-boosting"><span class="header-section-number">9.5.1.2</span> Early stopping in Gradient Boosting</a></li>
  <li><a href="#effect-of-learning-rate-on-cross-validation-error" id="toc-effect-of-learning-rate-on-cross-validation-error" class="nav-link" data-scroll-target="#effect-of-learning-rate-on-cross-validation-error"><span class="header-section-number">9.5.1.3</span> Effect of Learning Rate on Cross-Validation Error</a></li>
  <li><a href="#learning-rate-and-number-of-trees-are-closely-linked" id="toc-learning-rate-and-number-of-trees-are-closely-linked" class="nav-link" data-scroll-target="#learning-rate-and-number-of-trees-are-closely-linked"><span class="header-section-number">9.5.1.4</span> Learning Rate and Number of Trees Are Closely Linked</a></li>
  <li><a href="#effect-of-stochastic-gradient-boosting-on-cross-validation-error" id="toc-effect-of-stochastic-gradient-boosting-on-cross-validation-error" class="nav-link" data-scroll-target="#effect-of-stochastic-gradient-boosting-on-cross-validation-error"><span class="header-section-number">9.5.1.5</span> Effect of Stochastic Gradient Boosting on Cross-Validation Error</a></li>
  <li><a href="#effect-of-tree-complexity-on-cross-validation-error-not-tuned-here" id="toc-effect-of-tree-complexity-on-cross-validation-error-not-tuned-here" class="nav-link" data-scroll-target="#effect-of-tree-complexity-on-cross-validation-error-not-tuned-here"><span class="header-section-number">9.5.1.6</span> Effect of Tree Complexity on Cross-Validation Error (Not Tuned Here)</a></li>
  <li><a href="#loss-function-loss" id="toc-loss-function-loss" class="nav-link" data-scroll-target="#loss-function-loss"><span class="header-section-number">9.5.1.7</span> Loss Function (<code>loss</code>)</a></li>
  </ul></li>
  <li><a href="#joint-hyperparameter-optimization" id="toc-joint-hyperparameter-optimization" class="nav-link" data-scroll-target="#joint-hyperparameter-optimization"><span class="header-section-number">9.5.2</span> Joint Hyperparameter Optimization</a>
  <ul class="collapse">
  <li><a href="#using-bayessearchcv-for-hyperparameter-tuning" id="toc-using-bayessearchcv-for-hyperparameter-tuning" class="nav-link" data-scroll-target="#using-bayessearchcv-for-hyperparameter-tuning"><span class="header-section-number">9.5.2.1</span> Using <code>BayesSearchCV</code> for Hyperparameter Tuning</a></li>
  <li><a href="#hyperparameter-optimization-with-optuna" id="toc-hyperparameter-optimization-with-optuna" class="nav-link" data-scroll-target="#hyperparameter-optimization-with-optuna"><span class="header-section-number">9.5.2.2</span> Hyperparameter Optimization with Optuna</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#independent-study" id="toc-independent-study" class="nav-link" data-scroll-target="#independent-study"><span class="header-section-number">9.6</span> Independent Study</a></li>
  <li><a href="#foundational-paper" id="toc-foundational-paper" class="nav-link" data-scroll-target="#foundational-paper"><span class="header-section-number">9.7</span> Foundational Paper</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regression_tree_sp25.html">Tree based models</a></li><li class="breadcrumb-item"><a href="./Gradient_Boosting.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Gradient Boosting</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Gradient Boosting</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="c030794c" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display" data-execution_count="3">
<img src="./Datasets/vanilla_gradient_boosting.webp">
</div>
</div>
<p>Gradient Boosting is a <strong>boosting technique</strong> that builds an additive model in a forward stage-wise manner. Unlike AdaBoost, which adjusts weights on training instances, Gradient Boosting fits new models to the <strong>residual errors</strong> made by prior models using the gradient of a specified loss function.</p>
<p>At each stage, a new weak learner is trained to minimize the loss function by correcting the errors of the current ensemble.</p>
<section id="gradient-boosting-intuition" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="gradient-boosting-intuition"><span class="header-section-number">9.1</span> Gradient Boosting Intuition</h2>
<p>Gradient Boosting can be understood as <strong>functional gradient descent</strong>:</p>
<ul>
<li>We start with an initial prediction (e.g., the mean of the targets).</li>
<li>At each iteration, we fit a new model to the <strong>negative gradient</strong> of the loss function with respect to the current predictions.</li>
<li>This negative gradient plays a similar role to residuals in squared loss regression—it points in the direction that most reduces the loss.</li>
<li>The new model’s predictions are then added to the current model, scaled by a learning rate.</li>
</ul>
<p>By sequentially adding models that reduce the remaining error, the ensemble gradually improves.</p>
</section>
<section id="how-gradient-boosting-works-regression-example" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="how-gradient-boosting-works-regression-example"><span class="header-section-number">9.2</span> How Gradient Boosting Works (Regression Example)</h2>
<ol type="1">
<li><p><strong>Initialize</strong> the model with a constant prediction:<br>
<span class="math display">\[
\hat{f}^{(0)}(x) = \arg\min_c \sum_{i=1}^n L(y_i, c)
\]</span></p></li>
<li><p><strong>For</strong> <span class="math inline">\(m = 1\)</span> to <span class="math inline">\(M\)</span> (number of boosting rounds):</p>
<ul>
<li><p>Compute the <strong>negative gradient</strong> (pseudo-residuals):<br>
<span class="math display">\[
r_i^{(m)} = - \left[ \frac{\partial L(y_i, \hat{f}(x_i))}{\partial \hat{f}(x_i)} \right]_{\hat{f}(x) = \hat{f}^{(m-1)}(x)}
\]</span></p></li>
<li><p>Fit a <strong>base learner</strong> <span class="math inline">\(h^{(m)}(x)\)</span> to the residuals <span class="math inline">\(r_i^{(m)}\)</span>.</p></li>
<li><p>Determine the <strong>optimal step size</strong> (line search):<br>
<span class="math display">\[
\gamma^{(m)} = \arg\min_\gamma \sum_{i=1}^n L\left(y_i, \hat{f}^{(m-1)}(x_i) + \gamma \cdot h^{(m)}(x_i)\right)
\]</span></p></li>
<li><p><strong>Update the model</strong>:<br>
<span class="math display">\[
\hat{f}^{(m)}(x) = \hat{f}^{(m-1)}(x) + \eta \cdot \gamma^{(m)} h^{(m)}(x)
\]</span> where <span class="math inline">\(\eta\)</span> is the learning rate.</p></li>
</ul></li>
<li><p><strong>Final prediction</strong>:<br>
<span class="math display">\[
\hat{f}^{(M)}(x)
\]</span></p></li>
</ol>
<div id="c17072e2" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score,train_test_split, KFold, cross_val_predict</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error, mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, <span class="op">\</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>recall_score, precision_score, confusion_matrix</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor,DecisionTreeClassifier</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV, ParameterGrid, StratifiedKFold</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder, FunctionTransformer</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools <span class="im">as</span> it</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time <span class="im">as</span> time</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt <span class="im">import</span> BayesSearchCV</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt.space <span class="im">import</span> Real, Categorical, Integer</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt.plots <span class="im">import</span> plot_objective, plot_histogram, plot_convergence</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython <span class="im">import</span> display</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradient-boosting-in-scikit-learn" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="gradient-boosting-in-scikit-learn"><span class="header-section-number">9.3</span> Gradient Boosting in Scikit-Learn</h2>
<p>Scikit-learn offers a standard implementation of Gradient Boosting through two primary estimators:</p>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"><code>GradientBoostingClassifier</code></a> for classification tasks</li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"><code>GradientBoostingRegressor</code></a> for regression tasks</li>
</ul>
<p>These estimators build an additive model in a forward stage-wise fashion, allowing for the optimization of arbitrary differentiable loss functions. They are suitable for small to medium-sized datasets and provide flexibility in model tuning.</p>
<p>For larger datasets (typically with <code>n_samples &gt;= 10,000</code>), consider using the histogram-based variants:</p>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html"><code>HistGradientBoostingClassifier</code></a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html"><code>HistGradientBoostingRegressor</code></a></li>
</ul>
</section>
<section id="core-hyperparameters-categories" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="core-hyperparameters-categories"><span class="header-section-number">9.4</span> Core Hyperparameters Categories</h2>
<p>The primary hyperparameters for <code>GradientBoostingClassifier</code> and <code>GradientBoostingRegressor</code> can be grouped into the following categories:</p>
<ol type="1">
<li><p><strong>Number of Trees</strong> (<code>n_estimators</code>)</p>
<ul>
<li>Use <strong>early stopping</strong> (via <code>n_iter_no_change</code> and <code>validation_fraction</code> in scikit-learn) to avoid overfitting.<br>
</li>
<li>Start with a large value (e.g., 500–1000) and let early stopping prune unnecessary trees.</li>
</ul></li>
<li><p><strong>Early Stopping</strong></p>
<ul>
<li>Prevents overfitting by halting training once the validation performance stops improving.<br>
</li>
<li>Controlled using:
<ul>
<li><code>n_iter_no_change</code>: Number of rounds with no improvement before stopping (e.g., 10).</li>
<li><code>validation_fraction</code>: Fraction of training data reserved as internal validation set (e.g., 0.1).</li>
<li><code>tol</code>: Minimum improvement to be considered significant (e.g., <code>1e-4</code>).</li>
</ul></li>
<li>Set a large <code>n_estimators</code>, and let early stopping determine the optimal number of boosting iterations.</li>
</ul></li>
<li><p><strong>Learning Rate</strong> (<code>learning_rate</code>)</p>
<ul>
<li>Shrinks the contribution of each tree to improve generalization.<br>
</li>
<li><em>Typical range</em>: 0.01–0.2 (lower values require more trees).</li>
</ul></li>
<li><p><strong>Tree Complexity</strong></p>
<ul>
<li><code>max_depth</code>: Depth of individual trees. Start shallow (3–6) to limit overfitting.<br>
</li>
<li><code>min_samples_split</code>: Minimum samples required to split a node (e.g., 10–50).<br>
</li>
<li><code>min_samples_leaf</code>: Minimum samples required in a leaf node (e.g., 5–20).</li>
</ul></li>
<li><p><strong>Stochastic Gradient Boosting</strong></p>
<ul>
<li><code>subsample</code>: Fraction of training data sampled per tree (e.g., 0.5–1.0).<br>
</li>
<li><code>max_features</code>: Fraction/absolute number of features used per split (e.g., <code>sqrt(n_features)</code> or <code>0.8</code>).</li>
</ul></li>
<li><p><strong>Loss Function</strong> (<code>loss</code>)</p>
<ul>
<li>Matches the problem type:
<ul>
<li>Regression: <code>squared_error</code>, <code>absolute_error</code></li>
<li>Classification: <code>log_loss</code> (binary/multinomial deviance)</li>
</ul></li>
</ul></li>
</ol>
<p>For a comprehensive list and detailed explanations of all hyperparameters, refer to the official Scikit-learn documentation:</p>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">GradientBoostingClassifier Documentation</a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html">GradientBoostingRegressor Documentation</a></li>
</ul>
</section>
<section id="hyperparameter-tuning" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="hyperparameter-tuning"><span class="header-section-number">9.5</span> Hyperparameter Tuning</h2>
<p>Let’s reuse the car dataset to evaluate how different hyperparameter settings affect the performance of gradient boosting</p>
<div id="a9036ef3" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>car <span class="op">=</span> pd.read_csv(<span class="st">'Datasets/car.csv'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>car.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">brand</th>
<th data-quarto-table-cell-role="th">model</th>
<th data-quarto-table-cell-role="th">year</th>
<th data-quarto-table-cell-role="th">transmission</th>
<th data-quarto-table-cell-role="th">mileage</th>
<th data-quarto-table-cell-role="th">fuelType</th>
<th data-quarto-table-cell-role="th">tax</th>
<th data-quarto-table-cell-role="th">mpg</th>
<th data-quarto-table-cell-role="th">engineSize</th>
<th data-quarto-table-cell-role="th">price</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>vw</td>
<td>Beetle</td>
<td>2014</td>
<td>Manual</td>
<td>55457</td>
<td>Diesel</td>
<td>30</td>
<td>65.3266</td>
<td>1.6</td>
<td>7490</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>vauxhall</td>
<td>GTC</td>
<td>2017</td>
<td>Manual</td>
<td>15630</td>
<td>Petrol</td>
<td>145</td>
<td>47.2049</td>
<td>1.4</td>
<td>10998</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>merc</td>
<td>G Class</td>
<td>2012</td>
<td>Automatic</td>
<td>43000</td>
<td>Diesel</td>
<td>570</td>
<td>25.1172</td>
<td>3.0</td>
<td>44990</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>audi</td>
<td>RS5</td>
<td>2019</td>
<td>Automatic</td>
<td>10</td>
<td>Petrol</td>
<td>145</td>
<td>30.5593</td>
<td>2.9</td>
<td>51990</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>merc</td>
<td>X-CLASS</td>
<td>2018</td>
<td>Automatic</td>
<td>14000</td>
<td>Diesel</td>
<td>240</td>
<td>35.7168</td>
<td>2.3</td>
<td>28990</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="db6b5f99" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> car.drop(columns<span class="op">=</span>[<span class="st">'price'</span>])</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> car[<span class="st">'price'</span>]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the categorical columns and put them in a list</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>categorical_feature <span class="op">=</span> X.select_dtypes(include<span class="op">=</span>[<span class="st">'object'</span>]).columns.tolist()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the numerical columns and put them in a list</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>numerical_feature <span class="op">=</span> X.select_dtypes(include<span class="op">=</span>[<span class="st">'int64'</span>, <span class="st">'float64'</span>]).columns.tolist()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c7d5fb7e" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">'ignore'</span>, sparse_output<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>X_train_encoded <span class="op">=</span> encoder.fit_transform(X_train[categorical_feature])</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>X_test_encoded <span class="op">=</span> encoder.transform(X_test[categorical_feature])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the encoded features back to DataFrame</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>X_train_encoded_df <span class="op">=</span> pd.DataFrame(X_train_encoded, columns<span class="op">=</span>encoder.get_feature_names_out(categorical_feature))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>X_test_encoded_df <span class="op">=</span> pd.DataFrame(X_test_encoded, columns<span class="op">=</span>encoder.get_feature_names_out(categorical_feature))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate the encoded features with the original numerical features</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>X_train_final <span class="op">=</span> pd.concat([X_train_encoded_df, X_train[numerical_feature].reset_index(drop<span class="op">=</span><span class="va">True</span>)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>X_test_final <span class="op">=</span> pd.concat([X_test_encoded_df, X_test[numerical_feature].reset_index(drop<span class="op">=</span><span class="va">True</span>)], axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="individual-hyperparameter-impact-analysis" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="individual-hyperparameter-impact-analysis"><span class="header-section-number">9.5.1</span> Individual Hyperparameter Impact Analysis</h3>
<section id="effect-of-number-of-trees-on-cross-validation-error" class="level4" data-number="9.5.1.1">
<h4 data-number="9.5.1.1" class="anchored" data-anchor-id="effect-of-number-of-trees-on-cross-validation-error"><span class="header-section-number">9.5.1.1</span> Effect of Number of Trees on Cross-Validation Error</h4>
<p>Effect of Number of Trees on Cross-Validation Error In Gradient Boosting, the number of trees (<code>n_estimators</code>) controls how many boosting rounds the model performs. Adding more trees can reduce bias and improve training accuracy, but it also increases the risk of overfitting, especially with a high learning rate.</p>
<p>The optimal number of trees is often found by balancing <strong>model complexity</strong> and <strong>generalization performance</strong> using cross-validation.</p>
<div id="452c9c3a" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_models():</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define number of trees to consider</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    n_trees <span class="op">=</span> [<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">800</span>, <span class="dv">1000</span>, <span class="dv">1500</span>, <span class="dv">2000</span>]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> n_trees:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        models[<span class="bu">str</span>(n)] <span class="op">=</span> GradientBoostingRegressor(n_estimators<span class="op">=</span>n,random_state<span class="op">=</span><span class="dv">1</span>,loss<span class="op">=</span><span class="st">'huber'</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> models</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate a given model using cross-validation</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_model(model, X, y):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define the evaluation procedure</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    cv <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate the model and collect the results</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.sqrt(<span class="op">-</span>cross_val_score(model, X, y, scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>, cv<span class="op">=</span>cv, n_jobs<span class="op">=-</span><span class="dv">1</span>))</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scores</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># get the models to evaluate</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> get_models()</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate the models and store results</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>results, names <span class="op">=</span> <span class="bu">list</span>(), <span class="bu">list</span>()</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate the model</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> evaluate_model(model, X_train_final, y_train)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># store the results</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    results.append(scores)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    names.append(name)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># summarize the performance along the way</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'&gt;</span><span class="sc">%s</span><span class="st"> </span><span class="sc">%.3f</span><span class="st"> (</span><span class="sc">%.3f</span><span class="st">)'</span> <span class="op">%</span> (name, np.mean(scores), np.std(scores)))</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>plt.boxplot(results, labels<span class="op">=</span>names, showmeans<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross validation error'</span>,fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of trees'</span>,fontsize<span class="op">=</span><span class="dv">15</span>)<span class="op">;</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="co"># get the optimal number of trees</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>best_index <span class="op">=</span> np.argmin([np.mean(r) <span class="cf">for</span> r <span class="kw">in</span> results])</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>best_n_trees <span class="op">=</span> names[best_index]</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> np.mean(results[best_index])</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight the best model on the plot</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>best_index<span class="op">+</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>plt.text(best_index <span class="op">+</span> <span class="dv">1</span> <span class="op">-</span> <span class="fl">0.4</span>, best_score<span class="op">+</span><span class="dv">700</span>, </span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>         <span class="ss">f'Best: </span><span class="sc">{</span>best_n_trees<span class="sc">}</span><span class="ss"> (RMSE: </span><span class="sc">{</span>best_score<span class="sc">:.1f}</span><span class="ss">)'</span>, </span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">'red'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best number of trees: </span><span class="sc">{</span>best_n_trees<span class="sc">}</span><span class="ss"> with RMSE: </span><span class="sc">{</span>best_score<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt;50 6549.576 (722.462)
&gt;100 5232.949 (656.216)
&gt;500 3419.467 (262.753)
&gt;800 3202.489 (194.072)
&gt;1000 3106.002 (184.799)
&gt;1500 3039.520 (210.989)
&gt;2000 3194.874 (293.134)
Best number of trees: 1500 with RMSE: 3039.520</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Gradient_Boosting_files/figure-html/cell-7-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="early-stopping-in-gradient-boosting" class="level4" data-number="9.5.1.2">
<h4 data-number="9.5.1.2" class="anchored" data-anchor-id="early-stopping-in-gradient-boosting"><span class="header-section-number">9.5.1.2</span> Early stopping in Gradient Boosting</h4>
<p><strong>Why Early Stopping Matters</strong></p>
<p>Specifying a fixed number of trees means deciding in advance how many boosting rounds (i.e., trees) the model will train.</p>
<p>This approach can be inefficient or risky:</p>
<ul>
<li>If <strong>too few</strong> trees are used, the model may <strong>underfit</strong>.</li>
<li>If <strong>too many</strong>, the model may <strong>overfit</strong> or <strong>waste computation</strong>.</li>
</ul>
<p>That’s why <strong>early stopping</strong> is useful — it allows the model to <strong>stop training once performance on a validation set no longer improves</strong>, effectively selecting the optimal number of trees automatically.</p>
<p><strong>How Early Stopping Works</strong></p>
<p>Instead of specifying a fixed number of trees (<code>n_estimators</code>), the algorithm monitors performance on a <strong>validation set</strong> and stops adding new trees once the model’s improvement has plateaued.</p>
<p>In scikit-learn, early stopping can be enabled using:</p>
<ul>
<li><code>early_stopping=True</code></li>
<li><code>validation_fraction</code>: The fraction of training data used as a validation set</li>
<li><code>n_iter_no_change</code>: Number of iterations to wait without improvement before stopping</li>
</ul>
<p>This approach not only improves generalization but also reduces training time by avoiding unnecessary trees.</p>
<div id="2231df15" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> <span class="bu">dict</span>(n_estimators<span class="op">=</span><span class="dv">2000</span>, max_depth<span class="op">=</span><span class="dv">5</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>gbm_full <span class="op">=</span> GradientBoostingRegressor(<span class="op">**</span>params)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>gbm_early_stopping <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="op">**</span>params,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    validation_fraction<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    n_iter_no_change<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>gbm_full.fit(X_train_final, y_train)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>training_time_full <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>n_estimators_full <span class="op">=</span> gbm_full.n_estimators_</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>gbm_early_stopping.fit(X_train_final, y_train)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>training_time_early_stopping <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>estimators_early_stopping <span class="op">=</span> gbm_early_stopping.n_estimators_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s calculate the RMSE on both the training and test datasets for each model, which will be used for later visualization.</p>
<div id="cabba68f" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># import root mean squared error function</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>train_errors_without <span class="op">=</span> []</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>test_errors_without <span class="op">=</span> []</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>train_errors_with <span class="op">=</span> []</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>test_errors_with <span class="op">=</span> []</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (train_pred, test_pred) <span class="kw">in</span> <span class="bu">enumerate</span>(</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">zip</span>(</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        gbm_full.staged_predict(X_train_final),</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        gbm_full.staged_predict(X_test_final),</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    train_errors_without.append(root_mean_squared_error(y_train, train_pred))</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    test_errors_without.append(root_mean_squared_error(y_test, test_pred))</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (train_pred, test_pred) <span class="kw">in</span> <span class="bu">enumerate</span>(</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">zip</span>(</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        gbm_early_stopping.staged_predict(X_train_final),</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        gbm_early_stopping.staged_predict(X_test_final),</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    train_errors_with.append(root_mean_squared_error(y_train, train_pred))</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    test_errors_with.append(root_mean_squared_error(y_test, test_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s visulize Comparison. It includes three subplots:</p>
<ol type="1">
<li>Plotting training errors of both models over boosting iterations.</li>
<li>Plotting test errors of both models over boosting iterations.</li>
<li>Creating a bar chart to compare the training times and the number of estimators used by the models with and without early stopping.</li>
</ol>
<div id="784add04" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(train_errors_without, label<span class="op">=</span><span class="st">"gbm_full"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(train_errors_with, label<span class="op">=</span><span class="st">"gbm_early_stopping"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">"Boosting Iterations"</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">"RMSE (Training)"</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yscale(<span class="st">"log"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"Training Error"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(test_errors_without, label<span class="op">=</span><span class="st">"gbm_full"</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(test_errors_with, label<span class="op">=</span><span class="st">"gbm_early_stopping"</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">"Boosting Iterations"</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">"RMSE (Test)"</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yscale(<span class="st">"log"</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"Test Error"</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>training_times <span class="op">=</span> [training_time_full, training_time_early_stopping]</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">"gbm_full"</span>, <span class="st">"gbm_early_stopping"</span>]</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>bars <span class="op">=</span> axes[<span class="dv">2</span>].bar(labels, training_times)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">"Training Time (s)"</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bar, n_estimators <span class="kw">in</span> <span class="bu">zip</span>(bars, [n_estimators_full, estimators_early_stopping]):</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    height <span class="op">=</span> bar.get_height()</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].text(</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        bar.get_x() <span class="op">+</span> bar.get_width() <span class="op">/</span> <span class="dv">2</span>,</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        height <span class="op">+</span> <span class="fl">0.001</span>,</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"Estimators: </span><span class="sc">{</span>n_estimators<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        ha<span class="op">=</span><span class="st">"center"</span>,</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        va<span class="op">=</span><span class="st">"bottom"</span>,</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Gradient_Boosting_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The difference in training error between the <code>gbm_full</code> and the <code>gbm_early_stopping</code> stems from the fact that<br>
<code>gbm_early_stopping</code> sets aside <code>validation_fraction</code> of the training data as an internal validation set.</p>
<p>Early stopping is decided based on this internal validation score.</p>
<p><strong>Benefits of Using Early Stopping in Boosting:</strong></p>
<ul>
<li><p><strong>Preventing Overfitting</strong><br>
Early stopping helps avoid overfitting by monitoring the test error.<br>
When the error stabilizes or starts increasing, training stops — resulting in better generalization to unseen data.</p></li>
<li><p><strong>Improving Training Efficiency</strong><br>
Models with early stopping often require <strong>fewer estimators</strong> while achieving similar accuracy.<br>
This reduces training time significantly compared to training without early stopping.</p></li>
</ul>
</section>
<section id="effect-of-learning-rate-on-cross-validation-error" class="level4" data-number="9.5.1.3">
<h4 data-number="9.5.1.3" class="anchored" data-anchor-id="effect-of-learning-rate-on-cross-validation-error"><span class="header-section-number">9.5.1.3</span> Effect of Learning Rate on Cross-Validation Error</h4>
<p>The learning rate (<code>learning_rate</code>) determines how much each new tree contributes to the overall model. A <strong>smaller learning rate</strong> results in slower learning and often requires more trees to achieve good performance. A <strong>larger learning rate</strong> speeds up learning but increases the risk of overfitting.</p>
<p>Finding the optimal learning rate involves balancing: - <strong>High learning rate</strong> → faster convergence, but higher risk of overfitting<br>
- <strong>Low learning rate</strong> → better generalization, but requires more trees and longer training time</p>
<p>Cross-validation helps identify the learning rate that minimizes prediction error while ensuring model stability.</p>
<div id="3553d5cc" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_models():</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create 9 evenly spaced values between 0.2 and 1.0</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    learning_rates <span class="op">=</span> np.linspace(<span class="fl">0.2</span>, <span class="fl">1.0</span>, <span class="dv">9</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> learning_rate <span class="kw">in</span> learning_rates:</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Round to 2 decimal places for clean keys</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        lr_rounded <span class="op">=</span> <span class="bu">round</span>(learning_rate, <span class="dv">2</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>lr_rounded<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        models[key] <span class="op">=</span> GradientBoostingRegressor(learning_rate<span class="op">=</span>lr_rounded, random_state<span class="op">=</span><span class="dv">1</span>, loss<span class="op">=</span><span class="st">'huber'</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> models</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate a given model using cross-validation</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_model(model, X, y):</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define the evaluation procedure</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    cv <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate the model and collect the results</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.sqrt(<span class="op">-</span>cross_val_score(model, X, y, scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>, cv<span class="op">=</span>cv, n_jobs<span class="op">=-</span><span class="dv">1</span>))</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scores</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co"># get the models to evaluate</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> get_models()</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate the models and store results</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>results, names <span class="op">=</span> <span class="bu">list</span>(), <span class="bu">list</span>()</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>mean_scores <span class="op">=</span> []  <span class="co"># Track mean scores separately</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, model <span class="kw">in</span> models.items():</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate the model</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> evaluate_model(model, X_train_final, y_train)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># store the results</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    results.append(scores)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    names.append(name)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate and store mean score</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    mean_score <span class="op">=</span> np.mean(scores)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    mean_scores.append(mean_score)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># summarize the performance along the way</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'&gt;</span><span class="sc">%s</span><span class="st"> </span><span class="sc">%.1f</span><span class="st"> (</span><span class="sc">%.1f</span><span class="st">)'</span> <span class="op">%</span> (name, mean_score, np.std(scores)))</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="co"># plot model performance for comparison</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>plt.boxplot(results, labels<span class="op">=</span>names, showmeans<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross validation error'</span>, fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Learning rate'</span>, fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Performance by Learning Rate'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the best model using the saved mean scores</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>best_index <span class="op">=</span> np.argmin(mean_scores)</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>best_lr <span class="op">=</span> names[best_index]</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> mean_scores[best_index]</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight the best model on the plot</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>best_index<span class="op">+</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>plt.text(best_index<span class="op">+</span><span class="fl">1.2</span>, <span class="bu">min</span>(mean_scores)<span class="op">*</span><span class="fl">0.95</span>, </span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>         <span class="ss">f'Best: </span><span class="sc">{</span>best_lr<span class="sc">}</span><span class="ss"> (RMSE: </span><span class="sc">{</span>best_score<span class="sc">:.1f}</span><span class="ss">)'</span>, </span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">'red'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the best model information</span></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Best model: </span><span class="sc">{</span>best_lr<span class="sc">}</span><span class="ss"> with RMSE: </span><span class="sc">{</span>best_score<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt;0.20 4193.7 (301.2)
&gt;0.30 3740.3 (306.3)
&gt;0.40 3630.0 (212.2)
&gt;0.50 3529.6 (181.5)
&gt;0.60 3650.2 (169.0)
&gt;0.70 3644.7 (142.5)
&gt;0.80 3908.9 (260.6)
&gt;0.90 3968.7 (201.1)
&gt;1.00 4208.3 (368.5)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Gradient_Boosting_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Best model: 0.50 with RMSE: 3529.563</code></pre>
</div>
</div>
</section>
<section id="learning-rate-and-number-of-trees-are-closely-linked" class="level4" data-number="9.5.1.4">
<h4 data-number="9.5.1.4" class="anchored" data-anchor-id="learning-rate-and-number-of-trees-are-closely-linked"><span class="header-section-number">9.5.1.4</span> Learning Rate and Number of Trees Are Closely Linked</h4>
<p>The <strong>learning rate</strong> and <strong>number of trees</strong> (<code>n_estimators</code>) are tightly coupled hyperparameters in gradient boosting. Their balance plays a key role in model performance and overfitting control.</p>
<ul>
<li>A <strong>lower learning rate</strong> slows the learning process, requiring <strong>more trees</strong> to achieve strong performance.</li>
<li>A <strong>higher learning rate</strong> speeds up training but may cause the model to <strong>overfit</strong> if not regularized properly.</li>
</ul>
<p>⚠️ A high learning rate with too few trees can lead to poor generalization, while a very low learning rate with too many trees may improve accuracy but increase training time significantly.</p>
<p><strong>Best practice:</strong> Use a low to moderate learning rate (e.g., <code>0.01</code>–<code>0.1</code>) combined with <strong>early stopping</strong> to find the optimal number of trees.</p>
</section>
<section id="effect-of-stochastic-gradient-boosting-on-cross-validation-error" class="level4" data-number="9.5.1.5">
<h4 data-number="9.5.1.5" class="anchored" data-anchor-id="effect-of-stochastic-gradient-boosting-on-cross-validation-error"><span class="header-section-number">9.5.1.5</span> Effect of Stochastic Gradient Boosting on Cross-Validation Error</h4>
<p><strong>Stochastic Gradient Boosting</strong> enhances generalization by introducing randomness into the model-building process. Two key hyperparameters that control this are <code>subsample</code> and <code>max_features</code>, and they operate on <strong>different dimensions</strong> of the data:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 20%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Applies To</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>subsample</code></td>
<td>Rows (data points)</td>
<td>Randomly samples a fraction of the training data for each tree</td>
</tr>
<tr class="even">
<td><code>max_features</code></td>
<td>Columns (features)</td>
<td>Randomly samples a fraction of the features for each tree or split</td>
</tr>
</tbody>
</table>
<p>By tuning these parameters, we can reduce overfitting and increase model robustness. However, setting them too low may lead to underfitting due to insufficient information per tree.</p>
<div id="e06e6340" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> make_scorer, mean_squared_error</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define model</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GradientBoostingRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, max_depth<span class="op">=</span><span class="dv">4</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define param grid</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'subsample'</span>: np.linspace(<span class="fl">0.2</span>, <span class="fl">1.0</span>, <span class="dv">9</span>),</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: np.linspace(<span class="fl">0.2</span>, <span class="fl">1.0</span>, <span class="dv">9</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># RMSE scoring</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>scorer <span class="op">=</span> make_scorer(mean_squared_error, greater_is_better<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Grid search</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>model, param_grid<span class="op">=</span>param_grid,</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>                    scoring<span class="op">=</span>scorer, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>grid.fit(X_train_final, y_train)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame from results</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(grid.cv_results_)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>results_df[<span class="st">'mean_rmse'</span>] <span class="op">=</span> np.sqrt(<span class="op">-</span>results_df[<span class="st">'mean_test_score'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 5 folds for each of 81 candidates, totalling 405 fits</code></pre>
</div>
</div>
<div id="9cd37cec" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Round subsample and max_features to 2 decimal places for display</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>results_df[<span class="st">'subsample'</span>] <span class="op">=</span> results_df[<span class="st">'param_subsample'</span>].astype(<span class="bu">float</span>).<span class="bu">round</span>(<span class="dv">2</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>results_df[<span class="st">'max_features'</span>] <span class="op">=</span> results_df[<span class="st">'param_max_features'</span>].astype(<span class="bu">float</span>).<span class="bu">round</span>(<span class="dv">2</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Then pivot using the rounded values</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>heatmap_data <span class="op">=</span> results_df.pivot(index<span class="op">=</span><span class="st">'subsample'</span>, columns<span class="op">=</span><span class="st">'max_features'</span>, values<span class="op">=</span><span class="st">'mean_rmse'</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot heatmap</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">9</span>))</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>sns.heatmap(heatmap_data, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">".3f"</span>, cmap<span class="op">=</span><span class="st">"YlGnBu"</span>, cbar_kws<span class="op">=</span>{<span class="st">'label'</span>: <span class="st">'CV RMSE'</span>})</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Grid Search: CV RMSE by Subsample and Max Features'</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Subsample'</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Max Features'</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the location (subsample, max_features) of the minimum RMSE</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>min_rmse <span class="op">=</span> heatmap_data.<span class="bu">min</span>().<span class="bu">min</span>()</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>best_location <span class="op">=</span> heatmap_data.stack().idxmin()  <span class="co"># returns a tuple: (subsample, max_features)</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best RMSE: </span><span class="sc">{</span>min_rmse<span class="sc">:.3f}</span><span class="ss"> at subsample = </span><span class="sc">{</span>best_location[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">, max_features = </span><span class="sc">{</span>best_location[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Gradient_Boosting_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Best RMSE: 3748.534 at subsample = 0.5, max_features = 0.7</code></pre>
</div>
</div>
</section>
<section id="effect-of-tree-complexity-on-cross-validation-error-not-tuned-here" class="level4" data-number="9.5.1.6">
<h4 data-number="9.5.1.6" class="anchored" data-anchor-id="effect-of-tree-complexity-on-cross-validation-error-not-tuned-here"><span class="header-section-number">9.5.1.6</span> Effect of Tree Complexity on Cross-Validation Error (Not Tuned Here)</h4>
<p><strong>Tree complexity</strong> controls how expressive and flexible each individual tree in the gradient boosting ensemble can be. While deeper and more complex trees can capture intricate patterns in the data, they are also more prone to overfitting, especially when combined with many trees.</p>
<p>Key parameters include:</p>
<ul>
<li><code>max_depth</code>: Limits the depth of each tree. Shallower trees (e.g., depth 3–6) are preferred for reducing overfitting.</li>
<li><code>min_samples_split</code>: Specifies the minimum number of samples required to split an internal node. Higher values make the tree more conservative.</li>
<li><code>min_samples_leaf</code>: Sets the minimum number of samples required to be at a leaf node. This also helps smooth the model and avoid capturing noise.</li>
</ul>
<p>These parameters influence the bias-variance trade-off by adjusting how expressive each tree can be.</p>
<p>Since we have already discussed and tuned these parameters in earlier lessons (decision trees and random forests), we will <strong>not tune them again here</strong>.</p>
</section>
<section id="loss-function-loss" class="level4" data-number="9.5.1.7">
<h4 data-number="9.5.1.7" class="anchored" data-anchor-id="loss-function-loss"><span class="header-section-number">9.5.1.7</span> Loss Function (<code>loss</code>)</h4>
<p>In gradient boosting, the loss function determines how the model measures prediction errors and guides the optimization process during training. Here’s a breakdown of common loss functions for regression and classification tasks:</p>
<ul>
<li><strong>Regression</strong>:
<ul>
<li><code>squared_error</code>: Penalizes larger errors more heavily; sensitive to outliers. <em>(Default for regression)</em></li>
<li><code>absolute_error</code>: Penalizes all errors equally; more robust to outliers.</li>
<li><code>huber</code>: Combines squared and absolute error; less sensitive to outliers than <code>squared_error</code> and smoother than <code>absolute_error</code>.</li>
</ul></li>
<li><strong>Classification</strong>:
<ul>
<li><code>log_loss</code>: Also known as logistic loss or deviance; commonly used for binary and multiclass classification.</li>
<li><code>exponential</code>: Used by AdaBoost; heavily penalizes misclassified points, making it more sensitive to outliers.</li>
</ul></li>
</ul>
<p>Choosing an appropriate loss function ensures the model is optimized for the specific structure and goals of the problem.</p>
</section>
</section>
<section id="joint-hyperparameter-optimization" class="level3" data-number="9.5.2">
<h3 data-number="9.5.2" class="anchored" data-anchor-id="joint-hyperparameter-optimization"><span class="header-section-number">9.5.2</span> Joint Hyperparameter Optimization</h3>
<p>Since the optimal values of hyperparameters are often interdependent, they should be tuned <strong>together</strong> rather than in isolation to achieve the best performance.Next we will simultaneously tune multiple core hyperparameters to find the best combination for overall model performance.</p>
<section id="using-bayessearchcv-for-hyperparameter-tuning" class="level4" data-number="9.5.2.1">
<h4 data-number="9.5.2.1" class="anchored" data-anchor-id="using-bayessearchcv-for-hyperparameter-tuning"><span class="header-section-number">9.5.2.1</span> Using <code>BayesSearchCV</code> for Hyperparameter Tuning</h4>
<p>We can use <code>BayesSearchCV</code> with early stopping to <strong>simultaneously tune multiple hyperparameters</strong> in a more efficient and automated way.</p>
<div id="65054b73" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># time the search</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the search space</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>search_space <span class="op">=</span> {</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: Real(<span class="fl">0.01</span>, <span class="fl">0.8</span>, prior<span class="op">=</span><span class="st">'log-uniform'</span>),  <span class="co"># Prefer lower rates</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: Integer(<span class="dv">4</span>, <span class="dv">32</span>),          <span class="co"># Shallow trees to prevent overfitting</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: Integer(<span class="dv">2</span>, <span class="dv">100</span>), <span class="co"># Regularize splits</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: Integer(<span class="dv">1</span>, <span class="dv">30</span>),  <span class="co"># Regularize leaves</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'subsample'</span>: Real(<span class="fl">0.1</span>, <span class="fl">1.0</span>),         <span class="co"># Stochastic sampling</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: Categorical([</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'sqrt'</span>, <span class="st">'log2'</span>, <span class="va">None</span>,  <span class="co"># String options</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>  <span class="co"># Fractional options (discrete)</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    ])  <span class="co"># Feature sampling</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>model_with_early_stopping <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">10000</span>,  <span class="co"># Start with a large number of trees</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    validation_fraction<span class="op">=</span><span class="fl">0.1</span>,  <span class="co"># Reserve 10% of training data for validation</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    n_iter_no_change<span class="op">=</span><span class="dv">10</span>,      <span class="co"># Stop after 20 rounds of no improvement</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    tol<span class="op">=</span><span class="fl">0.001</span>,           <span class="co"># Tolerance for early stopping</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the search</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>bayes_cv  <span class="op">=</span> BayesSearchCV(</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    model_with_early_stopping,</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    search_space,</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">50</span>,  <span class="co"># Number of iterations</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>,</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">5</span>,  <span class="co"># Cross-validation folds</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,  <span class="co"># Use all available cores</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Verbosity level</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>  <span class="co"># For reproducibility</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>bayes_cv.fit(X_train_final, y_train)</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Stop the timer</span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>end <span class="op">=</span> time.time()</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate elapsed time</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>elapsed_time <span class="op">=</span> (end <span class="op">-</span> start)<span class="op">/</span><span class="dv">60</span>  <span class="co"># Convert to minutes</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Print elapsed time</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Elapsed time for Bayesian optimization with early stopping: </span><span class="sc">{</span>elapsed_time<span class="sc">:.2f}</span><span class="ss"> minutes"</span>)</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the best parameters and score</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> bayes_cv.best_params_</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> np.sqrt(<span class="op">-</span>bayes_cv.best_score_)</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best Parameters: </span><span class="sc">{</span>best_params<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best CV RMSE: </span><span class="sc">{</span>best_score<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Fitting 5 folds for each of 1 candidates, totalling 5 fits
Elapsed time for Bayesian optimization with early stopping: 10.00 minutes
Best Parameters: OrderedDict({'learning_rate': 0.01, 'max_depth': 32, 'max_features': 0.6, 'min_samples_leaf': 1, 'min_samples_split': 2, 'subsample': 0.302790110221997})
Best CV RMSE: 3142.483</code></pre>
</div>
</div>
<div id="a7695e37" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the optimization results</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>plot_convergence(bayes_cv.optimizer_results_)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Gradient_Boosting_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="6c1d110e" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the objective function</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>plot_objective(bayes_cv.optimizer_results_[<span class="dv">0</span>])</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Bayesian Optimization: Objective Function'</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Parameter Value'</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Objective Value (RMSE)'</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Gradient_Boosting_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="hyperparameter-optimization-with-optuna" class="level4" data-number="9.5.2.2">
<h4 data-number="9.5.2.2" class="anchored" data-anchor-id="hyperparameter-optimization-with-optuna"><span class="header-section-number">9.5.2.2</span> Hyperparameter Optimization with Optuna</h4>
<div id="681af397" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(trial):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define hyperparameters to optimize</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> {</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">'learning_rate'</span>: trial.suggest_float(<span class="st">'learning_rate'</span>, <span class="fl">0.01</span>, <span class="fl">0.8</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">'max_depth'</span>: trial.suggest_int(<span class="st">'max_depth'</span>, <span class="dv">4</span>, <span class="dv">32</span>),</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">'min_samples_split'</span>: trial.suggest_int(<span class="st">'min_samples_split'</span>, <span class="dv">2</span>, <span class="dv">100</span>),</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">'min_samples_leaf'</span>: trial.suggest_int(<span class="st">'min_samples_leaf'</span>, <span class="dv">1</span>, <span class="dv">30</span>),</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">'subsample'</span>: trial.suggest_float(<span class="st">'subsample'</span>, <span class="fl">0.1</span>, <span class="fl">1.0</span>),</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">'max_features'</span>: trial.suggest_categorical(</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">'max_features'</span>, </span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>            [<span class="st">'sqrt'</span>, <span class="st">'log2'</span>, <span class="va">None</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>]</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">'n_iter_no_change'</span>: <span class="dv">10</span>,  <span class="co"># Stop if no improvement in 50 rounds</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">'validation_fraction'</span>: <span class="fl">0.1</span>,  <span class="co"># 10% of training data for validation</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">'tol'</span>: <span class="fl">0.001</span>,  <span class="co"># Tolerance for early stopping</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">'n_estimators'</span>: <span class="dv">10000</span>,  <span class="co"># Start with a large number of trees</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">'random_state'</span>: <span class="dv">42</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the model with the parameters, adding early stopping</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>params</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> GradientBoostingRegressor(<span class="op">**</span>params)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define the evaluation procedure</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    cv <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform cross-validation</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> cross_val_score(model, X_train_final, y_train, cv<span class="op">=</span>cv, scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(np.sqrt(<span class="op">-</span>scores)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="0fb4bec0" class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a study object</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study(direction<span class="op">=</span><span class="st">"minimize"</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>study.optimize(objective, n_trials<span class="op">=</span><span class="dv">70</span>, timeout<span class="op">=</span><span class="dv">600</span>)  <span class="co"># 50 trials or 10 min</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Stop the timer</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>end <span class="op">=</span> time.time()</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate elapsed time</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>elapsed_time <span class="op">=</span> (end <span class="op">-</span> start)<span class="op">/</span><span class="dv">60</span>  <span class="co"># Convert to minutes</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Elapsed time for Optuna optimization: </span><span class="sc">{</span>elapsed_time<span class="sc">:.2f}</span><span class="ss"> minutes"</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the best parameters and score</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>best_params_optuna <span class="op">=</span> study.best_params</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>best_score_optuna <span class="op">=</span> study.best_value</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best Parameters: </span><span class="sc">{</span>best_params_optuna<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best CV RMSE: </span><span class="sc">{</span>best_score_optuna<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[I 2025-05-08 11:07:29,094] A new study created in memory with name: no-name-84a8f2a9-1577-4460-b0fa-ddf16779a99d
[I 2025-05-08 11:07:30,013] Trial 0 finished with value: 6402.677876019744 and parameters: {'learning_rate': 0.035073388881695464, 'max_depth': 30, 'min_samples_split': 46, 'min_samples_leaf': 26, 'subsample': 0.1177589817042714, 'max_features': 0.8}. Best is trial 0 with value: 6402.677876019744.
[I 2025-05-08 11:07:31,621] Trial 1 finished with value: 5817.716936023959 and parameters: {'learning_rate': 0.01478220034105809, 'max_depth': 10, 'min_samples_split': 36, 'min_samples_leaf': 21, 'subsample': 0.15640172328228477, 'max_features': 0.2}. Best is trial 1 with value: 5817.716936023959.
[I 2025-05-08 11:07:33,048] Trial 2 finished with value: 4639.695965157119 and parameters: {'learning_rate': 0.03787502889529579, 'max_depth': 7, 'min_samples_split': 42, 'min_samples_leaf': 16, 'subsample': 0.4581205314481861, 'max_features': 'log2'}. Best is trial 2 with value: 4639.695965157119.
[I 2025-05-08 11:07:33,778] Trial 3 finished with value: 3888.640108615851 and parameters: {'learning_rate': 0.18174509299300778, 'max_depth': 17, 'min_samples_split': 78, 'min_samples_leaf': 14, 'subsample': 0.5795668347371169, 'max_features': 0.1}. Best is trial 3 with value: 3888.640108615851.
[I 2025-05-08 11:07:34,527] Trial 4 finished with value: 5637.440291328167 and parameters: {'learning_rate': 0.1380752210199847, 'max_depth': 19, 'min_samples_split': 90, 'min_samples_leaf': 28, 'subsample': 0.23325390500017087, 'max_features': 0.7}. Best is trial 3 with value: 3888.640108615851.
[I 2025-05-08 11:08:15,193] Trial 5 finished with value: 4349.349334283657 and parameters: {'learning_rate': 0.012573622109971098, 'max_depth': 11, 'min_samples_split': 59, 'min_samples_leaf': 29, 'subsample': 0.8662915238008442, 'max_features': None}. Best is trial 3 with value: 3888.640108615851.
[I 2025-05-08 11:08:16,690] Trial 6 finished with value: 3538.201600150815 and parameters: {'learning_rate': 0.20506627729566493, 'max_depth': 22, 'min_samples_split': 29, 'min_samples_leaf': 1, 'subsample': 0.7644887945675666, 'max_features': 0.4}. Best is trial 6 with value: 3538.201600150815.
[I 2025-05-08 11:08:40,300] Trial 7 finished with value: 4222.0969339973235 and parameters: {'learning_rate': 0.013692887049401567, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 29, 'subsample': 0.7647042705751687, 'max_features': 0.7}. Best is trial 6 with value: 3538.201600150815.
[I 2025-05-08 11:08:40,844] Trial 8 finished with value: 5987.876440565123 and parameters: {'learning_rate': 0.07695507379792597, 'max_depth': 4, 'min_samples_split': 47, 'min_samples_leaf': 30, 'subsample': 0.2875628082638476, 'max_features': 0.1}. Best is trial 6 with value: 3538.201600150815.
[I 2025-05-08 11:08:41,963] Trial 9 finished with value: 5930.236597113848 and parameters: {'learning_rate': 0.03751759109605761, 'max_depth': 23, 'min_samples_split': 89, 'min_samples_leaf': 14, 'subsample': 0.12412065494979885, 'max_features': 0.4}. Best is trial 6 with value: 3538.201600150815.
[I 2025-05-08 11:08:43,184] Trial 10 finished with value: 4249.647691193036 and parameters: {'learning_rate': 0.7095372554176897, 'max_depth': 29, 'min_samples_split': 11, 'min_samples_leaf': 2, 'subsample': 0.9605533508261285, 'max_features': 0.4}. Best is trial 6 with value: 3538.201600150815.
[I 2025-05-08 11:08:43,792] Trial 11 finished with value: 3721.412018003831 and parameters: {'learning_rate': 0.30049152507822724, 'max_depth': 18, 'min_samples_split': 70, 'min_samples_leaf': 4, 'subsample': 0.6346922402417411, 'max_features': 0.1}. Best is trial 6 with value: 3538.201600150815.
[I 2025-05-08 11:08:44,200] Trial 12 finished with value: 3992.3681134868325 and parameters: {'learning_rate': 0.3980500012172675, 'max_depth': 25, 'min_samples_split': 24, 'min_samples_leaf': 1, 'subsample': 0.673568941345692, 'max_features': 'sqrt'}. Best is trial 6 with value: 3538.201600150815.
[I 2025-05-08 11:08:45,596] Trial 13 finished with value: 3745.0231375714984 and parameters: {'learning_rate': 0.2685874728814681, 'max_depth': 18, 'min_samples_split': 64, 'min_samples_leaf': 7, 'subsample': 0.46246048029745535, 'max_features': 0.6}. Best is trial 6 with value: 3538.201600150815.
[I 2025-05-08 11:08:46,415] Trial 14 finished with value: 4496.274561417728 and parameters: {'learning_rate': 0.5835585949486959, 'max_depth': 23, 'min_samples_split': 73, 'min_samples_leaf': 7, 'subsample': 0.6950785249386102, 'max_features': 0.5}. Best is trial 6 with value: 3538.201600150815.
[I 2025-05-08 11:08:50,798] Trial 15 finished with value: 3532.351997763184 and parameters: {'learning_rate': 0.09745878730555008, 'max_depth': 15, 'min_samples_split': 25, 'min_samples_leaf': 6, 'subsample': 0.8242687895348413, 'max_features': 0.9}. Best is trial 15 with value: 3532.351997763184.
[I 2025-05-08 11:08:57,644] Trial 16 finished with value: 3712.683820416855 and parameters: {'learning_rate': 0.08381685480205123, 'max_depth': 14, 'min_samples_split': 24, 'min_samples_leaf': 9, 'subsample': 0.8376225281110229, 'max_features': 0.9}. Best is trial 15 with value: 3532.351997763184.
[I 2025-05-08 11:09:08,351] Trial 17 finished with value: 3878.4225603020363 and parameters: {'learning_rate': 0.13671338236610117, 'max_depth': 26, 'min_samples_split': 25, 'min_samples_leaf': 10, 'subsample': 0.9981438257438544, 'max_features': 0.9}. Best is trial 15 with value: 3532.351997763184.
[I 2025-05-08 11:09:12,426] Trial 18 finished with value: 3258.6536376666345 and parameters: {'learning_rate': 0.054298295833061686, 'max_depth': 15, 'min_samples_split': 32, 'min_samples_leaf': 5, 'subsample': 0.8705317335073132, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.
[I 2025-05-08 11:09:14,871] Trial 19 finished with value: 3299.8057419420575 and parameters: {'learning_rate': 0.05307237898099509, 'max_depth': 15, 'min_samples_split': 15, 'min_samples_leaf': 5, 'subsample': 0.8923755963839227, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.
[I 2025-05-08 11:09:22,909] Trial 20 finished with value: 3558.1548802314146 and parameters: {'learning_rate': 0.024374699406738868, 'max_depth': 8, 'min_samples_split': 13, 'min_samples_leaf': 11, 'subsample': 0.9312911241232275, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.
[I 2025-05-08 11:09:24,627] Trial 21 finished with value: 3344.1360986274194 and parameters: {'learning_rate': 0.05641174821037, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 5, 'subsample': 0.8413643277096872, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.
[I 2025-05-08 11:09:26,856] Trial 22 finished with value: 3346.290872146259 and parameters: {'learning_rate': 0.05576222901451701, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 4, 'subsample': 0.8899852988884344, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.
[I 2025-05-08 11:09:30,886] Trial 23 finished with value: 3682.572188125887 and parameters: {'learning_rate': 0.053636711663155555, 'max_depth': 20, 'min_samples_split': 12, 'min_samples_leaf': 18, 'subsample': 0.7533492065291382, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.
[I 2025-05-08 11:09:36,517] Trial 24 finished with value: 3324.1356706329725 and parameters: {'learning_rate': 0.023446567191311374, 'max_depth': 16, 'min_samples_split': 17, 'min_samples_leaf': 5, 'subsample': 0.8976166400827316, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.
[I 2025-05-08 11:09:48,021] Trial 25 finished with value: 3477.877832840085 and parameters: {'learning_rate': 0.02114726465183569, 'max_depth': 12, 'min_samples_split': 33, 'min_samples_leaf': 11, 'subsample': 0.925317351270135, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.
[I 2025-05-08 11:09:51,103] Trial 26 finished with value: 3543.4521345710564 and parameters: {'learning_rate': 0.022722851512216705, 'max_depth': 16, 'min_samples_split': 16, 'min_samples_leaf': 8, 'subsample': 0.4238807472210913, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.
[I 2025-05-08 11:09:54,116] Trial 27 finished with value: 3234.0013427023514 and parameters: {'learning_rate': 0.0283832867263049, 'max_depth': 21, 'min_samples_split': 18, 'min_samples_leaf': 3, 'subsample': 0.5528086908100067, 'max_features': 0.3}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:10:00,202] Trial 28 finished with value: 3508.54835466015 and parameters: {'learning_rate': 0.046930229833580556, 'max_depth': 27, 'min_samples_split': 38, 'min_samples_leaf': 3, 'subsample': 0.5371065252040612, 'max_features': None}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:10:02,298] Trial 29 finished with value: 5004.42107047821 and parameters: {'learning_rate': 0.031927515240275074, 'max_depth': 20, 'min_samples_split': 52, 'min_samples_leaf': 21, 'subsample': 0.39582140307352154, 'max_features': 'log2'}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:10:03,666] Trial 30 finished with value: 4324.686970916422 and parameters: {'learning_rate': 0.07372182576303132, 'max_depth': 21, 'min_samples_split': 54, 'min_samples_leaf': 12, 'subsample': 0.3360673538685467, 'max_features': 0.5}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:10:11,068] Trial 31 finished with value: 3484.424151386007 and parameters: {'learning_rate': 0.028420783094759556, 'max_depth': 13, 'min_samples_split': 18, 'min_samples_leaf': 5, 'subsample': 0.7793810894071969, 'max_features': 0.8}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:10:18,699] Trial 32 finished with value: 3291.106187349627 and parameters: {'learning_rate': 0.015581688189605165, 'max_depth': 17, 'min_samples_split': 19, 'min_samples_leaf': 3, 'subsample': 0.7003002093460347, 'max_features': 0.3}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:10:22,168] Trial 33 finished with value: 3325.6927625938033 and parameters: {'learning_rate': 0.01749834216320663, 'max_depth': 9, 'min_samples_split': 34, 'min_samples_leaf': 2, 'subsample': 0.5707418196808265, 'max_features': 0.2}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:10:27,550] Trial 34 finished with value: 3243.0601177315375 and parameters: {'learning_rate': 0.01860910283944705, 'max_depth': 32, 'min_samples_split': 40, 'min_samples_leaf': 3, 'subsample': 0.519199766539664, 'max_features': 0.3}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:10:36,064] Trial 35 finished with value: 3258.8499173154833 and parameters: {'learning_rate': 0.010243612859981979, 'max_depth': 30, 'min_samples_split': 43, 'min_samples_leaf': 1, 'subsample': 0.623847104984506, 'max_features': 0.3}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:10:40,944] Trial 36 finished with value: 4733.54257152398 and parameters: {'learning_rate': 0.010552375239783827, 'max_depth': 31, 'min_samples_split': 44, 'min_samples_leaf': 24, 'subsample': 0.4832981433471137, 'max_features': 'sqrt'}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:10:55,316] Trial 37 finished with value: 3330.1658474531714 and parameters: {'learning_rate': 0.011303182659432521, 'max_depth': 32, 'min_samples_split': 40, 'min_samples_leaf': 1, 'subsample': 0.6192904496322136, 'max_features': 0.6}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:11:08,984] Trial 38 finished with value: 3657.4081420035955 and parameters: {'learning_rate': 0.01817928172328691, 'max_depth': 29, 'min_samples_split': 47, 'min_samples_leaf': 8, 'subsample': 0.5215847207874392, 'max_features': 0.8}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:11:11,528] Trial 39 finished with value: 3246.3324692045935 and parameters: {'learning_rate': 0.03864849232157439, 'max_depth': 28, 'min_samples_split': 58, 'min_samples_leaf': 2, 'subsample': 0.600994007175315, 'max_features': 0.2}. Best is trial 27 with value: 3234.0013427023514.
[I 2025-05-08 11:11:14,072] Trial 40 finished with value: 3223.908916424979 and parameters: {'learning_rate': 0.04161574178039716, 'max_depth': 27, 'min_samples_split': 61, 'min_samples_leaf': 3, 'subsample': 0.38840542153155744, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.
[I 2025-05-08 11:11:16,764] Trial 41 finished with value: 3262.2664150628366 and parameters: {'learning_rate': 0.040900390028730825, 'max_depth': 28, 'min_samples_split': 57, 'min_samples_leaf': 3, 'subsample': 0.39000153769914736, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.
[I 2025-05-08 11:11:18,027] Trial 42 finished with value: 3536.1048831110393 and parameters: {'learning_rate': 0.0373869523765574, 'max_depth': 25, 'min_samples_split': 63, 'min_samples_leaf': 3, 'subsample': 0.22414441067489127, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.
[I 2025-05-08 11:11:20,006] Trial 43 finished with value: 3725.757422297686 and parameters: {'learning_rate': 0.028313793871723975, 'max_depth': 31, 'min_samples_split': 79, 'min_samples_leaf': 6, 'subsample': 0.33977679725241394, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.
[I 2025-05-08 11:11:21,612] Trial 44 finished with value: 3401.5545066433783 and parameters: {'learning_rate': 0.06669527321370879, 'max_depth': 24, 'min_samples_split': 48, 'min_samples_leaf': 4, 'subsample': 0.48718984715872016, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.
[I 2025-05-08 11:11:25,264] Trial 45 finished with value: 3635.4930797564184 and parameters: {'learning_rate': 0.0961058290284304, 'max_depth': 27, 'min_samples_split': 30, 'min_samples_leaf': 7, 'subsample': 0.5935867492892174, 'max_features': 0.7}. Best is trial 40 with value: 3223.908916424979.
[I 2025-05-08 11:11:26,855] Trial 46 finished with value: 3297.8485341280875 and parameters: {'learning_rate': 0.04204785692165129, 'max_depth': 32, 'min_samples_split': 61, 'min_samples_leaf': 2, 'subsample': 0.5167714519960092, 'max_features': 'log2'}. Best is trial 40 with value: 3223.908916424979.
[I 2025-05-08 11:11:28,066] Trial 47 finished with value: 3484.8710748592084 and parameters: {'learning_rate': 0.031490405131204394, 'max_depth': 29, 'min_samples_split': 68, 'min_samples_leaf': 1, 'subsample': 0.20255680688589325, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.
[I 2025-05-08 11:11:29,916] Trial 48 finished with value: 5067.6932650619365 and parameters: {'learning_rate': 0.13316769809796983, 'max_depth': 22, 'min_samples_split': 81, 'min_samples_leaf': 18, 'subsample': 0.31148972269250275, 'max_features': None}. Best is trial 40 with value: 3223.908916424979.
[I 2025-05-08 11:11:33,570] Trial 49 finished with value: 3519.5312534022605 and parameters: {'learning_rate': 0.020121256369718885, 'max_depth': 27, 'min_samples_split': 37, 'min_samples_leaf': 6, 'subsample': 0.43073583851342867, 'max_features': 0.1}. Best is trial 40 with value: 3223.908916424979.
[I 2025-05-08 11:11:38,292] Trial 50 finished with value: 4947.269431523515 and parameters: {'learning_rate': 0.01401704983937264, 'max_depth': 30, 'min_samples_split': 56, 'min_samples_leaf': 13, 'subsample': 0.2680374684875922, 'max_features': 0.4}. Best is trial 40 with value: 3223.908916424979.
[I 2025-05-08 11:11:42,794] Trial 51 finished with value: 3223.184387572671 and parameters: {'learning_rate': 0.026412005607226167, 'max_depth': 30, 'min_samples_split': 42, 'min_samples_leaf': 2, 'subsample': 0.65936728502067, 'max_features': 0.3}. Best is trial 51 with value: 3223.184387572671.
[I 2025-05-08 11:11:47,351] Trial 52 finished with value: 3215.28175313329 and parameters: {'learning_rate': 0.026909377347601734, 'max_depth': 31, 'min_samples_split': 30, 'min_samples_leaf': 2, 'subsample': 0.6638539074973421, 'max_features': 0.3}. Best is trial 52 with value: 3215.28175313329.
[I 2025-05-08 11:11:51,995] Trial 53 finished with value: 3251.8248895524334 and parameters: {'learning_rate': 0.029697182156018534, 'max_depth': 31, 'min_samples_split': 51, 'min_samples_leaf': 2, 'subsample': 0.6712307703997793, 'max_features': 0.2}. Best is trial 52 with value: 3215.28175313329.
[I 2025-05-08 11:12:03,513] Trial 54 finished with value: 3397.6799413335893 and parameters: {'learning_rate': 0.01643638758357564, 'max_depth': 28, 'min_samples_split': 67, 'min_samples_leaf': 3, 'subsample': 0.5673934972094804, 'max_features': 0.7}. Best is trial 52 with value: 3215.28175313329.
[I 2025-05-08 11:12:10,304] Trial 55 finished with value: 3260.8279178294406 and parameters: {'learning_rate': 0.026362863130942972, 'max_depth': 30, 'min_samples_split': 27, 'min_samples_leaf': 4, 'subsample': 0.7153814770338778, 'max_features': 0.3}. Best is trial 52 with value: 3215.28175313329.
[I 2025-05-08 11:12:12,294] Trial 56 finished with value: 3192.9791005502093 and parameters: {'learning_rate': 0.03446122898299111, 'max_depth': 25, 'min_samples_split': 22, 'min_samples_leaf': 1, 'subsample': 0.6677588032576087, 'max_features': 'sqrt'}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:12:13,927] Trial 57 finished with value: 3269.998258903558 and parameters: {'learning_rate': 0.033447603873263364, 'max_depth': 26, 'min_samples_split': 21, 'min_samples_leaf': 1, 'subsample': 0.669383626299888, 'max_features': 'sqrt'}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:12:17,716] Trial 58 finished with value: 3457.122430183835 and parameters: {'learning_rate': 0.019680446694802907, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 8, 'subsample': 0.6399247722507589, 'max_features': 'sqrt'}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:12:20,307] Trial 59 finished with value: 3247.949537776105 and parameters: {'learning_rate': 0.04792784341369982, 'max_depth': 32, 'min_samples_split': 29, 'min_samples_leaf': 4, 'subsample': 0.7314524099096733, 'max_features': 'sqrt'}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:12:51,502] Trial 60 finished with value: 4050.280737550346 and parameters: {'learning_rate': 0.013362536181651579, 'max_depth': 25, 'min_samples_split': 21, 'min_samples_leaf': 26, 'subsample': 0.7856311694762674, 'max_features': 0.6}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:12:56,091] Trial 61 finished with value: 3249.9684847878243 and parameters: {'learning_rate': 0.025286437646103447, 'max_depth': 28, 'min_samples_split': 35, 'min_samples_leaf': 2, 'subsample': 0.5916004227404528, 'max_features': 0.3}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:13:01,774] Trial 62 finished with value: 3328.4822044271473 and parameters: {'learning_rate': 0.037252636958638265, 'max_depth': 29, 'min_samples_split': 60, 'min_samples_leaf': 2, 'subsample': 0.5509938512062063, 'max_features': 0.5}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:13:12,800] Trial 63 finished with value: 3490.4301182885783 and parameters: {'learning_rate': 0.04480753984980458, 'max_depth': 26, 'min_samples_split': 40, 'min_samples_leaf': 6, 'subsample': 0.6501238052668078, 'max_features': 0.9}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:13:15,262] Trial 64 finished with value: 3540.8369062274187 and parameters: {'learning_rate': 0.0651918522339886, 'max_depth': 5, 'min_samples_split': 50, 'min_samples_leaf': 1, 'subsample': 0.6047519410402453, 'max_features': 0.3}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:13:20,471] Trial 65 finished with value: 3294.353628295925 and parameters: {'learning_rate': 0.02196859905859851, 'max_depth': 23, 'min_samples_split': 98, 'min_samples_leaf': 3, 'subsample': 0.5103851076291236, 'max_features': 0.3}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:13:29,893] Trial 66 finished with value: 3326.4094166662317 and parameters: {'learning_rate': 0.03527083345645445, 'max_depth': 31, 'min_samples_split': 23, 'min_samples_leaf': 4, 'subsample': 0.687510121444283, 'max_features': 0.4}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:13:44,533] Trial 67 finished with value: 3548.591414006273 and parameters: {'learning_rate': 0.026050250772540858, 'max_depth': 29, 'min_samples_split': 74, 'min_samples_leaf': 16, 'subsample': 0.8079428723552805, 'max_features': 0.3}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:13:51,439] Trial 68 finished with value: 3228.1005724210245 and parameters: {'learning_rate': 0.018440524791042332, 'max_depth': 28, 'min_samples_split': 45, 'min_samples_leaf': 5, 'subsample': 0.7401907036295743, 'max_features': 0.1}. Best is trial 56 with value: 3192.9791005502093.
[I 2025-05-08 11:13:58,247] Trial 69 finished with value: 3264.495066558836 and parameters: {'learning_rate': 0.015627930519455754, 'max_depth': 19, 'min_samples_split': 31, 'min_samples_leaf': 5, 'subsample': 0.6551335248065494, 'max_features': 0.1}. Best is trial 56 with value: 3192.9791005502093.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Elapsed time for Optuna optimization: 6.49 minutes
Best Parameters: {'learning_rate': 0.03446122898299111, 'max_depth': 25, 'min_samples_split': 22, 'min_samples_leaf': 1, 'subsample': 0.6677588032576087, 'max_features': 'sqrt'}
Best CV RMSE: 3192.979</code></pre>
</div>
</div>
<div id="e9ad2bb2" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>optuna.visualization.plot_optimization_history(study).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
<div id="567e894a" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>optuna.visualization.plot_param_importances(study).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
<div id="d452edcb" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>optuna.visualization.plot_parallel_coordinate(study).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="independent-study" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="independent-study"><span class="header-section-number">9.6</span> Independent Study</h2>
<p>In this notebook, we used the car dataset for a guided regression task to illustrate the core hyperparameters in gradient boosting and how to tune them to balance bias and variance.</p>
<p>For your practice, please work with the <strong>diabetes dataset</strong> and complete the following:</p>
<ul>
<li>Fit a baseline gradient boosting classifier.</li>
<li>Tune key hyperparameters: <code>learning_rate</code>, <code>n_estimators</code>, <code>max_depth</code>, and <code>subsample</code>.</li>
<li>Use <strong>early stopping</strong> to determine the optimal number of trees.</li>
<li>Compare training and test <code>roc_auc</code> before and after tuning.</li>
<li>Visualize the learning curve (training vs test error across iterations).</li>
<li>Summarize what combination of hyperparameters yielded the best performance and how they impacted bias and variance.</li>
</ul>
<p>Feel free to use <code>GridSearchCV</code>, <code>BayesSearchCV</code>, or other tuning as you prefer.</p>
<div id="7b50e2fd" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> pd.read_csv(<span class="st">'./Datasets/diabetes_train.csv'</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> pd.read_csv(<span class="st">'./Datasets/diabetes_test.csv'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="41f5f44f" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train.shape, test.shape)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>train.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(614, 9) (154, 9)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="51">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Pregnancies</th>
<th data-quarto-table-cell-role="th">Glucose</th>
<th data-quarto-table-cell-role="th">BloodPressure</th>
<th data-quarto-table-cell-role="th">SkinThickness</th>
<th data-quarto-table-cell-role="th">Insulin</th>
<th data-quarto-table-cell-role="th">BMI</th>
<th data-quarto-table-cell-role="th">DiabetesPedigreeFunction</th>
<th data-quarto-table-cell-role="th">Age</th>
<th data-quarto-table-cell-role="th">Outcome</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>2</td>
<td>88</td>
<td>74</td>
<td>19</td>
<td>53</td>
<td>29.0</td>
<td>0.229</td>
<td>22</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>2</td>
<td>129</td>
<td>84</td>
<td>0</td>
<td>0</td>
<td>28.0</td>
<td>0.284</td>
<td>27</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0</td>
<td>102</td>
<td>78</td>
<td>40</td>
<td>90</td>
<td>34.5</td>
<td>0.238</td>
<td>24</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>0</td>
<td>123</td>
<td>72</td>
<td>0</td>
<td>0</td>
<td>36.3</td>
<td>0.258</td>
<td>52</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1</td>
<td>144</td>
<td>82</td>
<td>46</td>
<td>180</td>
<td>46.1</td>
<td>0.335</td>
<td>46</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="68b1eac9" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># check the distribution of the target variable</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>train[<span class="st">'Outcome'</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>Outcome
0    0.662866
1    0.337134
Name: proportion, dtype: float64</code></pre>
</div>
</div>
<div id="646083d6" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the features and target variable</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train.drop(columns<span class="op">=</span>[<span class="st">'Outcome'</span>])</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train[<span class="st">'Outcome'</span>]</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test.drop(columns<span class="op">=</span>[<span class="st">'Outcome'</span>])</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> test[<span class="st">'Outcome'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="foundational-paper" class="level2" data-number="9.7">
<h2 data-number="9.7" class="anchored" data-anchor-id="foundational-paper"><span class="header-section-number">9.7</span> Foundational Paper</h2>
<p>The foundational paper introducing “vanilla” Gradient Boosting is:</p>
<p><strong>Greedy Function Approximation: A Gradient Boosting Machine</strong><br>
<em>Author</em>: Jerome H. Friedman<br>
<em>Published in</em>: <em>The Annals of Statistics</em>, 2001, Vol. 29, No.&nbsp;5, pp.&nbsp;1189–1232<br>
<em>DOI</em>: <a href="https://doi.org/10.1214/aos/1013203451">10.1214/aos/1013203451</a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./adaboost.html" class="pagination-link" aria-label="Adaptive Boosting">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Adaptive Boosting</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./XGBoost.html" class="pagination-link" aria-label="XGBoost">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">XGBoost</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>