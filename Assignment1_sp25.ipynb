{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7fc45304",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Assignment 1\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    self-contained: true\n",
    "    number-sections: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672791b9",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity. \n",
    "\n",
    "2. Write your code in the **Code cells** and your answers in the **Markdown cells** of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\n",
    "\n",
    "3. Use [Quarto](https://quarto.org/docs/output-formats/html-basics.html) to render the **.ipynb** file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: `quarto render filename.ipynb --to html`. Submit the HTML file.\n",
    "\n",
    "4. The assignment is worth 100 points, and is due on **Thursday, 11th April 2025 at 11:59 pm**. \n",
    "\n",
    "5. **Five points are properly formatting the assignment**. The breakdown is as follows:\n",
    "    - Must be an HTML file rendered using Quarto **(2 points)**. *If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.* \n",
    "    - There aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) **(1 point)**\n",
    "    - Final answers to each question are written in the Markdown cells. **(1 point)**\n",
    "    - There is no piece of unnecessary / redundant code, and no unnecessary / redundant text. **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae5120",
   "metadata": {},
   "source": [
    "## 1) Bias-Variance Trade-off for Regression **(50 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988fbe55",
   "metadata": {},
   "source": [
    "The main goal of this question is to understand and visualize the bias-variance trade-off in a regression model by performing repetitive simulations.\n",
    "\n",
    "The conceptual clarity about bias and variance will help with the main logic behind creating many models that will come up later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dce7cd",
   "metadata": {},
   "source": [
    "### a) Define the True Relationship (Signal)\n",
    "\n",
    "First, you need to implement the underlying true relationship (Signal) you want to sample data from. Assume that the function is the [Bukin function](https://www.sfu.ca/~ssurjano/bukin6.html). Implement it as a user-defined function and run it with the test cases below to make sure it is implemented correctly. **(5 points)**\n",
    "\n",
    "**Note:** It would be more useful to have only one input to the function. You can treat the input as an array of two elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Bukin(np.array([1,2]))) # The output should be 141.177\n",
    "print(Bukin(np.array([6,-4]))) # The output should be 208.966\n",
    "print(Bukin(np.array([0,1]))) # The output should be 100.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099bf25c",
   "metadata": {},
   "source": [
    "### b) Generate Test Set (No Noise)\n",
    "\n",
    "Create a noiseless test set with 100 observations from the underlying function to isolate bias and variance. Remember how the test dataset is supposed to be sampled for bias-variance calculations. **No loops are allowed for this question - `.apply` should be very useful and actually simpler to use.** **(5 points)**\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "- The first predictor, $x_1$, comes from a uniform distribution between -15 and -5. ($U[-15, -5]$)\n",
    "- The second predictor, $x_2$, comes from a uniform distribution between -3 and 3. ($U[-3, 3]$)\n",
    "- Use `np.random.seed(100)` for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07670584",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "Create an empty DataFrame with columns named **degree**, **bias_sq** and **var**. This will be useful to store the analysis results in this question. **(3 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235129a9",
   "metadata": {},
   "source": [
    "### d) Generate Training Set (With Noise)\n",
    "\n",
    "Sample 100 training datasets to calculate the bias and variance of a Linear Regression model that predicts data coming from the underlying Bukin function. You need to repeat this process with polynomial transformations from degree 1 (which is the original predictors) to degree 7. For each degree, store the `degree`, `bias-squared` and `variance` values in the DataFrame. **(25 points)**\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- For a linear regression model, `bias` refers to squared bias\n",
    "- Assume that the noise in the population is a zero-mean Gaussian with a standard deviation of 10. ($N(0,10)$)\n",
    "- Keep the training data size the same as the test data size.\n",
    "- You need both the interactions and the higher-order transformations in your polynomial predictors.\n",
    "- For $i^{th}$ training dataset, you can consider using `np.random.seed(i)` for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa921c",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "Using the results stored in the DataFrame, plot the (1) expected mean squared error, (2) expected squared bias, (3) expected variance, and (4) the expected sum of squared bias, variance and noise variance *(i.e., summation of 2, 3, and noise variance)*, against the `degree` of the predictors in the model. **(5 points)** \n",
    "\n",
    "Make sure you add a legend to label the three lineplots. **(2 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7a6716",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "* What is the `degree` of the optimal model? **(2 point)** \n",
    "* What are the squared bias, variance and mean squared error for that degree? **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfeaf3",
   "metadata": {},
   "source": [
    "## 2) Low-Bias-Low-Variance Model via Regularization (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b5efe5",
   "metadata": {},
   "source": [
    "The main goal of this question is to further reduce the total error by regularization - in other words, to implement the low-bias-low-variance model for the underlying function and the data coming from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358665d",
   "metadata": {},
   "source": [
    "### a) \n",
    "\n",
    "First of all, explain why it is not guaranteed for the optimal model (with the optimal `degree`) in Question 1 to be the low-bias-low-variance model. **(2 points)** Why would regularization be necessary to achieve that model? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac93ce1",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "Before repeating the process in Question 1, you should see from the figure in 1e and the results in 1f that there is no point in trying some degrees again with regularization. Find out these degrees and explain why you should not use them for this question, **considering how regularization affects the bias and the variance of a model.** **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef62211",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "Repeat 1c and 1d with Ridge regularization. **Exclude the degrees you found in 2b and also degree 7**. Use 5-fold cross-validation (CV) to tune the model hyperparameter and use `neg_root_mean_squared_error` as the scoring metric. **(10 points)**\n",
    "\n",
    "Consider hyperparamter values in the range \\[1, 100\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27831de",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Repeat part 1e with Ridge regularization, using the results from 2c. **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50095e50",
   "metadata": {},
   "source": [
    "### e) \n",
    "\n",
    "What is the degree of the optimal Ridge Regression model? **(3 point)** What are the bias-squared, variance and total error values for that degree? **(3 point)** How do they compare to the Linear Regression model results? **(4 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7d0a6",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "Is the regularization successful in reducing the total error of the regression model? **(2 points)** Explain the results in 2e in terms of how bias and variance change with regularization. **(3 points)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
