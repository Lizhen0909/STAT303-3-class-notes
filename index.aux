\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\newlabel{preface}{{}{3}{}{chapter*.2}{}}
\@writefile{toc}{\contentsline {chapter}{Preface}{3}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{\numberline {I}Bias \& Variance; KNN}{4}{part.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}KNN}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{knn}{{1}{5}{KNN}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}KNN for regression}{5}{section.1.1}\protected@file@percent }
\newlabel{knn-for-regression}{{1.1}{5}{KNN for regression}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Feature Scaling in KNN}{7}{section.1.2}\protected@file@percent }
\newlabel{feature-scaling-in-knn}{{1.2}{7}{Feature Scaling in KNN}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Hyperparameters in KNN}{8}{section.1.3}\protected@file@percent }
\newlabel{hyperparameters-in-knn}{{1.3}{8}{Hyperparameters in KNN}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Tuning \emph  {k} in KNN}{9}{subsection.1.3.1}\protected@file@percent }
\newlabel{tuning-k-in-knn}{{1.3.1}{9}{\texorpdfstring {Tuning \emph {k} in KNN}{Tuning k in KNN}}{subsection.1.3.1}{}}
\gdef \LT@i {\LT@entry 
    {3}{27.90001pt}\LT@entry 
    {1}{40.03201pt}\LT@entry 
    {3}{56.12851pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{55.27501pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Tuning Other KNN Hyperparameters}{12}{subsection.1.3.2}\protected@file@percent }
\newlabel{tuning-other-knn-hyperparameters}{{1.3.2}{12}{Tuning Other KNN Hyperparameters}{subsection.1.3.2}{}}
\gdef \LT@ii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{115.48843pt}\LT@entry 
    {1}{145.07533pt}\LT@entry 
    {1}{91.13565pt}\LT@entry 
    {1}{120.38309pt}\LT@entry 
    {3}{260.0722pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\gdef \LT@iii {\LT@entry 
    {3}{16.95001pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{115.48843pt}\LT@entry 
    {1}{145.07533pt}\LT@entry 
    {1}{91.13565pt}\LT@entry 
    {1}{120.38309pt}\LT@entry 
    {3}{267.98904pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Hyperparameter Tuning}{16}{section.1.4}\protected@file@percent }
\newlabel{hyperparameter-tuning}{{1.4}{16}{Hyperparameter Tuning}{section.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.0.1}RandomizedSearchCV}{16}{subsubsection.1.4.0.1}\protected@file@percent }
\newlabel{randomizedsearchcv}{{1.4.0.1}{16}{RandomizedSearchCV}{subsubsection.1.4.0.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.0.2}BayesSearchCV}{19}{subsubsection.1.4.0.2}\protected@file@percent }
\newlabel{bayessearchcv}{{1.4.0.2}{19}{BayesSearchCV}{subsubsection.1.4.0.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bias-variance tradeoff}{23}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{bias-variance-tradeoff}{{2}{23}{Bias-variance tradeoff}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Simple model (Less flexible)}{23}{section.2.1}\protected@file@percent }
\newlabel{simple-model-less-flexible}{{2.1}{23}{Simple model (Less flexible)}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Complex model (more flexible)}{26}{section.2.2}\protected@file@percent }
\newlabel{complex-model-more-flexible}{{2.2}{26}{Complex model (more flexible)}{section.2.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Basic Hyperparameter tuning}{29}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{basic-hyperparameter-tuning}{{3}{29}{Basic Hyperparameter tuning}{chapter.3}{}}
\gdef \LT@iv {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{39.37502pt}\LT@entry 
    {1}{40.03201pt}\LT@entry 
    {3}{48.58395pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}\href  {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{\texttt  {GridSearchCV}}}{30}{section.3.1}\protected@file@percent }
\newlabel{gridsearchcv}{{3.1}{30}{\texorpdfstring {\href {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{\texttt {GridSearchCV}}}{GridSearchCV}}{section.3.1}{}}
\gdef \LT@v {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{81.10545pt}\LT@entry 
    {1}{110.69234pt}\LT@entry 
    {1}{86.00009pt}\LT@entry 
    {3}{243.31866pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\gdef \LT@vi {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{81.10545pt}\LT@entry 
    {1}{110.69234pt}\LT@entry 
    {1}{86.00009pt}\LT@entry 
    {3}{243.31866pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}\href  {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html}{\texttt  {RandomizedSearchCV()}}}{33}{section.3.2}\protected@file@percent }
\newlabel{randomizedsearchcv-1}{{3.2}{33}{\texorpdfstring {\href {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html}{\texttt {RandomizedSearchCV()}}}{RandomizedSearchCV()}}{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}\href  {https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html}{\texttt  {BayesSearchCV()}}}{35}{section.3.3}\protected@file@percent }
\newlabel{bayessearchcv-1}{{3.3}{35}{\texorpdfstring {\href {https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html}{\texttt {BayesSearchCV()}}}{BayesSearchCV()}}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Diagonosis of cross-validated score optimization}{38}{subsection.3.3.1}\protected@file@percent }
\newlabel{diagonosis-of-cross-validated-score-optimization}{{3.3.1}{38}{Diagonosis of cross-validated score optimization}{subsection.3.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Live monitoring of cross-validated score}{43}{subsection.3.3.2}\protected@file@percent }
\newlabel{live-monitoring-of-cross-validated-score}{{3.3.2}{43}{Live monitoring of cross-validated score}{subsection.3.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}\href  {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html}{\texttt  {cross\_validate()}}}{44}{section.3.4}\protected@file@percent }
\newlabel{cross_validate}{{3.4}{44}{\texorpdfstring {\href {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html}{\texttt {cross\_validate()}}}{cross\_validate()}}{section.3.4}{}}
\gdef \LT@vii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{27.81181pt}\LT@entry 
    {1}{26.95772pt}\LT@entry 
    {1}{22.95001pt}\LT@entry 
    {1}{50.4783pt}\LT@entry 
    {1}{31.1625pt}\LT@entry 
    {1}{25.75322pt}\LT@entry 
    {1}{44.92665pt}\LT@entry 
    {1}{46.98526pt}\LT@entry 
    {1}{39.68161pt}\LT@entry 
    {1}{49.1205pt}\LT@entry 
    {1}{36.09001pt}\LT@entry 
    {1}{22.3368pt}\LT@entry 
    {1}{30.86687pt}\LT@entry 
    {1}{34.6233pt}}
\@writefile{toc}{\contentsline {part}{\numberline {II}Tree based models}{48}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Regression trees}{49}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{regression-trees}{{4}{49}{Regression trees}{chapter.4}{}}
\gdef \LT@viii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Native Support for Missing Values}{50}{section.4.1}\protected@file@percent }
\newlabel{native-support-for-missing-values}{{4.1}{50}{Native Support for Missing Values}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Build a regression tree using mileage as the solo predictor}{51}{subsection.4.1.1}\protected@file@percent }
\newlabel{build-a-regression-tree-using-mileage-as-the-solo-predictor}{{4.1.1}{51}{Build a regression tree using mileage as the solo predictor}{subsection.4.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Building regression trees}{52}{section.4.2}\protected@file@percent }
\newlabel{building-regression-trees}{{4.2}{52}{Building regression trees}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Using only mileage feature}{52}{subsection.4.2.1}\protected@file@percent }
\newlabel{using-only-mileage-feature}{{4.2.1}{52}{Using only mileage feature}{subsection.4.2.1}{}}
\gdef \LT@ix {\LT@entry 
    {3}{27.90001pt}\LT@entry 
    {3}{44.8719pt}\LT@entry 
    {3}{61.5378pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{55.27501pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Using mileage and brand as predictors}{54}{subsection.4.2.2}\protected@file@percent }
\newlabel{using-mileage-and-brand-as-predictors}{{4.2.2}{54}{Using mileage and brand as predictors}{subsection.4.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Using all predictors}{56}{subsection.4.2.3}\protected@file@percent }
\newlabel{using-all-predictors}{{4.2.3}{56}{Using all predictors}{subsection.4.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Key Hyperparameters in Decision Tree}{58}{section.4.3}\protected@file@percent }
\newlabel{key-hyperparameters-in-decision-tree}{{4.3}{58}{Key Hyperparameters in Decision Tree}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Underfitting}{58}{subsection.4.3.1}\protected@file@percent }
\newlabel{underfitting}{{4.3.1}{58}{Underfitting}{subsection.4.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Overfitting}{58}{subsection.4.3.2}\protected@file@percent }
\newlabel{overfitting}{{4.3.2}{58}{Overfitting}{subsection.4.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}\texttt  {max\_depth}}{58}{subsection.4.3.3}\protected@file@percent }
\newlabel{max_depth}{{4.3.3}{58}{\texorpdfstring {\texttt {max\_depth}}{max\_depth}}{subsection.4.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}\texttt  {min\_samples\_split}}{58}{subsection.4.3.4}\protected@file@percent }
\newlabel{min_samples_split}{{4.3.4}{58}{\texorpdfstring {\texttt {min\_samples\_split}}{min\_samples\_split}}{subsection.4.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}\texttt  {min\_samples\_leaf}}{59}{subsection.4.3.5}\protected@file@percent }
\newlabel{min_samples_leaf}{{4.3.5}{59}{\texorpdfstring {\texttt {min\_samples\_leaf}}{min\_samples\_leaf}}{subsection.4.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}\texttt  {max\_features}}{59}{subsection.4.3.6}\protected@file@percent }
\newlabel{max_features}{{4.3.6}{59}{\texorpdfstring {\texttt {max\_features}}{max\_features}}{subsection.4.3.6}{}}
\gdef \LT@x {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{159.46362pt}\LT@entry 
    {1}{165.24522pt}\LT@entry 
    {1}{175.36302pt}\LT@entry 
    {1}{197.8981pt}\LT@entry 
    {1}{201.91676pt}\LT@entry 
    {3}{257.27994pt}\LT@entry 
    {1}{21.13231pt}\LT@entry 
    {1}{92.42775pt}\LT@entry 
    {1}{99.4029pt}\LT@entry 
    {1}{85.87965pt}\LT@entry 
    {1}{85.87965pt}\LT@entry 
    {1}{85.87965pt}\LT@entry 
    {1}{85.87965pt}\LT@entry 
    {1}{85.87965pt}\LT@entry 
    {1}{85.20074pt}\LT@entry 
    {1}{74.31645pt}\LT@entry 
    {1}{75.2916pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}Output feature importance}{62}{subsection.4.3.7}\protected@file@percent }
\newlabel{output-feature-importance}{{4.3.7}{62}{Output feature importance}{subsection.4.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Cost-Complexity Pruning (\texttt  {ccp\_alpha})}{64}{section.4.4}\protected@file@percent }
\newlabel{cost-complexity-pruning-ccp_alpha}{{4.4}{64}{\texorpdfstring {Cost-Complexity Pruning (\texttt {ccp\_alpha})}{Cost-Complexity Pruning (ccp\_alpha)}}{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Key Idea}{64}{subsection.4.4.1}\protected@file@percent }
\newlabel{key-idea}{{4.4.1}{64}{Key Idea}{subsection.4.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Parameter: \texttt  {ccp\_alpha} in scikit-learn}{64}{subsection.4.4.2}\protected@file@percent }
\newlabel{parameter-ccp_alpha-in-scikit-learn}{{4.4.2}{64}{\texorpdfstring {Parameter: \texttt {ccp\_alpha} in scikit-learn}{Parameter: ccp\_alpha in scikit-learn}}{subsection.4.4.2}{}}
\gdef \LT@xi {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{68.94pt}\LT@entry 
    {1}{71.05334pt}\LT@entry 
    {1}{67.4508pt}\LT@entry 
    {1}{81.11641pt}\LT@entry 
    {1}{71.38185pt}\LT@entry 
    {1}{75.3786pt}\LT@entry 
    {1}{78.35701pt}\LT@entry 
    {1}{88.4091pt}\LT@entry 
    {1}{61.932pt}\LT@entry 
    {1}{89.33984pt}\LT@entry 
    {1}{21.13231pt}\LT@entry 
    {1}{90.68669pt}\LT@entry 
    {1}{98.87729pt}\LT@entry 
    {1}{95.7018pt}\LT@entry 
    {1}{90.21585pt}\LT@entry 
    {1}{91.278pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{55.27501pt}}
\gdef \LT@xii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{27.81181pt}\LT@entry 
    {1}{26.95772pt}\LT@entry 
    {1}{22.95001pt}\LT@entry 
    {1}{50.4783pt}\LT@entry 
    {1}{31.1625pt}\LT@entry 
    {1}{25.75322pt}\LT@entry 
    {1}{44.92665pt}\LT@entry 
    {1}{46.98526pt}\LT@entry 
    {1}{39.68161pt}\LT@entry 
    {1}{49.1205pt}\LT@entry 
    {1}{36.09001pt}\LT@entry 
    {1}{22.3368pt}\LT@entry 
    {1}{30.86687pt}\LT@entry 
    {1}{34.6233pt}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Classification trees}{69}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{classification-trees}{{5}{69}{Classification trees}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Building a Classification Tree}{70}{section.5.1}\protected@file@percent }
\newlabel{building-a-classification-tree}{{5.1}{70}{Building a Classification Tree}{section.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Pre-pruning: Hyperparameters Tuning}{72}{section.5.2}\protected@file@percent }
\newlabel{pre-pruning-hyperparameters-tuning}{{5.2}{72}{Pre-pruning: Hyperparameters Tuning}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Gini or entropy}{74}{subsection.5.2.1}\protected@file@percent }
\newlabel{gini-or-entropy}{{5.2.1}{74}{Gini or entropy}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Post-pruning: Cost complexity pruning}{76}{section.5.3}\protected@file@percent }
\newlabel{post-pruning-cost-complexity-pruning}{{5.3}{76}{Post-pruning: Cost complexity pruning}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}step 1: calculate the cost complexity pruning path}{77}{subsection.5.3.1}\protected@file@percent }
\newlabel{step-1-calculate-the-cost-complexity-pruning-path}{{5.3.1}{77}{step 1: calculate the cost complexity pruning path}{subsection.5.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}step 2: Create trees with different ccp\_alpha values and evaluate their performance}{77}{subsection.5.3.2}\protected@file@percent }
\newlabel{step-2-create-trees-with-different-ccp_alpha-values-and-evaluate-their-performance}{{5.3.2}{77}{step 2: Create trees with different ccp\_alpha values and evaluate their performance}{subsection.5.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Step 3: Visualize the results}{77}{subsection.5.3.3}\protected@file@percent }
\newlabel{step-3-visualize-the-results}{{5.3.3}{77}{Step 3: Visualize the results}{subsection.5.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Step 4: Create the final model with the optimal alpha}{80}{subsection.5.3.4}\protected@file@percent }
\newlabel{step-4-create-the-final-model-with-the-optimal-alpha}{{5.3.4}{80}{Step 4: Create the final model with the optimal alpha}{subsection.5.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Feature Importance in Decision Trees}{81}{section.5.4}\protected@file@percent }
\newlabel{feature-importance-in-decision-trees}{{5.4}{81}{Feature Importance in Decision Trees}{section.5.4}{}}
\gdef \LT@xiii {\LT@entry 
    {3}{16.95001pt}\LT@entry 
    {3}{50.4783pt}\LT@entry 
    {1}{60.78285pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Do We Need Feature Selection or Regularization with Tree Models?}{83}{subsection.5.4.1}\protected@file@percent }
\newlabel{do-we-need-feature-selection-or-regularization-with-tree-models}{{5.4.1}{83}{Do We Need Feature Selection or Regularization with Tree Models?}{subsection.5.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Bottom Line}{84}{subsection.5.4.2}\protected@file@percent }
\newlabel{bottom-line}{{5.4.2}{84}{Bottom Line}{subsection.5.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Next Lecture}{84}{section.5.5}\protected@file@percent }
\newlabel{next-lecture}{{5.5}{84}{Next Lecture}{section.5.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bagging}{85}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{bagging}{{6}{85}{Bagging}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Bagging: A Variance Reduction Technique}{85}{section.6.1}\protected@file@percent }
\newlabel{bagging-a-variance-reduction-technique}{{6.1}{85}{Bagging: A Variance Reduction Technique}{section.6.1}{}}
\gdef \LT@xiv {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Bagging Regression Trees}{86}{section.6.2}\protected@file@percent }
\newlabel{bagging-regression-trees}{{6.2}{86}{Bagging Regression Trees}{section.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Bagging Doesn't Reduce Bias}{90}{section.6.3}\protected@file@percent }
\newlabel{bagging-doesnt-reduce-bias}{{6.3}{90}{Bagging Doesn't Reduce Bias}{section.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Model Performance vs.\nobreakspace  {}Number of Trees}{92}{section.6.4}\protected@file@percent }
\newlabel{model-performance-vs.-number-of-trees}{{6.4}{92}{Model Performance vs.~Number of Trees}{section.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}OOB Sample and OOB Score in Bagging}{94}{section.6.5}\protected@file@percent }
\newlabel{oob-sample-and-oob-score-in-bagging}{{6.5}{94}{OOB Sample and OOB Score in Bagging}{section.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}What is an OOB Sample?}{94}{subsection.6.5.1}\protected@file@percent }
\newlabel{what-is-an-oob-sample}{{6.5.1}{94}{What is an OOB Sample?}{subsection.6.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}What is OOB Score?}{95}{subsection.6.5.2}\protected@file@percent }
\newlabel{what-is-oob-score}{{6.5.2}{95}{What is OOB Score?}{subsection.6.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Bagging Hyperparameter Tuning}{97}{section.6.6}\protected@file@percent }
\newlabel{bagging-hyperparameter-tuning}{{6.6}{97}{Bagging Hyperparameter Tuning}{section.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Tuning with Cross-Validation}{98}{subsection.6.6.1}\protected@file@percent }
\newlabel{tuning-with-cross-validation}{{6.6.1}{98}{Tuning with Cross-Validation}{subsection.6.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Tuning with Out-of-Bag (OOB) Score}{99}{subsection.6.6.2}\protected@file@percent }
\newlabel{tuning-with-out-of-bag-oob-score}{{6.6.2}{99}{Tuning with Out-of-Bag (OOB) Score}{subsection.6.6.2}{}}
\gdef \LT@xv {\LT@entry 
    {1}{92.415pt}\LT@entry 
    {1}{181.74005pt}\LT@entry 
    {1}{172.65012pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.3}Comparing Hyperparameter Tuning: Cross-Validation vs.\nobreakspace  {}OOB Score}{102}{subsection.6.6.3}\protected@file@percent }
\newlabel{comparing-hyperparameter-tuning-cross-validation-vs.-oob-score}{{6.6.3}{102}{Comparing Hyperparameter Tuning: Cross-Validation vs.~OOB Score}{subsection.6.6.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.3.1}✅ Best Practices for Imbalanced Classification}{102}{subsubsection.6.6.3.1}\protected@file@percent }
\newlabel{best-practices-for-imbalanced-classification}{{6.6.3.1}{102}{✅ Best Practices for Imbalanced Classification}{subsubsection.6.6.3.1}{}}
\gdef \LT@xvi {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{27.81181pt}\LT@entry 
    {1}{26.95772pt}\LT@entry 
    {1}{22.95001pt}\LT@entry 
    {1}{50.4783pt}\LT@entry 
    {1}{31.1625pt}\LT@entry 
    {1}{25.75322pt}\LT@entry 
    {1}{44.92665pt}\LT@entry 
    {1}{46.98526pt}\LT@entry 
    {1}{39.68161pt}\LT@entry 
    {1}{49.1205pt}\LT@entry 
    {1}{36.09001pt}\LT@entry 
    {1}{22.3368pt}\LT@entry 
    {1}{30.86687pt}\LT@entry 
    {1}{34.6233pt}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Bagging Classification Trees}{103}{section.6.7}\protected@file@percent }
\newlabel{bagging-classification-trees}{{6.7}{103}{Bagging Classification Trees}{section.6.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Random Forests}{105}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{random-forests}{{7}{105}{Random Forests}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Motivation: Bagging Revisited}{105}{section.7.1}\protected@file@percent }
\newlabel{motivation-bagging-revisited}{{7.1}{105}{Motivation: Bagging Revisited}{section.7.1}{}}
\gdef \LT@xvii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Let's build a single decision tree and output its performance}{106}{subsection.7.1.1}\protected@file@percent }
\newlabel{lets-build-a-single-decision-tree-and-output-its-performance}{{7.1.1}{106}{Let's build a single decision tree and output its performance}{subsection.7.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Let's Build a Bagging Tree with Bootstrap Sampling to Reduce Variance}{107}{subsection.7.1.2}\protected@file@percent }
\newlabel{lets-build-a-bagging-tree-with-bootstrap-sampling-to-reduce-variance}{{7.1.2}{107}{Let's Build a Bagging Tree with Bootstrap Sampling to Reduce Variance}{subsection.7.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Let's Build a Bagging Tree Without Bootstrap Sampling}{108}{subsection.7.1.3}\protected@file@percent }
\newlabel{lets-build-a-bagging-tree-without-bootstrap-sampling}{{7.1.3}{108}{Let's Build a Bagging Tree Without Bootstrap Sampling}{subsection.7.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.4}❓ Why Does Bagging Without Bootstrap Perform Worse?}{110}{subsection.7.1.4}\protected@file@percent }
\newlabel{why-does-bagging-without-bootstrap-perform-worse}{{7.1.4}{110}{❓ Why Does Bagging Without Bootstrap Perform Worse?}{subsection.7.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.5}❓ Why Can Bagging Without Bootstrap Still Show Slight Improvement?}{110}{subsection.7.1.5}\protected@file@percent }
\newlabel{why-can-bagging-without-bootstrap-still-show-slight-improvement}{{7.1.5}{110}{❓ Why Can Bagging Without Bootstrap Still Show Slight Improvement?}{subsection.7.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Random Forest}{110}{section.7.2}\protected@file@percent }
\newlabel{random-forest}{{7.2}{110}{Random Forest}{section.7.2}{}}
\gdef \LT@xviii {\LT@entry 
    {1}{78.71349pt}\LT@entry 
    {1}{180.17252pt}\LT@entry 
    {1}{187.82887pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Idea Behind Random Forest}{111}{subsection.7.2.1}\protected@file@percent }
\newlabel{idea-behind-random-forest}{{7.2.1}{111}{Idea Behind Random Forest}{subsection.7.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Key Hyperparameter Comparison}{111}{subsection.7.2.2}\protected@file@percent }
\newlabel{key-hyperparameter-comparison}{{7.2.2}{111}{Key Hyperparameter Comparison}{subsection.7.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Let's Build a Random Forest Model Using the Default Settings}{111}{subsection.7.2.3}\protected@file@percent }
\newlabel{lets-build-a-random-forest-model-using-the-default-settings}{{7.2.3}{111}{Let's Build a Random Forest Model Using the Default Settings}{subsection.7.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Let's Build a Random Forest Model with \texttt  {sqrt} max\_features}{112}{subsection.7.2.4}\protected@file@percent }
\newlabel{lets-build-a-random-forest-model-with-sqrt-max_features}{{7.2.4}{112}{\texorpdfstring {Let's Build a Random Forest Model with \texttt {sqrt} max\_features}{Let's Build a Random Forest Model with sqrt max\_features}}{subsection.7.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Let's Explore How \texttt  {max\_features} Affects Performance}{114}{section.7.3}\protected@file@percent }
\newlabel{lets-explore-how-max_features-affects-performance}{{7.3}{114}{\texorpdfstring {Let's Explore How \texttt {max\_features} Affects Performance}{Let's Explore How max\_features Affects Performance}}{section.7.3}{}}
\gdef \LT@xix {\LT@entry 
    {1}{145.21484pt}\LT@entry 
    {1}{197.58969pt}\LT@entry 
    {1}{103.95549pt}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Other Hyperparameters in Random Forest}{116}{section.7.4}\protected@file@percent }
\newlabel{other-hyperparameters-in-random-forest}{{7.4}{116}{Other Hyperparameters in Random Forest}{section.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Why Bagging Uses Unpruned Trees}{116}{subsection.7.4.1}\protected@file@percent }
\newlabel{why-bagging-uses-unpruned-trees}{{7.4.1}{116}{Why Bagging Uses Unpruned Trees}{subsection.7.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Hyperparameters That Control Tree Complexity in Random Forest}{116}{subsection.7.4.2}\protected@file@percent }
\newlabel{hyperparameters-that-control-tree-complexity-in-random-forest}{{7.4.2}{116}{Hyperparameters That Control Tree Complexity in Random Forest}{subsection.7.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Why Random Forest Often Limits Tree Depth}{117}{subsection.7.4.3}\protected@file@percent }
\newlabel{why-random-forest-often-limits-tree-depth}{{7.4.3}{117}{Why Random Forest Often Limits Tree Depth}{subsection.7.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.4}Let's Tune Multiple Hyperparameters Simultaneously Using Cross-Validation}{117}{subsection.7.4.4}\protected@file@percent }
\newlabel{lets-tune-multiple-hyperparameters-simultaneously-using-cross-validation}{{7.4.4}{117}{Let's Tune Multiple Hyperparameters Simultaneously Using Cross-Validation}{subsection.7.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Feature Importance in Random Forest}{120}{section.7.5}\protected@file@percent }
\newlabel{feature-importance-in-random-forest}{{7.5}{120}{Feature Importance in Random Forest}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}How Feature Importance Is Calculated}{120}{subsection.7.5.1}\protected@file@percent }
\newlabel{how-feature-importance-is-calculated}{{7.5.1}{120}{How Feature Importance Is Calculated}{subsection.7.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Accessing Feature Importances}{121}{subsection.7.5.2}\protected@file@percent }
\newlabel{accessing-feature-importances}{{7.5.2}{121}{Accessing Feature Importances}{subsection.7.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}In Summary}{122}{section.7.6}\protected@file@percent }
\newlabel{in-summary}{{7.6}{122}{In Summary}{section.7.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Next Step}{122}{section.7.7}\protected@file@percent }
\newlabel{next-step}{{7.7}{122}{Next Step}{section.7.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Adaptive Boosting}{124}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{adaptive-boosting}{{8}{124}{Adaptive Boosting}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}What is AdaBoost?}{125}{section.8.1}\protected@file@percent }
\newlabel{what-is-adaboost}{{8.1}{125}{What is AdaBoost?}{section.8.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}AdaBoost Intuition}{125}{section.8.2}\protected@file@percent }
\newlabel{adaboost-intuition}{{8.2}{125}{AdaBoost Intuition}{section.8.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}How AdaBoost Works (High-Level Steps)}{125}{section.8.3}\protected@file@percent }
\newlabel{how-adaboost-works-high-level-steps}{{8.3}{125}{How AdaBoost Works (High-Level Steps)}{section.8.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Key Hyperparameters in AdaBoost}{125}{section.8.4}\protected@file@percent }
\newlabel{key-hyperparameters-in-adaboost}{{8.4}{125}{Key Hyperparameters in AdaBoost}{section.8.4}{}}
\gdef \LT@xx {\LT@entry 
    {1}{90.5507pt}\LT@entry 
    {1}{265.6586pt}\LT@entry 
    {1}{90.5507pt}}
\gdef \LT@xxi {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}AdaBoost for Regression}{126}{section.8.5}\protected@file@percent }
\newlabel{adaboost-for-regression}{{8.5}{126}{AdaBoost for Regression}{section.8.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}Let's build a adaboost regressor model with default settings}{127}{subsection.8.5.1}\protected@file@percent }
\newlabel{lets-build-a-adaboost-regressor-model-with-default-settings}{{8.5.1}{127}{Let's build a adaboost regressor model with default settings}{subsection.8.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.1.1}❓ Why AdaBoost Perform Worse Here}{128}{subsubsection.8.5.1.1}\protected@file@percent }
\newlabel{why-adaboost-perform-worse-here}{{8.5.1.1}{128}{❓ Why AdaBoost Perform Worse Here}{subsubsection.8.5.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}Impact of Tree Depth}{129}{subsection.8.5.2}\protected@file@percent }
\newlabel{impact-of-tree-depth}{{8.5.2}{129}{Impact of Tree Depth}{subsection.8.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}Impact of Learning Rate}{131}{subsection.8.5.3}\protected@file@percent }
\newlabel{impact-of-learning-rate}{{8.5.3}{131}{Impact of Learning Rate}{subsection.8.5.3}{}}
\gdef \LT@xxii {\LT@entry 
    {1}{124.37228pt}\LT@entry 
    {1}{168.41914pt}\LT@entry 
    {1}{153.96858pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.3.1}Effect on Performance}{132}{subsubsection.8.5.3.1}\protected@file@percent }
\newlabel{effect-on-performance}{{8.5.3.1}{132}{Effect on Performance}{subsubsection.8.5.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.4}Impact of Number of Trees in Boosting}{134}{subsection.8.5.4}\protected@file@percent }
\newlabel{impact-of-number-of-trees-in-boosting}{{8.5.4}{134}{Impact of Number of Trees in Boosting}{subsection.8.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.5}Tuning Hyperparameters Simultaneously}{137}{subsection.8.5.5}\protected@file@percent }
\newlabel{tuning-hyperparameters-simultaneously}{{8.5.5}{137}{Tuning Hyperparameters Simultaneously}{subsection.8.5.5}{}}
\gdef \LT@xxiii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{168.16887pt}\LT@entry 
    {1}{116.13449pt}\LT@entry 
    {1}{114.38249pt}\LT@entry 
    {3}{255.21037pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{21.13231pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.5.1}Analyzing \texttt  {BayesSearchCV} Results}{141}{subsubsection.8.5.5.1}\protected@file@percent }
\newlabel{analyzing-bayessearchcv-results}{{8.5.5.1}{141}{\texorpdfstring {Analyzing \texttt {BayesSearchCV} Results}{Analyzing BayesSearchCV Results}}{subsubsection.8.5.5.1}{}}
\gdef \LT@xxiv {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{116.13449pt}\LT@entry 
    {1}{168.16887pt}\LT@entry 
    {1}{114.38249pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.6}Using Optuna for Hyperparameter Tuning}{146}{subsection.8.5.6}\protected@file@percent }
\newlabel{using-optuna-for-hyperparameter-tuning}{{8.5.6}{146}{Using Optuna for Hyperparameter Tuning}{subsection.8.5.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Gradient Boosting}{151}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{gradient-boosting}{{9}{151}{Gradient Boosting}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Gradient Boosting Intuition}{151}{section.9.1}\protected@file@percent }
\newlabel{gradient-boosting-intuition}{{9.1}{151}{Gradient Boosting Intuition}{section.9.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}How Gradient Boosting Works (Regression Example)}{151}{section.9.2}\protected@file@percent }
\newlabel{how-gradient-boosting-works-regression-example}{{9.2}{151}{How Gradient Boosting Works (Regression Example)}{section.9.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Gradient Boosting in Scikit-Learn}{153}{section.9.3}\protected@file@percent }
\newlabel{gradient-boosting-in-scikit-learn}{{9.3}{153}{Gradient Boosting in Scikit-Learn}{section.9.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Core Hyperparameters Categories}{153}{section.9.4}\protected@file@percent }
\newlabel{core-hyperparameters-categories}{{9.4}{153}{Core Hyperparameters Categories}{section.9.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Hyperparameter Tuning}{154}{section.9.5}\protected@file@percent }
\newlabel{hyperparameter-tuning-1}{{9.5}{154}{Hyperparameter Tuning}{section.9.5}{}}
\gdef \LT@xxv {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Individual Hyperparameter Impact Analysis}{155}{subsection.9.5.1}\protected@file@percent }
\newlabel{individual-hyperparameter-impact-analysis}{{9.5.1}{155}{Individual Hyperparameter Impact Analysis}{subsection.9.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1.1}Effect of Number of Trees on Cross-Validation Error}{155}{subsubsection.9.5.1.1}\protected@file@percent }
\newlabel{effect-of-number-of-trees-on-cross-validation-error}{{9.5.1.1}{155}{Effect of Number of Trees on Cross-Validation Error}{subsubsection.9.5.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1.2}Early stopping in Gradient Boosting}{158}{subsubsection.9.5.1.2}\protected@file@percent }
\newlabel{early-stopping-in-gradient-boosting}{{9.5.1.2}{158}{Early stopping in Gradient Boosting}{subsubsection.9.5.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1.3}Effect of Learning Rate on Cross-Validation Error}{161}{subsubsection.9.5.1.3}\protected@file@percent }
\newlabel{effect-of-learning-rate-on-cross-validation-error}{{9.5.1.3}{161}{Effect of Learning Rate on Cross-Validation Error}{subsubsection.9.5.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1.4}Learning Rate and Number of Trees Are Closely Linked}{164}{subsubsection.9.5.1.4}\protected@file@percent }
\newlabel{learning-rate-and-number-of-trees-are-closely-linked}{{9.5.1.4}{164}{Learning Rate and Number of Trees Are Closely Linked}{subsubsection.9.5.1.4}{}}
\gdef \LT@xxvi {\LT@entry 
    {1}{66.41179pt}\LT@entry 
    {1}{98.83429pt}\LT@entry 
    {1}{281.55263pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1.5}Effect of Stochastic Gradient Boosting on Cross-Validation Error}{165}{subsubsection.9.5.1.5}\protected@file@percent }
\newlabel{effect-of-stochastic-gradient-boosting-on-cross-validation-error}{{9.5.1.5}{165}{Effect of Stochastic Gradient Boosting on Cross-Validation Error}{subsubsection.9.5.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1.6}Effect of Tree Complexity on Cross-Validation Error (Not Tuned Here)}{167}{subsubsection.9.5.1.6}\protected@file@percent }
\newlabel{effect-of-tree-complexity-on-cross-validation-error-not-tuned-here}{{9.5.1.6}{167}{Effect of Tree Complexity on Cross-Validation Error (Not Tuned Here)}{subsubsection.9.5.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1.7}Loss Function (\texttt  {loss})}{168}{subsubsection.9.5.1.7}\protected@file@percent }
\newlabel{loss-function-loss}{{9.5.1.7}{168}{\texorpdfstring {Loss Function (\texttt {loss})}{Loss Function (loss)}}{subsubsection.9.5.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.2}Joint Hyperparameter Optimization}{168}{subsection.9.5.2}\protected@file@percent }
\newlabel{joint-hyperparameter-optimization}{{9.5.2}{168}{Joint Hyperparameter Optimization}{subsection.9.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.2.1}Using \texttt  {BayesSearchCV} for Hyperparameter Tuning}{169}{subsubsection.9.5.2.1}\protected@file@percent }
\newlabel{using-bayessearchcv-for-hyperparameter-tuning}{{9.5.2.1}{169}{\texorpdfstring {Using \texttt {BayesSearchCV} for Hyperparameter Tuning}{Using BayesSearchCV for Hyperparameter Tuning}}{subsubsection.9.5.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.2.2}Hyperparameter Optimization with Optuna}{173}{subsubsection.9.5.2.2}\protected@file@percent }
\newlabel{hyperparameter-optimization-with-optuna}{{9.5.2.2}{173}{Hyperparameter Optimization with Optuna}{subsubsection.9.5.2.2}{}}
\gdef \LT@xxvii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{68.81955pt}\LT@entry 
    {1}{49.24095pt}\LT@entry 
    {1}{80.93025pt}\LT@entry 
    {1}{79.9557pt}\LT@entry 
    {1}{44.62006pt}\LT@entry 
    {1}{33.7467pt}\LT@entry 
    {1}{136.35913pt}\LT@entry 
    {1}{30.54932pt}\LT@entry 
    {1}{49.18681pt}}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Independent Study}{177}{section.9.6}\protected@file@percent }
\newlabel{independent-study}{{9.6}{177}{Independent Study}{section.9.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.7}Foundational Paper}{178}{section.9.7}\protected@file@percent }
\newlabel{foundational-paper}{{9.7}{178}{Foundational Paper}{section.9.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}XGBoost}{179}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{xgboost}{{10}{179}{XGBoost}{chapter.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}XGBoost Intuition}{179}{section.10.1}\protected@file@percent }
\newlabel{xgboost-intuition}{{10.1}{179}{XGBoost Intuition}{section.10.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}How XGBoost Works (Regression Example)}{179}{section.10.2}\protected@file@percent }
\newlabel{how-xgboost-works-regression-example}{{10.2}{179}{How XGBoost Works (Regression Example)}{section.10.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Using XGBoost}{180}{section.10.3}\protected@file@percent }
\newlabel{using-xgboost}{{10.3}{180}{Using XGBoost}{section.10.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.4}Core Hyperparameter Categories}{181}{section.10.4}\protected@file@percent }
\newlabel{core-hyperparameter-categories}{{10.4}{181}{Core Hyperparameter Categories}{section.10.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.1}Model Complexity}{181}{subsection.10.4.1}\protected@file@percent }
\newlabel{model-complexity}{{10.4.1}{181}{Model Complexity}{subsection.10.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.2}Learning and Regularization}{181}{subsection.10.4.2}\protected@file@percent }
\newlabel{learning-and-regularization}{{10.4.2}{181}{Learning and Regularization}{subsection.10.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.3}Regularization}{181}{subsection.10.4.3}\protected@file@percent }
\newlabel{regularization}{{10.4.3}{181}{Regularization}{subsection.10.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.4}Optimization Control}{181}{subsection.10.4.4}\protected@file@percent }
\newlabel{optimization-control}{{10.4.4}{181}{Optimization Control}{subsection.10.4.4}{}}
\gdef \LT@xxviii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.5}Baseline Model}{184}{subsection.10.4.5}\protected@file@percent }
\newlabel{baseline-model}{{10.4.5}{184}{Baseline Model}{subsection.10.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.6}Early Stopping in XGBoost}{184}{subsection.10.4.6}\protected@file@percent }
\newlabel{early-stopping-in-xgboost}{{10.4.6}{184}{Early Stopping in XGBoost}{subsection.10.4.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.6.1}How It Works}{184}{subsubsection.10.4.6.1}\protected@file@percent }
\newlabel{how-it-works}{{10.4.6.1}{184}{How It Works}{subsubsection.10.4.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.6.2}Requirements}{185}{subsubsection.10.4.6.2}\protected@file@percent }
\newlabel{requirements}{{10.4.6.2}{185}{Requirements}{subsubsection.10.4.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.7}\texttt  {gamma} in XGBoost}{186}{subsection.10.4.7}\protected@file@percent }
\newlabel{gamma-in-xgboost}{{10.4.7}{186}{\texorpdfstring {\texttt {gamma} in XGBoost}{gamma in XGBoost}}{subsection.10.4.7}{}}
\gdef \LT@xxix {\LT@entry 
    {1}{97.91109pt}\LT@entry 
    {1}{195.81572pt}\LT@entry 
    {1}{153.03322pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.8}\texttt  {reg\_lambda} and \texttt  {reg\_alpha} in XGBoost}{189}{subsection.10.4.8}\protected@file@percent }
\newlabel{reg_lambda-and-reg_alpha-in-xgboost}{{10.4.8}{189}{\texorpdfstring {\texttt {reg\_lambda} and \texttt {reg\_alpha} in XGBoost}{reg\_lambda and reg\_alpha in XGBoost}}{subsection.10.4.8}{}}
\gdef \LT@xxx {\LT@entry 
    {1}{172.25246pt}\LT@entry 
    {1}{274.50757pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.9}Exploring Regularization Hyperparameters Simultaneously}{193}{subsection.10.4.9}\protected@file@percent }
\newlabel{exploring-regularization-hyperparameters-simultaneously}{{10.4.9}{193}{Exploring Regularization Hyperparameters Simultaneously}{subsection.10.4.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.10}Comprehensive Hyperparameter Tuning}{195}{subsection.10.4.10}\protected@file@percent }
\newlabel{comprehensive-hyperparameter-tuning}{{10.4.10}{195}{Comprehensive Hyperparameter Tuning}{subsection.10.4.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.10.1}Why \texttt  {GridSearchCV} Is Not a Practical Option}{196}{subsubsection.10.4.10.1}\protected@file@percent }
\newlabel{why-gridsearchcv-is-not-a-practical-option}{{10.4.10.1}{196}{\texorpdfstring {Why \texttt {GridSearchCV} Is Not a Practical Option}{Why GridSearchCV Is Not a Practical Option}}{subsubsection.10.4.10.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.10.2}Smarter Tuning with Optuna or BayesSearchCV}{197}{subsubsection.10.4.10.2}\protected@file@percent }
\newlabel{smarter-tuning-with-optuna-or-bayessearchcv}{{10.4.10.2}{197}{Smarter Tuning with Optuna or BayesSearchCV}{subsubsection.10.4.10.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {10.4.10.2.1}\texttt  {BayesSearchCV} (from \texttt  {skopt})}{198}{paragraph.10.4.10.2.1}\protected@file@percent }
\newlabel{bayessearchcv-from-skopt}{{10.4.10.2.1}{198}{\texorpdfstring {\texttt {BayesSearchCV} (from \texttt {skopt})}{BayesSearchCV (from skopt)}}{paragraph.10.4.10.2.1}{}}
\gdef \LT@xxxi {\LT@entry 
    {3}{16.95001pt}\LT@entry 
    {3}{115.94833pt}\LT@entry 
    {1}{60.78285pt}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {10.4.10.2.2}Tuning with Optuna}{205}{paragraph.10.4.10.2.2}\protected@file@percent }
\newlabel{tuning-with-optuna}{{10.4.10.2.2}{205}{Tuning with Optuna}{paragraph.10.4.10.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.10.3}After Training: Analyze and Refine}{208}{subsubsection.10.4.10.3}\protected@file@percent }
\newlabel{after-training-analyze-and-refine}{{10.4.10.3}{208}{After Training: Analyze and Refine}{subsubsection.10.4.10.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.5}XGBoost for Imbalanced Classification}{208}{section.10.5}\protected@file@percent }
\newlabel{xgboost-for-imbalanced-classification}{{10.5}{208}{XGBoost for Imbalanced Classification}{section.10.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.1}Common Strategies Across Libraries}{208}{subsection.10.5.1}\protected@file@percent }
\newlabel{common-strategies-across-libraries}{{10.5.1}{208}{Common Strategies Across Libraries}{subsection.10.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.2}Handling Class Imbalance with \texttt  {scale\_pos\_weight} in XGBoost}{209}{subsection.10.5.2}\protected@file@percent }
\newlabel{handling-class-imbalance-with-scale_pos_weight-in-xgboost}{{10.5.2}{209}{\texorpdfstring {Handling Class Imbalance with \texttt {scale\_pos\_weight} in XGBoost}{Handling Class Imbalance with scale\_pos\_weight in XGBoost}}{subsection.10.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.2.1}What Does \texttt  {scale\_pos\_weight} Do?}{209}{subsubsection.10.5.2.1}\protected@file@percent }
\newlabel{what-does-scale_pos_weight-do}{{10.5.2.1}{209}{\texorpdfstring {What Does \texttt {scale\_pos\_weight} Do?}{What Does scale\_pos\_weight Do?}}{subsubsection.10.5.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.2.2}When to Use It}{209}{subsubsection.10.5.2.2}\protected@file@percent }
\newlabel{when-to-use-it}{{10.5.2.2}{209}{When to Use It}{subsubsection.10.5.2.2}{}}
\gdef \LT@xxxii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{68.81955pt}\LT@entry 
    {1}{49.24095pt}\LT@entry 
    {1}{80.93025pt}\LT@entry 
    {1}{79.9557pt}\LT@entry 
    {1}{44.62006pt}\LT@entry 
    {1}{33.7467pt}\LT@entry 
    {1}{136.35913pt}\LT@entry 
    {1}{30.54932pt}\LT@entry 
    {1}{49.18681pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.3}How to Set It}{210}{subsection.10.5.3}\protected@file@percent }
\newlabel{how-to-set-it}{{10.5.3}{210}{How to Set It}{subsection.10.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.4}Using \texttt  {scale\_pos\_weight}}{212}{subsection.10.5.4}\protected@file@percent }
\newlabel{using-scale_pos_weight}{{10.5.4}{212}{\texorpdfstring {Using \texttt {scale\_pos\_weight}}{Using scale\_pos\_weight}}{subsection.10.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.5}Threshold adjustment}{216}{subsection.10.5.5}\protected@file@percent }
\newlabel{threshold-adjustment}{{10.5.5}{216}{Threshold adjustment}{subsection.10.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.6}Alternative Method: Custom Instance Weights (\texttt  {sample\_weight})}{217}{subsection.10.5.6}\protected@file@percent }
\newlabel{alternative-method-custom-instance-weights-sample_weight}{{10.5.6}{217}{\texorpdfstring {Alternative Method: Custom Instance Weights (\texttt {sample\_weight})}{Alternative Method: Custom Instance Weights (sample\_weight)}}{subsection.10.5.6}{}}
\gdef \LT@xxxiii {\LT@entry 
    {1}{56.64528pt}\LT@entry 
    {1}{201.86012pt}\LT@entry 
    {1}{188.25462pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.7}\texttt  {scale\_pos\_weight} vs.\nobreakspace  {}\texttt  {sample\_weight}}{218}{subsection.10.5.7}\protected@file@percent }
\newlabel{scale_pos_weight-vs.-sample_weight}{{10.5.7}{218}{\texorpdfstring {\texttt {scale\_pos\_weight} vs.~\texttt {sample\_weight}}{scale\_pos\_weight vs.~sample\_weight}}{subsection.10.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.6}Resources for Learning XGBoost}{218}{section.10.6}\protected@file@percent }
\newlabel{resources-for-learning-xgboost}{{10.6}{218}{Resources for Learning XGBoost}{section.10.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}LightGBM and CatBoost}{219}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{lightgbm-and-catboost}{{11}{219}{LightGBM and CatBoost}{chapter.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}What They Share with XGBoost}{219}{section.11.1}\protected@file@percent }
\newlabel{what-they-share-with-xgboost}{{11.1}{219}{What They Share with XGBoost}{section.11.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}LightGBM}{220}{section.11.2}\protected@file@percent }
\newlabel{lightgbm}{{11.2}{220}{LightGBM}{section.11.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}What is LightGBM?}{220}{subsection.11.2.1}\protected@file@percent }
\newlabel{what-is-lightgbm}{{11.2.1}{220}{What is LightGBM?}{subsection.11.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.2}What Makes LightGBM Lighting Fast?}{220}{subsection.11.2.2}\protected@file@percent }
\newlabel{what-makes-lightgbm-lighting-fast}{{11.2.2}{220}{What Makes LightGBM Lighting Fast?}{subsection.11.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.2.1}Leaf-Wise Tree Growth}{220}{subsubsection.11.2.2.1}\protected@file@percent }
\newlabel{leaf-wise-tree-growth}{{11.2.2.1}{220}{Leaf-Wise Tree Growth}{subsubsection.11.2.2.1}{}}
\gdef \LT@xxxiv {\LT@entry 
    {1}{51.99pt}\LT@entry 
    {1}{57.99pt}\LT@entry 
    {1}{86.4825pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.2.2}GOSS (Gradient-based One-Side Sampling)}{221}{subsubsection.11.2.2.2}\protected@file@percent }
\newlabel{goss-gradient-based-one-side-sampling}{{11.2.2.2}{221}{GOSS (Gradient-based One-Side Sampling)}{subsubsection.11.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.2.3}Exclusive Feature Bundling (EFB)}{221}{subsubsection.11.2.2.3}\protected@file@percent }
\newlabel{exclusive-feature-bundling-efb}{{11.2.2.3}{221}{Exclusive Feature Bundling (EFB)}{subsubsection.11.2.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.3}Using LightGBM}{222}{subsection.11.2.3}\protected@file@percent }
\newlabel{using-lightgbm}{{11.2.3}{222}{Using LightGBM}{subsection.11.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3.1}Core LightGBM Hyperparameters}{222}{subsubsection.11.2.3.1}\protected@file@percent }
\newlabel{core-lightgbm-hyperparameters}{{11.2.3.1}{222}{Core LightGBM Hyperparameters}{subsubsection.11.2.3.1}{}}
\gdef \LT@xxxv {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3.2}Building a Baseline Model Using LightGBM's Native Categorical Feature Support}{225}{subsubsection.11.2.3.2}\protected@file@percent }
\newlabel{building-a-baseline-model-using-lightgbms-native-categorical-feature-support}{{11.2.3.2}{225}{Building a Baseline Model Using LightGBM's Native Categorical Feature Support}{subsubsection.11.2.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3.3}Enabling GOSS and EFB in LightGBM}{226}{subsubsection.11.2.3.3}\protected@file@percent }
\newlabel{enabling-goss-and-efb-in-lightgbm}{{11.2.3.3}{226}{Enabling GOSS and EFB in LightGBM}{subsubsection.11.2.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {11.2.3.3.1}GOSS is \textbf  {not enabled by default}.}{226}{paragraph.11.2.3.3.1}\protected@file@percent }
\newlabel{goss-is-not-enabled-by-default.}{{11.2.3.3.1}{226}{\texorpdfstring {GOSS is \textbf {not enabled by default}.}{GOSS is not enabled by default.}}{paragraph.11.2.3.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {11.2.3.3.2}EFB is \textbf  {enabled by default}}{226}{paragraph.11.2.3.3.2}\protected@file@percent }
\newlabel{efb-is-enabled-by-default}{{11.2.3.3.2}{226}{\texorpdfstring {EFB is \textbf {enabled by default}}{EFB is enabled by default}}{paragraph.11.2.3.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3.4}Tuning \texttt  {top\_rate} and \texttt  {other\_rate} in GOSS}{227}{subsubsection.11.2.3.4}\protected@file@percent }
\newlabel{tuning-top_rate-and-other_rate-in-goss}{{11.2.3.4}{227}{\texorpdfstring {Tuning \texttt {top\_rate} and \texttt {other\_rate} in GOSS}{Tuning top\_rate and other\_rate in GOSS}}{subsubsection.11.2.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3.5}Optimizing LightGBM with \texttt  {BayesSearchCV}}{228}{subsubsection.11.2.3.5}\protected@file@percent }
\newlabel{optimizing-lightgbm-with-bayessearchcv}{{11.2.3.5}{228}{\texorpdfstring {Optimizing LightGBM with \texttt {BayesSearchCV}}{Optimizing LightGBM with BayesSearchCV}}{subsubsection.11.2.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.3}CatBoost}{230}{section.11.3}\protected@file@percent }
\newlabel{catboost}{{11.3}{230}{CatBoost}{section.11.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.1}What is CatBoost?}{230}{subsection.11.3.1}\protected@file@percent }
\newlabel{what-is-catboost}{{11.3.1}{230}{What is CatBoost?}{subsection.11.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.2}What Makes CatBoost Unique?}{230}{subsection.11.3.2}\protected@file@percent }
\newlabel{what-makes-catboost-unique}{{11.3.2}{230}{What Makes CatBoost Unique?}{subsection.11.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2.1}Symmetric (Oblivious) Trees}{230}{subsubsection.11.3.2.1}\protected@file@percent }
\newlabel{symmetric-oblivious-trees}{{11.3.2.1}{230}{Symmetric (Oblivious) Trees}{subsubsection.11.3.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2.2}Advanced Categorical Feature Handling}{231}{subsubsection.11.3.2.2}\protected@file@percent }
\newlabel{advanced-categorical-feature-handling}{{11.3.2.2}{231}{Advanced Categorical Feature Handling}{subsubsection.11.3.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2.3}Ordered Boosting (vs.\nobreakspace  {}Standard Boosting)}{231}{subsubsection.11.3.2.3}\protected@file@percent }
\newlabel{ordered-boosting-vs.-standard-boosting}{{11.3.2.3}{231}{Ordered Boosting (vs.~Standard Boosting)}{subsubsection.11.3.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2.4}Handling of Text and Embedding Features}{231}{subsubsection.11.3.2.4}\protected@file@percent }
\newlabel{handling-of-text-and-embedding-features}{{11.3.2.4}{231}{Handling of Text and Embedding Features}{subsubsection.11.3.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2.5}Ease of Use and Defaults}{232}{subsubsection.11.3.2.5}\protected@file@percent }
\newlabel{ease-of-use-and-defaults}{{11.3.2.5}{232}{Ease of Use and Defaults}{subsubsection.11.3.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.3}Using CatBoost}{232}{subsection.11.3.3}\protected@file@percent }
\newlabel{using-catboost}{{11.3.3}{232}{Using CatBoost}{subsection.11.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.4}Installation}{232}{subsection.11.3.4}\protected@file@percent }
\newlabel{installation}{{11.3.4}{232}{Installation}{subsection.11.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.5}CatBoost for Regression}{232}{subsection.11.3.5}\protected@file@percent }
\newlabel{catboost-for-regression}{{11.3.5}{232}{CatBoost for Regression}{subsection.11.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.6}Tuning \texttt  {CatBoostRegressor}}{233}{subsection.11.3.6}\protected@file@percent }
\newlabel{tuning-catboostregressor}{{11.3.6}{233}{\texorpdfstring {Tuning \texttt {CatBoostRegressor}}{Tuning CatBoostRegressor}}{subsection.11.3.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.6.1}❌ Hyperparameters \textbf  {not used} in CatBoost:}{233}{subsubsection.11.3.6.1}\protected@file@percent }
\newlabel{hyperparameters-not-used-in-catboost}{{11.3.6.1}{233}{\texorpdfstring {❌ Hyperparameters \textbf {not used} in CatBoost:}{❌ Hyperparameters not used in CatBoost:}}{subsubsection.11.3.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.6.2}✅ Unique Hyperparameters in CatBoost}{234}{subsubsection.11.3.6.2}\protected@file@percent }
\newlabel{unique-hyperparameters-in-catboost}{{11.3.6.2}{234}{✅ Unique Hyperparameters in CatBoost}{subsubsection.11.3.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.7}When to Use \textbf  {CatBoost} Over \textbf  {XGBoost}}{240}{subsection.11.3.7}\protected@file@percent }
\newlabel{when-to-use-catboost-over-xgboost}{{11.3.7}{240}{\texorpdfstring {When to Use \textbf {CatBoost} Over \textbf {XGBoost}}{When to Use CatBoost Over XGBoost}}{subsection.11.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Handling Imbalanced Classification: XGBoost vs.\nobreakspace  {}LightGBM vs.\nobreakspace  {}CatBoost}{240}{section.11.4}\protected@file@percent }
\newlabel{handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost}{{11.4}{240}{Handling Imbalanced Classification: XGBoost vs.~LightGBM vs.~CatBoost}{section.11.4}{}}
\gdef \LT@xxxvi {\LT@entry 
    {1}{49.04655pt}\LT@entry 
    {1}{168.49821pt}\LT@entry 
    {1}{109.8012pt}\LT@entry 
    {1}{119.45164pt}}
\@writefile{toc}{\contentsline {section}{\numberline {11.5}Summary: XGBoost vs.\nobreakspace  {}LightGBM vs.\nobreakspace  {}CatBoost}{241}{section.11.5}\protected@file@percent }
\newlabel{summary-xgboost-vs.-lightgbm-vs.-catboost}{{11.5}{241}{Summary: XGBoost vs.~LightGBM vs.~CatBoost}{section.11.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.6}References}{242}{section.11.6}\protected@file@percent }
\newlabel{references}{{11.6}{242}{References}{section.11.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Smarter Hyperparameter Tuning}{243}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{smarter-hyperparameter-tuning}{{12}{243}{Smarter Hyperparameter Tuning}{chapter.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Cross-Validation Basics}{243}{section.12.1}\protected@file@percent }
\newlabel{cross-validation-basics}{{12.1}{243}{Cross-Validation Basics}{section.12.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}\texttt  {GridSearchCV}: Exhaustive but Limited}{243}{section.12.2}\protected@file@percent }
\newlabel{gridsearchcv-exhaustive-but-limited}{{12.2}{243}{\texorpdfstring {\texttt {GridSearchCV}: Exhaustive but Limited}{GridSearchCV: Exhaustive but Limited}}{section.12.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}\texttt  {RandomizedSearchCV}: Efficient but Random}{244}{section.12.3}\protected@file@percent }
\newlabel{randomizedsearchcv-efficient-but-random}{{12.3}{244}{\texorpdfstring {\texttt {RandomizedSearchCV}: Efficient but Random}{RandomizedSearchCV: Efficient but Random}}{section.12.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.4}Smarter Tuning for Complex Models}{244}{section.12.4}\protected@file@percent }
\newlabel{smarter-tuning-for-complex-models}{{12.4}{244}{Smarter Tuning for Complex Models}{section.12.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.1}\texttt  {BayesSearchCV} (Bayesian Optimization)}{245}{subsection.12.4.1}\protected@file@percent }
\newlabel{bayessearchcv-bayesian-optimization}{{12.4.1}{245}{\texorpdfstring {\texttt {BayesSearchCV} (Bayesian Optimization)}{BayesSearchCV (Bayesian Optimization)}}{subsection.12.4.1}{}}
\gdef \LT@xxxvii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.1.1}Data Preprocessing}{246}{subsubsection.12.4.1.1}\protected@file@percent }
\newlabel{data-preprocessing}{{12.4.1.1}{246}{Data Preprocessing}{subsubsection.12.4.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.1.2}Baseline Performance}{247}{subsubsection.12.4.1.2}\protected@file@percent }
\newlabel{baseline-performance}{{12.4.1.2}{247}{Baseline Performance}{subsubsection.12.4.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.1.3}\texttt  {BayesSearchCV} inital tuning}{247}{subsubsection.12.4.1.3}\protected@file@percent }
\newlabel{bayessearchcv-inital-tuning}{{12.4.1.3}{247}{\texorpdfstring {\texttt {BayesSearchCV} inital tuning}{BayesSearchCV inital tuning}}{subsubsection.12.4.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.1.4}Visualizing \texttt  {BayesSearchCV} Results}{249}{subsubsection.12.4.1.4}\protected@file@percent }
\newlabel{visualizing-bayessearchcv-results}{{12.4.1.4}{249}{\texorpdfstring {Visualizing \texttt {BayesSearchCV} Results}{Visualizing BayesSearchCV Results}}{subsubsection.12.4.1.4}{}}
