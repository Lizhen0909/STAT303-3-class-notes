[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science III with python (Class notes)",
    "section": "",
    "text": "Preface\nThis book serves as the course notes for STAT 303 Sec20, Spring 2025 at Northwestern University. To enhance your understanding of the material, you are expected to read the textbook before using these notes.\nIt is an evolving resource designed to support the course’s learning objectives. This edition builds upon the foundational work of Professor Arvind Krishna, whose contributions have provided a strong framework for this resource. We are deeply grateful for his efforts, which continue to shape the book’s development.\nThroughout the quarter, the content will be updated and refined in real time to enhance clarity, depth, and relevance. These modifications ensure alignment with current teaching objectives and methodologies.\nAs a living document, this book welcomes feedback, suggestions, and contributions from students, instructors, and the broader academic community. Your input helps improve its quality and effectiveness.\nThank you for being part of this journey—we hope this resource serves as a valuable guide in your learning.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Bias_variance_code.html",
    "href": "Bias_variance_code.html",
    "title": "1  Bias-variance tradeoff",
    "section": "",
    "text": "1.1 Simple model (Less flexible)\nRead section 2.2.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nIn this chapter, we will show that a flexible model is likely to have high variance and low bias, while a relatively less flexible model is likely to have a high bias and low variance.\nThe examples considered below are motivated from the examples shown in the documentation of the bias_variance_decomp() function from the mlxtend library. We will first manually compute the bias and variance for understanding of the concept. Later, we will show application of the bias_variance_decomp() function to estimate bias and variance.\nLet us consider a linear regression model as the less-flexible (or relatively simple) model.\nWe will first simulate the test dataset for which we will compute the bias and variance.\nnp.random.seed(101)\n\n# Simulating predictor values of test data\nxtest = np.random.uniform(-15, 10, 200)\n\n# Assuming the true mean response is square of the predictor value\nfxtest = xtest**2\n\n# Simulating noiseless test response \nytest = fxtest\n\n# We will find bias and variance using a linear regression model for prediction\nmodel = LinearRegression()\n# Visualizing the data and the true mean response\nsns.scatterplot(x = xtest, y = ytest)\nsns.lineplot(x = xtest, y = fxtest, color = 'grey', linewidth = 2)\n\n# Initializing objects to store predictions and mean squared error\n# of 100 models developed on 100 distinct training datasets samples\npred_test = []; mse_test = []\n\n# Iterating over each of the 100 models\nfor i in range(100):\n    np.random.seed(i)\n    \n    # Simulating the ith training data\n    x = np.random.uniform(-15, 10, 200)\n    fx = x**2\n    y = fx + np.random.normal(0, 10, 200)\n    \n    # Fitting the ith model on the ith training data\n    model.fit(x.reshape(-1,1), y)\n    \n    # Plotting the ith model\n    sns.lineplot(x = x, y = model.predict(x.reshape(-1,1)))\n    \n    # Storing the predictions of the ith model on test data\n    pred_test.append(model.predict(xtest.reshape(-1,1)))\n    \n    # Storing the mean squared error of the ith model on test data\n    mse_test.append(mean_squared_error(model.predict(xtest.reshape(-1,1)), ytest))\nThe above plots show that the 100 models seem to have low variance, but high bias. Note that the bias is low only around a couple of points (x = -10 & x = 5).\nLet us compute the average squared bias over all the test data points.\nmean_pred = np.array(pred_test).mean(axis = 0)\nsq_bias = ((mean_pred - fxtest)**2).mean()\nsq_bias\n\n2042.104126728109\nLet us compute the average variance over all the test data points.\nmean_var = np.array(pred_test).var(axis = 0).mean()\nmean_var\n\n28.37397844429763\nLet us compute the mean squared error over all the test data points.\nnp.array(mse_test).mean()\n\n2070.4781051724062\nNote that the mean squared error should be the same as the sum of squared bias and variance\nThe sum of squared bias and model variance is:\nsq_bias + mean_var\n\n2070.4781051724067\nNote that this is exactly the same as the mean squared error computed above as we are developing a finite number of models, and making predictions on a finite number of test data points.",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "Bias_variance_code.html#complex-model-more-flexible",
    "href": "Bias_variance_code.html#complex-model-more-flexible",
    "title": "1  Bias-variance tradeoff",
    "section": "1.2 Complex model (more flexible)",
    "text": "1.2 Complex model (more flexible)\nLet us consider a decion tree as the more flexible model.\n\nnp.random.seed(101)\nxtest = np.random.uniform(-15, 10, 200)\nfxtest = xtest**2\nytest = fxtest\nmodel = DecisionTreeRegressor()\n\n\nsns.scatterplot(x = xtest, y = ytest)\nsns.lineplot(x = xtest, y = fxtest, color = 'grey', linewidth = 2)\npred_test = []; mse_test = []\nfor i in range(100):\n    np.random.seed(i)\n    x = np.random.uniform(-15, 10, 200)\n    fx = x**2\n    y = fx + np.random.normal(0, 10, 200)\n    model.fit(x.reshape(-1,1), y)\n    sns.lineplot(x = x, y = model.predict(x.reshape(-1,1)))\n    pred_test.append(model.predict(xtest.reshape(-1,1)))\n    mse_test.append(mean_squared_error(model.predict(xtest.reshape(-1,1)), ytest))\n\n\n\n\n\n\n\n\nThe above plots show that the 100 models seem to have high variance, but low bias.\nLet us compute the average squared bias over all the test data points.\n\nmean_pred = np.array(pred_test).mean(axis = 0)\nsq_bias = ((mean_pred - fxtest)**2).mean()\nsq_bias\n\n1.3117561629333938\n\n\nLet us compute the average model variance over all the test data points.\n\nmean_var = np.array(pred_test).var(axis = 0).mean()\nmean_var\n\n102.5226748977198\n\n\nLet us compute the average mean squared error over all the test data points.\n\nnp.array(mse_test).mean()\n\n103.83443106065317\n\n\nNote that the above error is still the same as the sum of the squared bias, model variance and the irreducible error.\nNote that the relatively more flexible model has a higher variance, but lower bias as compared to the less flexible linear model. This will typically be the case, but may not be true in all scenarios. We will discuss one such scenario later.",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "KNN.html",
    "href": "KNN.html",
    "title": "2  KNN",
    "section": "",
    "text": "2.1 KNN for regression\nRead section 4.7.6 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\n\n# Split the dataset into features and target variable\nX = car.drop(columns=['price'])\ny = car['price']\n\n# split the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# extract the categorical columns and put them in a list\ncat_cols = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnum_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n# First transform categorical variables\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', num_cols),  # Just pass numerical features through\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n    ])\n\n# Create pipeline that scales all features together\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('scaler', StandardScaler()),  # Scale everything together\n    ('knn', KNeighborsRegressor(n_neighbors=5))\n])\n\n# Fit the pipeline to the training data\npipeline.fit(X_train, y_train)\n# Predict on the test data\ny_pred = pipeline.predict(X_test)\n# Calculate RMSE\nrmse = root_mean_squared_error(y_test, y_pred)\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"R² Score: {pipeline.score(X_test, y_test):.2f}\")\n\nRMSE: 4364.84\nR² Score: 0.94\n# show the features in the numerical transformer    \npipeline.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out()\nprint(\"numerical features in the pipeline:\", pipeline.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out())\n\n# show the features in the categorical transformer\npipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out()\nprint(\"categorical features in the pipeline:\", pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out())\n\nnumerical features in the pipeline: ['year' 'mileage' 'tax' 'mpg' 'engineSize']\ncategorical features in the pipeline: ['brand_audi' 'brand_bmw' 'brand_ford' 'brand_hyundi' 'brand_merc'\n 'brand_skoda' 'brand_toyota' 'brand_vauxhall' 'brand_vw'\n 'model_ 6 Series' 'model_ 7 Series' 'model_ 8 Series' 'model_ A7'\n 'model_ A8' 'model_ Agila' 'model_ Amarok' 'model_ Antara'\n 'model_ Arteon' 'model_ Avensis' 'model_ Beetle' 'model_ CC'\n 'model_ CLA Class' 'model_ CLK' 'model_ CLS Class' 'model_ Caddy'\n 'model_ Caddy Life' 'model_ Caddy Maxi Life' 'model_ California'\n 'model_ Camry' 'model_ Caravelle' 'model_ Combo Life' 'model_ Edge'\n 'model_ Eos' 'model_ Fusion' 'model_ G Class' 'model_ GL Class'\n 'model_ GLB Class' 'model_ GLS Class' 'model_ GT86' 'model_ GTC'\n 'model_ Galaxy' 'model_ Getz' 'model_ Grand C-MAX'\n 'model_ Grand Tourneo Connect' 'model_ Hilux' 'model_ I40' 'model_ I800'\n 'model_ IQ' 'model_ IX20' 'model_ IX35' 'model_ Jetta' 'model_ KA'\n 'model_ Kamiq' 'model_ Land Cruiser' 'model_ M Class' 'model_ M2'\n 'model_ M3' 'model_ M4' 'model_ M5' 'model_ M6' 'model_ Mustang'\n 'model_ PROACE VERSO' 'model_ Prius' 'model_ Puma' 'model_ Q8'\n 'model_ R8' 'model_ RS3' 'model_ RS4' 'model_ RS5' 'model_ RS6'\n 'model_ Rapid' 'model_ Roomster' 'model_ S Class' 'model_ S3' 'model_ S4'\n 'model_ SLK' 'model_ SQ5' 'model_ SQ7' 'model_ Santa Fe' 'model_ Scala'\n 'model_ Scirocco' 'model_ Shuttle' 'model_ Supra'\n 'model_ Tiguan Allspace' 'model_ Tourneo Connect' 'model_ Tourneo Custom'\n 'model_ V Class' 'model_ Verso' 'model_ Vivaro' 'model_ X-CLASS'\n 'model_ X4' 'model_ X6' 'model_ X7' 'model_ Yeti' 'model_ Z3' 'model_ Z4'\n 'model_ Zafira Tourer' 'model_ i3' 'model_ i8' 'transmission_Automatic'\n 'transmission_Manual' 'transmission_Other' 'transmission_Semi-Auto'\n 'fuelType_Diesel' 'fuelType_Electric' 'fuelType_Hybrid' 'fuelType_Other'\n 'fuelType_Petrol']",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "KNN.html#feature-scaling-in-knn",
    "href": "KNN.html#feature-scaling-in-knn",
    "title": "2  KNN",
    "section": "2.2 Feature Scaling in KNN",
    "text": "2.2 Feature Scaling in KNN\nFeature scaling is essential when using K-Nearest Neighbors (KNN) because the algorithm relies on calculating distances between data points. If features are measured on different scales (e.g., mileage in thousands and mpg in tens), the features with larger numeric ranges can dominate the distance calculations and distort the results.\nTo ensure that all features contribute equally, it’s important to standardize or normalize them before applying KNN. Common scaling techniques include:\n\nStandardization (zero mean, unit variance) using StandardScaler\nMin-max scaling to bring values into the [0, 1] range\n\nWithout scaling, KNN may produce biased or misleading predictions.\nThe example below illustrates how the same KNN model performs without feature scaling, highlighting the importance of preprocessing your data.\n\npreprocessor_no_scaling = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', num_cols),  # Pass numerical features through without scaling\n        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)  # Only one-hot encode categorical\n    ])\n\n# Create pipeline without any scaling\npipeline_no_scaling = Pipeline(steps=[\n    ('preprocessor', preprocessor_no_scaling),\n    ('knn', KNeighborsRegressor(n_neighbors=5))\n])\n\n# Fit the pipeline\npipeline_no_scaling.fit(X_train, y_train)\n\n# Evaluate\ny_pred_no_scaling = pipeline_no_scaling.predict(X_test)\n\nrmse_no_scaling = root_mean_squared_error(y_test, y_pred_no_scaling)\nprint(f\"RMSE without scaling: {rmse_no_scaling:.2f}\")\nprint(f\"R² Score without scaling: {pipeline_no_scaling.score(X_test, y_test):.2f}\")\n\nRMSE without scaling: 13758.38\nR² Score without scaling: 0.35",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "KNN.html#hyperparameters-in-knn",
    "href": "KNN.html#hyperparameters-in-knn",
    "title": "2  KNN",
    "section": "2.3 Hyperparameters in KNN",
    "text": "2.3 Hyperparameters in KNN\nThe most important hyperparameter in K-Nearest Neighbors (KNN) is k, which determines the number of neighbors considered when making predictions. Tuning k helps balance the model’s bias and variance:\n\nA small k (e.g., 1 or 3) can lead to low bias but high variance, making the model sensitive to noise in the training data.\nA large k results in higher bias but lower variance, producing smoother predictions that may underfit the data.\n\n\n2.3.1 Tuning k in KNN\nTo find the optimal value of k, it’s common to use cross-validation, which evaluates model performance on different subsets of the data. A popular tool for this is GridSearchCV, which automates the search process by testing multiple values of k using cross-validation behind the scenes. It selects the value of k that minimizes prediction error on unseen data—helping you achieve a good balance between underfitting and overfitting.\n\n# Create parameter grid for k values\nparam_grid = {\n    'knn__n_neighbors': list(range(1, 20))  # Test k values from 1 to 20\n}\n\n# Set up GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    cv=5,  # 5-fold cross-validation\n    scoring='neg_root_mean_squared_error',  # Optimize for RMSE\n    n_jobs=-1,  # Use all available cores\n    verbose=1\n)\n\n# Fit grid search\nprint(\"Tuning k parameter...\")\ngrid_search.fit(X_train, y_train)\n\n# Get best parameters and results\nbest_k = grid_search.best_params_['knn__n_neighbors']\nbest_score = -grid_search.best_score_  # Convert back from negative RMSE\n\nprint(f\"Best k: {best_k}\")\nprint(f\"Best CV RMSE: {best_score:.2f}\")\n\n# Evaluate on test set using best model\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\ntest_rmse = root_mean_squared_error(y_test, y_pred)\ntest_r2 = r2_score(y_test, y_pred)\n\nprint(f\"Test RMSE with k={best_k}: {test_rmse:.2f}\")\nprint(f\"Test R² Score with k={best_k}: {test_r2:.2f}\")\n\nTuning k parameter...\nFitting 5 folds for each of 19 candidates, totalling 95 fits\nBest k: 3\nBest CV RMSE: 4117.42\nTest RMSE with k=3: 4051.06\nTest R² Score with k=3: 0.94\n\n\n\n# Plot performance across different k values\ncv_results = grid_search.cv_results_\nk_values = param_grid['knn__n_neighbors']\nmean_rmse = -cv_results['mean_test_score'] \n\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, mean_rmse, marker='o')\nplt.xlabel('k (Number of Neighbors)')\nplt.ylabel('RMSE (Cross-Validation)')\nplt.title('KNN Performance for Different k Values')\nplt.grid(True)\nplt.xticks(k_values)\nplt.axvline(x=best_k, color='r', linestyle='--', label=f'Best k = {best_k}')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe distances and the indices of the nearest K observations to each test observation can be obtained using the kneighbors() method.\n\n# Get the KNN estimator from the pipeline\nknn_estimator = best_model.named_steps['knn']\n\n# Get indices of K-nearest neighbors for each test observation\nneighbor_indices = knn_estimator.kneighbors(best_model.named_steps['preprocessor'].transform(X_test), \n                                           return_distance=False)\n# neighbor_indices will contain the indices of the K nearest neighbors for each test observation\n# Note: The indices are relative to the training set, not the test set.\n# To get the actual neighbor observations, you can use these indices to index into the training set\n# For example, to get the actual neighbor observations for the first test observation:\nneighbors = X_train.iloc[neighbor_indices[0]]\nneighbors\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\n\n\n\n\n4580\nmerc\nV Class\n2010\nAutomatic\n259000\nDiesel\n540\n30.8345\n3.0\n\n\n5651\nmerc\nCLK\n2003\nAutomatic\n185000\nPetrol\n330\n18.0803\n4.3\n\n\n3961\nvw\nCaravelle\n2006\nManual\n178000\nDiesel\n325\n34.5738\n2.5\n\n\n\n\n\n\n\n\n\n2.3.2 Tuning Other KNN Hyperparameters\nIn addition to the number of neighbors (k), KNN has several other important hyperparameters that can significantly affect the model’s performance. Fine-tuning these settings helps you get the most out of the algorithm. Key hyperparameters include:\n\nweights: Determines how the neighbors contribute to the prediction.\n\n'uniform': All neighbors are weighted equally (default).\n\n'distance': Closer neighbors have more influence.\n\nChoosing 'distance' can improve performance, especially when data points are unevenly distributed.\n\nmetric: Defines the distance function used to measure similarity between data points.\n\n'minkowski' (default) is a general-purpose metric that includes both Euclidean and Manhattan distances.\n\nOther options include 'euclidean', 'manhattan', or even custom distance functions.\n\np: Used when metric='minkowski'.\n\np=2 gives Euclidean distance (standard for continuous features).\n\np=1 gives Manhattan distance (useful when features are sparse or grid-based).\n\nalgorithm: Controls the method used to compute nearest neighbors.\n\n'auto', 'ball_tree', 'kd_tree', or 'brute'.\n\nMost users can leave this as 'auto', which lets scikit-learn choose the best algorithm based on the data.\n\n\nThese hyperparameters can be tuned using GridSearchCV to find the combination that yields the best performance on validation data.\nThe model hyperparameters can be obtained using the get_params() method. Note that there are other hyperparameters to tune in addition to number of neighbors. However, the number of neighbours may be the most influential hyperparameter in most cases.\n\n# Get the best model parameters\nbest_model.get_params()\n\n{'memory': None,\n 'steps': [('preprocessor',\n   ColumnTransformer(transformers=[('num', 'passthrough',\n                                    ['year', 'mileage', 'tax', 'mpg',\n                                     'engineSize']),\n                                   ('cat',\n                                    OneHotEncoder(handle_unknown='ignore',\n                                                  sparse_output=False),\n                                    ['brand', 'model', 'transmission',\n                                     'fuelType'])])),\n  ('scaler', StandardScaler()),\n  ('knn', KNeighborsRegressor(n_neighbors=3))],\n 'transform_input': None,\n 'verbose': False,\n 'preprocessor': ColumnTransformer(transformers=[('num', 'passthrough',\n                                  ['year', 'mileage', 'tax', 'mpg',\n                                   'engineSize']),\n                                 ('cat',\n                                  OneHotEncoder(handle_unknown='ignore',\n                                                sparse_output=False),\n                                  ['brand', 'model', 'transmission',\n                                   'fuelType'])]),\n 'scaler': StandardScaler(),\n 'knn': KNeighborsRegressor(n_neighbors=3),\n 'preprocessor__force_int_remainder_cols': True,\n 'preprocessor__n_jobs': None,\n 'preprocessor__remainder': 'drop',\n 'preprocessor__sparse_threshold': 0.3,\n 'preprocessor__transformer_weights': None,\n 'preprocessor__transformers': [('num',\n   'passthrough',\n   ['year', 'mileage', 'tax', 'mpg', 'engineSize']),\n  ('cat',\n   OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n   ['brand', 'model', 'transmission', 'fuelType'])],\n 'preprocessor__verbose': False,\n 'preprocessor__verbose_feature_names_out': True,\n 'preprocessor__num': 'passthrough',\n 'preprocessor__cat': OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n 'preprocessor__cat__categories': 'auto',\n 'preprocessor__cat__drop': None,\n 'preprocessor__cat__dtype': numpy.float64,\n 'preprocessor__cat__feature_name_combiner': 'concat',\n 'preprocessor__cat__handle_unknown': 'ignore',\n 'preprocessor__cat__max_categories': None,\n 'preprocessor__cat__min_frequency': None,\n 'preprocessor__cat__sparse_output': False,\n 'scaler__copy': True,\n 'scaler__with_mean': True,\n 'scaler__with_std': True,\n 'knn__algorithm': 'auto',\n 'knn__leaf_size': 30,\n 'knn__metric': 'minkowski',\n 'knn__metric_params': None,\n 'knn__n_jobs': None,\n 'knn__n_neighbors': 3,\n 'knn__p': 2,\n 'knn__weights': 'uniform'}\n\n\n\n# Extended parameter grid\nparam_grid = {\n    'knn__n_neighbors': list(range(1, 20, 2)),  # Test odd k values from 1 to 19 (step=2 for efficiency)\n    'knn__weights': ['uniform', 'distance'],  # Uniform: equal weight; Distance: closer neighbors weigh more\n    'knn__metric': ['euclidean', 'manhattan', 'minkowski'],  # Common distance metrics\n    'knn__p': [1, 2]  # p=1 (Manhattan), p=2 (Euclidean) - only relevant for Minkowski\n}\n\n# Set up GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    cv=5,  # 5-fold cross-validation\n    scoring='neg_root_mean_squared_error',  # Optimize for RMSE\n    n_jobs=-1,  # Use all available cores\n    verbose=1\n)\n\n# Fit grid search\nprint(\"Tuning KNN hyperparameters...\")\ngrid_search.fit(X_train, y_train)\n\n# Get best parameters and results\nbest_params = grid_search.best_params_\nbest_score = -grid_search.best_score_  # Convert negative RMSE to positive\n\n# Display results\nprint(\"\\nBest Parameters:\")\nfor param, value in best_params.items():\n    print(f\"{param}: {value}\")\nprint(f\"Best CV RMSE: {best_score:.2f}\")\n\n# Evaluate on test set using best model\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\ntest_rmse = root_mean_squared_error(y_test, y_pred)  # Calculate RMSE\nprint(f\"Test RMSE: {test_rmse:.2f}\")\n\nTuning KNN hyperparameters...\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\nBest Parameters:\nknn__metric: euclidean\nknn__n_neighbors: 3\nknn__p: 1\nknn__weights: distance\nBest CV RMSE: 4001.34\nTest RMSE: 3826.94\n\n\nThe results for each cross-validation are stored in the cv_results_ attribute.\n\npd.DataFrame(grid_search.cv_results_).head()\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_knn__metric\nparam_knn__n_neighbors\nparam_knn__p\nparam_knn__weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.033124\n0.003042\n0.127347\n0.023598\neuclidean\n1\n1\nuniform\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4656.637196\n-3474.998033\n-4250.919748\n-4620.623046\n-4839.806784\n-4368.596961\n485.981480\n64\n\n\n1\n0.035615\n0.010407\n0.179835\n0.013115\neuclidean\n1\n1\ndistance\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4656.637196\n-3474.998033\n-4250.919748\n-4620.623046\n-4839.806784\n-4368.596961\n485.981480\n64\n\n\n2\n0.027877\n0.002536\n0.148597\n0.018612\neuclidean\n1\n2\nuniform\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4656.637196\n-3474.998033\n-4250.919748\n-4620.623046\n-4839.806784\n-4368.596961\n485.981480\n64\n\n\n3\n0.043631\n0.016927\n0.168392\n0.027444\neuclidean\n1\n2\ndistance\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4656.637196\n-3474.998033\n-4250.919748\n-4620.623046\n-4839.806784\n-4368.596961\n485.981480\n64\n\n\n4\n0.043071\n0.009615\n0.184532\n0.042681\neuclidean\n3\n1\nuniform\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4227.667178\n-3303.871045\n-3851.430697\n-4603.426146\n-4600.719641\n-4117.422942\n492.858432\n22\n\n\n\n\n\n\n\nThese results can be useful to see if other hyperparameter values are equally good.\n\npd.DataFrame(grid_search.cv_results_).sort_values(by = 'rank_test_score').head()\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_knn__metric\nparam_knn__n_neighbors\nparam_knn__p\nparam_knn__weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n87\n0.038193\n0.010690\n0.149261\n0.050225\nminkowski\n3\n2\ndistance\n{'knn__metric': 'minkowski', 'knn__n_neighbors...\n-4298.611714\n-3197.944286\n-3735.321059\n-4407.722340\n-4367.108381\n-4001.341556\n469.790238\n1\n\n\n5\n0.047902\n0.013181\n0.185623\n0.049865\neuclidean\n3\n1\ndistance\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4298.611714\n-3197.944286\n-3735.321059\n-4407.722340\n-4367.108381\n-4001.341556\n469.790238\n1\n\n\n7\n0.040595\n0.005817\n0.132290\n0.009807\neuclidean\n3\n2\ndistance\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4298.611714\n-3197.944286\n-3735.321059\n-4407.722340\n-4367.108381\n-4001.341556\n469.790238\n1\n\n\n51\n0.034996\n0.001900\n0.744842\n0.052065\nmanhattan\n5\n2\ndistance\n{'knn__metric': 'manhattan', 'knn__n_neighbors...\n-4090.438714\n-3258.873954\n-3680.152758\n-4846.570061\n-4192.419206\n-4013.690938\n531.510686\n4\n\n\n49\n0.031465\n0.004676\n0.718503\n0.057517\nmanhattan\n5\n1\ndistance\n{'knn__metric': 'manhattan', 'knn__n_neighbors...\n-4090.438714\n-3258.873954\n-3680.152758\n-4846.570061\n-4192.419206\n-4013.690938\n531.510686\n4\n\n\n\n\n\n\n\nThe results show that the next two best hyperparameter values yield the same performance as the printed one",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "KNN.html#hyperparameter-tuning",
    "href": "KNN.html#hyperparameter-tuning",
    "title": "2  KNN",
    "section": "2.4 Hyperparameter Tuning",
    "text": "2.4 Hyperparameter Tuning\nWe used GridSearchCV to tune the hyperparameters of our KNN model above. Given a relatively simple set of hyperparameters and a limited number of combinations, this approach was sufficient to reduce the RMSE.\nHowever, when the number of possible hyperparameter values grows large, GridSearchCV can become computationally expensive. In such cases, RandomizedSearchCV provides a more efficient alternative by sampling a fixed number of random combinations from the specified hyperparameter space. This makes it well-suited for scenarios with limited computational resources.\n\n2.4.0.1 RandomizedSearchCV\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n# Set up RandomizedSearchCV\n\n# Define parameter distributions for randomized search\nparam_distributions = {\n    'knn__n_neighbors': randint(1, 20),  # Random ints from 1 to 19\n    'knn__weights': ['uniform', 'distance'],\n    'knn__metric': ['euclidean', 'manhattan', 'minkowski'],\n    'knn__p': [1, 2]  # Only relevant for Minkowski\n}\n\n\n# Set up RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    estimator=pipeline,\n    param_distributions=param_distributions,\n    n_iter=30,  # Number of random combinations to try\n    cv=5,\n    scoring='neg_root_mean_squared_error',\n    n_jobs=-1,\n    random_state=42,\n    verbose=1\n)\n\n\n# Fit randomized search\nprint(\"Tuning KNN hyperparameters with RandomizedSearchCV...\")\nrandom_search.fit(X_train, y_train)\n\n# Best results\nbest_params = random_search.best_params_\nbest_score = -random_search.best_score_\n\n# Display results\nprint(\"\\nBest Parameters (RandomizedSearchCV):\")\nfor param, value in best_params.items():\n    print(f\"{param}: {value}\")\nprint(f\"Best CV RMSE: {best_score:.2f}\")\n\nTuning KNN hyperparameters with RandomizedSearchCV...\nFitting 5 folds for each of 30 candidates, totalling 150 fits\n\nBest Parameters (RandomizedSearchCV):\nknn__metric: manhattan\nknn__n_neighbors: 8\nknn__p: 2\nknn__weights: distance\nBest CV RMSE: 4005.70\n\n\n\n# Evaluate on test set\nbest_model = random_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\n# Calculate RMSE\ntest_rmse = root_mean_squared_error(y_test, y_pred)\nprint(f\"Test RMSE: {test_rmse:.2f}\")\n\nTest RMSE: 3811.89\n\n\nWhy might RandomizedSearchCV outperform GridSearchCV?\nAlthough GridSearchCV systematically evaluates all combinations of hyperparameter values from a predefined grid, it doesn’t guarantee the best performance. In some cases, RandomizedSearchCV can actually perform better. Here’s why:\n\nLimited Grid Resolution:\nGridSearchCV evaluates only the specific values you include in the grid. If the true optimal value lies between grid points, it may be missed entirely.\nBroader Exploration:\nRandomizedSearchCV samples from distributions (e.g., continuous or discrete ranges), allowing it to explore a wider range of hyperparameter values, including combinations not explicitly considered in a grid.\nIn this case,\n\nlist(range(1, 20, 2)) in GridSearchCV\nBut in RandomizedSearchCV, it samples from randint(1, 20)\n\nThe best n_neighbors happens to be 11, only RandomizedSearchCV can find it unless you explicitly included it in your grid.\nEfficiency in High Dimensions:\nIn high-dimensional search spaces, the number of combinations in a grid grows exponentially. RandomizedSearchCV remains efficient by sampling a fixed number of combinations, avoiding the “curse of dimensionality.”\nBetter Use of Time Budget:\nGiven the same computational budget, RandomizedSearchCV may cover more diverse regions of the search space and stumble upon better-performing configurations.\n\nIn summary, RandomizedSearchCV is not only faster but can also lead to better models—especially when the hyperparameter space is large, continuous, or contains irrelevant parameters.\n\n\n2.4.0.2 BayesSearchCV\nIn addition to these methods, BayesSearchCV, based on Bayesian optimization, provides a more intelligent approach to hyperparameter tuning. It models the performance landscape and selects hyperparameter combinations to evaluate based on past results, often requiring fewer evaluations to find optimal or near-optimal values. This makes BayesSearchCV a powerful option, especially when training models is costly.\n\n# Step 1: Install scikit-optimize if not already installed\n!pip install scikit-optimize\n\nCollecting scikit-optimize\n  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: joblib&gt;=0.11 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.4.2)\nCollecting pyaml&gt;=16.9 (from scikit-optimize)\n  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy&gt;=1.20.3 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.26.4)\nRequirement already satisfied: scipy&gt;=1.1.0 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.13.1)\nRequirement already satisfied: scikit-learn&gt;=1.0.0 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.6.1)\nRequirement already satisfied: packaging&gt;=21.3 in c:\\users\\lsi8012\\appdata\\roaming\\python\\python312\\site-packages (from scikit-optimize) (24.2)\nRequirement already satisfied: PyYAML in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from pyaml&gt;=16.9-&gt;scikit-optimize) (6.0.1)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn&gt;=1.0.0-&gt;scikit-optimize) (3.5.0)\nDownloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n   ---------------------------------------- 0.0/107.8 kB ? eta -:--:--\n   ------------------------------------- -- 102.4/107.8 kB 5.8 MB/s eta 0:00:01\n   ---------------------------------------- 107.8/107.8 kB 3.1 MB/s eta 0:00:00\nDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\nInstalling collected packages: pyaml, scikit-optimize\nSuccessfully installed pyaml-25.1.0 scikit-optimize-0.10.2\n\n\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Integer, Categorical\n\n\n# Step 3: Define search space for Bayesian optimization\nsearch_space = {\n    'knn__n_neighbors': Integer(1, 19),  # Odd values will be sampled if needed\n    'knn__weights': Categorical(['uniform', 'distance']),\n    'knn__metric': Categorical(['euclidean', 'manhattan', 'minkowski']),\n    'knn__p': Integer(1, 2)  # Used only when metric is minkowski\n}\n\n\n# Step 4: Set up BayesSearchCV\nbayes_search = BayesSearchCV(\n    estimator=pipeline,\n    search_spaces=search_space,\n    n_iter=30,  # Number of different combinations to try\n    scoring='neg_root_mean_squared_error',\n    cv=5,\n    n_jobs=-1,\n    verbose=1,\n    random_state=42\n)\n\n\n# Step 5: Fit BayesSearchCV\nprint(\"Tuning KNN hyperparameters with Bayesian Optimization...\")\nbayes_search.fit(X_train, y_train)\n\n# Get best parameters and best score\nbest_params = bayes_search.best_params_\nbest_score = -bayes_search.best_score_  # Convert negative RMSE to positive\n\n# Display results\nprint(\"\\nBest Parameters (Bayesian Optimization):\")\nfor param, value in best_params.items():\n    print(f\"{param}: {value}\")\nprint(f\"Best CV RMSE: {best_score:.2f}\")\n\nTuning KNN hyperparameters with Bayesian Optimization...\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 2, 'distance'] before, using random point ['euclidean', 12, 1, 'distance']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 2, 'distance'] before, using random point ['euclidean', 9, 2, 'distance']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 2, 'distance'] before, using random point ['manhattan', 5, 1, 'distance']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 1, 'distance'] before, using random point ['manhattan', 4, 2, 'uniform']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 1, 'distance'] before, using random point ['euclidean', 6, 1, 'uniform']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 1, 'distance'] before, using random point ['euclidean', 13, 1, 'distance']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 1, 'distance'] before, using random point ['euclidean', 6, 1, 'uniform']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 1, 'distance'] before, using random point ['manhattan', 2, 2, 'uniform']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\nBest Parameters (Bayesian Optimization):\nknn__metric: manhattan\nknn__n_neighbors: 8\nknn__p: 2\nknn__weights: distance\nBest CV RMSE: 4005.70\n\n\n\n# Step 6: Evaluate on test set\nbest_model = bayes_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\n# Calculate RMSE on test set\ntest_rmse = root_mean_squared_error(y_test, y_pred)\nprint(f\"Test RMSE: {test_rmse:.2f}\")\n\nTest RMSE: 3811.89",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html",
    "href": "Hyperparameter tuning.html",
    "title": "3  Hyperparameter tuning",
    "section": "",
    "text": "3.1 GridSearchCV\nIn this chapter we’ll introduce several functions that help with tuning hyperparameters of a machine learning model.\nLet us read and pre-process data first. Then we’ll be ready to tune the model hyperparameters. We’ll use KNN as the model. Note that KNN has multiple hyperparameters to tune, such as number of neighbors, distance metric, weights of neighbours, etc.\nThe function is used to compute the cross-validated score (MSE, RMSE, accuracy, etc.) over a grid of hyperparameter values. This helps avoid nested for() loops if multiple hyperparameter values need to be tuned.\n# GridSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor() # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n    # the keys should be EXACTLY the same as the names of the model inputs\n    # the values should be an array or list of hyperparam values you want to try out\n    \n# 30 K values x 2 weight settings x 3 metric settings = 180 different combinations in this grid\ngrid = {'n_neighbors': np.arange(5, 151, 5), 'weights':['uniform', 'distance'], \n        'metric': ['manhattan', 'euclidean', 'chebyshev']}\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, use it if you \n# have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within the function\ngcv = GridSearchCV(model, grid, cv = kfold, scoring = 'neg_root_mean_squared_error', n_jobs = -1, verbose = 10)\n\n# Fit the models, and cross-validate\ngcv.fit(X_train_scaled, y_train)\n\nFitting 5 folds for each of 180 candidates, totalling 900 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(), n_jobs=-1,\n             param_grid={'metric': ['manhattan', 'euclidean', 'chebyshev'],\n                         'n_neighbors': array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n        70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125, 130,\n       135, 140, 145, 150]),\n                         'weights': ['uniform', 'distance']},\n             scoring='neg_root_mean_squared_error', verbose=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(), n_jobs=-1,\n             param_grid={'metric': ['manhattan', 'euclidean', 'chebyshev'],\n                         'n_neighbors': array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n        70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125, 130,\n       135, 140, 145, 150]),\n                         'weights': ['uniform', 'distance']},\n             scoring='neg_root_mean_squared_error', verbose=10)estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()\nThe optimal estimator based on cross-validation is:\ngcv.best_estimator_\n\nKNeighborsRegressor(metric='manhattan', n_neighbors=10, weights='distance')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor(metric='manhattan', n_neighbors=10, weights='distance')\nThe optimal hyperparameter values (based on those considered in the grid search) are:\ngcv.best_params_\n\n{'metric': 'manhattan', 'n_neighbors': 10, 'weights': 'distance'}\nThe cross-validated root mean squared error for the optimal hyperparameter values is:\n-gcv.best_score_\n\n5740.928686723918\nThe RMSE on test data for the optimal hyperparameter values is:\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n5747.466851437544\nNote that the error is further reduced as compared to the case when we tuned only one hyperparameter in the previous chatper. We must tune all the hyperparameters that can effect prediction accuracy, in order to get the most accurate model.\nThe results for each cross-validation are stored in the cv_results_ attribute.\npd.DataFrame(gcv.cv_results_).head()\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_metric\nparam_n_neighbors\nparam_weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.011169\n0.005060\n0.011768\n0.001716\nmanhattan\n5\nuniform\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6781.316742\n-5997.969637\n-6726.786770\n-6488.191029\n-6168.502006\n-6432.553237\n306.558600\n19\n\n\n1\n0.009175\n0.001934\n0.009973\n0.000631\nmanhattan\n5\ndistance\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6449.449369\n-5502.975790\n-6306.888303\n-5780.902979\n-5365.980081\n-5881.239304\n429.577113\n3\n\n\n2\n0.008976\n0.001092\n0.012168\n0.001323\nmanhattan\n10\nuniform\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6668.299079\n-6116.693116\n-6387.505084\n-6564.727623\n-6219.094608\n-6391.263902\n205.856097\n16\n\n\n3\n0.007979\n0.000001\n0.011970\n0.000892\nmanhattan\n10\ndistance\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6331.374493\n-5326.304310\n-5787.179591\n-5809.777811\n-5450.007229\n-5740.928687\n349.872624\n1\n\n\n4\n0.006781\n0.000748\n0.012367\n0.001017\nmanhattan\n15\nuniform\n{'metric': 'manhattan', 'n_neighbors': 15, 'we...\n-6871.063499\n-6412.214411\n-6544.343677\n-7008.348770\n-6488.345118\n-6664.863095\n232.385843\n33\nThese results can be useful to see if other hyperparameter values are almost equally good.\nFor example, the next two best optimal values of the hyperparameter correspond to neighbors being 15 and 5 respectively. As the test error has a high variance, the best hyperparameter values need not necessarily be actually optimal.\npd.DataFrame(gcv.cv_results_).sort_values(by = 'rank_test_score').head()\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_metric\nparam_n_neighbors\nparam_weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n3\n0.007979\n0.000001\n0.011970\n0.000892\nmanhattan\n10\ndistance\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6331.374493\n-5326.304310\n-5787.179591\n-5809.777811\n-5450.007229\n-5740.928687\n349.872624\n1\n\n\n5\n0.009374\n0.004829\n0.013564\n0.001850\nmanhattan\n15\ndistance\n{'metric': 'manhattan', 'n_neighbors': 15, 'we...\n-6384.403268\n-5427.978762\n-5742.606651\n-6041.135255\n-5563.240077\n-5831.872803\n344.192700\n2\n\n\n1\n0.009175\n0.001934\n0.009973\n0.000631\nmanhattan\n5\ndistance\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6449.449369\n-5502.975790\n-6306.888303\n-5780.902979\n-5365.980081\n-5881.239304\n429.577113\n3\n\n\n7\n0.007977\n0.001092\n0.017553\n0.002054\nmanhattan\n20\ndistance\n{'metric': 'manhattan', 'n_neighbors': 20, 'we...\n-6527.825519\n-5534.609170\n-5860.837805\n-6100.919269\n-5679.403544\n-5940.719061\n349.270714\n4\n\n\n9\n0.007777\n0.000748\n0.019349\n0.003374\nmanhattan\n25\ndistance\n{'metric': 'manhattan', 'n_neighbors': 25, 'we...\n-6620.272336\n-5620.462675\n-5976.406911\n-6181.847891\n-5786.081991\n-6037.014361\n346.791650\n5\nLet us compute the RMSE on test data based on the 2nd and 3rd best hyperparameter values.\nmodel = KNeighborsRegressor(n_neighbors=15, metric='manhattan', weights='distance').fit(X_train_scaled, y_train)\nmean_squared_error(model.predict(X_test_scaled), y_test, squared = False)\n\n5800.418957612656\nmodel = KNeighborsRegressor(n_neighbors=5, metric='manhattan', weights='distance').fit(X_train_scaled, y_train)\nmean_squared_error(model.predict(X_test_scaled), y_test, squared = False)\n\n5722.4859230146685\nWe can see that the RMSE corresponding to the 3rd best hyperparameter value is the least. Due to variance in test errors, it may be a good idea to consider the set of top few best hyperparameter values, instead of just considering the best one.",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#randomizedsearchcv",
    "href": "Hyperparameter tuning.html#randomizedsearchcv",
    "title": "3  Hyperparameter tuning",
    "section": "3.2 RandomizedSearchCV()",
    "text": "3.2 RandomizedSearchCV()\nIn case of many possible values of hyperparameters, it may be comptaionally very expensive to use GridSearchCV(). In such cases, RandomizedSearchCV() can be used to compute the cross-validated score on a randomly selected subset of hyperparameter values from the specified grid. The number of values can be fixed by the user, as per the available budget.\n\n# RandomizedSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor() # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n    # the keys should be EXACTLY the same as the names of the model inputs\n    # the values should be an array or list of hyperparam values, or distribution of hyperparameter values\n    \n    \ngrid = {'n_neighbors': range(1, 500), 'weights':['uniform', 'distance'], \n        'metric': ['minkowski'], 'p': uniform(loc=1, scale=10)} #We can specify a distribution \n                                                                #for continuous hyperparameter values\n\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, use it if you \n# have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within the function\ngcv = RandomizedSearchCV(model, param_distributions = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1, verbose = 10)\n\n# Fit the models, and cross-validate\ngcv.fit(X_train_scaled, y_train)\n\nFitting 5 folds for each of 180 candidates, totalling 900 fits\n\n\nRandomizedSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n                   estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n                   param_distributions={'metric': ['minkowski'],\n                                        'n_neighbors': range(1, 500),\n                                        'p': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000226D6E70700&gt;,\n                                        'weights': ['uniform', 'distance']},\n                   random_state=10, scoring='neg_root_mean_squared_error',\n                   verbose=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n                   estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n                   param_distributions={'metric': ['minkowski'],\n                                        'n_neighbors': range(1, 500),\n                                        'p': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000226D6E70700&gt;,\n                                        'weights': ['uniform', 'distance']},\n                   random_state=10, scoring='neg_root_mean_squared_error',\n                   verbose=10)estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()\n\n\n\ngcv.best_params_\n\n{'metric': 'minkowski',\n 'n_neighbors': 3,\n 'p': 1.252639454318171,\n 'weights': 'uniform'}\n\n\n\ngcv.best_score_\n\n-6239.171627183809\n\n\n\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n6176.533397589911\n\n\nNote that in this example, RandomizedSearchCV() helps search for optimal values of the hyperparameter \\(p\\) over a continuous domain space. In this dataset, \\(p = 1\\) seems to be the optimal value. However, if the optimal value was somewhere in the middle of a larger continuous domain space (instead of the boundary of the domain space), and there were several other hyperparameters, some of which were not influencing the response (effect sparsity), RandomizedSearchCV() is likely to be more effective in estimating the optimal value of the continuous hyperparameter.\nThe advantages of RandomizedSearchCV() over GridSearchCV() are:\n\nRandomizedSearchCV() fixes the computational cost in case of large number of hyperparameters / large number of levels of individual hyperparameters. If there are \\(n\\) hyper parameters, each with 3 levels, the number of all possible hyperparameter values will be \\(3^n\\). The computational cost increase exponentially with increase in number of hyperparameters.\nIn case of a hyperparameter having continuous values, the distribution of the hyperparameter can be specified in RandomizedSearchCV().\nIn case of effect sparsity of hyperparameters, i.e., if only a few hyperparameters significantly effect prediction accuracy, RandomizedSearchCV() is likely to consider more unique values of the influential hyperparameters as compared to GridSearchCV(), and is thus likely to provide more optimal hyperparameter values as compared to GridSearchCV(). The figure below shows effect sparsity where there are 2 hyperparameters, but only one of them is associated with the cross-validated score, Here, it is more likely that the optimal cross-validated score will be obtained by RandomizedSearchCV(), as it is evaluating the model on 9 unique values of the relevant hyperparameter, instead of just 3.",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#bayessearchcv",
    "href": "Hyperparameter tuning.html#bayessearchcv",
    "title": "3  Hyperparameter tuning",
    "section": "3.3 BayesSearchCV()",
    "text": "3.3 BayesSearchCV()\nUnlike the grid search and random search, which treat hyperparameter sets independently, the Bayesian optimization is an informed search method, meaning that it learns from previous iterations. The number of trials in this approach is determined by the user.\n\nThe function begins by computing the cross-validated score by randomly selecting a few hyperparameter values from the specified disttribution of hyperparameter values.\nBased on the data of hyperparameter values tested (predictors), and the cross-validated score (the response), a Gaussian process model is developed to estimate the cross-validated score & the uncertainty in the estimate in the entire space of the hyperparameter values\nA criterion that “explores” uncertain regions of the space of hyperparameter values (where it is difficult to predict cross-validated score), and “exploits” promising regions of the space are of hyperparameter values (where the cross-validated score is predicted to minimize) is used to suggest the next hyperparameter value that will potentially minimize the cross-validated score\nCross-validated score is computed at the suggested hyperparameter value, the Gaussian process model is updated, and the previous step is repeated, until a certain number of iterations specified by the user.\n\nTo summarize, instead of blindly testing the model for the specified hyperparameter values (as in GridSearchCV()), or randomly testing the model on certain hyperparameter values (as in RandomizedSearchCV()), BayesSearchCV() smartly tests the model for those hyperparameter values that are likely to reduce the cross-validated score. The algorithm becomes “smarter” as it “learns” more with increasing iterations.\nHere is a nice blog, if you wish to understand more about the Bayesian optimization procedure.\n\n# BayesSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor(metric = 'minkowski') # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n# the keys should be EXACTLY the same as the names of the model inputs\n# the values should be the distribution of hyperparameter values. Lists and NumPy arrays can\n# also be used\n    \ngrid = {'n_neighbors': Integer(1, 500), 'weights': Categorical(['uniform', 'distance']), \n       'p': Real(1, 10, prior = 'uniform')} \n\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, \n# use it if you have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within \n# the function\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n\n# Fit the models, and cross-validate\n\n# Sometimes the Gaussian process model predicting the cross-validated score suggests a \n# \"promising point\" (i.e., set of hyperparameter values) for cross-validation that it has \n# already suggested earlier. In such  a case a warning is raised, and the objective \n# function (i.e., the cross-validation score) is computed at a randomly selected point \n# (as in RandomizedSearchCV()). This feature helps the algorithm explore other regions of\n# the hyperparameter space, rather than only searching in the promising regions. Thus, it \n# balances exploration (of the hyperparameter space) with exploitation (of the promising \n# regions of the hyperparameter space)\n\nwarnings.filterwarnings(\"ignore\")\ngcv.fit(X_train_scaled, y_train)\nwarnings.resetwarnings()\n\nThe optimal hyperparameter values (based on Bayesian search) on the provided distribution of hyperparameter values are:\n\ngcv.best_params_\n\nOrderedDict([('n_neighbors', 9),\n             ('p', 1.0008321732366932),\n             ('weights', 'distance')])\n\n\nThe cross-validated root mean squared error for the optimal hyperparameter values is:\n\n-gcv.best_score_\n\n5756.172382596493\n\n\nThe RMSE on test data for the optimal hyperparameter values is:\n\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n5740.432278861367\n\n\n\n3.3.1 Diagonosis of cross-validated score optimization\nBelow are the partial dependence plots of the objective function (i.e., the cross-validated score). The cross-validated score predictions are based on the most recently updated model (i.e., the updated Gaussian Process model at the end of n_iter iterations specified by the user) that predicts the cross-validated score.\nCheck the plot_objective() documentation to interpret the plots.\n\nplot_objective(gcv.optimizer_results_[0],\n                   dimensions=[\"n_neighbors\", \"p\", \"weights\"], size = 3)\nplt.show();\n\n\n\n\n\n\n\n\nThe frequence of individual hyperparameter values considered can also be visualized as below.\n\nfig, ax = plt.subplots(1, 3, figsize = (10, 3))\nplt.subplots_adjust(wspace=0.4)\nplot_histogram(gcv.optimizer_results_[0], 0, ax = ax[0])\nplot_histogram(gcv.optimizer_results_[0], 1, ax = ax[1])\nplot_histogram(gcv.optimizer_results_[0], 2, ax = ax[2])\nplt.show()\n\n\n\n\n\n\n\n\nBelow is the plot showing the minimum cross-validated score computed obtained until ‘n’ hyperparameter values are considered for cross-validation.\n\nplot_convergence(gcv.optimizer_results_)\nplt.show()\n\n\n\n\n\n\n\n\nNote that the cross-validated error is close to the optmial value in the 53rd iteration itself.\nThe cross-validated error at the 53rd iteration is:\n\ngcv.optimizer_results_[0]['func_vals'][53]\n\n5831.87280274334\n\n\nThe hyperparameter values at the 53rd iterations are:\n\ngcv.optimizer_results_[0]['x_iters'][53]\n\n[15, 1.0, 'distance']\n\n\nNote that this is the 2nd most optimal hyperparameter value based on GridSearchCV().\nBelow is the plot showing the cross-validated score computed at each of the 180 hyperparameter values considered for cross-validation. The plot shows that the algorithm seems to explore new regions of the domain space, instead of just exploting the promising ones. There is a balance between exploration and exploitation for finding the optimal hyperparameter values that minimize the objective function (i.e., the function that models the cross-validated score).\n\nsns.lineplot(x = range(1, 181), y = gcv.optimizer_results_[0]['func_vals'])\nplt.xlabel('Iteration')\nplt.ylabel('Cross-validated score')\nplt.show();\n\n\n\n\n\n\n\n\nThe advantages of BayesSearchCV() over GridSearchCV() and RandomizedSearchCV() are:\n\nThe Bayesian Optimization approach gives the benefit that we can give a much larger range of possible values, since over time we identify and exploit the most promising regions and discard the not so promising ones. Plain grid-search would burn computational resources to explore all regions of the domain space with the same granularity, even the not promising ones. Since we search much more effectively in Bayesian search, we can search over a larger domain space.\nBayesSearch CV may help us identify the optimal hyperparameter value in fewer iterations if the Gaussian process model estimating the cross-validated score is relatively accurate. However, this is not certain. Grid and random search are completely uninformed by past evaluations, and as a result, often spend a significant amount of time evaluating “bad” hyperparameters.\nBayesSearch CV is more reliable in cases of a large search space, where random selection may miss sampling values from optimal regions of the search space.\n\nThe disadvantages of BayesSearchCV() over GridSearchCV() and RandomizedSearchCV() are:\n\nBayesSearchCV() has a cost of learning from past data, i.e., updating the model that predicts the cross-validated score after every iteration of evaluating the cross-validated score on a new hyperparameter value. This cost will continue to increase as more and more data is collected. There is no such cost in GridSearchCV() and RandomizedSearchCV() as there is no learning. This implies that each iteration of BayesSearchCV() will take a longer time than each iteration of GridSearchCV() / RandomizedSearchCV(). Thus, even if BayesSearchCV() finds the optimal hyperparameter value in fewer iterations, it may take more time than GridSearchCV() / RandomizedSearchCV() for the same.\nThe success of BayesSearchCV() depends on the predictions and associated uncertainty estimated by the Gaussian process (GP) model that predicts the cross-validated score. The GP model, although works well in general, may not be suitable for certain datasets, or may take a relatively large number of iterations to learn for certain datasets.\n\n\n\n3.3.2 Live monitoring of cross-validated score\nNote that it will be useful monitor the cross-validated score while the Bayesian Search CV code is running, and stop the code as soon as the desired accuracy is reached, or the optimal cross-validated score doesn’t seem to improve. The fit() method of the BayesSeaerchCV() object has a callback argument that can be used as follows:\n\nmodel = KNeighborsRegressor(metric = 'minkowski') # No inputs defined inside the model\ngrid = {'n_neighbors': Integer(1, 500), 'weights': Categorical(['uniform', 'distance']), \n       'p': Real(1, 10, prior = 'uniform')} \n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n\n\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\n\n\ngcv.fit(X_train_scaled, y_train, callback = monitor)\n\n['n_neighbors', 'p', 'weights'] = [9, 1.0008321732366932, 'distance'] 5756.172382596493\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'n_neighbors': Integer(low=1, high=500, prior='uniform', transform='normalize'),\n                             'p': Real(low=1, high=10, prior='uniform', transform='normalize'),\n                             'weights': Categorical(categories=('uniform', 'distance'), prior=None)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'n_neighbors': Integer(low=1, high=500, prior='uniform', transform='normalize'),\n                             'p': Real(low=1, high=10, prior='uniform', transform='normalize'),\n                             'weights': Categorical(categories=('uniform', 'distance'), prior=None)})estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#cross_validate",
    "href": "Hyperparameter tuning.html#cross_validate",
    "title": "3  Hyperparameter tuning",
    "section": "3.4 cross_validate()",
    "text": "3.4 cross_validate()\nWe have used cross_val_score() and cross_val_predict() so far.\nWhen can we use one over the other?\nThe function cross_validate() is similar to cross_val_score() except that it has the option to return multiple cross-validated metrics, instead of a single one.\nConsider the heart disease classification problem, where the response is target (whether the person has a heart disease or not).\n\ndata = pd.read_csv('Datasets/heart_disease_classification.csv')\ndata.head()\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\nLet us pre-process the data.\n\n# First, separate the response and the predictors\ny = data['target']\nX = data.drop('target', axis=1)\n\n\n# Separate the data (X,y) into training and test\n\n# Inputs:\n    # data\n    # train-test ratio\n    # random_state for reproducible code\n    \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20, stratify=y) # 80%-20% split\n\n# stratify=y makes sure the class 0 to class 1 ratio in the training and test sets are kept the same as the entire dataset.\n\n\nmodel = KNeighborsClassifier() \nsc = StandardScaler()\nsc.fit(X_train)\nX_train_scaled = sc.transform(X_train)\nX_test_scaled = sc.transform(X_test)\n\nSuppose we want to take recall above a certain threshold with the highest precision possible. cross_validate() computes the cross-validated score for multiple metrics - rest is the same as cross_val_score().\n\nKs = np.arange(10,200,10)\n\nscores = []\n\nfor K in Ks:\n    model = KNeighborsClassifier(n_neighbors=K) # Keeping distance uniform\n    scores.append(cross_validate(model, X_train_scaled, y_train, cv=5, scoring = ['accuracy','recall', 'precision']))\n\n\nscores\n\n# The output is now a list of dicts - easy to convert to a df\n\ndf_scores = pd.DataFrame(scores) # We need to handle test_recall and test_precision cols\n\ndf_scores['CV_recall'] = df_scores['test_recall'].apply(np.mean)\ndf_scores['CV_precision'] = df_scores['test_precision'].apply(np.mean)\ndf_scores['CV_accuracy'] = df_scores['test_accuracy'].apply(np.mean)\n\ndf_scores.index = Ks # We can set K values as indices for convenience\n\n\n#df_scores\n# What happens as K increases?\n    # Recall increases (not monotonically)\n    # Precision decreases (not monotonically)\n# Why?\n    # Check the class distribution in the data - more obs with class 1\n    # As K gets higher, the majority class overrules (visualized in the slides)\n    # More 1s means less FNs - higher recall\n    # More 1s means more FPs - lower precision\n# Would this be the case for any dataset?\n    # NO!! Depends on what the majority class is!\n\nSuppose we wish to have the maximum possible precision for at least 95% recall.\nThe optimal K will be:\n\ndf_scores.loc[df_scores['CV_recall'] &gt; 0.95, 'CV_precision'].idxmax()\n\n120\n\n\nThe cross-validated precision, recall and accuracy for the optimal K are:\n\ndf_scores.loc[120, ['CV_recall', 'CV_precision', 'CV_accuracy']]\n\nCV_recall       0.954701\nCV_precision    0.734607\nCV_accuracy     0.785374\nName: 120, dtype: object\n\n\n\nsns.lineplot(x = df_scores.index, y = df_scores.CV_precision, color = 'blue', label = 'precision')\nsns.lineplot(x = df_scores.index, y = df_scores.CV_recall, color = 'red', label = 'recall')\nsns.lineplot(x = df_scores.index, y = df_scores.CV_accuracy, color = 'green', label = 'accuracy')\nplt.ylabel('Metric')\nplt.xlabel('K')\nplt.show()",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "regression_tree_sp25.html",
    "href": "regression_tree_sp25.html",
    "title": "4  Regression trees",
    "section": "",
    "text": "4.1 Native Support for Missing Values\nRead section 8.1.1 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nWe will use the same dataset as in the KNN model for regression trees.\nStarting with scikit-learn version 1.3, classical tree-based models in scikit-learn have added native support for missing values, which simplifies preprocessing and improves model robustness:\nThis means you no longer need to impute missing values manually before training these models.\nTo take advantage of this feature, first check your scikit-learn version:\nimport sklearn\nprint(sklearn.__version__)\n\n1.6.1\nIf your version is below 1.4.0, you can upgrade by running:\n# pip install --upgrade scikit-learn\n# Make a copy of the original dataset\ncar_missing = car.copy()\n\n# Randomly add missing values\n# Inject missing values into 10% of the 'mileage' column\ncar_missing.loc[car_missing.sample(frac=0.1, random_state=42).index, 'mileage'] = np.nan\n# Inject missing values into 10% of the 'fuelType' and 'engineSize' columns\n\ncar_missing.loc[car_missing.sample(frac=0.1, random_state=42).index, 'fuelType'] = np.nan\ncar_missing.loc[car_missing.sample(frac=0.1, random_state=42).index, 'engineSize'] = np.nan\ncar_missing.isna().sum()\n\nbrand             0\nmodel             0\nyear              0\ntransmission      0\nmileage         763\nfuelType        763\ntax               0\nmpg               0\nengineSize      763\nprice             0\ndtype: int64\n# Split the car_missing dataset into features and target\nX_missing = car_missing.drop(columns=['price'])\ny_missing = car_missing['price']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "regression_tree_sp25.html#native-support-for-missing-values",
    "href": "regression_tree_sp25.html#native-support-for-missing-values",
    "title": "4  Regression trees",
    "section": "",
    "text": "DecisionTreeClassifier supports missing values as of version 1.3.0\n\nRandomForestClassifier adds support in version 1.4.0\n\n\n\n\n\n\n\n\n\n\n4.1.1 Build a regression tree using mileage as the solo predictor\n\n# Use only 'mileage' as the feature\nX_mileage = X_missing[['mileage']]\ny_mileage = y_missing\n\n\n# Create a DecisionTreeRegressor model\nreg_tree = DecisionTreeRegressor(random_state=42)\n\n# Fit the model to the data\nreg_tree.fit(X_mileage, y_mileage)\n\nDecisionTreeRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(random_state=42) \n\n\n\n# Predict the target variable using the model\ny_pred = reg_tree.predict(X_mileage)\n\n# Calculate the RMSE and R² score\nrmse = np.sqrt(np.mean((y_missing - y_pred) ** 2))\nr2 = r2_score(y_missing, y_pred)\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"R²: {r2:.2f}\")\n\nRMSE: 8797.85\nR²: 0.71",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "regression_tree_sp25.html#building-regression-trees",
    "href": "regression_tree_sp25.html#building-regression-trees",
    "title": "4  Regression trees",
    "section": "4.2 Building regression trees",
    "text": "4.2 Building regression trees\n\nX = car.drop(columns=['price'])\ny = car['price']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n4.2.1 Using only mileage feature\n\n# Use only 'mileage' as the feature\nX_train_mileage = X_train[['mileage']]\nX_test_mileage = X_test[['mileage']]\n\n# Create a DecisionTreeRegressor model\nreg_tree = DecisionTreeRegressor(random_state=42, max_depth=3)\n\n# Fit the model to the training data\nreg_tree.fit(X_train_mileage, y_train)\n\n# Predict the target variable using the model\ny_pred = reg_tree.predict(X_test_mileage)\n\n# Calculate the RMSE and R² score\nrmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\nr2 = r2_score(y_test, y_pred)\nprint(f\"RMSE using only mileage predictor: {rmse:.2f}\")\nprint(f\"R² using only mileage predictor: {r2:.2f}\")\n\nRMSE using only mileage predictor: 14437.80\nR² using only mileage predictor: 0.29\n\n\nLet’s visualize the tree structure\n\n# Plot the tree\nplt.figure(figsize=(18, 6))\nplot_tree(reg_tree, feature_names=['mileage'], filled=True, rounded=True)\nplt.title(\"Regression Tree Using Mileage\")\nplt.show()\n\n\n\n\n\n\n\n\nLet’s visualize how mileage is used in the decision tree below:\n\n# Create evenly spaced mileage values within the range of training data\nXtest = np.linspace(X_train_mileage['mileage'].min(), X_train_mileage['mileage'].max(), 100).reshape(-1, 1)\n\n# Convert Xtest to a DataFrame with the correct column name\nXtest_df = pd.DataFrame(Xtest, columns=['mileage'])\n\n# Predict using the DataFrame instead of NumPy array\nytest_pred = reg_tree.predict(Xtest_df)\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=X_train_mileage['mileage'], y=y_train, color='orange', label='Training data')\n\n# Step plot to reflect piecewise constant predictions\nplt.step(Xtest_df['mileage'], ytest_pred, color='blue', label='Tree prediction', where='mid')\n\nplt.xlabel(\"Mileage\")\nplt.ylabel(\"Price\")\nplt.title(\"Decision Tree Regression: Mileage vs. Price\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAll cars falling within the same terminal node have the same predicted price, which is seen as flat line segments in the above model curve.\n\n\n4.2.2 Using mileage and brand as predictors\n\nX_train.head() \n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\n\n\n\n\n216\nvw\nScirocco\n2016\nManual\n41167\nDiesel\n20\n55.2654\n2.0\n\n\n4381\nmerc\nCLS Class\n2018\nSemi-Auto\n12078\nDiesel\n145\n47.7624\n2.9\n\n\n6891\nhyundi\nSanta Fe\n2019\nAutomatic\n623\nDiesel\n145\n43.0887\n2.2\n\n\n421\nhyundi\nIX35\n2014\nManual\n37095\nDiesel\n145\n53.4862\n1.7\n\n\n505\nford\nEdge\n2016\nSemi-Auto\n15727\nDiesel\n160\n49.0741\n2.0\n\n\n\n\n\n\n\n\n# Select features and target\nX_train_tree = X_train[['mileage', 'brand']]\n\nX_test_tree = X_test[['mileage', 'brand']]\n\n\n# One-hot encode the categorical variable 'brand'\nX_train_tree_encoded = pd.get_dummies(X_train_tree, columns=['brand'])\nX_test_tree_encoded = pd.get_dummies(X_test_tree, columns=['brand'])\n\n\nmodel = DecisionTreeRegressor(max_depth=3, random_state=42)\nmodel.fit(X_train_tree_encoded, y_train)\n\nDecisionTreeRegressor(max_depth=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(max_depth=3, random_state=42) \n\n\n\nplt.figure(figsize=(12, 6))\nplot_tree(model, feature_names=X_train_tree_encoded.columns, filled=True, rounded=True)\nplt.title(\"Decision Tree Using Mileage and Brand to Predict Price\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Predict the target variable using the model\ny_pred_tree = model.predict(X_test_tree_encoded)\n\n# Calculate the RMSE and R² score\nrmse_tree = np.sqrt(np.mean((y_test - y_pred_tree) ** 2))\nr2_tree = r2_score(y_test, y_pred_tree)\nprint(f\"RMSE using mileage and brand predictor: {rmse_tree:.2f}\")\nprint(f\"R² using mileage and brand predictor: {r2_tree:.2f}\")\n\n# Compare the performance of the two models\nprint(f\"RMSE using only mileage predictor: {rmse:.2f}\")\nprint(f\"RMSE using mileage and brand predictor: {rmse_tree:.2f}\")\n# The RMSE using mileage and brand predictor is lower than using only mileage predictor.\n# This indicates that adding the brand feature improves the model's performance\n\nRMSE using mileage and brand predictor: 12531.44\nR² using mileage and brand predictor: 0.46\nRMSE using only mileage predictor: 14437.80\nRMSE using mileage and brand predictor: 12531.44\n\n\n\n\n4.2.3 Using all predictors\nNow that we’ve explored a single predictor (mileage) and added a second predictor (brand), let’s take it a step further and use all available features to build a more robust model.\nWe’ll construct a pipeline that handles necessary preprocessing steps (e.g., categorical encoding) and fits a Decision Tree Regressor in a streamlined and reproducible way.\n\n# extract the categorical columns and put them in a list\ncategorical_feature = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumerical_feature = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n\n# Create a ColumnTransformer to handle encoding\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_feature)\n    ],\n    remainder='passthrough',  # Keep numerical feature (mileage) unchanged\n    force_int_remainder_cols=False\n)\n\n# Create the pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', DecisionTreeRegressor(max_depth=4, random_state=42))\n])\n\n# Usage:\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n\n# Calculate the RMSE and R² score\nrmse_pipeline = np.sqrt(np.mean((y_test - y_pred) ** 2))\nr2_pipeline = r2_score(y_test, y_pred)\nprint(f\"RMSE using pipeline: {rmse_pipeline:.2f}\")\nprint(f\"R² using pipeline: {r2_pipeline:.2f}\")\n\nRMSE using pipeline: 8186.34\nR² using pipeline: 0.77\n\n\n\n# let's visuzalize the decision tree\n\nplt.figure(figsize=(12, 6))\nplot_tree(pipeline.named_steps['regressor'], feature_names=pipeline.named_steps['preprocessor'].get_feature_names_out(), filled=True, rounded=True)\nplt.title(\"Decision Tree Using Pipeline\")\nplt.show()",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "regression_tree_sp25.html#key-hyperparameters-in-decision-tree",
    "href": "regression_tree_sp25.html#key-hyperparameters-in-decision-tree",
    "title": "4  Regression trees",
    "section": "4.3 Key Hyperparameters in Decision Tree",
    "text": "4.3 Key Hyperparameters in Decision Tree\nIn regression trees, model complexity is controlled by hyperparameters, tuning them is crucial for balancing underfitting and overfitting.\n\n4.3.1 Underfitting\n\nThe model is too simple to capture patterns in the data.\nHigh bias, low variance.\nOften caused by:\n\nShallow trees (max_depth is too small)\nToo strict constraints (min_samples_split or min_samples_leaf is too high)\n\n\n\n\n4.3.2 Overfitting\n\nThe model is too complex and learns noise from the training data.\nLow bias, high variance.\nOften caused by:\n\nDeep trees with many splits\nVery small min_samples_leaf or min_samples_split\n\n\nBelow are the most commonly used hyperparameters:\n\n\n4.3.3 max_depth\n\nControls the maximum depth of the tree.\nIf None, the tree will expand until all leaves are pure or contain fewer than min_samples_split samples.\nControls overfitting (deep trees → overfit, shallow trees → underfit)\nTypical values: 3 to 20 (start with lower values).\n\n\n\n4.3.4 min_samples_split\n\nThe minimum number of samples required to split an internal node.\nhigher values → simpler trees (reducing overfitting)\n\n\n\n4.3.5 min_samples_leaf\n\nThe minimum number of samples required to be at a leaf node.\nSetting this to a higher number can smooth the model by reducing variance.\n\n\n\n4.3.6 max_features\n\nNumber of features to consider when looking for the best split.\nCan be set to:\n\n\"auto\" or None: use all features\n\"sqrt\": use the square root of the number of features\n\"log2\": use log base 2\n\n\n\n# Define your parameter grid with pipeline step prefix\nparam_grid = {\n    'regressor__max_depth': [3, 5, 7, 10, None],\n    'regressor__min_samples_split': [2, 5, 10],\n    'regressor__min_samples_leaf': [1, 2, 4],\n    'regressor__max_features': ['sqrt', None]\n}\n\n# Create custom scorer for RMSE\nrmse_scorer = make_scorer(lambda y_true, y_pred: root_mean_squared_error(y_true, y_pred),\n                          greater_is_better=False)\n# Create GridSearchCV object\ngrid_search = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    scoring={\n        'RMSE': rmse_scorer,\n        'R2': 'r2'\n    },\n    refit='R2',\n    cv=5,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit the grid search to training data\ngrid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 270 candidates, totalling 1350 fits\n\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(force_int_remainder_cols=False,\n                                                          remainder='passthrough',\n                                                          transformers=[('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['brand',\n                                                                          'model',\n                                                                          'transmission',\n                                                                          'fuelType'])])),\n                                       ('regressor',\n                                        DecisionTreeRegressor(max_depth=4,\n                                                              random_state=42))]),\n             n_jobs=-1,\n             param_grid={'regressor__ccp_alpha': [0.001, 0.01, 0.1],\n                         'regressor__max_depth': [3, 5, 7, 10, None],\n                         'regressor__max_features': ['sqrt', None],\n                         'regressor__min_samples_leaf': [1, 2, 4],\n                         'regressor__min_samples_split': [2, 5, 10]},\n             refit='R2',\n             scoring={'R2': 'r2',\n                      'RMSE': make_scorer(&lt;lambda&gt;, greater_is_better=False, response_method='predict')},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(force_int_remainder_cols=False,\n                                                          remainder='passthrough',\n                                                          transformers=[('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['brand',\n                                                                          'model',\n                                                                          'transmission',\n                                                                          'fuelType'])])),\n                                       ('regressor',\n                                        DecisionTreeRegressor(max_depth=4,\n                                                              random_state=42))]),\n             n_jobs=-1,\n             param_grid={'regressor__ccp_alpha': [0.001, 0.01, 0.1],\n                         'regressor__max_depth': [3, 5, 7, 10, None],\n                         'regressor__max_features': ['sqrt', None],\n                         'regressor__min_samples_leaf': [1, 2, 4],\n                         'regressor__min_samples_split': [2, 5, 10]},\n             refit='R2',\n             scoring={'R2': 'r2',\n                      'RMSE': make_scorer(&lt;lambda&gt;, greater_is_better=False, response_method='predict')},\n             verbose=1) best_estimator_: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(force_int_remainder_cols=False,\n                                   remainder='passthrough',\n                                   transformers=[('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('regressor',\n                 DecisionTreeRegressor(ccp_alpha=0.001, min_samples_split=5,\n                                       random_state=42))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(force_int_remainder_cols=False, remainder='passthrough',\n                  transformers=[('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') remainder['year', 'mileage', 'tax', 'mpg', 'engineSize'] passthroughpassthrough DecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(ccp_alpha=0.001, min_samples_split=5, random_state=42) \n\n\nThe GridSearchCV setup evaluates both RMSE and R² during cross-validation.\n\nR² is used to select the best model and is also used to refit the model on the entire training set.\nRMSE is computed during the process for evaluation purposes, but it is not used to determine the best model.\n\nThis allows for more comprehensive model assessment while still optimizing based on a single selected metric.\n\n# Get best estimator and predictions\nbest_model = grid_search.best_estimator_\ny_pred_tuned = best_model.predict(X_test)\n\n# Calculate metrics for tuned model\nrmse_tuned = root_mean_squared_error(y_test, y_pred_tuned)\nr2_tuned = r2_score(y_test, y_pred_tuned)\n\n\nprint(\"\\n=== Best Parameters ===\")\nprint(grid_search.best_params_)\nprint(\"\\n=== Tuned Model Performance ===\")\nprint(f\"RMSE (Tuned): {rmse_tuned:.2f}\")\nprint(f\"R² (Tuned): {r2_tuned:.2f}\")\nprint(f\"Improvement in R²: {(r2_tuned - r2_pipeline):.2%}\")\n\n\n=== Best Parameters ===\n{'regressor__ccp_alpha': 0.001, 'regressor__max_depth': None, 'regressor__max_features': None, 'regressor__min_samples_leaf': 1, 'regressor__min_samples_split': 5}\n\n=== Tuned Model Performance ===\nRMSE (Tuned): 4726.17\nR² (Tuned): 0.92\nImprovement in R²: 15.23%\n\n\n\nprint(\"\\n=== Best Parameters ===\")\nprint(grid_search.best_params_)\nprint(\"\\n=== Tuned Model Performance ===\")\nprint(f\"RMSE (Tuned): {rmse_tuned:.2f}\")\nprint(f\"R² (Tuned): {r2_tuned:.2f}\")\nprint(f\"Improvement in R²: {(r2_tuned - r2_pipeline):.2%}\")\n\n\n=== Best Parameters ===\n{'regressor__ccp_alpha': 0.001, 'regressor__max_depth': None, 'regressor__max_features': None, 'regressor__min_samples_leaf': 1, 'regressor__min_samples_split': 5}\n\n=== Tuned Model Performance ===\nRMSE (Tuned): 4726.17\nR² (Tuned): 0.92\nImprovement in R²: 15.23%\n\n\nGridSearchCV improves the r squared from 0.77 to 0.92, increased by 15.23%, Let us visualize the mean squared error based on the hyperparameter values. We’ll use the cross validation results stored in the cv_results_ attribute of the GridSearchCV fit() object.\n\n#Detailed results of k-fold cross validation\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results.head()\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_regressor__ccp_alpha\nparam_regressor__max_depth\nparam_regressor__max_features\nparam_regressor__min_samples_leaf\nparam_regressor__min_samples_split\nparams\n...\nstd_test_RMSE\nrank_test_RMSE\nsplit0_test_R2\nsplit1_test_R2\nsplit2_test_R2\nsplit3_test_R2\nsplit4_test_R2\nmean_test_R2\nstd_test_R2\nrank_test_R2\n\n\n\n\n0\n0.075184\n0.013049\n0.009999\n0.001052\n0.001\n3\nsqrt\n1\n2\n{'regressor__ccp_alpha': 0.001, 'regressor__ma...\n...\n607.458245\n250\n0.42527\n0.37778\n0.538252\n0.37182\n0.364829\n0.415590\n0.064903\n250\n\n\n1\n0.078505\n0.014115\n0.010804\n0.001051\n0.001\n3\nsqrt\n1\n5\n{'regressor__ccp_alpha': 0.001, 'regressor__ma...\n...\n607.458245\n250\n0.42527\n0.37778\n0.538252\n0.37182\n0.364829\n0.415590\n0.064903\n250\n\n\n2\n0.077370\n0.011081\n0.010873\n0.000946\n0.001\n3\nsqrt\n1\n10\n{'regressor__ccp_alpha': 0.001, 'regressor__ma...\n...\n606.926403\n244\n0.42527\n0.37778\n0.538252\n0.37182\n0.365155\n0.415655\n0.064852\n244\n\n\n3\n0.036274\n0.036182\n0.010617\n0.000345\n0.001\n3\nsqrt\n2\n2\n{'regressor__ccp_alpha': 0.001, 'regressor__ma...\n...\n607.458245\n250\n0.42527\n0.37778\n0.538252\n0.37182\n0.364829\n0.415590\n0.064903\n250\n\n\n4\n0.018702\n0.003564\n0.012120\n0.004090\n0.001\n3\nsqrt\n2\n5\n{'regressor__ccp_alpha': 0.001, 'regressor__ma...\n...\n607.458245\n250\n0.42527\n0.37778\n0.538252\n0.37182\n0.364829\n0.415590\n0.064903\n250\n\n\n\n\n5 rows × 26 columns\n\n\n\n\n# Plotting the RMSE for different max_depth values\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=cv_results, x='param_regressor__max_depth', y=np.abs(cv_results['mean_test_RMSE']), marker='o')\nplt.xlabel('Max Depth')\nplt.ylabel('Mean Test RMSE')\nplt.title('RMSE vs Max Depth');\n\n\n\n\n\n\n\n\n\n\n4.3.7 Output feature importance\n\n# Get feature importances and names\nfeature_importances = best_model.named_steps['regressor'].feature_importances_\nfeature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n\n# Create DataFrame and select top 10\nfeature_importance_df = (\n    pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n    .sort_values(by='Importance', ascending=False)\n    .head(10)  # Keep only top 10 features\n)\n\n# Print top 10 features\nprint(\"=== Top 10 Feature Importances ===\")\nprint(feature_importance_df)\n\n# Plot top 10 features\nplt.figure(figsize=(12, 6))\nsns.barplot(data=feature_importance_df, x='Importance', y='Feature')\nplt.title('Top 10 Feature Importances from Decision Tree')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.tight_layout()\nplt.show()\n\n=== Top 10 Feature Importances ===\n                   Feature  Importance\n112  remainder__engineSize    0.437921\n109     remainder__mileage    0.173215\n108        remainder__year    0.149303\n111         remainder__mpg    0.086922\n98          cat__model_ i8    0.017971\n2          cat__brand_ford    0.013837\n92          cat__model_ X7    0.013477\n110         remainder__tax    0.011502\n60     cat__model_ Mustang    0.009517\n86     cat__model_ V Class    0.008147",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "regression_tree_sp25.html#cost-complexity-pruning-ccp_alpha",
    "href": "regression_tree_sp25.html#cost-complexity-pruning-ccp_alpha",
    "title": "4  Regression trees",
    "section": "4.4 Cost-Complexity Pruning (ccp_alpha)",
    "text": "4.4 Cost-Complexity Pruning (ccp_alpha)\nCost-complexity pruning is a post-pruning technique used to reduce the size of a decision tree by removing sections that provide little to no improvement in prediction accuracy. It helps prevent overfitting and improves model generalization.\n\n4.4.1 Key Idea\nEach subtree in a decision tree has an associated cost-complexity score:\n$ R_(T) = R(T) + |T| $\n\n$ R(T) $: Total training error of the tree ( T )\n$ |T| $: Number of leaf nodes in the tree\n\\(alpha\\) (ccp_alpha): Complexity parameter that penalizes tree size\n\nAs \\(alpha\\) increases, the tree is pruned more aggressively.\n\n\n4.4.2 Parameter: ccp_alpha in scikit-learn\n\nAvailable in DecisionTreeRegressor and DecisionTreeClassifier\nDefault: ccp_alpha = 0.0 (no pruning)\nIncreasing ccp_alpha encourages simpler trees by penalizing extra leaf nodes\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, mean_squared_error\nimport numpy as np\n\n# Define your parameter grid with pipeline step prefix\nparam_grid = {\n    'regressor__ccp_alpha': [0.0, 0.001, 0.01, 0.1]\n}\n\n# Create custom scorer for RMSE\nrmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n                          greater_is_better=False)\n\n# Create GridSearchCV object\ngrid_search_ccp = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    scoring={\n        'RMSE': rmse_scorer,\n        'R2': 'r2'\n    },\n    refit='R2',\n    cv=5,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit the grid search to training data\ngrid_search_ccp.fit(X_train, y_train)\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(force_int_remainder_cols=False,\n                                                          remainder='passthrough',\n                                                          transformers=[('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['brand',\n                                                                          'model',\n                                                                          'transmission',\n                                                                          'fuelType'])])),\n                                       ('regressor',\n                                        DecisionTreeRegressor(max_depth=4,\n                                                              random_state=42))]),\n             n_jobs=-1,\n             param_grid={'regressor__ccp_alpha': [0.0, 0.001, 0.01, 0.1]},\n             refit='R2',\n             scoring={'R2': 'r2',\n                      'RMSE': make_scorer(&lt;lambda&gt;, greater_is_better=False, response_method='predict')},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(force_int_remainder_cols=False,\n                                                          remainder='passthrough',\n                                                          transformers=[('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['brand',\n                                                                          'model',\n                                                                          'transmission',\n                                                                          'fuelType'])])),\n                                       ('regressor',\n                                        DecisionTreeRegressor(max_depth=4,\n                                                              random_state=42))]),\n             n_jobs=-1,\n             param_grid={'regressor__ccp_alpha': [0.0, 0.001, 0.01, 0.1]},\n             refit='R2',\n             scoring={'R2': 'r2',\n                      'RMSE': make_scorer(&lt;lambda&gt;, greater_is_better=False, response_method='predict')},\n             verbose=1) best_estimator_: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(force_int_remainder_cols=False,\n                                   remainder='passthrough',\n                                   transformers=[('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('regressor',\n                 DecisionTreeRegressor(max_depth=4, random_state=42))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(force_int_remainder_cols=False, remainder='passthrough',\n                  transformers=[('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') remainder['year', 'mileage', 'tax', 'mpg', 'engineSize'] passthroughpassthrough DecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(max_depth=4, random_state=42) \n\n\n\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\nX_train_encoded = encoder.fit_transform(X_train[categorical_feature])\nX_test_encoded = encoder.transform(X_test[categorical_feature])\n\n# Convert the encoded features back to DataFrame\nX_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_feature))\nX_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_feature))\n\n# Concatenate the encoded features with the original numerical features\nX_train_final = pd.concat([X_train_encoded_df, X_train[numerical_feature].reset_index(drop=True)], axis=1)\nX_test_final = pd.concat([X_test_encoded_df, X_test[numerical_feature].reset_index(drop=True)], axis=1)\n\n# Check the final shape of the training and testing sets\nprint(\"Training set shape:\", X_train_final.shape)\nprint(\"Testing set shape:\", X_test_final.shape)\n# Check the first few rows of the final training set\nX_train_final.head()\n# Check the first few rows of the final testing set\n\nTraining set shape: (6105, 113)\nTesting set shape: (1527, 113)\n\n\n\n\n\n\n\n\n\nbrand_audi\nbrand_bmw\nbrand_ford\nbrand_hyundi\nbrand_merc\nbrand_skoda\nbrand_toyota\nbrand_vauxhall\nbrand_vw\nmodel_ 6 Series\n...\nfuelType_Diesel\nfuelType_Electric\nfuelType_Hybrid\nfuelType_Other\nfuelType_Petrol\nyear\nmileage\ntax\nmpg\nengineSize\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n2016\n41167\n20\n55.2654\n2.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n2018\n12078\n145\n47.7624\n2.9\n\n\n2\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n2019\n623\n145\n43.0887\n2.2\n\n\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n2014\n37095\n145\n53.4862\n1.7\n\n\n4\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n2016\n15727\n160\n49.0741\n2.0\n\n\n\n\n5 rows × 113 columns\n\n\n\n\nmodel = DecisionTreeRegressor(random_state = 1)#model without any restrictions\npath= model.cost_complexity_pruning_path(X_train_final,y_train)# Compute the pruning path during Minimal Cost-Complexity Pruning.\n\n\n# Extract the effective alphas and the corresponding performance metrics\nccp_alphas = path.ccp_alphas\nimpurities = path.impurities\n# Create a DataFrame to store the results\nccp_results = pd.DataFrame({'ccp_alpha': ccp_alphas, 'impurity': impurities})\n# Fit the model for each alpha value and calculate the mean test score\nmean_test_scores = []\nfor alpha in ccp_alphas:\n    model = DecisionTreeRegressor(random_state=1, ccp_alpha=alpha)\n    model.fit(X_train_final, y_train)\n    y_pred = model.predict(X_test_final)\n    mean_test_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n# Add the mean test scores to the DataFrame\nccp_results['mean_test_score'] = mean_test_scores\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.plot(ccp_results['ccp_alpha'], ccp_results['mean_test_score'], marker='o')\nplt.xlabel('ccp_alpha')\nplt.ylabel('Mean Test RMSE')\nplt.title('Effect of ccp_alpha on Test RMSE')\nplt.xscale('log')\nplt.grid()",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "Classification _Tree.html",
    "href": "Classification _Tree.html",
    "title": "5  Classification trees",
    "section": "",
    "text": "5.1 Building a Classification Tree\nRead section 8.1.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nImport libraries\nWe will build a classification tree to predict whether a person has heart disease, using the default parameters of the decision tree classifier.\n# create a decision tree classifier\ntree = DecisionTreeClassifier(random_state=42)\n\n# fit the model to the training data\ntree.fit(X_train, y_train)\n\n# make predictions on the test data\ny_pred = tree.predict(X_test)\n\n# calculate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Test Accuracy: {accuracy:.2f}')\n\n# train accuracy\ny_train_pred = tree.predict(X_train)\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\nprint(f'Train Accuracy: {train_accuracy:.2f}')\n\nTest Accuracy: 0.75\nTrain Accuracy: 1.00\n# plot the decision tree\nplt.figure(figsize=(12, 8))\nplot_tree(tree, filled=True, feature_names=X.columns, class_names=['No Disease', 'Disease'])\nplt.title('Decision Tree for Heart Disease Classification');\n# get the number of leaves in the tree\nnum_leaves = tree.get_n_leaves()\nprint(f'Number of leaves: {num_leaves}')\n\n# get the depth of the tree\ntree_depth = tree.get_depth()\nprint(f'Depth of the tree: {tree_depth}')\n\nNumber of leaves: 41\nDepth of the tree: 9\nClearly, the model is overfitting, as indicated by a training accuracy of 100% and a much lower test accuracy of 75%.\nNext, we will explore different strategies to address and reduce overfitting.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Classification _Tree.html#pre-pruning-hyperparameters-tuning",
    "href": "Classification _Tree.html#pre-pruning-hyperparameters-tuning",
    "title": "5  Classification trees",
    "section": "5.2 Pre-pruning: Hyperparameters Tuning",
    "text": "5.2 Pre-pruning: Hyperparameters Tuning\nMaximum depth of tree (max_depth) - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\nMinimum samples for a node split (min_samples_split) - Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting. - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\nMinimum samples for a terminal node (min_samples_leaf) - Defines the minimum samples (or observations) required in a terminal node or leaf. - Used to control over-fitting similar to min_samples_split. - Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.\nMaximum number of terminal nodes (max_leaf_nodes) - The maximum number of terminal nodes or leaves in a tree.\n\n# hyperparameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\n# define the parameter grid\nparam_grid = {\n    'max_depth': list(range(1, 9)) + [None], \n    'min_samples_split': [2, 5, 10, 15, 20],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# create a grid search object\ngrid_search = GridSearchCV(estimator=tree, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n# fit the grid search to the training data\ngrid_search.fit(X_train, y_train)\n# print the best parameters\nprint(\"Best parameters found: \", grid_search.best_params_)\n\n# print the best score\nprint(\"Best score: \", grid_search.best_score_)\n# get the best estimator\nbest_tree = grid_search.best_estimator_\n\n# make predictions on the test data with the best estimator\ny_pred_best = best_tree.predict(X_test)\n# calculate the accuracy of the best estimator\nbest_accuracy = accuracy_score(y_test, y_pred_best)\nprint(f'Best Test Accuracy: {best_accuracy:.2f}')\n\nFitting 5 folds for each of 135 candidates, totalling 675 fits\nBest parameters found:  {'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 10}\nBest score:  0.7687074829931972\nBest Test Accuracy: 0.85\n\n\n\n# print out the best tree depth and number of leaves\nbest_num_leaves = best_tree.get_n_leaves()\nprint(f'Best Number of leaves: {best_num_leaves}')\n\nbest_tree_depth = best_tree.get_depth()\nprint(f'Best Depth of the tree: {best_tree_depth}')\n\nBest Number of leaves: 21\nBest Depth of the tree: 6\n\n\n\n# plot the best decision tree\nplt.figure(figsize=(12, 8))\nplot_tree(best_tree, filled=True, feature_names=X.columns, class_names=['No Disease', 'Disease'])\nplt.title('Best Decision Tree for Heart Disease Classification')\nplt.show()\n\n\n\n\n\n\n\n\n\n5.2.1 Gini or entropy\n\n# define the parameter grid\nparam_grid_metric = {\n    'max_depth': list(range(1, 9)) + [None], \n    'min_samples_split': [2, 5, 10, 15, 20],\n    'min_samples_leaf': [1, 2, 4],\n    # adding the criterion parameter to the grid search\n    'criterion': ['gini', 'entropy']\n}\n\n# create a grid search object\ngrid_search_metric = GridSearchCV(estimator=tree, param_grid=param_grid_metric, cv=5, n_jobs=-1, verbose=2)\n# fit the grid search to the training data\ngrid_search_metric.fit(X_train, y_train)\n# print the best parameters\nprint(\"Best parameters found: \", grid_search_metric.best_params_)\n\n# print the best score\nprint(\"Best score: \", grid_search_metric.best_score_)\n# get the best estimator\nbest_tree = grid_search_metric.best_estimator_\n\n# make predictions on the test data with the best estimator\ny_pred_best = best_tree.predict(X_test)\n# calculate the accuracy of the best estimator\nbest_accuracy = accuracy_score(y_test, y_pred_best)\nprint(f'Best Test Accuracy: {best_accuracy:.2f}')\n\nFitting 5 folds for each of 270 candidates, totalling 1350 fits\nBest parameters found:  {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 20}\nBest score:  0.7811224489795918\nBest Test Accuracy: 0.85\n\n\n\n# print out the best tree depth and number of leaves\nbest_num_leaves = best_tree.get_n_leaves()\nprint(f'Best Number of leaves: {best_num_leaves}')\n\nbest_tree_depth = best_tree.get_depth()\nprint(f'Best Depth of the tree: {best_tree_depth}')\n\nBest Number of leaves: 11\nBest Depth of the tree: 4\n\n\n\n# plot the best decision tree\nplt.figure(figsize=(12, 8))\nplot_tree(best_tree, filled=True, feature_names=X.columns, class_names=['No Disease', 'Disease'])\nplt.title('Best Decision Tree for Heart Disease Classification')\nplt.show()\n\n\n\n\n\n\n\n\nBoth criteria aim to minimize impurity in splits, in pratice, they often lead to comparable performance in decision trees, even though their mathematical forulations differ\n\nGini: Faster to compute (no logarithms) and often used for large datasets. so it is a good default. May produce slightly more complex trees.\nEntropy: Slower but aligns with information theory. Prefers splits that balance node sizes, leading to more interpretable trees.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Classification _Tree.html#post-pruning-cost-complexity-pruning",
    "href": "Classification _Tree.html#post-pruning-cost-complexity-pruning",
    "title": "5  Classification trees",
    "section": "5.3 Post-pruning: Cost complexity pruning",
    "text": "5.3 Post-pruning: Cost complexity pruning\nPost-pruning, on the other hand, allows the decision tree to grow to its full extent and then prunes it back to reduce complexity. This approach first builds a complete tree and then removes or collapses branches that don’t significantly contribute to the model’s performance. One common post-pruning technique is called Cost-Complexity Pruning.\n\n5.3.1 step 1: calculate the cost complexity pruning path\n\npath = tree.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n\n\n5.3.2 step 2: Create trees with different ccp_alpha values and evaluate their performance\n\n\n# We'll skip the last alpha which would produce a single-node tree\nalphas = ccp_alphas[:-1]\n\n# Create empty lists to store the results\ntrain_scores = []\ntest_scores = []\ncv_scores = []\nnode_counts = []\n\n# For each alpha value, fit a tree and evaluate\nfor alpha in alphas:\n    # Create and train the model\n    clf = DecisionTreeClassifier(ccp_alpha=alpha, random_state=42)\n    clf.fit(X_train, y_train)\n    \n    # Record scores\n    train_scores.append(accuracy_score(y_train, clf.predict(X_train)))\n    test_scores.append(accuracy_score(y_test, clf.predict(X_test)))\n    \n    # Cross-validation score for robustness\n    cv_score = cross_val_score(clf, X_train, y_train, cv=5).mean()\n    cv_scores.append(cv_score)\n    \n    # Record tree complexity\n    node_counts.append(clf.tree_.node_count)\n\n\n\n5.3.3 Step 3: Visualize the results\n\n# Step 3: Visualize the results\nfig, ax = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot accuracy vs alpha\nax[0, 0].plot(alphas, train_scores, marker='o', label='Train')\nax[0, 0].plot(alphas, test_scores, marker='o', label='Test')\nax[0, 0].plot(alphas, cv_scores, marker='o', label='Cross-validation')\nax[0, 0].set_xlabel('ccp_alpha')\nax[0, 0].set_ylabel('Accuracy')\nax[0, 0].set_title('Accuracy vs. ccp_alpha')\nax[0, 0].legend()\nax[0, 0].grid(True)\n\n# Plot number of nodes vs alpha\nax[0, 1].plot(alphas, node_counts, marker='o')\nax[0, 1].set_xlabel('ccp_alpha')\nax[0, 1].set_ylabel('Number of nodes')\nax[0, 1].set_title('Tree complexity vs. ccp_alpha')\nax[0, 1].grid(True)\n\n# Log scale for better visualization of small alpha values\nax[1, 0].plot(alphas, train_scores, marker='o', label='Train')\nax[1, 0].plot(alphas, test_scores, marker='o', label='Test')\nax[1, 0].plot(alphas, cv_scores, marker='o', label='Cross-validation')\nax[1, 0].set_xlabel('ccp_alpha (log scale)')\nax[1, 0].set_ylabel('Accuracy')\nax[1, 0].set_title('Accuracy vs. ccp_alpha (log scale)')\nax[1, 0].set_xscale('log')\nax[1, 0].legend()\nax[1, 0].grid(True)\n\n# Find best alpha based on test score\nbest_test_idx = np.argmax(test_scores)\nbest_test_alpha = alphas[best_test_idx]\nbest_test_acc = test_scores[best_test_idx]\n\n# Find best alpha based on CV score (more robust)\nbest_cv_idx = np.argmax(cv_scores)\nbest_cv_alpha = alphas[best_cv_idx]\nbest_cv_acc = cv_scores[best_cv_idx]\n\n# Plot highlighting best points\nax[1, 1].plot(alphas, test_scores, 'b-', marker='o', label='Test accuracy')\nax[1, 1].plot(alphas, cv_scores, 'g-', marker='o', label='CV accuracy')\nax[1, 1].axvline(x=best_test_alpha, color='blue', linestyle='--', alpha=0.5, \n                label=f'Best test alpha: {best_test_alpha:.6f}')\nax[1, 1].axvline(x=best_cv_alpha, color='green', linestyle='--', alpha=0.5,\n                label=f'Best CV alpha: {best_cv_alpha:.6f}')\nax[1, 1].set_xlabel('ccp_alpha')\nax[1, 1].set_ylabel('Accuracy')\nax[1, 1].set_title('Finding optimal ccp_alpha')\nax[1, 1].legend(loc='lower left')\nax[1, 1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Print the optimal alpha values and corresponding metrics\nprint(f\"Best alpha based on test score: {best_test_alpha:.6f} (Accuracy: {best_test_acc:.4f}, Nodes: {node_counts[best_test_idx]})\")\nprint(f\"Best alpha based on CV score: {best_cv_alpha:.6f} (Accuracy: {best_cv_acc:.4f}, Nodes: {node_counts[best_cv_idx]})\")\n\n\n\n\n\n\n\n\n\n\n5.3.4 Step 4: Create the final model with the optimal alpha\n\n# Using CV-based alpha as it's more robust against overfitting\nfinal_model = DecisionTreeClassifier(ccp_alpha=best_cv_alpha, random_state=42)\nfinal_model.fit(X_train, y_train)\n\n# Evaluate the final model\ntrain_acc = accuracy_score(y_train, final_model.predict(X_train))\ntest_acc = accuracy_score(y_test, final_model.predict(X_test))\n\nprint(f\"\\nFinal model performance:\")\nprint(f\"Training accuracy: {train_acc:.4f}\")\nprint(f\"Test accuracy: {test_acc:.4f}\")\nprint(f\"Tree nodes: {final_model.tree_.node_count}\")\nprint(f\"Tree depth: {final_model.get_depth()}\")\n\nBest alpha based on test score: 0.007969 (Accuracy: 0.8852, Nodes: 23)\nBest alpha based on CV score: 0.007969 (Accuracy: 0.7604, Nodes: 23)\n\nFinal model performance:\nTraining accuracy: 0.8967\nTest accuracy: 0.8852\nTree nodes: 23\nTree depth: 6\n\n\n\n# plot the final decision tree\nplt.figure(figsize=(12, 8))\nplot_tree(final_model, filled=True, feature_names=X.columns, class_names=['No Disease', 'Disease'])\nplt.title('Final Decision Tree for Heart Disease Classification')\nplt.show()\n\n\n\n\n\n\n\n\nPost-pruning can potentially create more optimal trees, as it considers the entire tree structure before making pruning decisions. However, it can be more computationally expensive.\nBoth approaches aim to find a balance between model complexity and performance, with the goal of creating a model that generalizes well to unseen data. The choice between pre-pruning and post-pruning (or a combination of both) often depends on the specific dataset, the problem at hand, and of course, computational resources available.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Classification _Tree.html#feature-importance-in-decision-trees",
    "href": "Classification _Tree.html#feature-importance-in-decision-trees",
    "title": "5  Classification trees",
    "section": "5.4 Feature Importance in Decision Trees",
    "text": "5.4 Feature Importance in Decision Trees\nDecision tree algorithms, such as Classification and Regression Trees (CART), compute feature importance scores based on how much each feature contributes to reducing the splitting criterion (e.g., Gini impurity or entropy).\nThis methodology extends naturally to ensemble models like Random Forests and Gradient Boosting, which average feature importance across all trees in the ensemble.\nOnce a model is trained, the relative importance of each feature can be accessed using the .feature_importances_ attribute. These scores indicate how valuable each feature was in constructing the decision rules that led to the model’s predictions.\n\nimportances  = final_model.feature_importances_\n\n# Create a DataFrame for easier visualization\nfeature_importance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': importances\n}).sort_values(by='Importance', ascending=False)\n\n# Display the DataFrame\nfeature_importance_df\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n2\ncp\n0.340102\n\n\n11\nca\n0.145273\n\n\n9\noldpeak\n0.139508\n\n\n8\nexang\n0.113871\n\n\n0\nage\n0.095885\n\n\n4\nchol\n0.056089\n\n\n10\nslope\n0.041242\n\n\n12\nthal\n0.035531\n\n\n1\nsex\n0.032499\n\n\n3\ntrestbps\n0.000000\n\n\n5\nfbs\n0.000000\n\n\n6\nrestecg\n0.000000\n\n\n7\nthalach\n0.000000\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\nplt.xlabel(\"Importance Score\")\nplt.title(\"Feature Importances\")\nplt.gca().invert_yaxis()  # Most important feature at the top\nplt.show()",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Classification _Tree.html#using-bagging-to-combat-overfitting",
    "href": "Classification _Tree.html#using-bagging-to-combat-overfitting",
    "title": "5  Classification trees",
    "section": "5.5 Using Bagging to Combat Overfitting",
    "text": "5.5 Using Bagging to Combat Overfitting\nBagging, short for Bootstrap Aggregating, is an effective way to reduce overfitting in decision trees. It works by training multiple decision trees—each on a different bootstrap sample of the data—and then aggregating their predictions.\nEach individual decision tree is a weak learner and prone to overfitting, but by combining many such trees, bagging produces a strong learner with lower variance and improved generalization performance.\nThe number of trees to include in the ensemble is specified by the n_estimators hyperparameter.\n\n# using tree bagging to fit the data\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, random_state=0)\nbagging.fit(X_train, y_train)\n\ny_pred_train = bagging.predict(X_train)\ncm = pd.DataFrame(confusion_matrix(y_train, y_pred_train).T, index=['No', 'Yes'], columns=['No', 'Yes'])\nprint(cm)\n\ny_pred = bagging.predict(X_test)\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred).T, index=['No', 'Yes'], columns=['No', 'Yes'])\n\nprint(cm)\n\n#print out the accuracy on test set and training set\nprint(\"Train Accuracy is \", accuracy_score(y_train,y_pred_train)*100)\nprint(\"Test Accuracy is \", accuracy_score(y_test,y_pred)*100)\n\n      No  Yes\nNo   109    0\nYes    0  133\n     No  Yes\nNo   25    5\nYes   4   27\nTrain Accuracy is  100.0\nTest Accuracy is  85.24590163934425\n\n\n\n5.5.1 ✅ Takeaway:\nBagging reduces variance but does not affect bias.\nBy averaging the predictions of multiple high-variance models (like decision trees), bagging stabilizes the model and improves generalization, while the overall bias remains unchanged.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "bagging.html",
    "href": "bagging.html",
    "title": "6  Bagging",
    "section": "",
    "text": "6.1 Bagging: A Variance Reduction Technique\nRead section 8.2.1 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nBagging, short for Bootstrap Aggregating, is an effective way to reduce overfitting in decision trees. It works by training multiple decision trees—each on a different bootstrap sample of the data—and then aggregating their predictions.\nEach individual decision tree is a weak learner and prone to overfitting, but by combining many such trees, bagging produces a strong learner with lower variance and improved generalization performance.\nThe number of trees to include in the ensemble is specified by the n_estimators hyperparameter.\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.35)\n\n# import the decision tree regressor\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree, export_graphviz\nfrom sklearn.ensemble import BaggingRegressor,BaggingClassifier\n\n# split the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict, KFold\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.metrics import root_mean_squared_error, r2_score, make_scorer, accuracy_score",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#bagging-regression-trees",
    "href": "bagging.html#bagging-regression-trees",
    "title": "6  Bagging",
    "section": "6.2 Bagging Regression Trees",
    "text": "6.2 Bagging Regression Trees\nLet’s revisit the same dataset used for building a single regression tree and explore whether we can further improve its performance using bagging\n\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\ncar.head()\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\nvw\nBeetle\n2014\nManual\n55457\nDiesel\n30\n65.3266\n1.6\n7490\n\n\n1\nvauxhall\nGTC\n2017\nManual\n15630\nPetrol\n145\n47.2049\n1.4\n10998\n\n\n2\nmerc\nG Class\n2012\nAutomatic\n43000\nDiesel\n570\n25.1172\n3.0\n44990\n\n\n3\naudi\nRS5\n2019\nAutomatic\n10\nPetrol\n145\n30.5593\n2.9\n51990\n\n\n4\nmerc\nX-CLASS\n2018\nAutomatic\n14000\nDiesel\n240\n35.7168\n2.3\n28990\n\n\n\n\n\n\n\nSplit the predictors and target, then perform the train-test split\n\nX = car.drop(columns=['price'])\ny = car['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# extract the categorical columns and put them in a list\ncategorical_feature = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumerical_feature = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\nEncode categorical predictors\n\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\nX_train_encoded = encoder.fit_transform(X_train[categorical_feature])\nX_test_encoded = encoder.transform(X_test[categorical_feature])\n\n# Convert the encoded features back to DataFrame\nX_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_feature))\nX_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_feature))\n\n# Concatenate the encoded features with the original numerical features\nX_train_final = pd.concat([X_train_encoded_df, X_train[numerical_feature].reset_index(drop=True)], axis=1)\nX_test_final = pd.concat([X_test_encoded_df, X_test[numerical_feature].reset_index(drop=True)], axis=1)\n\nBy default, a single decision tree grows to its full depth, which often leads to overfitting as shown below\n\n# build a decision tree regressor using the default parameters\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(X_train_final, y_train)\ny_pred = tree_reg.predict(X_test_final)\nrmse = root_mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"Test RMSE: {rmse:.2f}, test R^2: {r2:.2f}\")\n\n# training rmse and r2\ny_train_pred = tree_reg.predict(X_train_final)\ntrain_rmse = root_mean_squared_error(y_train, y_train_pred)\ntrain_r2 = r2_score(y_train, y_train_pred)\nprint(f\"Train RMSE: {train_rmse:.2f}, train R^2: {train_r2:.2f}\")\n\n# print the depth of the tree\nprint(f\"Depth of the tree: {tree_reg.get_depth()}\")\n# print the number of leaves in the tree\nprint(f\"Number of leaves in the tree: {tree_reg.get_n_leaves()}\")\n\nTest RMSE: 6219.96, test R^2: 0.87\nTrain RMSE: 0.00, train R^2: 1.00\nDepth of the tree: 34\nNumber of leaves in the tree: 5925\n\n\nAs observed, the model achieves an RMSE of 0.00 and an R² of 100% on the training data with default parameters, indicating overfitting\nTo address this, we’ve previously explored pre-pruning and post-pruning techniques. Another effective approach is bagging, which helps reduce overfitting by lowering model variance.\nNext, we’ll explore how bagging can improve the performance of unpruned decision trees by reducing variance\n\n#Bagging the results of 10 decision trees with the default parameters to predict car price\nbagging_reg = BaggingRegressor(random_state=1, \n                        n_jobs=-1).fit(X_train_final, y_train)\n\n# make predictions on the test set\ny_pred_bagging = bagging_reg.predict(X_test_final)\n\n# calculate the RMSE and R^2 score\nrmse_bagging = root_mean_squared_error(y_test, y_pred_bagging)\nr2_bagging = r2_score(y_test, y_pred_bagging)\n\nprint(\"Test RMSE with Bagging unpruned trees:\", round(rmse_bagging, 2))\nprint(\"Test R^2 score with Bagging unpruned trees:\", round(r2_bagging, 2))\n\n# training RMSE and R^2 score\ny_pred_train_bagging = bagging_reg.predict(X_train_final)\n\n# calculate the RMSE and R^2 score\nrmse_train_bagging = root_mean_squared_error(y_train, y_pred_train_bagging)\nr2_train_bagging = r2_score(y_train, y_pred_train_bagging)\n\nprint(\"Train RMSE with Bagging unpruned trees:\", round(rmse_train_bagging, 2))\nprint(\"Train R^2 score with Bagging unpruned trees:\", round(r2_train_bagging, 2))\n\nTest RMSE with Bagging unpruned trees: 3758.1\nTest R^2 score with Bagging unpruned trees: 0.95\nTrain RMSE with Bagging unpruned trees: 1501.04\nTrain R^2 score with Bagging unpruned trees: 0.99\n\n\nWith the default settings, bagging unpruned trees improves performance, reducing the RMSE from 6219.96 to 3758.10 and increasing the R² score from 0.87 to 0.95.\nWhat about bagging pruned trees? Since pruning improves the performance of a single decision tree, does that mean bagging pruned trees will also outperform bagging unpruned trees? Let’s find out through implementation.\nBelow is the best model obtained by tuning the hyperparameters of a single decision tree.\n\n# fit the decision tree regressor\npruned_tree_reg = DecisionTreeRegressor(max_depth=None, min_samples_leaf=1, min_samples_split=5, random_state=42)\npruned_tree_reg.fit(X_train_final, y_train)\n\n# make predictions on the test set\ny_pred = pruned_tree_reg.predict(X_test_final)\n# calculate the RMSE and R^2 score\nrmse = root_mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# print the RMSE and R^2 score, keep the decimal points to 2\nprint(\"Test RMSE:\", round(rmse, 2))\nprint(\"Test R^2 score:\", round(r2, 2))\n\n#print the depth of the tree\nprint(f\"Depth of the tree: {pruned_tree_reg.get_depth()}\")\n#print the number of leaves in the tree\nprint(f\"Number of leaves in the tree: {pruned_tree_reg.get_n_leaves()}\")\n\nTest RMSE: 4726.17\nTest R^2 score: 0.92\nDepth of the tree: 33\nNumber of leaves in the tree: 2558\n\n\nNext, let’s apply bagging to these pruned trees using the default settings.\n\n#Bagging the results of 10 decision trees to predict car price\nbagging_reg = BaggingRegressor(estimator=pruned_tree_reg, random_state=1,\n                        n_jobs=-1).fit(X_train_final, y_train)\n\n# make predictions on the test set\ny_pred_bagging = bagging_reg.predict(X_test_final)\n\n# calculate the RMSE and R^2 score\nrmse_bagging = root_mean_squared_error(y_test, y_pred_bagging)\nr2_bagging = r2_score(y_test, y_pred_bagging)\n\nprint(\"Test RMSE with Bagging pruned trees:\", round(rmse_bagging, 2))\nprint(\"Test R^2 score with Bagging pruned trees:\", round(r2_bagging, 2))\n\n\n# training RMSE and R^2 score\ny_pred_train_bagging = bagging_reg.predict(X_train_final)\n\n# calculate the RMSE and R^2 score\nrmse_train_bagging = root_mean_squared_error(y_train, y_pred_train_bagging)\nr2_train_bagging = r2_score(y_train, y_pred_train_bagging)\n\nprint(\"Train RMSE with Bagging pruned trees:\", round(rmse_train_bagging, 2))\nprint(\"Train R^2 score with Bagging pruned trees:\", round(r2_train_bagging, 2))\n\nTest RMSE with Bagging pruned trees: 3806.7\nTest R^2 score with Bagging pruned trees: 0.95\nTrain RMSE with Bagging pruned trees: 1868.7\nTrain R^2 score with Bagging pruned trees: 0.99\n\n\nCompared to bagging the unpruned trees, the performance is slightly worse, with bagging pruned trees the RMSE is 3806.7, bagging the unpruned trees lead to RMSe 3758.1.\nWhy is bagging tuned trees worse than bagging untuned trees?\nIn the pruned tree, limiting the maximum depth reduces variance but increases bias, as reflected by the smaller depth and fewer leaves compared to the unpruned tree. Since bagging only reduces variance and does not affect bias, applying it to pruned trees—which have slightly higher bias—results in slightly worse performance than bagging unpruned trees\nThis suggests that when using bagging, we don’t necessarily need to tune the hyperparameters of the base decision tree—bagging itself effectively combats overfitting by reducing variance, much like hyperparameter tuning does.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#model-performance-vs.-number-of-trees",
    "href": "bagging.html#model-performance-vs.-number-of-trees",
    "title": "6  Bagging",
    "section": "6.4 Model Performance vs. Number of Trees",
    "text": "6.4 Model Performance vs. Number of Trees\nTo better understand how the number of base estimators affects the performance of a bagging model, we evaluate the test RMSE and R² score across different numbers of trees.\nThis analysis helps us determine whether adding more trees continues to improve performance or if the model reaches a performance plateau.\n\n# explore how the number of estimators affects the performance of the model, output both oob and test scores\nn_estimators = [ 10, 15, 20, 25,30, 35, 40, 45, 50]\nrmse_scores = []\nr2_scores = []\n\n# iterate through the number of estimators and fit the model\nfor n in n_estimators:\n    bagging_reg = BaggingRegressor(estimator=pruned_tree_reg, n_estimators=n, random_state=1,\n                        n_jobs=-1).fit(X_train_final, y_train)\n    y_pred_bagging = bagging_reg.predict(X_test_final)\n    rmse_scores.append(np.sqrt(np.mean((y_test - y_pred_bagging) ** 2)))\n    r2_scores.append(r2_score(y_test, y_pred_bagging))\n\n# plot the RMSE and R^2 scores against the number of estimators\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(n_estimators, rmse_scores, marker='o')\nplt.title('RMSE vs Number of Estimators')\nplt.xlabel('Number of Estimators')\nplt.ylabel('RMSE')\nplt.xticks(n_estimators)\nplt.grid()\nplt.subplot(1, 2, 2)\nplt.plot(n_estimators, r2_scores, marker='o')\nplt.title('R^2 Score vs Number of Estimators')\nplt.xlabel('Number of Estimators')\nplt.ylabel('R^2 Score')\nplt.xticks(n_estimators)\nplt.grid()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nQuick Takeaway\n\nIncreasing the number of estimators initially improves performance, as seen from the decreasing RMSE and increasing R² scores.\nPerformance stabilizes around 30–35 estimators. Beyond this point, additional trees provide minimal gains.\nDue to the small and possibly noisy test set, performance may appear to peak and then decline slightly. However, this is not typical—under normal circumstances, performance levels off and forms a plateau.\n\n\n# get the number of estimators that gives the best RMSE score\nbest_rmse_index = np.argmin(rmse_scores)\nbest_rmse_n_estimators = n_estimators[best_rmse_index]\nbest_rmse_value = rmse_scores[best_rmse_index]\nprint(\"Best number of estimators for RMSE:\", best_rmse_n_estimators)\nprint(\"Best RMSE value:\", round(best_rmse_value, 2))\n\n# get the number of estimators that gives the best R^2 score\nbest_r2_index = np.argmax(r2_scores)\nbest_r2_n_estimators = n_estimators[best_r2_index]\nbest_r2_value = r2_scores[best_r2_index]\nprint(\"Best number of estimators for R^2 score:\", best_r2_n_estimators)\nprint(\"Best R^2 score:\", round(best_r2_value, 2))\n\nBest number of estimators for RMSE: 30\nBest RMSE value: 3508.31\nBest number of estimators for R^2 score: 30\nBest R^2 score: 0.96",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#bagging-hyperparameter-tuning",
    "href": "bagging.html#bagging-hyperparameter-tuning",
    "title": "6  Bagging",
    "section": "6.6 Bagging Hyperparameter Tuning",
    "text": "6.6 Bagging Hyperparameter Tuning\nTo further improve the performance of our bagging model, we can tune key hyperparameters such as:\n\nn_estimators: the number of base estimators in the ensemble,\nmax_features: the maximum number of features considered at each split,\nmax_samples: the size of each bootstrap sample used to train base estimators,\nbootstrap: whether sampling is performed with replacement (True) or without (False),\nbootstrap_features: whether features are sampled with replacement when selecting subsets of features for each estimator.\n\nBy systematically exploring different combinations of these parameters, we aim to identify the optimal settings that enhance predictive accuracy while maintaining good generalization.\nThere are two common approaches for tuning these hyperparameters: - Cross-validation, which provides a robust estimate of model performance, and\n- Out-of-Bag (OOB) score, which offers an efficient built-in alternative without needing a separate validation set.\n\n6.6.1 Tuning with Cross-Validation\nNext, let’s use GridSearchCV to tune these hyperparameters and identify the best combination for improved model performance.\n\n# hyperparameter tuning using GridSearchCV\nparam_grid = {\n    'n_estimators': [10, 20, 30, 40, 50],\n    'max_samples': [0.5, 0.75, 1.0],\n    'max_features': [0.5, 0.75, 1.0],\n    'bootstrap': [True, False],\n    'bootstrap_features': [True, False]\n}\n\nbagging_reg_grid = BaggingRegressor(random_state=42, n_jobs=-1)\ngrid_search = GridSearchCV(bagging_reg_grid, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train_final, y_train)\n\n# get the best parameters and the best score\nbest_params = grid_search.best_params_\nbest_score = np.sqrt(-grid_search.best_score_)\nprint(\"Best parameters:\", best_params)\nprint(\"Best RMSE cv score:\", round(best_score, 2))\n\n# make predictions on the test set using the best parameters\nbest_bagging_reg = grid_search.best_estimator_\ny_pred_best_bagging = best_bagging_reg.predict(X_test_final)\n\n# calculate the RMSE and R^2 score\nrmse_best_bagging = root_mean_squared_error(y_test, y_pred_best_bagging)\nr2_best_bagging = r2_score(y_test, y_pred_best_bagging)\nprint(\"Test RMSE with best Bagging model:\", round(rmse_best_bagging, 2))\nprint(\"Test R^2 score with best Bagging model:\", round(r2_best_bagging, 2))\n\n# training RMSE and R^2 score\ny_pred_train_best_bagging = best_bagging_reg.predict(X_train_final)\n\n# calculate the RMSE and R^2 score\nrmse_train_best_bagging = root_mean_squared_error(y_train, y_pred_train_best_bagging)\nr2_train_best_bagging = r2_score(y_train, y_pred_train_best_bagging)\nprint(\"Train RMSE with best Bagging model:\", round(rmse_train_best_bagging, 2))\nprint(\"Train R^2 score with best Bagging model:\", round(r2_train_best_bagging, 2))\n\nBest parameters: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.75, 'max_samples': 1.0, 'n_estimators': 50}\nBest RMSE cv score: 3288.03\nTest RMSE with best Bagging model: 3348.45\nTest R^2 score with best Bagging model: 0.96\nTrain RMSE with best Bagging model: 1411.13\nTrain R^2 score with best Bagging model: 0.99\n\n\nAfter simultaneously tuning multiple hyperparameters of the bagging model, including n_estimators, max_features, and max_samples, we achieved the best performance:\n\nTest RMSE with best Bagging model: 3348.45\n\nTest R² score with best Bagging model: 0.96\n\nThis demonstrates that careful tuning of bagging-specific parameters can lead to further improvements beyond using default or even optimized single decision trees.\n\n\n6.6.2 Tuning with Out-of-Bag (OOB) Score\nAs an alternative to cross-validation, we can use the Out-of-Bag (OOB) score to evaluate model performance during training.\nThis method is more efficient for large datasets, as it avoids the need to split data or run multiple folds.\nBy enabling oob_score=True, we can monitor performance on unseen data points (those not included in each bootstrap sample) and use this score to guide hyperparameter tuning.\n\nfrom sklearn.model_selection import ParameterGrid\n# tune the hyperparameters of the decision tree regressor using oob score\n# Hyperparameter grid\nparam_grid = {\n    'n_estimators': [10, 20, 30, 40, 50],\n    'max_samples': [0.5, 0.75, 1.0],\n    'max_features': [0.5, 0.75, 1.0],\n    'bootstrap': [True],  # Required for OOB\n    'bootstrap_features': [True, False]\n}\n\n# Track best parameters and OOB score\nbest_score = -np.inf\nbest_params = {}\n\n# Iterate over all hyperparameter combinations\nfor params in ParameterGrid(param_grid):\n    # Train model with current params and OOB score enabled\n    model = BaggingRegressor(\n        estimator=tree_reg,\n        **params,\n        oob_score=True,\n        random_state=42,\n        n_jobs=-1\n    )\n    model.fit(X_train_final, y_train)\n    \n    # Get OOB score (higher is better for R², lower for RMSE)\n    current_score = model.oob_score_\n    \n    # Update best params if current score is better\n    if current_score &gt; best_score:\n        best_score = current_score\n        best_params = params\n\n# Best model\nbest_model = BaggingRegressor(\n    estimator=tree_reg,\n    **best_params,\n    oob_score=True,\n    random_state=42,\n    n_jobs=-1\n)\nbest_model.fit(X_train_final, y_train)\n\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best OOB Score:\", best_score)\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\n\n\nBest Hyperparameters: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 50}\nBest OOB Score: 0.9587800605892783\n\n\n\n# output the test RMSE and R^2 score with the best model\ny_pred_best_model = best_model.predict(X_test_final)\nrmse_best_model = root_mean_squared_error(y_test, y_pred_best_model)\nr2_best_model = r2_score(y_test, y_pred_best_model)\nprint(\"Test RMSE with best model:\", round(rmse_best_model, 2))\nprint(\"Test R^2 score with best model:\", round(r2_best_model, 2))\n\nTest RMSE with best model: 3418.55\nTest R^2 score with best model: 0.96\n\n\n\n\n6.6.3 Comparing Hyperparameter Tuning: Cross-Validation vs. OOB Score\n\n\n\n\n\n\n\n\nAspect\nCross-Validation (CV)\nOut-of-Bag (OOB) Score\n\n\n\n\nMechanism\nSplits training data into multiple folds to validate\nUses unused (out-of-bag) samples in each bootstrap\n\n\nRequires Manual Splits?\nYes\nNo — internal to bagging process\n\n\nEfficiency\nSlower, especially for large datasets\nFaster and more efficient for large datasets\n\n\nBias-Variance Tradeoff\nMore stable and less biased performance estimate\nSlightly more variable and can underestimate accuracy\n\n\nAvailability\nAvailable for all models\nOnly available when bootstrap=True in bagging\n\n\nIntegration in Sklearn\nBuilt-in via GridSearchCV\nMust be implemented manually for tuning\n\n\nFlexibility\nWorks with any model type\nOnly works with bagging-based models\n\n\nUse Case\nIdeal for robust model comparison\nGreat for quick tuning on large datasets\n\n\nScoring Access\n.best_score_ from GridSearchCV\n.oob_score_ from trained model\n\n\n\n\n6.6.3.1 ✅ Best Practices for Imbalanced Classification\n\nPrefer Cross-Validation, especially with:\n\nStratifiedKFold to maintain class distribution in each fold.\nCustom metrics (e.g., F1-score, ROC-AUC, balanced accuracy) using scoring=.\n\nBe cautious using OOB score:\n\nOOB score may be misleading for rare classes, especially when n_estimators is small.\nOnly use it for rough estimates or early tuning when computational efficiency is critical.\n\n\n#### ✅ Summary\n\nUse Cross-Validation when:\n\nYou want robust, model-agnostic performance evaluation.\nYou need precise comparisons between different model types or pipelines.\nYou work on imbalanced classification task\n\nUse OOB Score when:\n\nYou’re working with large datasets and want faster tuning.\nYour model is based on bagging (e.g., BaggingClassifier, RandomForestClassifier).\nYou want to avoid manual train/validation splits.\n\n\n\n⚠️ Note: Scikit-learn’s GridSearchCV does not use OOB score for tuning—even if oob_score=True is set. To use OOB for tuning, you must loop over hyperparameters manually and evaluate using .oob_score_.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#bagging-doesnt-reduce-bias",
    "href": "bagging.html#bagging-doesnt-reduce-bias",
    "title": "6  Bagging",
    "section": "6.3 Bagging Doesn’t Reduce Bias",
    "text": "6.3 Bagging Doesn’t Reduce Bias\nBagging high-variance models can effectively lower overall variance, as long as the individual models are not highly correlated. However, Bagging high-bias models will still produce a high-bias ensemble.\nTo demonstrate this, we first fit a shallow decision tree with max_depth=2, which severely underfits the data due to its limited capacity. Then, we apply bagging using 10 such shallow trees (default setting) as base estimators.\nSince each tree has high bias, the aggregated predictions from bagging still inherit that bias. In our results, both the single shallow tree and the bagged version yield similar (and poor) performance in terms of RMSE and R² on both the training and test sets.\nThis experiment shows that if your base model is too simple to capture the underlying patterns in the data, bagging will not help. To improve performance in such cases, we need to use more expressive base models or consider methods like boosting, which are better suited to reducing both bias and variance.\n\n# Single shallow decision tree (underfitting)\nshallow_tree_reg = DecisionTreeRegressor(max_depth=2, random_state=1)\nshallow_tree_reg.fit(X_train_final, y_train)\n\n# Predict and evaluate on test set\ny_pred_single = shallow_tree_reg.predict(X_test_final)\nrmse_single = root_mean_squared_error(y_test, y_pred_single)\nr2_single = r2_score(y_test, y_pred_single)\n\n# Predict and evaluate on training set\ny_pred_train_single = shallow_tree_reg.predict(X_train_final)\nrmse_train_single = root_mean_squared_error(y_train, y_pred_train_single)\nr2_train_single = r2_score(y_train, y_pred_train_single)\n\nprint(\"Single Shallow Tree - Test RMSE:\", round(rmse_single, 2))\nprint(\"Single Shallow Tree - Test R^2:\", round(r2_single, 2))\nprint(\"Single Shallow Tree - Train RMSE:\", round(rmse_train_single, 2))\nprint(\"Single Shallow Tree - Train R^2:\", round(r2_train_single, 2))\n\nSingle Shallow Tree - Test RMSE: 11084.97\nSingle Shallow Tree - Test R^2: 0.58\nSingle Shallow Tree - Train RMSE: 10314.42\nSingle Shallow Tree - Train R^2: 0.6\n\n\nLet’s bag these shallow trees\n\n\n# Bagging with 10 shallow trees\nbagging_shallow = BaggingRegressor(estimator=DecisionTreeRegressor(max_depth=2, random_state=1),\n                                    random_state=1,\n                                    n_jobs=-1)\nbagging_shallow.fit(X_train_final, y_train)\n\n# Predict and evaluate on test set\ny_pred_bagging = bagging_shallow.predict(X_test_final)\nrmse_bagging = root_mean_squared_error(y_test, y_pred_bagging)\nr2_bagging = r2_score(y_test, y_pred_bagging)\n\n# Predict and evaluate on training set\ny_pred_train_bagging = bagging_shallow.predict(X_train_final)\nrmse_train_bagging = root_mean_squared_error(y_train, y_pred_train_bagging)\nr2_train_bagging = r2_score(y_train, y_pred_train_bagging)\n\nprint(\"Bagged Shallow Trees - Test RMSE:\", round(rmse_bagging, 2))\nprint(\"Bagged Shallow Trees - Test R^2:\", round(r2_bagging, 2))\nprint(\"Bagged Shallow Trees - Train RMSE:\", round(rmse_train_bagging, 2))\nprint(\"Bagged Shallow Trees - Train R^2:\", round(r2_train_bagging, 2))\n\nBagged Shallow Trees - Test RMSE: 10894.92\nBagged Shallow Trees - Test R^2: 0.6\nBagged Shallow Trees - Train RMSE: 10114.07\nBagged Shallow Trees - Train R^2: 0.62\n\n\n✅ What you should observe:\n\nBoth models show low R² and high RMSE due to the shallow depth (max_depth=2).\nBagging cannot fix the high bias inherent in a shallow decision tree.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#bagging-classification-trees",
    "href": "bagging.html#bagging-classification-trees",
    "title": "6  Bagging",
    "section": "6.7 Bagging Classification Trees",
    "text": "6.7 Bagging Classification Trees\nLet’s revisit the same dataset used for building a single classification tree.\nWhen using the default settings, the tree tends to overfit the data, as shown here.\nIn that notebook, we addressed the overfitting issue using both pre-pruning and post-pruning techniques.\nNow, we’ll explore an alternative approach—bagging—to reduce overfitting and improve model performance.\n\n# load the dataset\nheart_df  = pd.read_csv('datasets/heart_disease_classification.csv')\nprint(heart_df .shape)\nheart_df .head()\n\n(303, 14)\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\n\n# split the x and y data\nX_cls = heart_df.drop(columns=['target'])\ny_cls = heart_df.target\n\n# split the data into train and test sets, add _cls to the variable names\nX_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)\n\n\n# using tree bagging to fit the data\n\nbagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, random_state=0)\nbagging.fit(X_train_cls, y_train_cls)\n\ny_pred_train_cls = bagging.predict(X_train_cls)\ny_pred_cls = bagging.predict(X_test_cls)\n\n\n#print out the accuracy on test set and training set\nprint(\"Train Accuracy is \", accuracy_score(y_train_cls,y_pred_train_cls)*100)\nprint(\"Test Accuracy is \", accuracy_score(y_test_cls,y_pred_cls)*100)\n\nTrain Accuracy is  100.0\nTest Accuracy is  85.24590163934425",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html",
    "href": "Bagging (OOB vs K-fold cross-validation).html",
    "title": "7  Bagging (addendum)",
    "section": "",
    "text": "7.1 Tree without tuning\nThis notebook provides examples to:\nmodel = DecisionTreeRegressor()\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n-np.mean(cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv = cv))\n\n7056.960817154941\nparam_grid = {'max_depth': Integer(2, 30)}\ngcv = BayesSearchCV(model, search_spaces = param_grid, cv = cv, n_iter = 40, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\ngcv.fit(X, y, callback = monitor)    \n\n['max_depth'] = [10] 6341.1481858990355\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=DecisionTreeRegressor(), n_iter=40, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'max_depth': Integer(low=2, high=30, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=DecisionTreeRegressor(), n_iter=40, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'max_depth': Integer(low=2, high=30, prior='uniform', transform='normalize')})estimator: DecisionTreeRegressorDecisionTreeRegressor()DecisionTreeRegressorDecisionTreeRegressor()",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#performance-of-tree-improves-with-tuning",
    "href": "Bagging (OOB vs K-fold cross-validation).html#performance-of-tree-improves-with-tuning",
    "title": "7  Bagging (addendum)",
    "section": "7.2 Performance of tree improves with tuning",
    "text": "7.2 Performance of tree improves with tuning\n\nmodel = DecisionTreeRegressor(max_depth=10)\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n-np.mean(cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv = cv))\n\n6442.494300778735",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#bagging-tuned-trees",
    "href": "Bagging (OOB vs K-fold cross-validation).html#bagging-tuned-trees",
    "title": "7  Bagging (addendum)",
    "section": "7.3 Bagging tuned trees",
    "text": "7.3 Bagging tuned trees\n\nmodel = BaggingRegressor(DecisionTreeRegressor(max_depth = 10), oob_score=True, n_estimators = 100).fit(X, y)\nmean_squared_error(model.oob_prediction_, y, squared = False)\n\n5354.357809020438",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#bagging-untuned-trees",
    "href": "Bagging (OOB vs K-fold cross-validation).html#bagging-untuned-trees",
    "title": "7  Bagging (addendum)",
    "section": "7.4 Bagging untuned trees",
    "text": "7.4 Bagging untuned trees\n\nmodel = BaggingRegressor(DecisionTreeRegressor(), oob_score=True, n_estimators = 100).fit(X, y)\nmean_squared_error(model.oob_prediction_, y, squared = False)\n\n5248.720845665685\n\n\nWhy is bagging tuned trees worse than bagging untuned trees?\nIn the tuned tree here, the reduction in variance by controlling maximum depth resulted in an increas in bias of indivudual trees. Bagging trees only reduces the variance, but not the bias of the indivudal trees. Thus, bagging high bias models will result in a high-bias model, while bagging high variance models may result in a low variance model if the models are not highly correlated.\nBagging tuned models may provide a better performance as compared to bagging untuned models if the reduction in variance of the individual models is high enough to overshadow the increase in bias, and increase in pairwise correlation of the individual models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#tuning-bagged-model---oob",
    "href": "Bagging (OOB vs K-fold cross-validation).html#tuning-bagged-model---oob",
    "title": "7  Bagging (addendum)",
    "section": "7.5 Tuning bagged model - OOB",
    "text": "7.5 Tuning bagged model - OOB\n\nparam_grid1 = {'max_samples': [0.25, 0.5, 0.75, 1.0],\n             'max_features': [2, 3, 4],\n             'bootstrap_features': [True, False]}\nparam_grid2 = {'max_samples': [0.25, 0.5, 0.75, 1.0],\n             'max_features': [1],\n              'bootstrap_features': [False]}\nparam_list1 = list(it.product(*[values for key, values in param_grid1.items()]))\nparam_list2 = list(it.product(*[values for key, values in param_grid2.items()]))\nparam_list = param_list1 + param_list2\n\n\noob_score_pr = []\nfor pr in param_list:\n    model = BaggingRegressor(DecisionTreeRegressor(), max_samples=pr[0], max_features=pr[1],\n                            bootstrap_features=pr[2], n_jobs = -1, oob_score=True, n_estimators = 50).fit(X, y)\n    oob_score_pr.append(mean_squared_error(model.oob_prediction_, y, squared=False))\n\nWhat is the benefit of OOB validation to tune hyperparameters in bagging?\nIt is much cheaper than \\(k\\)-fold cross-validation, as only \\(1/k\\) of the models are trained with OOB validation as compared to \\(k\\)-fold cross-validation. However, the cost of training individual models is lower in \\(k\\)-fold cross-validation as models are trained on a smaller dataset. Typically, OOB will be faster than \\(k\\)-fold cross-validation. The higher the value of \\(k\\), the more faster OOB validation will be as compared to \\(k\\)-fold cross-validation.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#tuning-without-k-fold-cross-validation",
    "href": "Bagging (OOB vs K-fold cross-validation).html#tuning-without-k-fold-cross-validation",
    "title": "7  Bagging (addendum)",
    "section": "7.6 Tuning without k-fold cross-validation",
    "text": "7.6 Tuning without k-fold cross-validation\nWhen hyperparameters can be tuned with OOB validation, what is the benefit of using k-fold cross-validation?\n\nHyperparameters cannot be tuned over continuous spaces with OOB validation.\nOOB score is not computed if samping is done without replacement (bootstrap = False). Thus, for tuning the bootstrap hyperparameter, \\(k\\)-fold cross-validation will need to be used.\n\n\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\n\n\nparam_grid = {'max_samples': Real(0.2, 1.0),\n             'max_features': Integer(1, 4),\n             'bootstrap_features': [True, False],\n              'bootstrap': [True, False]}\ngcv = BayesSearchCV(BaggingRegressor(DecisionTreeRegressor(), bootstrap=False), \n                    search_spaces = param_grid, cv = cv, n_jobs = -1,\n                  scoring='neg_root_mean_squared_error')\n\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ngcv.fit(X, y, callback=monitor)\n\n['bootstrap', 'bootstrap_features', 'max_features', 'max_samples'] = [True, False, 4, 0.8061354588503475] 5561.064432968422\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=BaggingRegressor(bootstrap=False,\n                                         estimator=DecisionTreeRegressor()),\n              n_jobs=-1, scoring='neg_root_mean_squared_error',\n              search_spaces={'bootstrap': [True, False],\n                             'bootstrap_features': [True, False],\n                             'max_features': Integer(low=1, high=4, prior='uniform', transform='normalize'),\n                             'max_samples': Real(low=0.2, high=1.0, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=BaggingRegressor(bootstrap=False,\n                                         estimator=DecisionTreeRegressor()),\n              n_jobs=-1, scoring='neg_root_mean_squared_error',\n              search_spaces={'bootstrap': [True, False],\n                             'bootstrap_features': [True, False],\n                             'max_features': Integer(low=1, high=4, prior='uniform', transform='normalize'),\n                             'max_samples': Real(low=0.2, high=1.0, prior='uniform', transform='normalize')})estimator: BaggingRegressorBaggingRegressor(bootstrap=False, estimator=DecisionTreeRegressor())estimator: DecisionTreeRegressorDecisionTreeRegressor()DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nplot_histogram(gcv.optimizer_results_[0],0)\n\n\n\n\n\n\n\n\n\nplot_objective(gcv.optimizer_results_[0])",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#warm-start",
    "href": "Bagging (OOB vs K-fold cross-validation).html#warm-start",
    "title": "7  Bagging (addendum)",
    "section": "7.7 warm start",
    "text": "7.7 warm start\nWhat is the purpose of warm_start?\nThe purpose of warm_start is to avoid developing trees from scratch, and incrementally add trees to monitor the validation error. However, note that OOB score is not computed with warm_start. Thus, a validation set approach will need to be adopted to tune number of trees.\nA cheaper approach to tune number of estimators is to just use trial and error, and stop increasing once the cross-validation error / OOB error / validation set error stabilizes.\n\nmodel = BaggingRegressor(DecisionTreeRegressor(), oob_score=False, n_estimators = 5,\n                        warm_start=True).fit(X, y)\nrmse = []\nfor i in range(10, 200, 10):\n    model.n_estimators = i\n    model.fit(X, y)\n    rmse.append(mean_squared_error(model.predict(Xtest), ytest, squared=False))\n    sns.lineplot(x = range(10, i+1, 10), y = rmse)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#bagging-knn",
    "href": "Bagging (OOB vs K-fold cross-validation).html#bagging-knn",
    "title": "7  Bagging (addendum)",
    "section": "7.8 Bagging KNN",
    "text": "7.8 Bagging KNN\nShould we bag a tuned KNN model or an untuned one?\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nmodel = KNeighborsRegressor(n_neighbors=9) # optimal neigbors\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n-np.mean(cross_val_score((model), X_scaled, y, cv = cv, \n                         scoring='neg_root_mean_squared_error', n_jobs = -1))\n\n6972.997277781689\n\n\n\nmodel = KNeighborsRegressor(n_neighbors=1)\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n-np.mean(cross_val_score(BaggingRegressor(model), X_scaled, y, cv = cv, \n                         scoring='neg_root_mean_squared_error', n_jobs = -1))\n\n6254.305462266355\n\n\n\nmodel = BaggingRegressor(DecisionTreeRegressor(), n_estimators=5, warm_start=True)\nmodel.fit(X, y)\nrmse = []\nfor i in range(10, 200,10):\n    model.n_estimators = i\n    model.fit(X, y)\n    rmse.append(mean_squared_error(model.predict(Xtest), ytest, squared=False))\n    sns.lineplot(x = range(10, i + 1, 10), y = rmse)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html",
    "href": "Lec6_RandomForest.html",
    "title": "8  Random Forest",
    "section": "",
    "text": "8.1 Random Forest for regression\nRead section 8.2.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nLet us make a bunch of small trees with bagging, so that we can visualize and see if they are being dominated by a particular predictor or predictor(s).\nEach of the 10 bagged trees seems to be dominated by the engineSize predictor, thereby making the trees highly correlated. Average of highly correlated random variables has a higher variance than the average of lesser correlated random variables. Thus, highly correlated trees will tend to have a relatively high prediction variance despite averaging their predictions.\nWe can see that engineSize has the highest importance among predictors, supporting the visualization that it dominates the trees.\nNow, let us visualize small trees with the random forest algorithm to see if a predictor dominates all the trees.\n#Averaging the results of 10 decision trees, while randomly considering sqrt(4)=2 predictors at each node\n#to split, to predict car price\nmodel = RandomForestRegressor(n_estimators=10, random_state=1,max_features=\"sqrt\",max_depth=3,\n                        n_jobs=-1).fit(X, y)\n#Change the index of model.estimators_[index] to visualize the 10 random forest trees, one at a time\ndot_data = StringIO()\nexport_graphviz(model.estimators_[4], out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage','mpg','year','engineSize'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n#graph.write_png('car_price_tree.png')\nImage(graph.create_png())\nAs two of the four predictors are randomly selected for splitting each node, engineSize no longer seems to dominate the trees. This will tend to reduce correlation among trees, thereby reducing the prediction variance, which in turn will tend to improve prediction accuracy.\n#Averaging the results of 10 decision trees, while randomly considering sqrt(4)=2 predictors at each node\n#to split, to predict car price\nmodel = RandomForestRegressor(n_estimators=10, random_state=1,max_features=\"sqrt\",\n                        n_jobs=-1).fit(X, y)\nmodel.feature_importances_\n\narray([0.16370584, 0.35425511, 0.18552673, 0.29651232])\nNote that the feature importance of engineSize is reduced in random forests (as compared to bagged trees), and it no longer dominates the trees.\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5856.022395768459\nThe RMSE is similar to that obtained by bagging. We will discuss the comparison later.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-for-regression",
    "href": "Lec6_RandomForest.html#random-forest-for-regression",
    "title": "8  Random Forest",
    "section": "",
    "text": "8.1.1 Model accuracy vs number of trees\nHow does the model accuracy vary with the number of trees?\nAs we increase the number of trees, it will tend to reduce the variance of individual trees leading to a more accurate prediction.\n\n#Finding model accuracy vs number of trees\nwarnings.filterwarnings(\"ignore\")\noob_rsquared={};test_rsquared={};oob_rmse={};test_rmse = {}\n\nfor i in np.linspace(10,400,40,dtype=int):\n    model = RandomForestRegressor(n_estimators=i, random_state=1,max_features=\"sqrt\",\n                        n_jobs=-1,oob_score=True).fit(X, y)\n    oob_rsquared[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_rsquared[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n    oob_rmse[i]=np.sqrt(mean_squared_error(model.oob_prediction_,y))\n    test_rmse[i]=np.sqrt(mean_squared_error(model.predict(Xtest),ytest))\n\nwarnings.resetwarnings()\n    \n# The hidden warning is: \"Some inputs do not have OOB scores. This probably means too few \n# estimators were used to compute any reliable oob estimates.\" This warning will appear\n# in case of small number of estimators. In such a case, some observations may be use\n# by all the estimators, and their OOB score can't be computed\n\nAs we are ensemble only 10 trees in the first iteration, some of the observations are selected in every bootstrapped sample, and thus they don’t have an out-of-bag error, which is producing the warning. For every observation to have an out-of-bag error, the number of trees must be sufficiently large.\nLet us visualize the out-of-bag (OOB) R-squared and R-squared on test data vs the number of trees.\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),label = 'Out of bag R-squared')\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),'o',color = 'blue')\nplt.plot(test_rsquared.keys(),test_rsquared.values(), label = 'Test data R-squared')\nplt.xlabel('Number of trees')\nplt.ylabel('Rsquared')\nplt.legend();\n\n\n\n\n\n\n\n\nThe out-of-bag \\(R\\)-squared initially increases, and then stabilizes after a certain number of trees (around 200 in this case). Note that increasing the number of trees further will not lead to overfitting. However, increasing the number of trees will increase the computations. Thus, the number of trees developed should be the number beyond which the \\(R\\)-squared stabilizes.\n\n#Visualizing out-of-bag RMSE and test data RMSE\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rmse.keys(),oob_rmse.values(),label = 'Out of bag RMSE')\nplt.plot(oob_rmse.keys(),oob_rmse.values(),'o',color = 'blue')\nplt.plot(test_rmse.keys(),test_rmse.values(), label = 'Test data RMSE')\nplt.xlabel('Number of trees')\nplt.ylabel('RMSE')\nplt.legend();\n\n\n\n\n\n\n\n\nA similar trend can be seen by plotting out-of-bag RMSE and test RMSE. Note that RMSE is proportional to R-squared. You only need to visualize one of RMSE or \\(R\\)-squared to find the optimal number of trees.\n\n#Bagging with 150 trees\nmodel = RandomForestRegressor(n_estimators=200, random_state=1,max_features=\"sqrt\",\n                        oob_score=True,n_jobs=-1).fit(X, y)\n\n\n#OOB R-squared\nmodel.oob_score_\n\n0.8998265006519903\n\n\n\n#RMSE on test data\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(test.price, pred))\n\n5647.195064555622\n\n\n\n\n8.1.2 Tuning random forest\nThe Random forest object has options to set parameters such as depth, leaves, minimum number of observations in a leaf etc., for individual trees. These parameters are useful to prune a decision tree model consisting of a single tree, in order to avoid overfitting due to high variance of an unpruned tree.\nPruning individual trees in random forests is not likely to add much value, since averaging a sufficient number of unpruned trees reduces the variance of the trees, which enhances prediction accuracy. Pruning individual trees is unlikely to further reduce the prediction variance.\nHere is a comment from page 596 of the The Elements of Statistical Learning that supports the above statement: Segal (2004) demonstrates small gains in performance by controlling the depths of the individual trees grown in random forests. Our experience is that using full-grown trees seldom costs much, and results in one less tuning parameter.\nBelow we attempt to optimize parameters that prune individual trees. However, as expected, it does not result in a substantial increase in prediction accuracy.\nAlso, note that we don’t need to tune the number of trees in random forest with GridSearchCV. As we know the prediction accuracy will keep increasing with number of trees, we can tune the other hyperparameters with a constant value for the number of trees.\n\nmodel.estimators_[0].get_n_leaves()\n\n3086\n\n\n\nmodel.estimators_[0].get_depth()\n\n29\n\n\nCoarse grid search\n\n#Optimizing with OOB score takes half the time as compared to cross validation. \n#The number of models developed with OOB score tuning is one-fifth of the number of models developed with\n#5-fold cross validation\nstart_time = time.time()\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'max_depth': [5, 10, 15, 20, 25, 30],\n          'max_leaf_nodes':[600, 1200, 1800, 2400, 3000],\n          'max_features': [1,2,3,4]}\n\nparam_list=list(it.product(*(params[Name] for Name in params)))\n\noob_score = [0]*len(param_list)\ni=0\nfor pr in param_list:\n    model = RandomForestRegressor(random_state=1,oob_score=True,verbose=False,\n                    n_estimators = 100, max_depth=pr[0],\n                    max_leaf_nodes=pr[1], max_features=pr[2], n_jobs=-1).fit(X,y)\n    oob_score[i] = mean_squared_error(model.oob_prediction_, y, squared=False)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"Best params = \", param_list[np.argmin(oob_score)])\nprint(\"Optimal OOB validation RMSE = \", np.min(oob_score))\n\ntime taken =  1.230358862876892  minutes\nBest params =  (15, 1800, 3)\nOptimal OOB validation RMSE =  5243.408784594606\n\n\nFiner grid search\nBased on the coarse grid search, hyperparameters will be tuned in a finer grid around the optimal hyperparamter values obtained.\n\n#Optimizing with OOB score takes half the time as compared to cross validation. \n#The number of models developed with OOB score tuning is one-fifth of the number of models developed with\n#5-fold cross validation\nstart_time = time.time()\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'max_depth': [12, 15, 18],\n          'max_leaf_nodes':[1600, 1800, 2000],\n          'max_features': [1,2,3,4]}\n\nparam_list=list(it.product(*(params[Name] for Name in params)))\n\noob_score = [0]*len(param_list)\ni=0\nfor pr in param_list:\n    model = RandomForestRegressor(random_state=1,oob_score=True,verbose=False,\n             n_estimators = 100, max_depth=pr[0], max_leaf_nodes=pr[1],\n                    max_features=pr[2], n_jobs=-1).fit(X,y)\n    oob_score[i] = mean_squared_error(model.oob_prediction_, y, squared=False)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"Best params = \", param_list[np.argmin(oob_score)])\nprint(\"Optimal OOB validation RMSE = \", np.min(oob_score))\n\ntime taken =  0.4222299337387085  minutes\nBest params =  (15, 1800, 3)\nBest score =  5243.408784594606\n\n\n\n#Model with optimal parameters\nmodel = RandomForestRegressor(n_estimators = 100, random_state=1, max_leaf_nodes = 1800, max_depth = 15,\n                        oob_score=True,n_jobs=-1, max_features=3).fit(X, y)\n\n\n#RMSE on test data\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5671.410705964455\n\n\nOptimizing depth and leaves of individual trees didn’t improve the prediction accuracy of the model. Important parameters to optimize in random forests will be the number of trees (n_estimators), and number of predictors considered at each split (max_features). However, sometimes individual pruning of trees may be useful. This may happen when the increase in bias in individual trees (when pruned) is lesser than the decrease in variance of the tree. However, if the pairwise correlation coefficient \\(\\rho\\) of the trees increases by a certain extent on pruning, pruning may again be not useful.\n\n#Tuning only n_estimators and max_features produces similar results\nstart_time = time.time()\nparams = {'max_features': [1,2,3,4]}\n\nparam_list=list(it.product(*(params[Name] for Name in params)))\n\noob_score = [0]*len(param_list)\ni=0\nfor pr in param_list:\n    model = RandomForestRegressor(random_state=1,oob_score=True,verbose=False,\n                      n_estimators = 100, max_features=pr[0], n_jobs=-1).fit(X,y)\n    oob_score[i] = mean_squared_error(model.oob_prediction_, y, squared=False)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"Best params = \", param_list[np.argmin(oob_score)])\nprint(\"Optimal OOB validation RMSE = \", np.min(oob_score))\n\ntime taken =  0.02856200933456421  minutes\nBest params =  (3,)\nBest score (R-squared) =  5252.291978670057\n\n\n\n#Model with optimal parameters\nmodel = RandomForestRegressor(n_estimators=100, random_state=1,\n                        n_jobs=-1, max_features=3).fit(X, y)\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5656.561522632323\n\n\nConsidering hyperparameters involving pruning, we observe a marginal decrease in the out-of-bag RMSE. Thus, other hyperparameters (such as max_features and max_samples) must be prioritized for tuning over hyperparameters involving pruning.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-for-classification",
    "href": "Lec6_RandomForest.html#random-forest-for-classification",
    "title": "8  Random Forest",
    "section": "8.2 Random forest for classification",
    "text": "8.2 Random forest for classification\nRandom forest model to predict if a person has diabetes.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n#Ensembling the results of 10 decision trees\nmodel = RandomForestClassifier(n_estimators=200, random_state=1,max_features=\"sqrt\",n_jobs=-1).fit(X, y)\n\n\n#Feature importance for Random forest\nnp.mean([tree.feature_importances_ for tree in model.estimators_],axis=0)\n\narray([0.08380406, 0.25403736, 0.09000104, 0.07151063, 0.07733353,\n       0.16976023, 0.12289303, 0.13066012])\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.23\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  72.72727272727273\nROC-AUC:  0.8744050766790058\nPrecision:  0.6021505376344086\nRecall:  0.9180327868852459\n\n\n\n\n\n\n\n\n\nThe model obtained above is similar to the one obtained by bagging. We’ll discuss the comparison later.\n\n8.2.1 Model accuracy vs number of trees\n\n#Finding model accuracy vs number of trees\noob_accuracy={};test_accuracy={};oob_precision={}; test_precision = {}\nfor i in np.linspace(50,500,45,dtype=int):\n    model = RandomForestClassifier(n_estimators=i, random_state=1,max_features=\"sqrt\",n_jobs=-1,oob_score=True).fit(X, y)\n    oob_accuracy[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_accuracy[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n    oob_pred = (model.oob_decision_function_[:,1]&gt;=0.5).astype(int)     \n    oob_precision[i] = precision_score(y, oob_pred)\n    test_pred = model.predict(Xtest)\n    test_precision[i] = precision_score(ytest, test_pred)\n\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),label = 'Out of bag accuracy')\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),'o',color = 'blue')\nplt.plot(test_accuracy.keys(),test_accuracy.values(), label = 'Test data accuracy')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Classification accuracy')\nplt.legend();\n\n\n\n\n\n\n\n\nWe can also plot other metrics of interest such as out-of-bag precision vs number of trees.\n\n#Precision vs number of trees\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_precision.keys(),oob_precision.values(),label = 'Out of bag precision')\nplt.plot(oob_precision.keys(),oob_precision.values(),'o',color = 'blue')\nplt.plot(test_precision.keys(),test_precision.values(), label = 'Test data precision')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Precision')\nplt.legend();\n\n\n\n\n\n\n\n\n\n\n8.2.2 Tuning random forest\nHere we tune the number of predictors to be considered at each node for the split to maximize recall.\n\nstart_time = time.time()\n\nparams = {'n_estimators': [500],\n          'max_features': range(1,9),\n         }\n\nparam_list=list(it.product(*(params[Name] for Name in list(params.keys()))))\noob_recall = [0]*len(param_list)\n\ni=0\nfor pr in param_list:\n    model = RandomForestClassifier(random_state=1,oob_score=True,verbose=False,n_estimators = pr[0],\n                                  max_features=pr[1], n_jobs=-1).fit(X,y)\n    \n    oob_pred = (model.oob_decision_function_[:,1]&gt;=0.5).astype(int)     \n    oob_recall[i] = recall_score(y, oob_pred)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"max recall = \", np.max(oob_recall))\nprint(\"params= \", param_list[np.argmax(oob_recall)])\n\ntime taken =  0.08032723267873128  minutes\nmax recall =  0.5990338164251208\nparams=  (500, 8)\n\n\n\nmodel = RandomForestClassifier(random_state=1,n_jobs=-1,max_features=8,n_estimators=500).fit(X, y)\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.23\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  76.62337662337663\nROC-AUC:  0.8787237793054822\nPrecision:  0.6404494382022472\nRecall:  0.9344262295081968\n\n\n\n\n\n\n\n\n\n\nmodel.feature_importances_\n\narray([0.069273  , 0.31211579, 0.08492953, 0.05225877, 0.06179047,\n       0.17732674, 0.12342981, 0.1188759 ])",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-vs-bagging",
    "href": "Lec6_RandomForest.html#random-forest-vs-bagging",
    "title": "8  Random Forest",
    "section": "8.3 Random forest vs Bagging",
    "text": "8.3 Random forest vs Bagging\nWe saw in the above examples that the performance of random forest was similar to that of bagged trees. This may happen in some cases including but not limited to:\n\nAll the predictors are more or less equally important, and the bagged trees are not highly correlated.\nOne of the predictors dominates the trees, resulting in highly correlated trees. However, each of the highly correlated trees have high prediction accuracy, leading to overall high prediction accuracy of the bagged trees despite the high correlation.\n\nWhen can random forests perform poorly: When the number of variables is large, but the fraction of relevant variables small, random forests are likely to perform poorly with small \\(m\\) (fraction of predictors considered for each split). At each split the chance can be small that the relevant variables will be selected. - Elements of Statistical Learning, page 596.\nHowever, in general, random forests are expected to decorrelate and improve the bagged trees.\nLet us consider a classification example.\n\ndata = pd.read_csv('Heart.csv')\ndata.dropna(inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPain\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nThal\nAHD\n\n\n\n\n0\n63\n1\ntypical\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\nfixed\nNo\n\n\n1\n67\n1\nasymptomatic\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\nnormal\nYes\n\n\n2\n67\n1\nasymptomatic\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\nreversable\nYes\n\n\n3\n37\n1\nnonanginal\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\nnormal\nNo\n\n\n4\n41\n0\nnontypical\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\nnormal\nNo\n\n\n\n\n\n\n\nIn the above dataset, we wish to predict if a person has acquired heart disease (AHD = ‘Yes’), based on their symptoms.\n\n#Response variable\ny = pd.get_dummies(data['AHD'])['Yes']\n\n#Creating a dataframe for predictors with dummy variables replacing the categorical variables\nX = data.drop(columns = ['AHD','ChestPain','Thal'])\nX = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\nX.head()\n\n\n\n\n\n\n\n\nAge\nSex\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nasymptomatic\nnonanginal\nnontypical\ntypical\nfixed\nnormal\nreversable\n\n\n\n\n0\n63\n1\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\n0\n0\n0\n1\n1\n0\n0\n\n\n1\n67\n1\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\n1\n0\n0\n0\n0\n1\n0\n\n\n2\n67\n1\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\n1\n0\n0\n0\n0\n0\n1\n\n\n3\n37\n1\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\n0\n1\n0\n0\n0\n1\n0\n\n\n4\n41\n0\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n\n\n\n\nX.shape\n\n(297, 18)\n\n\n\n#Creating train and test datasets\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#tuning-random-forest-2",
    "href": "Lec6_RandomForest.html#tuning-random-forest-2",
    "title": "8  Random Forest",
    "section": "Tuning random forest",
    "text": "Tuning random forest\n\n#Tuning the random forest parameters\nstart_time = time.time()\n\noob_score = {}\n\ni=0\nfor pr in range(1,19):\n    model = RandomForestClassifier(random_state=1,oob_score=True,verbose=False,n_estimators = 500,\n                                  max_features=pr, n_jobs=-1).fit(X,y)\n    oob_score[i] = model.oob_score_\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"max accuracy = \", np.max(list(oob_score.values())))\nprint(\"Best value of max_features= \", np.argmax(list(oob_score.values()))+1)\n\ntime taken =  0.21557459433873494  minutes\nmax accuracy =  0.8249158249158249\nBest value of max_features=  3\n\n\n\nsns.scatterplot(x = oob_score.keys(),y = oob_score.values())\nplt.xlabel('Max features')\nplt.ylabel('Classification accuracy')\n\nText(0, 0.5, 'Classification accuracy')\n\n\n\n\n\n\n\n\n\nNote that as the value of max_features is increasing, the accuracy is decreasing. This is probably due to the trees getting correlated as we consider more predictors for each split.\n\n#Finding model accuracy vs number of trees\noob_accuracy={};test_accuracy={};\noob_accuracy2={};test_accuacy2={};\n\nfor i in np.linspace(100,500,40,dtype=int):\n    #Bagging\n    model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=i, random_state=1,\n                        n_jobs=-1,oob_score=True).fit(Xtrain, ytrain)\n    oob_accuracy[i]=model.oob_score_  #Returns the out-of-bag classification accuracy of the model\n    test_accuracy[i]=model.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n    \n    #Random forest\n    model2 = RandomForestClassifier(n_estimators=i, random_state=1,max_features=3,\n                        n_jobs=-1,oob_score=True).fit(Xtrain, ytrain)\n    oob_accuracy2[i]=model2.oob_score_  #Returns the out-of-bag classification accuracy of the model\n    test_accuacy2[i]=model2.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n   \n\n\n#Feature importance for bagging\nnp.mean([tree.feature_importances_ for tree in model.estimators_],axis=0)\n\narray([0.04381883, 0.05913479, 0.08585651, 0.07165678, 0.00302965,\n       0.00903484, 0.05890448, 0.01223421, 0.072461  , 0.01337919,\n       0.17495662, 0.18224651, 0.00527156, 0.00953965, 0.00396654,\n       0.00163193, 0.09955286, 0.09332406])\n\n\nNote that no predictor is too important to consider. That’s why a small value of three for max_features is likely to decorrelate trees without compromising the quality of predictions.\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),label = 'Bagging OOB')\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),'o',color = 'blue')\nplt.plot(test_accuracy.keys(),test_accuracy.values(), label = 'Bagging test accuracy')\n\nplt.plot(oob_accuracy2.keys(),oob_accuracy2.values(),label = 'RF OOB')\nplt.plot(oob_accuracy2.keys(),oob_accuracy2.values(),'o',color = 'green')\nplt.plot(test_accuacy2.keys(),test_accuacy2.values(), label = 'RF test accuracy')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Classification accuracy')\nplt.legend(bbox_to_anchor=(0, -0.15, 1, 0), loc=2, ncol=2, mode=\"expand\", borderaxespad=0)\n\n\n\n\n\n\n\n\nIn the above example we observe that random forest does improve over bagged trees in terms of classification accuracy. Unlike the previous two examples, the optimal value of max_features for random forests is much smaller than the total number of available predictors, thereby making the random forest model much different than the bagged tree model.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html",
    "href": "Lec7_AdaBoost.html",
    "title": "9  Adaptive Boosting",
    "section": "",
    "text": "9.1 Hyperparameters\nRead section 8.2.3 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nFor the exact algorithms underlying the AdaBoost algorithm, check out the papers AdaBoostRegressor() and AdaBoostClassifier().\nThere are 3 important parameters to tune in AdaBoost:\nLet us visualize the accuracy of AdaBoost when we independently tweak each of the above parameters.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\nfrom sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\nrecall_score, precision_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold\nfrom sklearn.ensemble import BaggingRegressor,BaggingClassifier,AdaBoostRegressor,AdaBoostClassifier, \\\nRandomForestRegressor\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nimport itertools as it\nimport time as time\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_convergence\nimport warnings\nfrom IPython import display\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\nX = train[['mileage','mpg','year','engineSize']]\nXtest = test[['mileage','mpg','year','engineSize']]\ny = train['price']\nytest = test['price']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html#hyperparameters",
    "href": "Lec7_AdaBoost.html#hyperparameters",
    "title": "9  Adaptive Boosting",
    "section": "",
    "text": "Number of trees\nDepth of each tree\nLearning rate",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html#adaboost-for-regression",
    "href": "Lec7_AdaBoost.html#adaboost-for-regression",
    "title": "9  Adaptive Boosting",
    "section": "9.2 AdaBoost for regression",
    "text": "9.2 AdaBoost for regression\n\n9.2.1 Number of trees vs cross validation error\nAs the number of trees increases, the prediction bias will decrease, and the prediction variance will increase. Thus, there will be an optimal number of trees that minimizes the prediction error.\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [2, 5, 10, 50, 100, 500, 1000]\n    for n in n_trees:\n        models[str(n)] = AdaBoostRegressor(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15);\n\n&gt;2 9190.253 (757.408)\n&gt;5 8583.629 (341.406)\n&gt;10 8814.328 (248.891)\n&gt;50 10763.138 (465.677)\n&gt;100 11217.783 (602.642)\n&gt;500 11336.088 (763.288)\n&gt;1000 11390.043 (752.446)\n\n\n\n\n\n\n\n\n\n\n\n9.2.2 Depth of tree vs cross validation error\nAs the depth of each weak learner (decision tree) increases, the complexity of the weak learner will increase. As the complexity increases, the prediction bias will decrease, while the prediction variance will increase. Thus, there will be an optimal depth for each weak learner that minimizes the prediction error.\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define base model\n        base = DecisionTreeRegressor(max_depth=i)\n        # define ensemble model\n        models[str(i)] = AdaBoostRegressor(base_estimator=base,n_estimators=50)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15);\n\n&gt;1 12798.764 (490.538)\n&gt;2 11031.451 (465.520)\n&gt;3 10739.302 (636.517)\n&gt;4 9491.714 (466.764)\n&gt;5 7184.489 (324.484)\n&gt;6 6181.533 (411.394)\n&gt;7 5746.902 (407.451)\n&gt;8 5587.726 (473.619)\n&gt;9 5526.291 (541.512)\n&gt;10 5444.928 (554.170)\n&gt;11 5321.725 (455.899)\n&gt;12 5279.581 (492.785)\n&gt;13 5494.982 (393.469)\n&gt;14 5423.982 (488.564)\n&gt;15 5369.485 (441.799)\n&gt;16 5536.739 (409.166)\n&gt;17 5511.002 (517.384)\n&gt;18 5510.922 (478.285)\n&gt;19 5482.119 (465.565)\n&gt;20 5667.969 (468.964)\n\n\n\n\n\n\n\n\n\n\n\n9.2.3 Learning rate vs cross validation error\nThe optimal learning rate will depend on the number of trees, and vice-versa. If the learning rate is too low, it will take several trees to “learn” the response. If the learning rate is high, the response will be “learned” quickly (with fewer) trees. Learning too quickly will be prone to overfitting, while learning too slowly will be computationally expensive. Thus, there will be an optimal learning rate to minimize the prediction error.\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = AdaBoostRegressor(learning_rate=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15);\n\n&gt;0.1 8291.9 (452.4)\n&gt;0.2 8475.7 (465.3)\n&gt;0.3 8648.5 (458.8)\n&gt;0.4 8995.5 (438.6)\n&gt;0.5 9376.1 (388.2)\n&gt;0.6 9655.3 (551.8)\n&gt;0.7 9877.3 (319.8)\n&gt;0.8 10466.8 (528.3)\n&gt;0.9 10728.9 (386.8)\n&gt;1.0 10720.2 (410.6)\n&gt;1.1 11043.9 (432.5)\n&gt;1.2 10602.5 (570.0)\n&gt;1.3 11058.8 (362.1)\n&gt;1.4 11022.7 (616.0)\n&gt;1.5 11252.5 (839.3)\n&gt;1.6 11195.3 (604.5)\n&gt;1.7 11206.3 (636.1)\n&gt;1.8 11569.1 (674.6)\n&gt;1.9 11232.3 (605.6)\n&gt;2.0 11581.0 (824.8)\n\n\n\n\n\n\n\n\n\n\n\n9.2.4 Tuning AdaBoost for regression\nAs the optimal value of the parameters depend on each other, we need to optimize them simultaneously.\n\nmodel = AdaBoostRegressor(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['estimator'] = [DecisionTreeRegressor(max_depth=3), DecisionTreeRegressor(max_depth=5), \n                          DecisionTreeRegressor(max_depth=10),DecisionTreeRegressor(max_depth=15)]\n# define the evaluation procedure\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='neg_root_mean_squared_error')\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (-grid_result.best_score_, grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nBest: 5346.490675 using {'estimator': DecisionTreeRegressor(max_depth=10), 'learning_rate': 1.0, 'n_estimators': 50}\n\n\nNote that for tuning max_depth of the base estimator - decision tree, we specified 4 different base estimators with different depths. However, there is a more concise way to do that. We can specify the max_depth of the estimator by adding a double underscore “__” between the estimator and the hyperparameter that we wish to tune (max_depth here), and then specify its potential values in the grid itself as shown below. However, we’ll then need to add DecisionTreeRegressor() as the estimator within the AdaBoostRegressor() function.\n\nmodel = AdaBoostRegressor(random_state=1, estimator = DecisionTreeRegressor(random_state=1))\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['estimator__max_depth'] = [3, 5, 10, 15]\n# define the evaluation procedure\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='neg_root_mean_squared_error')\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (-grid_result.best_score_, grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nBest: 5346.490675 using {'estimator__max_depth': 10, 'learning_rate': 1.0, 'n_estimators': 50}\n\n\nThe BayesSearchCV() approach also coverges to a slightly different set of optimal hyperparameter values. However, it gives a similar cross-validated RMSE. This is possible. There may be multiple hyperparameter values that are different from each other, but similar in performance. It may be a good idea to ensemble models based on these two distinct set of hyperparameter values that give an equally accurate model.\n\nmodel = AdaBoostRegressor(estimator=DecisionTreeRegressor()) \ngrid = dict()\ngrid['n_estimators'] = Integer(2, 1000)\ngrid['learning_rate'] = Real(0.0001, 1.0)\ngrid['estimator__max_depth'] = Integer(1, 20)\n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\ngcv.fit(X, y, callback = monitor)\n\n['estimator__max_depth', 'learning_rate', 'n_estimators'] = [13, 1.0, 570] 5325.017602505734\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=AdaBoostRegressor(estimator=DecisionTreeRegressor()),\n              n_iter=180, n_jobs=-1, random_state=10,\n              scoring='neg_root_mean_squared_error',\n              search_spaces={'estimator__max_depth': Integer(low=1, high=20, prior='uniform', transform='normalize'),\n                             'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=1000, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=AdaBoostRegressor(estimator=DecisionTreeRegressor()),\n              n_iter=180, n_jobs=-1, random_state=10,\n              scoring='neg_root_mean_squared_error',\n              search_spaces={'estimator__max_depth': Integer(low=1, high=20, prior='uniform', transform='normalize'),\n                             'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=1000, prior='uniform', transform='normalize')})estimator: AdaBoostRegressorAdaBoostRegressor(estimator=DecisionTreeRegressor())estimator: DecisionTreeRegressorDecisionTreeRegressor()DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nplot_objective(gcv.optimizer_results_[0],\n                   dimensions=['estimator__max_depth', 'learning_rate', 'n_estimators'], size = 3)\nplt.show();\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 3, figsize = (10, 3))\nplt.subplots_adjust(wspace=0.4)\nplot_histogram(gcv.optimizer_results_[0], 0, ax = ax[0])\nplot_histogram(gcv.optimizer_results_[0], 1, ax = ax[1])\nplot_histogram(gcv.optimizer_results_[0], 2, ax = ax[2])\nplt.show()\n\n\n\n\n\n\n\n\n\n#Model based on the optimal hyperparameters\nmodel = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,learning_rate=1.0,\n                         random_state=1).fit(X,y)\n\n\n#RMSE of the optimized model on test data\npred1=model.predict(Xtest)\nprint(\"AdaBoost model RMSE = \", np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nAdaBoost model RMSE =  5693.165811600585\n\n\n\n#Model based on the optimal hyperparameters\nmodel = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=13),n_estimators=570,learning_rate=1.0,\n                         random_state=1).fit(X,y)\n\n\n#RMSE of the optimized model on test data\npred2=model.predict(Xtest)\nprint(\"AdaBoost model RMSE = \", np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nAdaBoost model RMSE =  5434.852990644646\n\n\n\nmodel = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\npred3 = model.predict(Xtest)\nprint(\"Random Forest model RMSE = \", np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nRandom Forest model RMSE =  5642.45839697972\n\n\n\n#Ensemble modeling\npred = 0.33*pred1+0.33*pred2 + 0.34*pred3\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5402.832128650372\n\n\nCombined, the random forest model and the Adaboost models do better than each of the individual models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html#adaboost-for-classification",
    "href": "Lec7_AdaBoost.html#adaboost-for-classification",
    "title": "9  Adaptive Boosting",
    "section": "9.3 AdaBoost for classification",
    "text": "9.3 AdaBoost for classification\nBelow is the AdaBoost implementation on a classification problem. The takeaways are the same as that of the regression problem above.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n9.3.1 Number of trees vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [10, 50, 100, 500, 1000, 5000]\n    for n in n_trees:\n        models[str(n)] = AdaBoostClassifier(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;10 0.718 (0.060)\n&gt;50 0.751 (0.051)\n&gt;100 0.748 (0.053)\n&gt;500 0.690 (0.045)\n&gt;1000 0.694 (0.048)\n&gt;5000 0.691 (0.044)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n9.3.2 Depth of each tree vs cross validation accuracy\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define base model\n        base = DecisionTreeClassifier(max_depth=i)\n        # define ensemble model\n        models[str(i)] = AdaBoostClassifier(estimator=base)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 0.751 (0.051)\n&gt;2 0.699 (0.063)\n&gt;3 0.696 (0.062)\n&gt;4 0.707 (0.055)\n&gt;5 0.713 (0.021)\n&gt;6 0.710 (0.061)\n&gt;7 0.733 (0.057)\n&gt;8 0.738 (0.044)\n&gt;9 0.727 (0.053)\n&gt;10 0.738 (0.065)\n&gt;11 0.748 (0.048)\n&gt;12 0.699 (0.044)\n&gt;13 0.738 (0.047)\n&gt;14 0.697 (0.041)\n&gt;15 0.697 (0.052)\n&gt;16 0.692 (0.052)\n&gt;17 0.702 (0.056)\n&gt;18 0.702 (0.045)\n&gt;19 0.700 (0.040)\n&gt;20 0.696 (0.042)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n9.3.3 Learning rate vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = AdaBoostClassifier(learning_rate=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.1 0.749 (0.052)\n&gt;0.2 0.743 (0.050)\n&gt;0.3 0.731 (0.057)\n&gt;0.4 0.736 (0.053)\n&gt;0.5 0.733 (0.062)\n&gt;0.6 0.738 (0.058)\n&gt;0.7 0.741 (0.056)\n&gt;0.8 0.741 (0.049)\n&gt;0.9 0.736 (0.048)\n&gt;1.0 0.741 (0.035)\n&gt;1.1 0.734 (0.037)\n&gt;1.2 0.736 (0.038)\n&gt;1.3 0.731 (0.057)\n&gt;1.4 0.728 (0.041)\n&gt;1.5 0.730 (0.036)\n&gt;1.6 0.720 (0.038)\n&gt;1.7 0.707 (0.045)\n&gt;1.8 0.730 (0.024)\n&gt;1.9 0.712 (0.033)\n&gt;2.0 0.454 (0.191)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n9.3.4 Tuning AdaBoost Classifier hyperparameters\n\nmodel = AdaBoostClassifier(random_state=1, estimator = DecisionTreeClassifier())\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['estimator__max_depth'] = [1, 2, 3, 4]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                          verbose = True)\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n#for mean, stdev, param in zip(means, stds, params):\n#    print(\"%f (%f) with: %r\" % (mean, stdev, param)\n\nFitting 5 folds for each of 100 candidates, totalling 500 fits\nBest: 0.763934 using {'estimator__max_depth': 3, 'learning_rate': 0.01, 'n_estimators': 200}\n\n\n\n\n9.3.5 Tuning the decision threshold probability\nWe’ll find a decision threshold probability that balances recall with precision.\n\n#Model based on the optimal parameters\nmodel = AdaBoostClassifier(random_state=1, estimator = DecisionTreeClassifier(max_depth=3),learning_rate=0.01,\n                          n_estimators=200).fit(X,y)\n\n# Note that we are using the cross-validated predicted probabilities, instead of directly using the \n# predicted probabilities on train data, as the model may be overfitting on the train data, and \n# may lead to misleading results\ncross_val_ypred = cross_val_predict(AdaBoostClassifier(random_state=1,base_estimator = DecisionTreeClassifier(max_depth=3),learning_rate=0.01,\n                          n_estimators=200), X, y, cv = 5, method = 'predict_proba')\n\np, r, thresholds = precision_recall_curve(y, cross_val_ypred[:,1])\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\n\n# Thresholds with precision and recall\nall_thresholds = np.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,2]&gt;0.8,:]\n# As the values in 'recall_more_than_80' are arranged in decreasing order of recall and increasing threshold,\n# the last value will provide the maximum threshold probability for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the maximum possible precision\nrecall_more_than_80[recall_more_than_80.shape[0]-1]\n\narray([0.33488762, 0.50920245, 0.80193237])\n\n\n\n#Optimal decision threshold probability\nthres = recall_more_than_80[recall_more_than_80.shape[0]-1][0]\nthres\n\n0.3348876199649718\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = thres\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8884188260179798\nPrecision:  0.6875\nRecall:  0.9016393442622951\n\n\n\n\n\n\n\n\n\nThe above model is similar to the one obtained with bagging / random forest. However, adaptive boosting may lead to better classification performance as compared to bagging / random forest.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html",
    "href": "Lec8_Gradient_Boosting.html",
    "title": "10  Gradient Boosting",
    "section": "",
    "text": "10.1 Hyperparameters\nCheck the gradient boosting algorithm in section 10.10.2 of the book, Elements of Statistical Learning before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nThere are 5 important parameters to tune in Gradient boosting:\nLet us visualize the accuracy of Gradient boosting when we independently tweak each of the above parameters.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\nfrom sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\nrecall_score, precision_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nimport itertools as it\nimport time as time\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_convergence\nimport warnings\nfrom IPython import display\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\nX = train[['mileage','mpg','year','engineSize']]\nXtest = test[['mileage','mpg','year','engineSize']]\ny = train['price']\nytest = test['price']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html#hyperparameters",
    "href": "Lec8_Gradient_Boosting.html#hyperparameters",
    "title": "10  Gradient Boosting",
    "section": "",
    "text": "Number of trees\nDepth of each tree\nLearning rate\nSubsample fraction\nMaximum features",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html#gradient-boosting-for-regression",
    "href": "Lec8_Gradient_Boosting.html#gradient-boosting-for-regression",
    "title": "10  Gradient Boosting",
    "section": "10.2 Gradient boosting for regression",
    "text": "10.2 Gradient boosting for regression\n\n10.2.1 Number of trees vs cross validation error\nAs per the documentation, Gradient boosting is fairly robust (as compared to AdaBoost) to over-fitting (why?) so a large number usually results in better performance. Note that the number of trees still need to be tuned for optimal performance.\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [2, 5, 10, 50, 100, 500, 1000, 2000, 5000]\n    for n in n_trees:\n        models[str(n)] = GradientBoostingRegressor(n_estimators=n,random_state=1,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;2 14927.566 (179.475)\n&gt;5 12743.148 (189.408)\n&gt;10 10704.199 (226.234)\n&gt;50 6869.066 (278.885)\n&gt;100 6354.656 (270.097)\n&gt;500 5515.622 (424.516)\n&gt;1000 5515.251 (427.767)\n&gt;2000 5600.041 (389.687)\n&gt;5000 5854.168 (362.223)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n10.2.2 Depth of tree vs cross validation error\nAs the depth of each weak learner (decision tree) increases, the complexity of the weak learner will increase. As the complexity increases, the prediction bias will decrease, while the prediction variance will increase. Thus, there will be an optimal depth of each weak learner that minimizes the prediction error.\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define ensemble model\n        models[str(i)] = GradientBoostingRegressor(n_estimators=50,random_state=1,max_depth=i,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 9693.731 (810.090)\n&gt;2 7682.569 (489.841)\n&gt;3 6844.225 (536.792)\n&gt;4 5972.203 (538.693)\n&gt;5 5664.563 (497.882)\n&gt;6 5329.130 (404.330)\n&gt;7 5210.934 (461.038)\n&gt;8 5197.204 (494.957)\n&gt;9 5227.975 (478.789)\n&gt;10 5299.782 (446.509)\n&gt;11 5433.822 (451.673)\n&gt;12 5617.946 (509.797)\n&gt;13 5876.424 (542.981)\n&gt;14 6030.507 (560.447)\n&gt;15 6125.914 (643.852)\n&gt;16 6294.784 (672.646)\n&gt;17 6342.327 (677.050)\n&gt;18 6372.418 (791.068)\n&gt;19 6456.471 (741.693)\n&gt;20 6503.622 (759.193)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n10.2.3 Learning rate vs cross validation error\nThe optimal learning rate will depend on the number of trees, and vice-versa. If the learning rate is too low, it will take several trees to “learn” the response. If the learning rate is high, the response will be “learned” quickly (with fewer) trees. Learning too quickly will be prone to overfitting, while learning too slowly will be computationally expensive. Thus, there will be an optimal learning rate to minimize the prediction error.\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = GradientBoostingRegressor(learning_rate=i,random_state=1,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.1 6329.8 (450.7)\n&gt;0.2 5942.9 (454.8)\n&gt;0.3 5618.4 (490.8)\n&gt;0.4 5665.9 (577.3)\n&gt;0.5 5783.5 (561.7)\n&gt;0.6 5773.8 (500.3)\n&gt;0.7 5875.5 (565.7)\n&gt;0.8 5878.5 (540.5)\n&gt;0.9 6214.4 (594.3)\n&gt;1.0 5986.1 (601.5)\n&gt;1.1 6216.5 (395.3)\n&gt;1.2 6667.5 (657.2)\n&gt;1.3 6717.4 (594.4)\n&gt;1.4 7048.4 (531.7)\n&gt;1.5 7265.0 (742.0)\n&gt;1.6 7404.4 (868.2)\n&gt;1.7 7425.8 (606.3)\n&gt;1.8 8283.0 (1345.3)\n&gt;1.9 8872.2 (1137.9)\n&gt;2.0 17713.3 (865.3)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n10.2.4 Subsampling vs cross validation error\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for s in np.arange(0.25, 1.1, 0.25):\n        key = '%.2f' % s\n        models[key] = GradientBoostingRegressor(random_state=1,subsample=s,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.2f (%.2f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Subsample',fontsize=15)\n\n&gt;0.25 6219.59 (569.97)\n&gt;0.50 6178.28 (501.87)\n&gt;0.75 6141.96 (432.66)\n&gt;1.00 6329.79 (450.72)\n\n\nText(0.5, 0, 'Subsample')\n\n\n\n\n\n\n\n\n\n\n\n10.2.5 Maximum features vs cross-validation error\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for s in np.arange(0.25, 1.1, 0.25):\n        key = '%.2f' % s\n        models[key] = GradientBoostingRegressor(random_state=1,max_features=s,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.2f (%.2f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Maximum features',fontsize=15)\n\n&gt;0.25 6654.27 (567.72)\n&gt;0.50 6373.92 (538.53)\n&gt;0.75 6325.55 (470.41)\n&gt;1.00 6329.79 (450.72)\n\n\nText(0.5, 0, 'Maximum features')\n\n\n\n\n\n\n\n\n\n\n\n10.2.6 Tuning Gradient boosting for regression\nAs the optimal value of the parameters depend on each other, we need to optimize them simultaneously.\n\nstart_time = time.time()\nmodel = GradientBoostingRegressor(random_state=1,loss='huber')\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['max_depth'] = [3,5,8,10,12,15]\n\n# define the evaluation procedure\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='neg_mean_squared_error',\n                          verbose = True)\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (np.sqrt(-grid_result.best_score_), grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n#for mean, stdev, param in zip(means, stds, params):\n#    print(\"%f (%f) with: %r\" % (mean, stdev, param)\nprint(\"Time taken = \",(time.time()-start_time)/60,\" minutes\")\n\nBest: 5190.765919 using {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 100}\nTime taken =  46.925597019990285  minutes\n\n\nNote that the code takes 46 minutes to run. In case of a lot of hyperparameters, RandomizedSearchCV may be preferred to trade-off between optimality of the solution and computational cost.\n\nmodel = GradientBoostingRegressor(random_state=1, loss='huber') \ngrid = dict()\ngrid['n_estimators'] = Integer(2, 1000)\ngrid['learning_rate'] = Real(0.0001, 1.0)\ngrid['max_leaf_nodes'] = Integer(4, 5000)\ngrid['subsample'] = Real(0.1, 1)\ngrid['max_features'] = Real(0.1, 1)\n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 100, random_state = 1,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nparas = list(gcv.search_spaces.keys())\nparas.sort()\nstart_time = time.time()\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    print(\"Time so far = \", np.round((time.time()-start_time)/60), \"minutes\")\n    sns.lineplot(cv_values)\n    plt.show()\ngcv.fit(X, y, callback = monitor)\n\n['learning_rate', 'max_features', 'max_leaf_nodes', 'n_estimators', 'subsample'] = [0.23102084158310995, 0.315075948850183, 5000, 817, 1.0] 5360.92726695485\nTime so far =  21.0 minutes\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=GradientBoostingRegressor(loss='huber', random_state=1),\n              n_iter=100, n_jobs=-1, random_state=1,\n              scoring='neg_root_mean_squared_error',\n              search_spaces={'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'max_features': Real(low=0.1, high=1, prior='uniform', transform='normalize'),\n                             'max_leaf_nodes': Integer(low=4, high=5000, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=1000, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.1, high=1, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=GradientBoostingRegressor(loss='huber', random_state=1),\n              n_iter=100, n_jobs=-1, random_state=1,\n              scoring='neg_root_mean_squared_error',\n              search_spaces={'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'max_features': Real(low=0.1, high=1, prior='uniform', transform='normalize'),\n                             'max_leaf_nodes': Integer(low=4, high=5000, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=1000, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.1, high=1, prior='uniform', transform='normalize')})estimator: GradientBoostingRegressorGradientBoostingRegressor(loss='huber', random_state=1)GradientBoostingRegressorGradientBoostingRegressor(loss='huber', random_state=1)\n\n\n\n#Model based on the optimal parameters\nmodel = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n                         random_state=1,loss='huber').fit(X,y)\n\n\n#RMSE of the optimized model on test data\nprint(\"Gradient boost RMSE = \",np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nGradient boost RMSE =  5405.787029062213\n\n\n\n#Model based on the optimal parameters\nmodel_bayes = GradientBoostingRegressor(max_leaf_nodes=5000,n_estimators=817,learning_rate=0.23, max_features=0.31,\n                         random_state=1,subsample=1.0,loss='huber').fit(X,y)\n\n\n#RMSE of the optimized model on test data\nprint(\"Gradient boost RMSE = \",np.sqrt(mean_squared_error(model_bayes.predict(Xtest),ytest)))\n\nGradient boost RMSE =  5734.200307094321\n\n\n\n#Let us combine the Gradient boost model with other models\nmodel2 = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,learning_rate=1.0,\n                         random_state=1).fit(X,y)\nprint(\"AdaBoost RMSE = \",np.sqrt(mean_squared_error(model2.predict(Xtest),ytest)))\nmodel3 = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\nprint(\"Random Forest RMSE = \",np.sqrt(mean_squared_error(model3.predict(Xtest),ytest)))\n\nAdaBoost RMSE =  5693.165811600585\nRandom Forest RMSE =  5642.45839697972\n\n\n\n#Ensemble model\npred1=model.predict(Xtest)#Gradient boost\npred2=model2.predict(Xtest)#Adaboost\npred3=model3.predict(Xtest)#Random forest\npred = 0.34*pred1+0.33*pred2+0.33*pred3 #Higher weight to the better model\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5364.478227748279\n\n\n\n\n10.2.7 Ensemble modeling (for regression models)\n\n#Ensemble model\npred1=model.predict(Xtest)#Gradient boost\npred2=model2.predict(Xtest)#Adaboost\npred3=model3.predict(Xtest)#Random forest\npred = 0.6*pred1+0.2*pred2+0.2*pred3 #Higher weight to the better model\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5323.119083375402\n\n\nCombined, the random forest model, gradient boost and the Adaboost model do better than each of the individual models.\nNote that ideally we should do K-fold cross validation to figure out the optimal weights. We’ll learn about ensembling techniques later in the course.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html#gradient-boosting-for-classification",
    "href": "Lec8_Gradient_Boosting.html#gradient-boosting-for-classification",
    "title": "10  Gradient Boosting",
    "section": "10.3 Gradient boosting for classification",
    "text": "10.3 Gradient boosting for classification\nBelow is the Gradient boost implementation on a classification problem. The takeaways are the same as that of the regression problem above.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n10.3.1 Number of trees vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [10, 50, 100, 500, 1000, 5000]\n    for n in n_trees:\n        models[str(n)] = GradientBoostingClassifier(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;10 0.738 (0.031)\n&gt;50 0.748 (0.054)\n&gt;100 0.722 (0.075)\n&gt;500 0.707 (0.066)\n&gt;1000 0.712 (0.075)\n&gt;5000 0.697 (0.061)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Depth of each tree vs cross validation accuracy\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define ensemble model\n        models[str(i)] = GradientBoostingClassifier(random_state=1,max_depth=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 0.746 (0.040)\n&gt;2 0.744 (0.046)\n&gt;3 0.722 (0.075)\n&gt;4 0.743 (0.049)\n&gt;5 0.738 (0.046)\n&gt;6 0.741 (0.047)\n&gt;7 0.735 (0.057)\n&gt;8 0.736 (0.051)\n&gt;9 0.728 (0.055)\n&gt;10 0.710 (0.050)\n&gt;11 0.697 (0.061)\n&gt;12 0.681 (0.056)\n&gt;13 0.709 (0.047)\n&gt;14 0.702 (0.048)\n&gt;15 0.705 (0.048)\n&gt;16 0.700 (0.042)\n&gt;17 0.699 (0.048)\n&gt;18 0.697 (0.050)\n&gt;19 0.696 (0.042)\n&gt;20 0.697 (0.048)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n10.3.3 Learning rate vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = GradientBoostingClassifier(learning_rate=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.1 0.747 (0.044)\n&gt;0.2 0.736 (0.028)\n&gt;0.3 0.726 (0.039)\n&gt;0.4 0.730 (0.034)\n&gt;0.5 0.726 (0.041)\n&gt;0.6 0.722 (0.043)\n&gt;0.7 0.717 (0.050)\n&gt;0.8 0.713 (0.033)\n&gt;0.9 0.694 (0.045)\n&gt;1.0 0.695 (0.032)\n&gt;1.1 0.718 (0.034)\n&gt;1.2 0.692 (0.045)\n&gt;1.3 0.708 (0.042)\n&gt;1.4 0.704 (0.050)\n&gt;1.5 0.702 (0.028)\n&gt;1.6 0.700 (0.050)\n&gt;1.7 0.694 (0.044)\n&gt;1.8 0.650 (0.075)\n&gt;1.9 0.551 (0.163)\n&gt;2.0 0.484 (0.123)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n10.3.4 Tuning Gradient boosting Classifier\n\nstart_time = time.time()\nmodel = GradientBoostingClassifier(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['max_depth'] = [1,2,3,4,5]\ngrid['subsample'] = [0.5,1.0]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, verbose = True, scoring = 'recall')\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nprint(\"Time taken = \", time.time() - start_time, \"seconds\")\n\nFitting 5 folds for each of 250 candidates, totalling 1250 fits\nBest: 0.701045 using {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}\nTime taken =  32.46394085884094\n\n\n\n#Model based on the optimal parameters\nmodel = GradientBoostingClassifier(random_state=1,max_depth=3,learning_rate=0.1,subsample=0.5,\n                          n_estimators=200).fit(X,y)\n\n# Note that we are using the cross-validated predicted probabilities, instead of directly using the \n# predicted probabilities on train data, as the model may be overfitting on the train data, and \n# may lead to misleading results\ncross_val_ypred = cross_val_predict(GradientBoostingClassifier(random_state=1,max_depth=3,\n                                                               learning_rate=0.1,subsample=0.5,\n                          n_estimators=200), X, y, cv = 5, method = 'predict_proba')\n\np, r, thresholds = precision_recall_curve(y, cross_val_ypred[:,1])\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\n\n# Thresholds with precision and recall\nall_thresholds = np.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,2]&gt;0.8,:]\n# As the values in 'recall_more_than_80' are arranged in decreasing order of recall and increasing threshold,\n# the last value will provide the maximum threshold probability for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the maximum possible precision\nrecall_more_than_80[recall_more_than_80.shape[0]-1]\n\narray([0.18497144, 0.53205128, 0.80193237])\n\n\n\n#Optimal decision threshold probability\nthres = recall_more_than_80[recall_more_than_80.shape[0]-1][0]\nthres\n\n0.18497143500912738\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = thres\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  77.92207792207793\nROC-AUC:  0.8704389212057112\nPrecision:  0.6626506024096386\nRecall:  0.9016393442622951\n\n\n\n\n\n\n\n\n\nThe model seems to be similar to the Adaboost model. However, gradient boosting algorithms with robust loss functions can perform better than Adaboost in the presence of outliers (in terms of response) in the data.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html#faster-algorithms-and-tuning-tips",
    "href": "Lec8_Gradient_Boosting.html#faster-algorithms-and-tuning-tips",
    "title": "10  Gradient Boosting",
    "section": "10.4 Faster algorithms and tuning tips",
    "text": "10.4 Faster algorithms and tuning tips\nCheck out HistGradientBoostingRegressor() and HistGradientBoostingClassifier() for a faster gradient boosting algorithm for big datasets (more than 10,000 observations).\nCheck out tips for faster hyperparameter tuning, such as tuning max_leaf_nodes instead of max_depth here.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html",
    "href": "Lec9_XGBoost.html",
    "title": "11  XGBoost",
    "section": "",
    "text": "11.1 Hyperparameters\nXGBoost is a very recently developed algorithm (2016). Thus, it’s not yet there in standard textbooks. Here are some resources for it.\nDocumentation\nSlides\nReference paper\nVideo by author (Tianqi Chen)\nVideo by StatQuest\nThe following are some of the important hyperparameters to tune in XGBoost:\nHowever, there are other hyperparameters that can be tuned as well. Check out the list of all hyperparameters in the XGBoost documentation.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\nfrom sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\nrecall_score, precision_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.ensemble import VotingRegressor, VotingClassifier, StackingRegressor, StackingClassifier, GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.linear_model import LinearRegression,LogisticRegression, LassoCV, RidgeCV, ElasticNetCV\nfrom sklearn.neighbors import KNeighborsRegressor\nimport itertools as it\nimport time as time\nimport xgboost as xgb\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_convergence\nimport warnings\nfrom IPython import display\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\nX = train[['mileage','mpg','year','engineSize']]\nXtest = test[['mileage','mpg','year','engineSize']]\ny = train['price']\nytest = test['price']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html#hyperparameters",
    "href": "Lec9_XGBoost.html#hyperparameters",
    "title": "11  XGBoost",
    "section": "",
    "text": "Number of trees (n_estimators)\nDepth of each tree (max_depth)\nLearning rate (learning_rate)\nSampling observations / predictors (subsample for observations, colsample_bytree for predictors)\nRegularization parameters (reg_lambda & gamma)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html#xgboost-for-regression",
    "href": "Lec9_XGBoost.html#xgboost-for-regression",
    "title": "11  XGBoost",
    "section": "11.2 XGBoost for regression",
    "text": "11.2 XGBoost for regression\n\n11.2.1 Number of trees vs cross validation error\nAs the number of trees increase, the prediction bias will decrease. Like gradient boosting is relatively robust (as compared to AdaBoost) to over-fitting (why?) so a large number usually results in better performance. Note that the number of trees still need to be tuned for optimal performance.\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [5, 10, 50, 100, 500, 1000, 2000, 5000]\n    for n in n_trees:\n        models[str(n)] = xgb.XGBRegressor(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;5 7961.485 (192.906)\n&gt;10 5837.134 (217.986)\n&gt;50 5424.788 (263.890)\n&gt;100 5465.396 (237.938)\n&gt;500 5608.350 (235.903)\n&gt;1000 5635.159 (236.664)\n&gt;2000 5642.669 (236.192)\n&gt;5000 5643.411 (236.074)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n11.2.2 Depth of tree vs cross validation error\nAs the depth of each weak learner (decision tree) increases, the complexity of the weak learner will increase. As the complexity increases, the prediction bias will decrease, while the prediction variance will increase. Thus, there will be an optimal depth of each weak learner that minimizes the prediction error.\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define ensemble model\n        models[str(i)] = xgb.XGBRegressor(random_state=1,max_depth=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 7541.827 (545.951)\n&gt;2 6129.425 (393.357)\n&gt;3 5647.783 (454.318)\n&gt;4 5438.481 (453.726)\n&gt;5 5358.074 (379.431)\n&gt;6 5281.675 (383.848)\n&gt;7 5495.163 (459.356)\n&gt;8 5399.145 (380.437)\n&gt;9 5469.563 (384.004)\n&gt;10 5461.549 (416.630)\n&gt;11 5443.210 (432.863)\n&gt;12 5546.447 (412.097)\n&gt;13 5532.414 (369.131)\n&gt;14 5556.761 (362.746)\n&gt;15 5540.366 (452.612)\n&gt;16 5586.004 (451.199)\n&gt;17 5563.137 (464.344)\n&gt;18 5594.919 (480.221)\n&gt;19 5641.226 (451.713)\n&gt;20 5616.462 (417.405)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n11.2.3 Learning rate vs cross validation error\nThe optimal learning rate will depend on the number of trees, and vice-versa. If the learning rate is too low, it will take several trees to “learn” the response. If the learning rate is high, the response will be “learned” quickly (with fewer) trees. Learning too quickly will be prone to overfitting, while learning too slowly will be computationally expensive. Thus, there will be an optimal learning rate to minimize the prediction error.\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in [0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.8,1.0]:\n        key = '%.4f' % i\n        models[key] = xgb.XGBRegressor(learning_rate=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.0100 12223.8 (636.7)\n&gt;0.0500 5298.5 (383.5)\n&gt;0.1000 5236.3 (397.5)\n&gt;0.2000 5221.5 (347.5)\n&gt;0.3000 5281.7 (383.8)\n&gt;0.4000 5434.1 (364.6)\n&gt;0.5000 5537.0 (471.9)\n&gt;0.6000 5767.4 (478.5)\n&gt;0.8000 6132.7 (472.5)\n&gt;1.0000 6593.6 (408.9)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n11.2.4 Regularization (reg_lambda) vs cross validation error\nThe parameter reg_lambda penalizes the L2 norm of the leaf scores. For example, in case of classification, it will penalize the summation of the square of log odds of the predicted probability. This penalization will tend to reduce the log odds, thereby reducing the tendency to overfit. “Reducing the log odds” in layman terms will mean not being overly sure about the prediction.\nWithout regularization, the algorithm will be closer to the gradient boosting algorithm. Regularization may provide some additional boost to prediction accuracy by reducing over-fitting. In the example below, regularization with reg_lambda=1 turns out to be better than no regularization (reg_lambda=0)*. Of course, too much regularization may increase bias so much such that it leads to a decrease in prediction accuracy.\n\ndef get_models():\n    models = dict()\n    # explore 'reg_lambda' from 0.1 to 2 in 0.1 increments\n    for i in [0,0.5,1.0,1.5,2,10,100]:\n        key = '%.4f' % i\n        models[key] = xgb.XGBRegressor(reg_lambda=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('reg_lambda',fontsize=15)\n\n&gt;0.0000 5359.2 (317.0)\n&gt;0.5000 5382.7 (363.1)\n&gt;1.0000 5281.7 (383.8)\n&gt;1.5000 5348.0 (383.9)\n&gt;2.0000 5336.4 (426.6)\n&gt;10.0000 5410.9 (521.9)\n&gt;100.0000 5801.1 (563.7)\n\n\nText(0.5, 0, 'reg_lambda')\n\n\n\n\n\n\n\n\n\n\n\n11.2.5 Regularization (gamma) vs cross validation error\nThe parameter gamma penalizes the tree based on the number of leaves. This is similar to the parameter alpha of cost complexity pruning. As gamma increases, more leaves will be pruned. Note that the previous parameter reg_lambda penalizes the leaf score, but does not prune the tree.\nWithout regularization, the algorithm will be closer to the gradient boosting algorithm. Regularization may provide some additional boost to prediction accuracy by reducing over-fitting. However, in the example below, no regularization (in terms of gamma=0) turns out to be better than a non-zero regularization. (reg_lambda=0).\n\ndef get_models():\n    models = dict()\n    # explore gamma from 0.1 to 2 in 0.1 increments\n    for i in [0,10,1e2,1e3,1e4,1e5,1e6,1e7,1e8,1e9]:\n        key = '%.4f' % i\n        models[key] = xgb.XGBRegressor(gamma=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('gamma',fontsize=15)\n#ax.set_xticklabels(x.astype(int))\nplt.xticks(ticks=plt.xticks()[0].astype(int), labels=np.round([0,10,1e2,1e3,1e4,1e5,1e6,1e7,1e8,1e9]),\n          rotation = 30);\n\n&gt;0.0000 5281.7 (383.8)\n&gt;10.0000 5281.7 (383.8)\n&gt;100.0000 5281.7 (383.8)\n&gt;1000.0000 5291.8 (381.8)\n&gt;10000.0000 5295.7 (370.2)\n&gt;100000.0000 5293.0 (402.5)\n&gt;1000000.0000 5322.2 (368.9)\n&gt;10000000.0000 5273.7 (409.8)\n&gt;100000000.0000 5345.4 (373.9)\n&gt;1000000000.0000 5932.3 (397.6)\n\n\n\n\n\n\n\n\n\n\n\n11.2.6 Tuning XGboost regressor\nAlong with max_depth, learning_rate, and n_estimators, here we tune reg_lambda - the regularization parameter for penalizing the tree predictions.\n\n#K-fold cross validation to find optimal parameters for XGBoost\nstart_time = time.time()\nparam_grid = {'max_depth': [4,6,8],\n              'learning_rate': [0.01, 0.05, 0.1],\n               'reg_lambda':[0, 1, 10],\n                'n_estimators':[100, 500, 1000],\n                'gamma': [0, 10, 100],\n                'subsample': [0.5, 0.75, 1.0],\n                'colsample_bytree': [0.5, 0.75, 1.0]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = RandomizedSearchCV(estimator=xgb.XGBRegressor(random_state=1),                                                       \n                             param_distributions = param_grid, n_iter = 200,\n                             verbose = 1,\n                             n_jobs=-1,\n                             cv = cv)\noptimal_params.fit(X,y)\nprint(\"Optimal parameter values =\", optimal_params.best_params_)\nprint(\"Optimal cross validation R-squared = \",optimal_params.best_score_)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\nOptimal parameter values = {'subsample': 0.75, 'reg_lambda': 1, 'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.01, 'gamma': 100, 'colsample_bytree': 1.0}\nOptimal cross validation R-squared =  0.9002580404500382\nTime taken =  4  minutes\n\n\n\n#RMSE based on the optimal parameter values\nnp.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))\n\n5497.553788113875\n\n\nLet us use Bayes search to tune the model.\n\nmodel = xgb.XGBRegressor(random_state = 1) \n\ngrid = {'max_leaves': Integer(4, 5000),\n              'learning_rate': Real(0.0001, 1.0),\n               'reg_lambda':Real(0, 1e4),\n                'n_estimators':Integer(2, 2000),\n                'gamma': Real(0, 1e11),\n                'subsample': Real(0.1,1.0),\n                'colsample_bytree': Real(0.1, 1.0)}\n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 100, random_state = 1,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\ngcv.fit(X, y, callback = monitor)\n\n['colsample_bytree', 'gamma', 'learning_rate', 'max_leaves', 'n_estimators', 'reg_lambda', 'subsample'] = [0.8455872906441244, 0.0, 0.9137620583590551, 802, 1023, 1394.0140479620452, 0.7987263539365876] 5340.272253124142\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=XGBRegressor(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False, eval_metric=None,\n                                     feature_types=None, gamma=None,\n                                     gpu_id=None, grow_policy=None,\n                                     importance_type=None,\n                                     inte...\n                             'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'max_leaves': Integer(low=4, high=5000, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=2000, prior='uniform', transform='normalize'),\n                             'reg_lambda': Real(low=0, high=10000.0, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.1, high=1.0, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=XGBRegressor(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False, eval_metric=None,\n                                     feature_types=None, gamma=None,\n                                     gpu_id=None, grow_policy=None,\n                                     importance_type=None,\n                                     inte...\n                             'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'max_leaves': Integer(low=4, high=5000, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=2000, prior='uniform', transform='normalize'),\n                             'reg_lambda': Real(low=0, high=10000.0, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.1, high=1.0, prior='uniform', transform='normalize')})estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=1, ...)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=1, ...)\n\n\n\nmodel1 = xgb.XGBRegressor(random_state = 1, colsample_bytree = 0.85, gamma = 0, learning_rate = 0.91, \n                          max_leaves = 802, n_estimators = 1023, reg_lambda = 1394, subsample = 0.8).fit(X, y)\n\n\nnp.sqrt(mean_squared_error(model1.predict(Xtest),ytest))\n\n5466.076861800755\n\n\nWe got a different set of optimal hyperparameters with Bayes search. Thus, ensembling the model based on the two sets of hyperparameters is likely to improve the accuracy over the individual models.\n\nmodel2 = xgb.XGBRegressor(random_state = 1, colsample_bytree = 1.0, gamma = 100, learning_rate = 0.01, \n                          max_depth = 8, n_estimators = 1000, reg_lambda = 1, subsample = 0.75).fit(X, y)\n\n\nnp.sqrt(mean_squared_error(0.5*model1.predict(Xtest)+0.5*model2.predict(Xtest),ytest))\n\n5393.379834226845\n\n\n\n\n11.2.7 Early stopping with XGBoost\nIf we have a test dataset (or we can further split the train data into a smaller train and test data), we can use it with the early_stopping_rounds argument of XGBoost, where it will stop growing trees once the model accuracy fails to increase for a certain number of consecutive iterations, given as early_stopping_rounds.\n\nX_train_sub, X_test_sub, y_train_sub, y_test_sub = \\\ntrain_test_split(X, y, test_size = 0.2, random_state = 45)\n\n\nmodel = xgb.XGBRegressor(random_state = 1, max_depth = 8, learning_rate = 0.01,\n                        n_estimators = 20000,reg_lambda = 1, gamma = 100, subsample = 0.75, colsample_bytree = 1.0)\nmodel.fit(X_train_sub, y_train_sub, eval_set = ([(X_test_sub, y_test_sub)]), early_stopping_rounds = 250)\n\nThe results of the code are truncated to save space. A snapshot of the beginning and end of the results is below. The algorithm keeps adding trees to the model until the RMSE ceases to decrease for 250 consecutive iterations.\n\n\n\n\n\n\nprint(\"XGBoost RMSE = \",np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nXGBoost RMSE =  5508.787454011525\n\n\nLet us further reduce the learning rate to 0.001 and see if the accuracy increases further on the test data. We’ll use the early_stopping_rounds argument to stop growing trees once the accuracy fails to increase for 250 consecutive iterations.\n\nmodel = xgb.XGBRegressor(random_state = 1, max_depth = 8, learning_rate = 0.001,\n                        n_estimators = 20000,reg_lambda = 1, gamma = 100, subsample = 0.75, colsample_bytree = 1.0)\nmodel.fit(X_train_sub, y_train_sub, eval_set = ([(X_test_sub, y_test_sub)]), early_stopping_rounds = 250)\n\n\n\n\n\n\n\nprint(\"XGBoost RMSE = \",np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nXGBoost RMSE =  5483.518711988693\n\n\nNote that the accuracy on this test data has further increased with a lower learning rate.\nLet us combine the XGBoost model with other tuned models from earlier chapters.\n\n#Tuned AdaBoost model from Section 7.2.4\nmodel_ada = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,learning_rate=1.0,\n                         random_state=1).fit(X,y)\nprint(\"AdaBoost RMSE = \", np.sqrt(mean_squared_error(model_ada.predict(Xtest),ytest)))\n\n#Tuned Random forest model from Section 6.1.2\nmodel_rf = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\nprint(\"Random Forest RMSE = \",np.sqrt(mean_squared_error(model_rf.predict(Xtest),ytest)))\n\n#Tuned gradient boosting model from Section 8.2.5\nmodel_gb = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n                         random_state=1,loss='huber').fit(X,y)\nprint(\"Gradient boost RMSE = \",np.sqrt(mean_squared_error(model_gb.predict(Xtest),ytest)))\n\nAdaBoost RMSE =  5693.165811600585\nRandom Forest RMSE =  5642.45839697972\nGradient boost RMSE =  5405.787029062213\n\n\n\n#Ensemble model\npred_xgb = model.predict(Xtest)    #XGBoost\npred_ada = model_ada.predict(Xtest)#AdaBoost\npred_rf = model_rf.predict(Xtest)  #Random Forest\npred_gb = model_gb.predict(Xtest)  #Gradient boost\npred = 0.25*pred_xgb + 0.25*pred_ada + 0.25*pred_rf + 0.25*pred_gb #Option 1 - All models are equally weighted\n#pred = 0.15*pred1+0.15*pred2+0.15*pred3+0.55*pred4 #Option 2 - Higher weight to the better model\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5352.145010078119\n\n\nCombined, the random forest model, gradient boost, XGBoost and the Adaboost model do better than each of the individual models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html#xgboost-for-classification",
    "href": "Lec9_XGBoost.html#xgboost-for-classification",
    "title": "11  XGBoost",
    "section": "11.3 XGBoost for classification",
    "text": "11.3 XGBoost for classification\n\ndata = pd.read_csv('./Datasets/Heart.csv')\ndata.dropna(inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPain\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nThal\nAHD\n\n\n\n\n0\n63\n1\ntypical\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\nfixed\nNo\n\n\n1\n67\n1\nasymptomatic\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\nnormal\nYes\n\n\n2\n67\n1\nasymptomatic\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\nreversable\nYes\n\n\n3\n37\n1\nnonanginal\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\nnormal\nNo\n\n\n4\n41\n0\nnontypical\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\nnormal\nNo\n\n\n\n\n\n\n\n\n#Response variable\ny = pd.get_dummies(data['AHD'])['Yes']\n\n#Creating a dataframe for predictors with dummy varibles replacing the categorical variables\nX = data.drop(columns = ['AHD','ChestPain','Thal'])\nX = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\nX.head()\n\n\n\n\n\n\n\n\nAge\nSex\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nasymptomatic\nnonanginal\nnontypical\ntypical\nfixed\nnormal\nreversable\n\n\n\n\n0\n63\n1\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\n0\n0\n0\n1\n1\n0\n0\n\n\n1\n67\n1\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\n1\n0\n0\n0\n0\n1\n0\n\n\n2\n67\n1\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\n1\n0\n0\n0\n0\n0\n1\n\n\n3\n37\n1\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\n0\n1\n0\n0\n0\n1\n0\n\n\n4\n41\n0\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n#Creating train and test datasets\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)\n\nXGBoost has an additional parameter for classification: scale_pos_weight\nGradients are used as the basis for fitting subsequent trees added to boost or correct errors made by the existing state of the ensemble of decision trees.\nThe scale_pos_weight value is used to scale the gradient for the positive class.\nThis has the effect of scaling errors made by the model during training on the positive class and encourages the model to over-correct them. In turn, this can help the model achieve better performance when making predictions on the positive class. Pushed too far, it may result in the model overfitting the positive class at the cost of worse performance on the negative class or both classes.\nAs such, the scale_pos_weight hyperparameter can be used to train a class-weighted or cost-sensitive version of XGBoost for imbalanced classification.\nA sensible default value to set for the scale_pos_weight hyperparameter is the inverse of the class distribution. For example, for a dataset with a 1 to 100 ratio for examples in the minority to majority classes, the scale_pos_weight can be set to 100. This will give classification errors made by the model on the minority class (positive class) 100 times more impact, and in turn, 100 times more correction than errors made on the majority class.\nReference\n\nstart_time = time.time()\nparam_grid = {'n_estimators':[25,100,500],\n                'max_depth': [6,7,8],\n              'learning_rate': [0.01,0.1,0.2],\n               'gamma': [0.1,0.25,0.5],\n               'reg_lambda':[0,0.01,0.001],\n                'scale_pos_weight':[1.25,1.5,1.75]#Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances).\n             }\n\ncv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = GridSearchCV(estimator=xgb.XGBClassifier(objective = 'binary:logistic',random_state=1,\n                                                         use_label_encoder=False),\n                             param_grid = param_grid,\n                             scoring = 'accuracy',\n                             verbose = 1,\n                             n_jobs=-1,\n                             cv = cv)\noptimal_params.fit(Xtrain,ytrain)\nprint(optimal_params.best_params_,optimal_params.best_score_)\nprint(\"Time taken = \", (time.time()-start_time)/60, \" minutes\")\n\nFitting 5 folds for each of 729 candidates, totalling 3645 fits\n[22:00:02] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n{'gamma': 0.25, 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 25, 'reg_lambda': 0.01, 'scale_pos_weight': 1.5} 0.872183908045977\n\n\n\ncv_results=pd.DataFrame(optimal_params.cv_results_)\ncv_results.sort_values(by = 'mean_test_score',ascending=False)[0:5]\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_gamma\nparam_learning_rate\nparam_max_depth\nparam_n_estimators\nparam_reg_lambda\nparam_scale_pos_weight\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n409\n0.111135\n0.017064\n0.005629\n0.000737\n0.25\n0.2\n6\n25\n0.01\n1.5\n{'gamma': 0.25, 'learning_rate': 0.2, 'max_dep...\n0.866667\n0.766667\n0.9\n0.931034\n0.896552\n0.872184\n0.05656\n1\n\n\n226\n0.215781\n0.007873\n0.005534\n0.001615\n0.1\n0.2\n8\n100\n0\n1.5\n{'gamma': 0.1, 'learning_rate': 0.2, 'max_dept...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n290\n1.391273\n0.107808\n0.007723\n0.006286\n0.25\n0.01\n7\n500\n0\n1.75\n{'gamma': 0.25, 'learning_rate': 0.01, 'max_de...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n266\n1.247463\n0.053597\n0.006830\n0.002728\n0.25\n0.01\n6\n500\n0.01\n1.75\n{'gamma': 0.25, 'learning_rate': 0.01, 'max_de...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n269\n1.394361\n0.087307\n0.005530\n0.001718\n0.25\n0.01\n6\n500\n0.001\n1.75\n{'gamma': 0.25, 'learning_rate': 0.01, 'max_de...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n\n\n\n\n\n\n#Function to compute confusion matrix and prediction accuracy on test/train data\ndef confusion_matrix_data(data,actual_values,model,cutoff=0.5):\n#Predict the values using the Logit model\n    pred_values = model.predict_proba(data)[:,1]\n# Specify the bins\n    bins=np.array([0,cutoff,1])\n#Confusion matrix\n    cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n    cm_df = pd.DataFrame(cm)\n    cm_df.columns = ['Predicted 0','Predicted 1']\n    cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n# Calculate the accuracy\n    accuracy = 100*(cm[0,0]+cm[1,1])/cm.sum()\n    fnr = 100*(cm[1,0])/(cm[1,0]+cm[1,1])\n    precision = 100*(cm[1,1])/(cm[0,1]+cm[1,1])\n    fpr = 100*(cm[0,1])/(cm[0,0]+cm[0,1])\n    tpr = 100*(cm[1,1])/(cm[1,0]+cm[1,1])\n    print(\"Accuracy = \", accuracy)\n    print(\"Precision = \", precision)\n    print(\"FNR = \", fnr)\n    print(\"FPR = \", fpr)\n    print(\"TPR or Recall = \", tpr)\n    print(\"Confusion matrix = \\n\", cm_df)\n    return (\" \")\n\n\nmodel4 = xgb.XGBClassifier(objective = 'binary:logistic',random_state=1,gamma=0.25,learning_rate = 0.01,max_depth=6,\n                              n_estimators = 500,reg_lambda = 0.01,scale_pos_weight=1.75)\nmodel4.fit(Xtrain,ytrain)\nmodel4.score(Xtest,ytest)\n\n0.7718120805369127\n\n\n\n#Computing the accuracy\ny_pred = model4.predict(Xtest)\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\ny_pred_prob = model4.predict_proba(Xtest)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  77.18120805369128\nROC-AUC:  0.8815070986530761\nPrecision:  0.726027397260274\nRecall:  0.7910447761194029\n\n\n\n\n\n\n\n\n\nIf we increase the value of scale_pos_weight, the model will focus on classifying positives more correctly. This will increase the recall (true positive rate) since the focus is on identifying all positives. However, this will lead to identifying positives aggressively, and observations ‘similar’ to observations of the positive class will also be predicted as positive resulting in an increase in false positives and a decrease in precision. See the trend below as we increase the value of scale_pos_weight.\n\n11.3.1 Precision & recall vs scale_pos_weight\n\ndef get_models():\n    models = dict()\n    # explore 'scale_pos_weight' from 0.1 to 2 in 0.1 increments\n    for i in [0,1,10,1e2,1e3,1e4,1e5,1e6,1e7,1e8,1e9]:\n        key = '%.0f' % i\n        models[key] = xgb.XGBClassifier(objective = 'binary:logistic',scale_pos_weight=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores_recall = cross_val_score(model, X, y, scoring='recall', cv=cv, n_jobs=-1)\n    scores_precision = cross_val_score(model, X, y, scoring='precision', cv=cv, n_jobs=-1)\n    return list([scores_recall,scores_precision])\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults_recall, results_precision, names = list(), list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    scores_recall = scores[0]\n    scores_precision = scores[1]\n    # store the results\n    results_recall.append(scores_recall)\n    results_precision.append(scores_precision)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.2f (%.2f)' % (name, np.mean(scores_recall), np.std(scores_recall)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nsns.set(font_scale = 1.5)\npdata = pd.DataFrame(results_precision)\npdata.columns = list(['p1','p2','p3','p4','p5'])\npdata['metric'] = 'precision'\nrdata = pd.DataFrame(results_recall)\nrdata.columns = list(['p1','p2','p3','p4','p5'])\nrdata['metric'] = 'recall'\npr_data = pd.concat([pdata,rdata])\npr_data.reset_index(drop=False,inplace= True)\n#sns.boxplot(x=\"day\", y=\"total_bill\", hue=\"time\",pr_data=tips, linewidth=2.5)\npr_data_melt=pr_data.melt(id_vars = ['index','metric'])\npr_data_melt['index']=pr_data_melt['index']-1\npr_data_melt['index'] = pr_data_melt['index'].astype('str')\npr_data_melt.replace(to_replace='-1',value =  '-inf',inplace=True)\nsns.boxplot(x='index', y=\"value\", hue=\"metric\", data=pr_data_melt, linewidth=2.5)\nplt.xlabel('$log_{10}$(scale_pos_weight)',fontsize=15)\nplt.ylabel('Precision / Recall ',fontsize=15)\nplt.legend(loc=\"lower right\", frameon=True, fontsize=15)\n\n&gt;0 0.00 (0.00)\n&gt;1 0.77 (0.13)\n&gt;10 0.81 (0.09)\n&gt;100 0.85 (0.11)\n&gt;1000 0.85 (0.10)\n&gt;10000 0.90 (0.06)\n&gt;100000 0.90 (0.08)\n&gt;1000000 0.90 (0.06)\n&gt;10000000 0.91 (0.10)\n&gt;100000000 0.96 (0.03)\n&gt;1000000000 1.00 (0.00)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Lec11_More boosting models.html",
    "href": "Lec11_More boosting models.html",
    "title": "12  LightGBM and CatBoost",
    "section": "",
    "text": "12.1 LightGBM\nWe’ll continue to use the same datasets that we have been using throughout the course.\nLightGBM is a gradient boosting decision tree algorithm developed by Microsoft in 2017. LightGBM outperforms XGBoost in terms of compuational speed, and provides comparable accuracy in general. The following two key features in LightGBM that make it faster than XGBoost:\n1. Gradient-based One-Side Sampling (GOSS): Recall, in gradient boosting, we fit trees on the gradient of the loss function (refer the gradient boosting algorithm in section 10.10.2 of the book, Elements of Statistical Learning):\n\\[r_m = -\\bigg[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}  \\bigg]_{f = f_{m-1}}. \\]\nObservations that correspond to relatively larger gradients contribute more to minimizing the loss function as compared to observations with smaller gradients. The algorithm down-samples the observations with small gradients, while selecting all the observations with large gradients. As observations with large gradients contribute the most to the reduction in loss function when considering a split, the accuracy of loss reduction estimate is maintained even with a reduced sample size. This leads to similar performance in terms of prediction accuracy while reducing computation speed due to reduction in sample size to fit trees.\n2. Exclusive feature bundling (EFB): This is useful when there are a lot of predictors, but the predictor space is sparse, i.e., most of the values are zero for several predictors, and the predictors rarely take non-zero values simultaneously. This can typically happen in case of a lot of dummy variables in the data. In such a case, the predictors are bundled to create a single predictor.\nIn the example below you can see that feature1 and feature2 are mutually exclusive. In order to achieve non overlapping buckets we add bundle size of feature1 to feature2. This makes sure that non zero data points of bundled features (feature1 and feature2) reside in different buckets. In feature_bundle buckets 1 to 4 contains non zero instances of feature1 and buckets 5,6 contain non zero instances of feature2 (Reference).\nRead the LightGBM paper for more details.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "Lec11_More boosting models.html#lightgbm",
    "href": "Lec11_More boosting models.html#lightgbm",
    "title": "12  LightGBM and CatBoost",
    "section": "",
    "text": "feature1\nfeature2\nfeature_bundle\n\n\n\n\n0\n2\n6\n\n\n0\n1\n5\n\n\n0\n2\n6\n\n\n1\n0\n1\n\n\n2\n0\n2\n\n\n3\n0\n3\n\n\n4\n0\n4\n\n\n\n\n\n12.1.1 LightGBM for regression\nLet us tune a lightGBM model for regression for our problem of predicting car price. We’ll use the function LGBMRegressor. For classification problems, LGBMClassifier can be used. Note that we are using the GOSS algorithm to downsample observations with smaller gradients.\n\n#K-fold cross validation to find optimal parameters for LightGBM regressor\nstart_time = time.time()\nparam_grid = {'num_leaves': [20, 31, 40],\n              'learning_rate': [0.01, 0.05, 0.1],\n               'reg_lambda':[0, 10, 100],\n                'n_estimators':[100, 500, 1000],\n                'reg_alpha': [0, 10, 100],\n                'subsample': [0.5, 0.75, 1.0],\n                'colsample_bytree': [0.5, 0.75, 1.0]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = RandomizedSearchCV(estimator=LGBMRegressor(boosting_type = 'goss'),                                                       \n                             param_distributions = param_grid, n_iter = 200,\n                             verbose = 1, scoring='neg_root_mean_squared_error',\n                             n_jobs=-1,random_state=1,\n                             cv = cv)\noptimal_params.fit(X,y)\nprint(\"Optimal parameter values =\", optimal_params.best_params_)\nprint(\"Optimal cross validation RMSE = \",optimal_params.best_score_)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\nOptimal parameter values = {'subsample': 1.0, 'reg_lambda': 10, 'reg_alpha': 0, 'num_leaves': 20, 'n_estimators': 1000, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\nOptimal cross validation R-squared =  -5670.309021679375\nTime taken =  1  minutes\n\n\n\n#RMSE based on the optimal parameter values of a LighGBM Regressor model\nnp.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))\n\n5614.374498193448\n\n\nNote that downsampling of small-gradient observations leads to faster execution time, but potentially by compromising some accuracy. We can expect to improve the accuracy by increasing the top_rate or the other_rate hyperparameters, but at an increased computational cost. In the cross-validation below, we have increased the top_rate to 0.5 from the default value of 0.2.\n\n#K-fold cross validation to find optimal parameters for LightGBM regressor\nstart_time = time.time()\nparam_grid = {'num_leaves': [20, 31, 40],\n              'learning_rate': [0.01, 0.05, 0.1],\n               'reg_lambda':[0, 10, 100],\n                'n_estimators':[100, 500, 1000],\n                'reg_alpha': [0, 10, 100],\n                'subsample': [0.5, 0.75, 1.0],\n                'colsample_bytree': [0.5, 0.75, 1.0]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = RandomizedSearchCV(estimator=LGBMRegressor(boosting_type = 'goss', top_rate = 0.5),                                                       \n                             param_distributions = param_grid, n_iter = 200,\n                             verbose = 1, scoring='neg_root_mean_squared_error',\n                             n_jobs=-1,random_state=1,\n                             cv = cv)\noptimal_params.fit(X,y)\nprint(\"Optimal parameter values =\", optimal_params.best_params_)\nprint(\"Optimal cross validation RMSE = \",optimal_params.best_score_)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\nOptimal parameter values = {'subsample': 0.5, 'reg_lambda': 0, 'reg_alpha': 100, 'num_leaves': 31, 'n_estimators': 500, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\nOptimal cross validation R-squared =  -5436.062435616846\nTime taken =  1  minutes\n\n\n\n#RMSE based on the optimal parameter values of a LighGBM Regressor model\nnp.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))\n\n5355.964600884197\n\n\nNote that the cross-validated RMSE has reduced. However, this is at an increased computational expense. In the simulations below, we compare the time taken to train models with increasing values of the top_rate hyperparameter.\n\ntime_list = []\nfor i in range(50):\n    start_time = time.time()\n    model = LGBMRegressor(boosting_type = 'goss', top_rate = 0.2, n_jobs=-1).fit(X, y)\n    time_list.append(time.time()-start_time)\n\n\ntime_list2 = []\nfor i in range(50):\n    start_time = time.time()\n    model = LGBMRegressor(boosting_type = 'goss', top_rate = 0.5, n_jobs=-1).fit(X, y)\n    time_list2.append(time.time()-start_time)\n\n\ntime_list3 = []\nfor i in range(50):\n    start_time = time.time()\n    model = LGBMRegressor(boosting_type = 'goss', top_rate = 0.8, n_jobs=-1).fit(X, y)\n    time_list3.append(time.time()-start_time)\n\n\nax = sns.boxplot([time_list, time_list2, time_list3]);\nax.set_xticklabels([0.2, 0.5, 0.75]);\nplt.ylabel('Time');\nplt.xlabel('top_rate');\nplt.xticks(rotation = 45);\n\n\n\n\n\n\n\n\n\n\n12.1.2 LightGBM vs XGBoost\nLightGBM model took 2 minutes for a random search with 1000 fits as compared to 7 minutes for an XGBoost model with 1000 fits on the same data (as shown below). In terms of prediction accuracy, we observe that the accuracy of LightGBM on test (unseen) data is comparable to that of XGBoost.\n\n#K-fold cross validation to find optimal parameters for XGBoost\nstart_time = time.time()\nparam_grid = {'max_depth': [4,6,8],\n              'learning_rate': [0.01, 0.05, 0.1],\n               'reg_lambda':[0, 1, 10],\n                'n_estimators':[100, 500, 1000],\n                'gamma': [0, 10, 100],\n                'subsample': [0.5, 0.75, 1.0],\n                'colsample_bytree': [0.5, 0.75, 1.0]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = RandomizedSearchCV(estimator=xgb.XGBRegressor(),                                                       \n                             param_distributions = param_grid, n_iter = 200,\n                             verbose = 1, scoring = 'neg_root_mean_squared_error',\n                             n_jobs=-1,random_state = 1,\n                             cv = cv)\noptimal_params.fit(X,y)\nprint(\"Optimal parameter values =\", optimal_params.best_params_)\nprint(\"Optimal cross validation R-squared = \",optimal_params.best_score_)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\nOptimal parameter values = {'subsample': 0.75, 'reg_lambda': 1, 'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.01, 'gamma': 100, 'colsample_bytree': 1.0}\nOptimal cross validation R-squared =  -5178.8689594137295\nTime taken =  7  minutes\n\n\n\n#RMSE based on the optimal parameter values\nnp.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))\n\n5420.661056398766",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "Lec11_More boosting models.html#catboost",
    "href": "Lec11_More boosting models.html#catboost",
    "title": "12  LightGBM and CatBoost",
    "section": "12.2 CatBoost",
    "text": "12.2 CatBoost\nCatBoost is a gradient boosting algorithm developed by Yandex (Russian Google) in 2017. Like LightGBM, CatBoost is also faster than XGBoost in training. However, unlike LightGBM, the authors have claimed that it outperforms both LightGBM and XGBoost in terms of prediction accuracy as well.\nThe key feature of CatBoost that address the issue with the gradient boosting procedure is the idea of ordered boosting. Classic boosting algorithms are prone to overfitting on small/noisy datasets due to a problem known as prediction shift. Recall, in gradient boosting, we fit trees on the gradient of the loss function (refer the gradient boosting algorithm in section 10.10.2 of the book, Elements of Statistical Learning):\n\\[r_m = -\\bigg[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}  \\bigg]_{f = f_{m-1}}. \\]\nWhen calculating the gradient estimate of an observation, these algorithms use the same observations that the model was built with, thus having no chances of experiencing unseen data. CatBoost, on the other hand, uses the concept of ordered boosting, a permutation-driven approach to train model on a subset of data while calculating residuals on another subset, thus preventing “target leakage” and overfitting. The residuals of an observation are computed based on a model developed on the previous observations, where the observations are randomly shuffled at each iteration, i.e., for each tree.\nThus, the gradient of the loss function is based on test (unseen) data, instead of the data on which the model has been trained, which improves the generalizability of the model, and avoids overfitting on train data.\nThe authors have also shown that CatBoost performs better than XGBoost and LightGBM without tuning, i.e., with default hyperparameter settings.\nRead the CatBoost paper for more details.\nHere is a good blog listing the key features of CatBoost.\n\n12.2.1 CatBoost for regression\nWe’ll use the function CatBoostRegressor for regression. For classification problems CatBoostClassifier can be used.\nLet us check the performance of CatBoostRegressor() without tuning, i.e., with default hyperparameter settings.\n\nmodel_cat = CatBoostRegressor(verbose=0).fit(X, y)\n\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\nnp.mean(-cross_val_score(CatBoostRegressor(verbose=0), X, y, cv = cv, n_jobs = -1, \n                scoring='neg_root_mean_squared_error'))\n\n5035.972129299527\n\n\n\nnp.sqrt(mean_squared_error(model_cat.predict(Xtest),ytest))\n\n5288.82153844634\n\n\nEven with default hyperparameter settings, CatBoost has outperformed both XGBoost and LightGBM in terms of cross-validated RMSE, and RMSE on test data for our example of predicting car prices.\n\n\n12.2.2 Target encoding with CatBoost\nTarget encoding for categorical variables can be used with CatBoost, that may benefit in terms of both speed and accuracy. However, the benefit is not gauranteed. Let us use target encoding for the categorical predictors brand, model, transmission and fuelType.\n\nX = train[['mileage','mpg','year','engineSize', 'brand', 'model', 'transmission', 'fuelType']]\nXtest = test[['mileage','mpg','year','engineSize', 'brand', 'model',  'transmission', 'fuelType']]\ny = train['price']\nytest = test['price']\n\nThe parameter cat_features will be used to specify the indices of the categorical predictors for target encoding.\n\nmodel = CatBoostRegressor(verbose = False, cat_features = range(4, 8)).fit(X, y)\nmean_squared_error(model.predict(Xtest), ytest, squared = False)\n\n3263.1348853593345\n\n\nLet us compare the resuts with one-hot encoding of the categorical predictors.\n\nX = train[['mileage','mpg','year','engineSize', 'brand', 'model', 'transmission', 'fuelType']]\nXtest = test[['mileage','mpg','year','engineSize', 'brand', 'model',  'transmission', 'fuelType']]\nX = pd.get_dummies(X)\nXtest = pd.get_dummies(Xtest)\n\nIn one-hot encoding, we need to make sure that both the datasets have the same predictors. Let us find the predictors in train data that are not in test data. Note that this is not necessary in target encoding.\n\nnp.setdiff1d(X.columns, Xtest.columns)\n\narray(['model_ M6'], dtype=object)\n\n\n\nX.drop(columns = 'model_ M6', inplace = True)\ny = train['price']\nytest = test['price']\n\n\nmodel = CatBoostRegressor(verbose = False).fit(X, y)\nmean_squared_error(model.predict(Xtest), ytest, squared = False)\n\n3219.857899121199\n\n\nIn this case, target encoding has a slightly higher RMSE as compared to one-hot encoding. However, it may do better than one-hot-encoding in a different problem.\nLet us use both target encoding and one-hot encoding together to see if it helps do better than each of the them individually.\n\nX = pd.concat([train[['brand', 'model', 'transmission', 'fuelType']], X], axis = 1)\nXtest = pd.concat([test[['brand', 'model', 'transmission', 'fuelType']], Xtest], axis = 1)\n\n\nmodel = CatBoostRegressor(verbose = False, cat_features = range(4)).fit(X, y)\nmean_squared_error(model.predict(Xtest), ytest, squared = False)\n\n3172.449374536484\n\n\nIn this case, using target-encoding and one-hot-encoding together does better on test data. Using both the encodings together will help reduce bias while increasing variance. The benefit of using both the encodings together depends on the bias-variance tradeoff.\n\n\n12.2.3 CatBoost vs XGBoost\nLet us see the performance of XGBoost with default hyperparameter settings.\n\nmodel_xgb = xgb.XGBRFRegressor().fit(X, y)\nnp.mean(-cross_val_score(xgb.XGBRFRegressor(), X, y, cv = cv, n_jobs = -1, \n                scoring='neg_root_mean_squared_error'))\n\n6273.043859096154\n\n\n\nnp.sqrt(mean_squared_error(model_xgb.predict(Xtest),ytest))\n\n6821.745153860935\n\n\nXGBoost performance deteriorates showing that hyperparameter tuning is more important in XGBoost.\nLet us see the performance of LightGBM with default hyperparameter settings.\n\nmodel_lgbm = LGBMRegressor().fit(X, y)\nnp.mean(-cross_val_score(LGBMRegressor(), X, y, cv = cv, n_jobs = -1, \n                scoring='neg_root_mean_squared_error'))\n\n5562.149251902867\n\n\n\nnp.sqrt(mean_squared_error(model_lgbm.predict(Xtest),ytest))\n\n5494.0777923513515\n\n\nLightGBM’s default hyperparameter settings also seem to be more robust as compared to those of XGBoost.\n\n\n12.2.4 Tuning CatBoostRegressor\nThe CatBoost hyperparameters can be tuned just like the XGBoost hyperparameters. However, there is some difference in the hyperparameters of both the packages. For example, reg_alpha (the L1 penalization on weights of leaves) and colsample_bytree (subsample ratio of columns when constructing each tree) hyperparameters are not there in CatBoost.\n\n#K-fold cross validation to find optimal parameters for CatBoost regressor\nstart_time = time.time()\nparam_grid = {'max_depth': [4,6,8, 10],\n              'num_leaves': [20, 31, 40, 60],\n              'learning_rate': [0.01, 0.05, 0.1],\n               'reg_lambda':[0, 10, 100],\n                'n_estimators':[500, 1000, 1500],\n                'subsample': [0.5, 0.75, 1.0],\n             'colsample_bylevel': [0.25, 0.5, 0.75, 1.0]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = RandomizedSearchCV(estimator=CatBoostRegressor(random_state=1, verbose=False, \n                            grow_policy='Lossguide'),                                                       \n                             param_distributions = param_grid, n_iter = 200,\n                             verbose = 1,random_state = 1, scoring='neg_root_mean_squared_error',\n                             n_jobs=-1,\n                             cv = cv)\noptimal_params.fit(X,y)\nprint(\"Optimal parameter values =\", optimal_params.best_params_)\nprint(\"Optimal cross validation RMSE = \",optimal_params.best_score_)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\nOptimal parameter values = {'subsample': 0.5, 'reg_lambda': 0, 'num_leaves': 40, 'n_estimators': 500, 'max_depth': 10, 'learning_rate': 0.05, 'colsample_bylevel': 0.75}\nOptimal cross validation RMSE =  -4993.129407810791\nTime taken =  23  minutes\n\n\n\n#RMSE based on the optimal parameter values\nnp.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))\n\n5249.434282204398\n\n\nIt takes 2 minutes to tune CatBoost, which is higher than LightGBM and lesser than XGBoost. CatBoost falls in between LightGBM and XGBoost in terms of speed. However, it is likely to be more accurate than XGBoost and LighGBM, and likely to require lesser tuning as compared to XGBoost.\n\nmodel = CatBoostRegressor(grow_policy='Lossguide') \n\ngrid = {'num_leaves': Integer(4, 64),\n              'learning_rate': Real(0.0001, 1.0),\n               'reg_lambda':Real(0, 1e4),\n                'n_estimators':Integer(2, 2000),\n                'subsample': Real(0.1,1.0),\n                'colsample_bylevel': Real(0.1, 1.0)}\n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 200, random_state = 1,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\ngcv.fit(X, y, callback = monitor)\n\n\n\n\n\n\n\n# Optimal values obtained\n#['colsample_bylevel', 'learning_rate', 'n_estimators', 'num_leaves', 'reg_lambda', 'subsample'] = \n#[0.3745508446405472, 0.1000958551500621, 2000, 11, 0.0, 0.3877212027881348] 5132.537839676808\n\n\n\n12.2.5 Tuning Tips\nCheck the documentation for some tuning tips.\n\nIt is not recommended to use values greater than 64 for num_leaves, since it can significantly slow down the training process.\nIn most cases, the optimal depth ranges from 4 to 10. Values in the range from 6 to 10 are recommended. The maximum possible value of max_depth is 16.\nDo not use one-hot encoding during preprocessing. This affects both the training speed and the resulting quality.\nSymmetric trees have a very good prediction speed (roughly 10 times faster than non-symmetric trees) and give better quality in many cases.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "Lec10_Ensemble.html",
    "href": "Lec10_Ensemble.html",
    "title": "13  Ensemble modeling",
    "section": "",
    "text": "13.1 Ensembling regression models\nEnsembling models can help reduce error by leveraging the diversity and collective wisdom of multiple models. When ensembling, several individual models are trained independently and their predictions are combined to make the final prediction.\nWe have already seen examples of ensemble models in chapters 5 - 13. The ensembled models may reduce error by reducing the bias (boosting) and / or reducing the variance (bagging / random forests / boosting).\nHowever, in this chapter we’ll ensemble different types of models, instead of the same type of model. We may ensemble a linear regression model, a random forest, a gradient boosting model, and as many different types of models as we wish.\nBelow are a couple of reasons why ensembling models can be effective in reducing error:\nMathematically also, we can show the effectiveness of an ensemble model. Let’s consider the case of regression, and let the predictors be denoted as \\(X\\), and the response as \\(Y\\). Let \\(f_1, ..., f_m\\) be individual models. The expected MSE of an ensemble can be written as:\n\\[ E(MSE_{Ensemble}) = E\\bigg[\\bigg( \\frac{1}{m} \\sum_{i = 1}^{m} f_i(X) - Y \\bigg)^2 \\bigg] = \\frac{1}{m^2} \\sum_{i = 1}^{m} E \\bigg[\\big(f_i(X) - Y\\big)^2 \\bigg] + \\frac{1}{m^2} \\sum_{i \\ne j} E\\bigg[\\big(f_i(X) - Y\\big)\\big(f_j(X) - Y\\big) \\bigg]\\]\n\\[ \\implies E(MSE_{Ensemble}) = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m E \\bigg[\\big(f_i(X) - Y\\big)^2 \\bigg]\\bigg) + \\frac{1}{m^2} \\sum_{i \\ne j} E\\bigg[\\big(f_i(X) - Y\\big)\\big(f_j(X) - Y\\big) \\bigg]\\]\n\\[ \\implies E(MSE_{Ensemble}) = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i})\\bigg) + \\frac{1}{m^2} \\sum_{i \\ne j} E\\bigg[\\big(f_i(X) - Y\\big)\\big(f_j(X) - Y\\big) \\bigg]\\]\nIf \\(f_1, ..., f_m\\) are unbiased, then,\n\\[ E(MSE_{Ensemble}) = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i})\\bigg) + \\frac{1}{m^2} \\sum_{i \\ne j} Cov(f_i(X), f_j(X))\\]\nAssuming the models are uncorrelated (i.e., they have a zero correlation), the second term (covariance of \\(f_i(.)\\) and \\(f_j(.)\\)) reduces to zero, and the expected MSE of the ensemble reduces to:\n\\[\nE(MSE_{Ensemble}) = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i})\\bigg)\n\\tag{13.1}\\]\nThus, the expected MSE of an ensemble model with uncorrelated models is much smaller than the average MSE of all the models. Unless there is a model that is much better than the rest of the models, the MSE of the ensemble model is likely to be lower than the MSE of the individual models. However, there is no guarantee that the MSE of the ensemble model will be lower than the MSE of the individual models. Consider an extreme case where only one of the models have a zero MSE. The MSE of this model will be lower than the expected MSE of the ensemble model.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ensemble modeling</span>"
    ]
  },
  {
    "objectID": "Lec10_Ensemble.html#ensembling-regression-models",
    "href": "Lec10_Ensemble.html#ensembling-regression-models",
    "title": "13  Ensemble modeling",
    "section": "",
    "text": "13.1.1 Voting Regressor\nHere, we will combine the predictions of different models. The function VotingRegressor() averages the predictions of all the models.\nBelow are the individual models tuned in the previous chapters.\n\n#Tuned AdaBoost model from Section 7.2.4\nmodel_ada = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,\n                    learning_rate=1.0,  random_state=1).fit(X, y)\nprint(\"RMSE for AdaBoost = \", np.sqrt(mean_squared_error(model_ada.predict(Xtest), ytest)))\n\n#Tuned Random forest model from Section 6.1.2\nmodel_rf = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\nprint(\"RMSE for Random forest = \", np.sqrt(mean_squared_error(model_rf.predict(Xtest), ytest)))\n\n# Tuned XGBoost model from Section 9.2.6\nmodel_xgb = xgb.XGBRegressor(random_state=1,max_depth=8,n_estimators=1000, subsample = 0.75, colsample_bytree = 1.0,\n                                         learning_rate = 0.01,reg_lambda=1, gamma = 100).fit(X, y)\nprint(\"RMSE for XGBoost = \", np.sqrt(mean_squared_error(model_xgb.predict(Xtest), ytest)))\n\n#Tuned gradient boosting model from Section 8.2.5\nmodel_gb = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n                         random_state=1,loss='huber').fit(X, y)\nprint(\"RMSE for Gradient Boosting = \", np.sqrt(mean_squared_error(model_gb.predict(Xtest), ytest)))\n\n# Tuned Light GBM model from Section 13.1.1\nmodel_lgbm = LGBMRegressor(subsample = 0.5, reg_lambda = 0, reg_alpha = 100, boosting_type = 'goss',\n            num_leaves = 31, n_estimators = 500, learning_rate = 0.05, colsample_bytree = 1.0,\n                          top_rate = 0.5).fit(X, y)\nprint(\"RMSE for LightGBM = \", np.sqrt(mean_squared_error(model_lgbm.predict(Xtest), ytest)))\n\n# Tuned CatBoost model from Section 13.2.3\nmodel_cat = CatBoostRegressor(subsample=0.5, num_leaves=40, n_estimators=500, max_depth=10, \n                              verbose = False, learning_rate = 0.05, colsample_bylevel=0.75, \n                              grow_policy='Lossguide', random_state = 1).fit(X, y)\nprint(\"RMSE for CatBoost = \", np.sqrt(mean_squared_error(model_cat.predict(Xtest), ytest)))\n\nRMSE for AdaBoost =  5693.165811600585\nRMSE for Random forest =  5642.45839697972\nRMSE for XGBoost =  5497.553788113875\nRMSE for Gradient Boosting =  5405.787029062213\nRMSE for LightGBM =  5355.964600884197\nRMSE for CatBoost =  5271.104736146779\n\n\nNote that we don’t need to fit the models individually before fitting them simultaneously in the voting ensemble. If we fit them individual, it will unnecessarily waste time.\nLet us ensemble the models using the voting ensemble with equal weights.\n\n#Voting ensemble: Averaging the predictions of all models\n\n#Tuned AdaBoost model from Section 7.2.4\nmodel_ada = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=10),\n                    n_estimators=50,learning_rate=1.0,  random_state=1)\n\n#Tuned Random forest model from Section 6.1.2\nmodel_rf = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2)\n\n# Tuned XGBoost model from Section 9.2.6\nmodel_xgb = xgb.XGBRegressor(random_state=1,max_depth=8,n_estimators=1000, subsample = 0.75, \n                colsample_bytree = 1.0, learning_rate = 0.01,reg_lambda=1, gamma = 100)\n\n#Tuned gradient boosting model from Section 8.2.5\nmodel_gb = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n                         random_state=1,loss='huber')\n\n# Tuned CatBoost model from Section 13.2.3\nmodel_cat = CatBoostRegressor(subsample=0.5, num_leaves=40, n_estimators=500, max_depth=10,\n                             learning_rate = 0.05, colsample_bylevel=0.75, grow_policy='Lossguide',\n                             random_state=1, verbose = False)\n\n# Tuned Light GBM model from Section 13.1.1\nmodel_lgbm = LGBMRegressor(subsample = 0.5, reg_lambda = 0, reg_alpha = 100, boosting_type = 'goss',\n                           num_leaves = 31, n_estimators = 500, learning_rate = 0.05, \n                           colsample_bytree = 1.0, top_rate = 0.5)\n\nstart_time = time.time()\nen = VotingRegressor(estimators = [('xgb',model_xgb),('ada',model_ada),('rf',model_rf),\n                    ('gb',model_gb), ('cat', model_cat), ('lgbm', model_lgbm)], n_jobs = -1)\nen.fit(X,y)\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nEnsemble model RMSE =  5259.899392611916\nTime taken =  0.21 minutes\n\n\nAs expected, RMSE of the ensembled model is less than that of each of the individual models.\nNote that the RMSE can be further improved by removing the weaker models from the ensemble. Let us remove the three weakest models - XGBoost, Random forest, and AdaBoost.\n\n#Voting ensemble: Averaging the predictions of all models\n\nstart_time = time.time()\nen = VotingRegressor(estimators = [('gb',model_gb), ('cat', model_cat), ('lgbm', model_lgbm)], n_jobs = -1)\nen.fit(X,y)\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nEnsemble model RMSE =  5191.814866810768\nTime taken =  0.18 minutes\n\n\n\n\n13.1.2 Stacking Regressor\nStacking is a more sophisticated method of ensembling models. The method is as follows:\n\nThe training data is split into K folds. Each of the K folds serves as a test data in one of the K iterations, and the rest of the folds serve as train data.\nEach model is used to make predictions on each of the K folds, after being trained on the remaining K-1 folds. In this manner, each model predicts the response on each train data point - when that train data point was not used to train the model.\nPredictions at each training data points are generated by each model in step 2 (the above step). These predictions are now used as predictors to train a meta-model (referred by the argument final_estimator), with the original response as the response. The meta-model (or final_estimator) learns to combine predictions of different models to make a better prediction.\n\n\n13.1.2.1 Metamodel: Linear regression\n\n#Stacking using LinearRegression as the metamodel\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n                                     ('gb', model_gb), ('cat', model_cat), ('lgbm', model_lgbm)],\n                     final_estimator=LinearRegression(),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nstart_time = time.time()\nen.fit(X,y)\nprint(\"Linear regression metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nLinear regression metamodel RMSE =  5220.456280327686\nTime taken =  2.03 minutes\n\n\n\n#Co-efficients of the meta-model\nen.final_estimator_.coef_\n\narray([ 0.05502964,  0.14566665,  0.01093624,  0.30478283,  0.57403909,\n       -0.07057344])\n\n\n\nsum(en.final_estimator_.coef_)\n\n1.0198810182715363\n\n\nNote the above coefficients of the meta-model. The model gives the highest weight to the gradient boosting model (with huber loss), and the catboost model, and the lowest weight to the relatively weak random forest model.\nAlso, note that the coefficients need not sum to one.\nLet us try improving the RMSE further by removing the weaker models from the ensemble. Let us remove the three weakest models based on the size of their coefficients in the linear regression metamodel.\n\n#Stacking using LinearRegression as the metamodel\nen = StackingRegressor(estimators = [('gb', model_gb), ('cat', model_cat), ('ada', model_ada)],\n                     final_estimator=LinearRegression(),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nstart_time = time.time()\nen.fit(X,y)\nprint(\"Linear regression metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nLinear regression metamodel RMSE =  5205.225710180056\nTime taken =  1.36 minutes\n\n\nThe metamodel accuracy improves further, when strong models are ensembled.\n\n#Co-efficients of the meta-model\nen.final_estimator_.coef_\n\narray([0.31824119, 0.54231032, 0.15998634])\n\n\n\nsum(en.final_estimator_.coef_)\n\n1.020537847948332\n\n\n\n\n13.1.2.2 Metamodel: Lasso\n\n#Stacking using Lasso as the metamodel\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n                        ('gb', model_gb),('cat', model_cat), ('lgbm', model_lgbm) ],\n                     final_estimator = LassoCV(),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nstart_time = time.time()\nen.fit(X,y)\nprint(\"Lasso metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nLasso metamodel RMSE =  5206.021083501416\nTime taken =  2.05 minutes\n\n\n\n#Coefficients of the lasso metamodel\nen.final_estimator_.coef_\n\narray([ 0.03524446,  0.15077605,  0.        ,  0.30392268,  0.52946243,\n       -0.        ])\n\n\nNote that lasso reduces the weight of the weak random forest model, and light gbm model to 0. Even though light GBM is a strong model, it may be correlated or collinear with XGBoost, or other models, and hence is not needed.\nNote that as lasso performs model selection on its own, removing models with zero coefficients or weights does not make a difference, as shown below.\n\n#Stacking using Lasso as the metamodel\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),\n                        ('gb', model_gb),('cat', model_cat) ],\n                     final_estimator = LassoCV(),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nstart_time = time.time()\nen.fit(X,y)\nprint(\"Lasso metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nLasso metamodel RMSE =  5205.93233977352\nTime taken =  1.79 minutes\n\n\n\n#Coefficients of the lasso metamodel\nen.final_estimator_.coef_\n\narray([0.03415944, 0.15053122, 0.30464838, 0.53006297])\n\n\n\n\n13.1.2.3 Metamodel: Random forest\nA highly flexible model such as a random forest may not be a good choice for ensembling correlated models. However, let us tune the random forest meta model, and check its accuracy.\n\n# Tuning hyperparameter of the random forest meta-model\nstart_time = time.time()\noob_score_i = []\nfor i in range(1, 7):\n    en = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n                        ('gb', model_gb),('cat', model_cat), ('lgbm', model_lgbm)],\n                     final_estimator = RandomForestRegressor(max_features = i, oob_score = True),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1)).fit(X,y)\n    oob_score_i.append(en.final_estimator_.oob_score_)\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nTime taken =  12.08 minutes\n\n\n\nprint(\"Optimal value of max_features =\", np.array(oob_score_i).argmax() + 1)\n\nOptimal value of max_features = 1\n\n\n\n# Training the tuned random forest metamodel\nstart_time = time.time()\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),\n                    ('rf', model_rf), ('gb', model_gb),('cat', model_cat), \n                    ('lgbm', model_lgbm)],\n                final_estimator = RandomForestRegressor(max_features = 1, \n                n_estimators=500), cv = KFold(n_splits = 5, shuffle = True, \n                random_state=1)).fit(X,y)\nprint(\"Random Forest metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nRandom Forest metamodel RMSE =  5441.9155087961\nTime taken =  1.71 minutes\n\n\nNote that highly flexible models may not be needed when the predictors are highly correlated with the response. However, in some cases, they may be useful, as in the classification example in the next section.\n\n\n13.1.2.4 Metamodel: CatBoost\n\n#Stacking using MARS as the meta-model\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n                        ('gb', model_gb),('cat', model_cat), ('lgbm', model_lgbm)],\n                     final_estimator = CatBoostRegressor(verbose = False),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nstart_time = time.time()\nen.fit(X,y)\nprint(\"Random Forest metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nRandom Forest metamodel RMSE =  5828.803609683251\nTime taken =  1.66 minutes",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ensemble modeling</span>"
    ]
  },
  {
    "objectID": "Lec10_Ensemble.html#ensembling-classification-models",
    "href": "Lec10_Ensemble.html#ensembling-classification-models",
    "title": "13  Ensemble modeling",
    "section": "13.2 Ensembling classification models",
    "text": "13.2 Ensembling classification models\nWe’ll ensemble models for predicting accuracy of identifying people having a heart disease.\n\ndata = pd.read_csv('./Datasets/Heart.csv')\ndata.dropna(inplace = True)\n#Response variable\ny = pd.get_dummies(data['AHD'])['Yes']\n\n#Creating a dataframe for predictors with dummy variables replacing the categorical variables\nX = data.drop(columns = ['AHD','ChestPain','Thal'])\nX = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\n\n#Creating train and test datasets\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)\n\nLet us tune the individual models first.\n\nAdaBoost\n\n# Tuning Adaboost for maximizing accuracy\nmodel = AdaBoostClassifier(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['base_estimator'] = [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), \n                          DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=4)]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',refit='accuracy')\n# execute the grid search\ngrid_result = grid_search.fit(Xtrain, ytrain)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\nBest: 0.871494 using {'base_estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 0.01, 'n_estimators': 200}\n\n\n\n\nGradient Boosting\n\n# Tuning gradient boosting for maximizing accuracy\nmodel = GradientBoostingClassifier(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['max_depth'] = [1,2,3,4,5]\ngrid['subsample'] = [0.5,1.0]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',refit='accuracy')\n# execute the grid search\ngrid_result = grid_search.fit(Xtrain, ytrain)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\nBest: 0.871954 using {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 100, 'subsample': 1.0}\n\n\n\n\nXGBoost\n\n# Tuning XGBoost for maximizing accuracy\nstart_time = time.time()\nparam_grid = {'n_estimators':[25, 100,250,500],\n                'max_depth': [4, 6 ,8],\n              'learning_rate': [0.01,0.1,0.2],\n               'gamma': [0, 1, 10, 100],\n               'reg_lambda':[0, 10, 100],\n               'subsample': [0.5, 0.75, 1.0]\n                'scale_pos_weight':[1.25,1.5,1.75]#Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances).\n             }\n\ncv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = GridSearchCV(estimator=xgb.XGBClassifier(random_state=1),\n                             param_grid = param_grid,\n                             scoring = 'accuracy',\n                             verbose = 1,\n                             n_jobs=-1,\n                             cv = cv)\noptimal_params.fit(Xtrain,ytrain)\nprint(optimal_params.best_params_,optimal_params.best_score_)\nprint(\"Time taken = \", (time.time()-start_time)/60, \" minutes\")\n\nFitting 5 folds for each of 972 candidates, totalling 4860 fits\n{'gamma': 0, 'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 25, 'reg_lambda': 0, 'scale_pos_weight': 1.25} 0.872183908045977\nTime taken =  0.9524135629336039  minutes\n\n\n\n#Tuned Adaboost model\nmodel_ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=200, \n                               random_state=1,learning_rate=0.01).fit(Xtrain, ytrain)    \ntest_accuracy_ada = model_ada.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n    \n#Tuned Random forest model from Section 6.3\nmodel_rf = RandomForestClassifier(n_estimators=500, random_state=1,max_features=3,\n                        n_jobs=-1,oob_score=False).fit(Xtrain, ytrain)\ntest_accuracy_rf = model_rf.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n    \n#Tuned gradient boosting model\nmodel_gb = GradientBoostingClassifier(n_estimators=100, random_state=1,max_depth=4,learning_rate=1.0,\n                                     subsample = 1.0).fit(Xtrain, ytrain)\ntest_accuracy_gb = model_gb.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n\n#Tuned XGBoost model\nmodel_xgb = xgb.XGBClassifier(random_state=1,gamma=0,learning_rate = 0.2,max_depth=4,\n                              n_estimators = 25,reg_lambda = 0,scale_pos_weight=1.25).fit(Xtrain,ytrain)\ntest_accuracy_xgb = model_xgb.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n\nprint(\"Adaboost accuracy = \",test_accuracy_ada)\nprint(\"Random forest accuracy = \",test_accuracy_rf)\nprint(\"Gradient boost accuracy = \",test_accuracy_gb)\nprint(\"XGBoost model accuracy = \",test_accuracy_xgb)\n\nAdaboost accuracy =  0.7986577181208053\nRandom forest accuracy =  0.8120805369127517\nGradient boost accuracy =  0.7986577181208053\nXGBoost model accuracy =  0.7785234899328859\n\n\n\n\n13.2.1 Voting classifier - hard voting\nIn this type of ensembling, the predicted class is the one predicted by the majority of the classifiers.\n\nensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)])\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.825503355704698\n\n\nNote that the prediction accuracy of the ensemble is higher than the prediction accuracy of each of the individual models on unseen data.\n\n\n13.2.2 Voting classifier - soft voting\nIn this type of ensembling, the predicted class is the one based on the average predicted probabilities of all the classifiers. The threshold probability is 0.5.\n\nensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                 voting='soft')\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.7919463087248322\n\n\nNote that soft voting will be good only for well calibrated classifiers, i.e., all the classifiers must have probabilities at the same scale.\n\n\n13.2.3 Stacking classifier\nConceptually, the idea is similar to that of Stacking regressor.\n\n#Using Logistic regression as the meta model (final_estimator)\nensemble_model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                   final_estimator=LogisticRegression(random_state=1,max_iter=10000),n_jobs=-1,\n                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1))\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.7986577181208053\n\n\n\n#Coefficients of the logistic regression metamodel\nensemble_model.final_estimator_.coef_\n\narray([[0.81748051, 1.28663164, 1.64593342, 1.50947087]])\n\n\n\n#Using random forests as the meta model (final_estimator). Note that random forest will require tuning\nensemble_model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                   final_estimator=RandomForestClassifier(n_estimators=500, max_features=1,\n                                                                          random_state=1,oob_score=True),n_jobs=-1,\n                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1))\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.8322147651006712\n\n\nNote that a complex final_estimator such as random forest will require tuning. In the above case, the max_features argument of random forests has been tuned to obtain the maximum OOB score. The tuning is shown below.\n\n#Tuning the random forest parameters\nstart_time = time.time()\noob_score = {}\n\ni=0\nfor pr in range(1,5):\n    model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                   final_estimator=RandomForestClassifier(n_estimators=500, max_features=pr,\n                                    random_state=1,oob_score=True),n_jobs=-1,\n                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)).fit(Xtrain, ytrain)\n    oob_score[pr] = model.final_estimator_.oob_score_\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"max accuracy = \", np.max(list(oob_score.values())))\nprint(\"Best value of max_features= \", np.argmax(list(oob_score.values()))+1)\n\ntime taken =  0.33713538646698  minutes\nmax accuracy =  0.8445945945945946\nBest value of max_features=  1\n\n\n\n#The final predictor (metamodel) - random forest obtains the maximum oob_score for max_features = 1\noob_score\n\n{1: 0.8445945945945946,\n 2: 0.831081081081081,\n 3: 0.8378378378378378,\n 4: 0.831081081081081}\n\n\n\n\n13.2.4 Tuning all models simultaneously\nIndividual model hyperparameters can be tuned simultaneously while ensembling them with a VotingClassifier(). However, this approach can be too expensive for even moderately-sized datasets.\n\n# Create the param grid with the names of the models as prefixes\n\nmodel_ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier())\nmodel_rf = RandomForestClassifier()\nmodel_gb = GradientBoostingClassifier()\nmodel_xgb = xgb.XGBClassifier()\n\nensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)])\n\nhp_grid = dict()\n\n# XGBoost\nhp_grid['xgb__n_estimators'] = [25, 100,250,50]\nhp_grid['xgb__max_depth'] = [4, 6 ,8]\nhp_grid['xgb__learning_rate'] = [0.01, 0.1, 1.0]\nhp_grid['xgb__gamma'] = [0, 1, 10, 100]\nhp_grid['xgb__reg_lambda'] = [0, 1, 10, 100]\nhp_grid['xgb__subsample'] = [0, 1, 10, 100]\nhp_grid['xgb__scale_pos_weight'] = [1.0, 1.25, 1.5]\nhp_grid['xgb__colsample_bytree'] = [0.5, 0.75, 1.0]\n\n# AdaBoost\nhp_grid['ada__n_estimators'] = [10, 50, 100,200,500]\nhp_grid['ada__base_estimator__max_depth'] = [1, 3, 5]\nhp_grid['ada__learning_rate'] = [0.01, 0.1, 0.2]\n\n# Random Forest\nhp_grid['rf__n_estimators'] = [100]\nhp_grid['rf__max_features'] = [3, 6, 9, 12, 15]\n\n# GradBoost\nhp_grid['gb__n_estimators'] = [10, 50, 100,200,500]\nhp_grid['gb__max_depth'] = [1, 3, 5]\nhp_grid['gb__learning_rate'] = [0.01, 0.1, 0.2, 1.0]\nhp_grid['gb__subsample'] = [0.5, 0.75, 1.0]\n\nstart_time = time.time()\ngrid = RandomizedSearchCV(ensemble_model, hp_grid, cv=5, scoring='accuracy', verbose = True,\n                         n_iter = 100, n_jobs=-1).fit(Xtrain, ytrain)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\n\ngrid.best_estimator_.score(Xtest, ytest)\n\n0.8120805369127517",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ensemble modeling</span>"
    ]
  },
  {
    "objectID": "Lec10_Ensemble.html#ensembling-models-based-on-different-sets-of-predictors",
    "href": "Lec10_Ensemble.html#ensembling-models-based-on-different-sets-of-predictors",
    "title": "13  Ensemble modeling",
    "section": "13.3 Ensembling models based on different sets of predictors",
    "text": "13.3 Ensembling models based on different sets of predictors\nGenerally, tree-based models such as CatBoost, and XGBoost are the most accurate, while other models, such as bagging, random forests, KNN, and linear models, may not be as accurate. Thus, sometimes, the weaker models, despite bringing-in diversity in the model ensemble may deteriorate the ensemble accuracy due to their poor individual performance (check slides for technical details). Thus, sometimes, another approach to bring-in model diversity is to develop strong models based on different sets of predictors, and ensemble them.\nDifferent feature selection methods (such as Lasso, feature importance returned by tree-based methods, stepwise k-fold feature selection, etc.), may be used to obtain different sets of important features, strong models can be tuned on these sets, and then ensembled. Even though the models may be of the same type, the different sets of predictors will help bring-in the element of diversity in the ensemble.\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\n\n\n\n\n\n\n\n\nX = train[['mileage','mpg','year','engineSize']]\nXtest = test[['mileage','mpg','year','engineSize']]\ny = train['price']\nytest = test['price']\n\nWe will create polynomial interactions to develop two sets of predictors - first order predictors, and second order predictors.\n\npoly_set = PolynomialFeatures(2, include_bias = False)\nX_poly = poly_set.fit_transform(X)\nX_poly = pd.DataFrame(X_poly, columns=poly_set.get_feature_names_out())\nX_poly.columns = X_poly.columns.str.replace(\"^\", \"_\", regex=True)\nXtest_poly = poly_set.fit_transform(Xtest)\nXtest_poly = pd.DataFrame(Xtest_poly, columns=poly_set.get_feature_names_out())\nXtest_poly.columns = Xtest_poly.columns.str.replace(\"^\", \"_\", regex=True)\n\nLet us use 2 different sets of predictors to introduce diversity in the ensemble.\n\ncol_set1 = ['mileage','mpg', 'year','engineSize']\ncol_set2 = X_poly.columns\n\nLet us use two types of strong tree-based models.\n\ncat = CatBoostRegressor(verbose=False)\ngb = GradientBoostingRegressor(loss = 'huber')\n\nWe will use the Pipeline() function along with ColumnTransformer() to map a predictor set to each model.\n\ncat_pipe1 = Pipeline([\n    ('column_transformer', ColumnTransformer([('cat1_transform', 'passthrough', col_set1)], remainder='drop')),\n    ('cat1', cat)\n])\n\ncat_pipe2 = Pipeline([\n    ('column_transformer', ColumnTransformer([('cat2_transform', 'passthrough', col_set2)], remainder='drop')),\n    ('cat2', cat)\n])\n\ngb_pipe1 = Pipeline([\n    ('column_transformer', ColumnTransformer([('gb1_transform', 'passthrough', col_set1)], remainder='drop')),\n    ('gb1', gb)\n])\n\ngb_pipe2 = Pipeline([\n    ('column_transformer', ColumnTransformer([('gb2_transform', 'passthrough', col_set2)], remainder='drop')),\n    ('gb2', gb)\n])\n\nWe will use Linear regression to ensemble the models.\n\nen_new.final_estimator_.coef_\n\narray([ 0.30127482,  0.79242981, -0.07168258, -0.01781781])\n\n\n\nen_new = StackingRegressor(estimators = [('cat1', cat_pipe1),('cat2', cat_pipe2),\n                                        ('gb1', gb_pipe1), ('gb2', gb_pipe2)],\n                     final_estimator=LinearRegression(),                                          \n                    cv = KFold(n_splits = 15, shuffle = True, random_state=1))\n\n\nen_new.fit(X_poly, y)\n\nStackingRegressor(cv=KFold(n_splits=15, random_state=1, shuffle=True),\n                  estimators=[('cat1',\n                               Pipeline(steps=[('column_transformer',\n                                                ColumnTransformer(transformers=[('cat1_transform',\n                                                                                 'passthrough',\n                                                                                 ['mileage',\n                                                                                  'mpg',\n                                                                                  'year',\n                                                                                  'engineSize'])])),\n                                               ('cat1',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;)])),\n                              ('cat2',\n                               Pipeline(steps=[('column_transformer',...\n                               Pipeline(steps=[('column_transformer',\n                                                ColumnTransformer(transformers=[('gb2_transform',\n                                                                                 'passthrough',\n                                                                                 Index(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object'))])),\n                                               ('gb2',\n                                                GradientBoostingRegressor(loss='huber'))]))],\n                  final_estimator=LinearRegression())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StackingRegressorStackingRegressor(cv=KFold(n_splits=15, random_state=1, shuffle=True),\n                  estimators=[('cat1',\n                               Pipeline(steps=[('column_transformer',\n                                                ColumnTransformer(transformers=[('cat1_transform',\n                                                                                 'passthrough',\n                                                                                 ['mileage',\n                                                                                  'mpg',\n                                                                                  'year',\n                                                                                  'engineSize'])])),\n                                               ('cat1',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;)])),\n                              ('cat2',\n                               Pipeline(steps=[('column_transformer',...\n                               Pipeline(steps=[('column_transformer',\n                                                ColumnTransformer(transformers=[('gb2_transform',\n                                                                                 'passthrough',\n                                                                                 Index(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object'))])),\n                                               ('gb2',\n                                                GradientBoostingRegressor(loss='huber'))]))],\n                  final_estimator=LinearRegression())cat1column_transformer: ColumnTransformerColumnTransformer(transformers=[('cat1_transform', 'passthrough',\n                                 ['mileage', 'mpg', 'year', 'engineSize'])])cat1_transform['mileage', 'mpg', 'year', 'engineSize']passthroughpassthroughCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;cat2column_transformer: ColumnTransformerColumnTransformer(transformers=[('cat2_transform', 'passthrough',\n                                 Index(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object'))])cat2_transformIndex(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object')passthroughpassthroughCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;gb1column_transformer: ColumnTransformerColumnTransformer(transformers=[('gb1_transform', 'passthrough',\n                                 ['mileage', 'mpg', 'year', 'engineSize'])])gb1_transform['mileage', 'mpg', 'year', 'engineSize']passthroughpassthroughGradientBoostingRegressorGradientBoostingRegressor(loss='huber')gb2column_transformer: ColumnTransformerColumnTransformer(transformers=[('gb2_transform', 'passthrough',\n                                 Index(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object'))])gb2_transformIndex(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object')passthroughpassthroughGradientBoostingRegressorGradientBoostingRegressor(loss='huber')final_estimatorLinearRegressionLinearRegression()\n\n\n\nmean_squared_error(en_new.predict(Xtest_poly), ytest, squared = False)\n\n5185.376722607323\n\n\nNote that the above model does better on test data than all the models developed so far. Using different sets of predictors introduces diversity in the ensemble, as an alternative to including “weaker” models in the ensemble to add diversity.\nCheck the idea being used in the Spring 2023 prediction problem in the appendix.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ensemble modeling</span>"
    ]
  },
  {
    "objectID": "Assignment1_sp25.html",
    "href": "Assignment1_sp25.html",
    "title": "Appendix A — Assignment 1",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1_sp25.html#instructions",
    "href": "Assignment1_sp25.html#instructions",
    "title": "Appendix A — Assignment 1",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Thursday, 11th April 2025 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (2 points). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1_sp25.html#bias-variance-trade-off-for-regression-50-points",
    "href": "Assignment1_sp25.html#bias-variance-trade-off-for-regression-50-points",
    "title": "Appendix A — Assignment 1",
    "section": "1) Bias-Variance Trade-off for Regression (50 points)",
    "text": "1) Bias-Variance Trade-off for Regression (50 points)\nThe main goal of this question is to understand and visualize the bias-variance trade-off in a regression model by performing repetitive simulations.\nThe conceptual clarity about bias and variance will help with the main logic behind creating many models that will come up later in the course.\n\na) Define the True Relationship (Signal)\nFirst, you need to implement the underlying true relationship (Signal) you want to sample data from. Assume that the function is the Bukin function. Implement it as a user-defined function and run it with the test cases below to make sure it is implemented correctly. (5 points)\nNote: It would be more useful to have only one input to the function. You can treat the input as an array of two elements.\n\nprint(Bukin(np.array([1,2]))) # The output should be 141.177\nprint(Bukin(np.array([6,-4]))) # The output should be 208.966\nprint(Bukin(np.array([0,1]))) # The output should be 100.1\n\n\n\nb) Generate Test Set (No Noise)\nGenerate a noiseless test set with 100 observations sampled from the true underlying function. This test set will be used to evaluate bias and variance, so make sure it follows the correct data generation process.\n(5 points)\nInstructions:\n\nDo not use loops for this question.\n.apply will be especially helpful (and often simpler).\n\nData generation assumptions:\n\nUse np.random.seed(100) for reproducibility.\nThe first predictor, \\(x_1\\), should be drawn from a uniform distribution over the interval \\([-15, -5]\\), i.e., \\(x_1 \\sim U[-15, -5]\\).\nThe second predictor, \\(x_2\\), should be drawn from a uniform distribution over the interval \\([-3, 3]\\), i.e., \\(x_2 \\sim U[-3, 3]\\).\nCompute the true function values using the underlying model as your response \\(y\\)\n\n\n\nc) Initialize Results DataFrame\nCreate an empty DataFrame with the following columns:\n\ndegree: the degree of the polynomial model\n\nbias_sq: estimated squared bias (averaged over test points)\n\nvar: estimated variance of predictions\n\nbias_var_sum: sum of bias squared and variance\n\nempirical_mse: mean squared error calculated using sklearn’s mean_squared_error() on model predictions vs. true function values\n\nThis DataFrame will be used to store the results of your bias–variance tradeoff analysis and for generating comparison plots.\n(3 points)\n\n\nd) Generate Training Sets (With Noise)\nTo estimate the bias, variance, and total error (MSE) of a Linear Regression model trained on noisy data from the underlying Bukin function, follow the steps below.\n🔁 Step 1: Generate 100 Training Sets\n\nCreate 100 independent training datasets, each with 100 observations (same size as the test set).\nFor each training dataset:\n\nUse np.random.seed(i) to ensure reproducibility, where i is the dataset index (0 to 99).\nSample predictors from the same distributions used to generate the test set.\nAdd Gaussian noise with mean 0 and standard deviation 10:\n\\(\\varepsilon \\sim \\mathcal{N}(0, 10)\\)\n\n\n🧠 Step 2: Train Polynomial Models (Degrees 1 to 7)\n\nFor each training dataset, train polynomial models with degrees 1 through 7.\nUse polynomial feature transformations that include both:\n\nHigher-order terms (e.g., \\(x_1^2\\), \\(x_2^3\\))\nInteraction terms (e.g., \\(x_1 \\cdot x_2\\))\n\nMake predictions on the fixed, noiseless test set for each trained model.\n\n📊 Step 3: Estimate Bias², Variance, and MSE\n\nFor each degree, and each test point, collect the 100 predicted values from the models trained on the different training sets.\nUsing these predictions, compute:\n\nBias squared: squared difference between the mean prediction and the true value.\nVariance: variance of the predictions.\nTheoretical MSE: sum of bias squared and variance.\nEmpirical MSE: compute using sklearn.metrics.mean_squared_error between each model’s prediction and the true values, then average over the 100 training runs.\n\nStore all four quantities for each degree in your results DataFrame:\n\ndegree\nbias_sq\nvar\nbias_var_sum (bias squared + variance)\nempirical_mse\n\n\n(25 points)\n💡 Reminder: Comparing Theoretical vs. Empirical MSE\nWhen evaluating model performance on the noiseless test set:\n\nThe irreducible error (i.e., noise in training data) does not affect the test targets.\nTherefore, the test error (MSE) can be decomposed as:\n\\(MSE\\) = \\({Bias}^2\\) + \\({Variance}\\)\nThe empirical MSE (from sklearn) should closely match the sum of bias² and variance, since the test data contains no noise.\n\n\n\ne) Visualize Bias–Variance Decomposition\nUsing the results stored in your DataFrame, create a plot with four lines, each plotted against the polynomial degree:\n\nBias squared\nVariance\nBias squared + Variance (i.e., the theoretical decomposition of MSE)\nEmpirical MSE calculated using sklearn.metrics.mean_squared_error()\n(computed from the predicted values vs. true function values on the noiseless test set)\n\nPlot requirements: - Use a single line plot with the polynomial degree on the x-axis and error values on the y-axis. - Include a legend to clearly label each line. - Use different line styles or markers for easy visual comparison.\nGoal: - Compare the empirical MSE to the sum of bias squared and variance. - If everything is implemented correctly, the two lines should be very close (or even identical, up to numerical precision).\n\n\nf) Identify the Optimal Model\n\nWhat is the optimal polynomial degree based on the lowest empirical MSE (calculated using sklearn)?\n(2 points)\nReport the corresponding values of:\n\nBias squared\n\nVariance\n\nBias squared + Variance\n\nEmpirical MSE\nfor that degree.\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1_sp25.html#building-a-low-bias-low-variance-model-via-regularization-50-points",
    "href": "Assignment1_sp25.html#building-a-low-bias-low-variance-model-via-regularization-50-points",
    "title": "Appendix A — Assignment 1",
    "section": "2) Building a Low-Bias, Low-Variance Model via Regularization (50 points)",
    "text": "2) Building a Low-Bias, Low-Variance Model via Regularization (50 points)\nThe main goal of this question is to further reduce the total prediction error by applying regularization.\nSpecifically, you’ll use Ridge regression to build a low-bias, low-variance model for data generated from the underlying Bukin function with noise.\n\na) Why Regularization?\nExplain why the model with the optimal polynomial degree (as identified in Question 1) is not guaranteed to be the true low-bias, low-variance model.\nWhy might regularization still be necessary to improve generalization performance, even after selecting the degree that minimizes MSE?\n(5 points)\n\n\nb) Which Degrees to Exclude?\nBased on your plot and results from 1e and 1f, identify which polynomial degrees should be excluded from regularization experiments because they are already too simple (high bias) or too complex (high variance).\nExplain which degrees you will exclude and why, using your understanding of how regularization affects bias and variance.\n(10 points)\n\n\nc) Apply Ridge Regularization\nRepeat the steps from 1c and 1d, but this time use Ridge regression instead of ordinary least squares.\n\nUse only the degrees not excluded in 2b (and also exclude degree 7 to avoid extreme overfitting).\nUse 5-fold cross-validation to tune the Ridge regularization strength.\nUse neg_root_mean_squared_error as the scoring metric for cross-validation.\nTune over a range of regularization strengths (e.g., from 1 to 100).\nFor each retained degree, compute:\n\nBias squared\nVariance\nBias squared + Variance\nEmpirical MSE (from sklearn.metrics.mean_squared_error)\n\n\nStore your results in a new DataFrame with the same structure as in Question 1.\n(10 points)\n\n\nd) Visualize Regularized Results\nRepeat the visualization from 1e, but using the results from 2c (Ridge regression).\nYour plot should include four lines plotted against polynomial degree:\n\nBias squared\nVariance\nBias squared + Variance\nEmpirical MSE (computed using sklearn)\n\nInclude a clear legend and label your axes.\nThis will help you visually assess how regularization impacts bias, variance, and overall model error.\n(10 points)\n\n\ne) Evaluate the Regularized Model\n\nWhat is the optimal polynomial degree for the Ridge Regression model, based on the lowest empirical MSE?\n(3 points)\nReport the corresponding values of:\n\nBias squared\n\nVariance\n\nEmpirical MSE\nfor that optimal Ridge model.\n(3 points)\n\nCompare these results to those of the optimal Linear Regression model from Question 1.\nDiscuss how regularization influenced the bias, variance, and overall prediction error (MSE).\n(4 points)\n\n\n\nf) Interpreting the Impact of Regularization\n\nWas regularization successful in reducing the total prediction error (MSE) compared to the unregularized model?\n(2 points)\nBased on your results from 2e, explain how bias and variance changed as a result of regularization.\nHow did these changes affect the final total error?\nSupport your explanation with values or observations from your analysis.\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment2_sp25.html",
    "href": "Assignment2_sp25.html",
    "title": "Appendix B — Assignment 2",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment2_sp25.html#instructions",
    "href": "Assignment2_sp25.html#instructions",
    "title": "Appendix B — Assignment 2",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Monday, 18th April 2025 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (1 point). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission. (1 point)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)\n\nThe maximum possible score in the assigment is 103+5 = 108 out of 100.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment2_sp25.html#optimizing-knn-for-classification-71-points",
    "href": "Assignment2_sp25.html#optimizing-knn-for-classification-71-points",
    "title": "Appendix B — Assignment 2",
    "section": "B.1 Optimizing KNN for Classification (71 points)",
    "text": "B.1 Optimizing KNN for Classification (71 points)\nIn this question, you will use classification_data.csv. Each row is a loan and the each column represents some financial information as follows:\n\nhi_int_prncp_pd: Indicates if a high percentage of the repayments went to interest rather than principal. This is the classification response.\nout_prncp_inv: Remaining outstanding principal for portion of total amount funded by investors\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\nint_rate: Interest Rate on the loan\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60.\n\nAs indicated above, hi_int_prncp_pd is the response and all the remaining columns are predictors. You will tune and train a K-Nearest Neighbors (KNN) classifier throughout this question.\n\nB.1.1 a) Load the Dataset (1 point)\nRead the dataset into your notebook.\n\n\nB.1.2 b) Define Predictor and Response Variables (1 point)\nCreate the predictor (features) and response (target) variables from the dataset.\n\n\nB.1.3 c) Split the Data into Training and Test Sets (1 points)\nCreate the training and test datasets using the following specifications:\n\nUse a 75%-25% split.\nEnsure that the class ratio is preserved in both training and test sets (i.e., stratify the split).\nSet random_state=45 for reproducibility.\n\n\n\nB.1.4 d) Check Class Ratios (2 points)\nPrint the class distribution (ratios) for:\n\nThe entire dataset\n\nThe training set\n\nThe test set\n\nThis is to verify that the class ratio is preserved after splitting.\n\n\nB.1.5 e) Scale the Dataset (2 points)\nUse StandardScaler to scale the dataset in order to prepare it for KNN modeling.\nScaling ensures that all features contribute equally to the distance calculations used by the KNN algorithm.\n\n\nB.1.6 f) Set Up Cross-Validation (2 points)\nBefore creating and tuning your model, you need to define cross-validation settings to ensure consistent and accurate evaluation across folds.\nPlease follow these specifications:\n\nUse 5 stratified folds to preserve class distributions in each split.\nShuffle the data before splitting to introduce randomness.\nSet random_state=14 for reproducibility.\n\nNote: You must use these exact cross-validation settings throughout the rest of this question to maintain consistency.\n\n\nB.1.7 g) Tune K for KNN Using Cross-Validation (12 points)\nTune a KNN Classifier using cross-validation with the following specifications:\n\nUse every odd K value from 1 to 50 (inclusive).\nKeep all other model settings at their defaults.\nUse the cross-validation settings defined in part (f).\nEvaluate performance using the F1 score as the metric.\n\n(4 points)\nThen, complete the following tasks:\n\nCreate a plot of K values vs. cross-validation F1 scores to visualize how K balances overfitting and underfitting. (2 points)\nPrint the best average cross-validation F1 score. (1 points)\nReport the K value corresponding to the best F1 score. (1 points)\nDetermine whether this is the only K value that results in the best F1 score. Use code to justify your answer. (2 points)\nReflect on whether accuracy is a good metric for tuning the model in this case. Explain your reasoning. (2 points)\n\n💡 Hint:\nIn addition to reporting the best K and best F1 score, you may also want to examine the full cross-validation results to check if other K values achieved the same F1 score.\n\n\nB.1.8 h) Optimize the Classification Threshold (4 points)\nUsing the optimal K value you identified in part (g), optimize the classification threshold to maximize the cross-validation F1 score.\n\nB.1.8.1 Specifications:\n\nSearch across all possible threshold values using a step size of 0.05.\nUse the cross-validation settings defined in part (f).\nEvaluate performance using the F1 score, consistent with part (g).\n\n\n\nB.1.8.2 Tasks:\n\nVisualize the F1 score vs. different threshold values. (2 points)\nIdentify and report the best threshold that yields the highest F1 score. (1 points)\nOutput the best cross-validation F1 score. (1 points)\n\n\n\n\nB.1.9 i) Evaluate the Tuning Method (2 points)\nIs the method we used in parts (g) and (h) guaranteed to find the best combination of K and threshold, i.e., to tune the classifier to its optimal values?\n(1 point)\nJustify your answer.\n(1 point)\n\n\nB.1.10 j) Evaluate Tuned Classifier on Test Set (3 points)\nUsing the tuned KNN classifier and the optimal threshold you identified, evaluate the model on the test set. Report the following metrics:\n\nF1 Score\n\nAccuracy\n\nPrecision\n\nRecall\n\nAUC\n\n\n\nB.1.11 k) Jointly Tune K and Threshold (6 points)\nNow, tune K and the classification threshold simultaneously, rather than sequentially.\n\nUse the same settings from the previous parts (i.e., odd K values from 1 to 50, threshold step size of 0.05, F1 score as the metric, and the same cross-validation strategy).\nIdentify the best F1 score, along with the K value and threshold that produce it.\n\n\n\nB.1.12 l) Visualize Cross-Validation Results with a Heatmap (3 points)\nCreate a heatmap to visualize the cross-validation results in two dimensions.\n\nThe x-axis should represent the K values.\nThe y-axis should represent the threshold values.\nThe color should represent the F1 score.\n\nNote: This question only requires one line of code. You’ll need to recall a data visualization function and a data reshaping method from 303-1.\n\n\nB.1.13 m) Compare Joint vs. Sequential Tuning Results (4 points)\n\nHow does the best cross-validation F1 score from part (k) compare to the scores from parts (g) and (h)? (1 point)\nDid the optimal K value and threshold change when tuning them jointly? (1 point)\nExplain why or why not. Consider how tuning the two parameters together might impact the result. (2 points)\n\n\n\nB.1.14 n) Evaluate Final Tuned Model on Test Set (3 points)\nUsing the tuned classifier and threshold from part (k), evaluate the model on the test set. Report the following metrics:\n\nF1 Score\n\nAccuracy\n\nPrecision\n\nRecall\n\nAUC\n\n\n\nB.1.15 o) Compare Tuning Strategies and Computational Cost (3 points)\nCompare the tuning approach used in parts (g) & (h) (separate tuning of K and threshold) with the approach in part (k) (joint tuning of K and threshold) in terms of computational cost.\n\nHow many K and threshold combinations did you evaluate in each approach? (2 points)\nBased on this comparison and your answer from part (l), explain the main trade-off involved in model tuning (e.g., between computation and performance). (2 points)\n\n\n\nB.1.16 p) Tune K Using Multiple Metrics (5 points)\nGridSearchCV and cross_val_score are designed to optimize based on a single metric.\nIn this section, you’ll practice tuning hyperparameters while evaluating multiple metrics simultaneously using cross_validate.\nFor this imbalanced classification task, instead of optimizing the F1 score directly, we’ll focus on precision and recall together.\nKeep in mind that the F1 score is the harmonic mean of precision and recall—it balances the trade-off between the two.\nCross-validate a KNN classifier using the following specifications:\n\nUse the same cross-validation setting and hyperparameter grid as before\nEvaluate the model using precision, recall, and f1-score, as metrics at the same time.\n\nSave the cross-validation results into a DataFrame, and compute the average score for each metric, and visualize how these metrics change with different values of K.\n\n\nB.1.17 q) Optimize for Recall with Precision Constraint (4 point)\nIdentify the K value that yields the highest recall, while maintaining a precision of at least 75%.\n(3 points)\nThen, print the average cross-validation metrics (f1-score, precision, recall) for that K value.\n(1 point)\n\n\nB.1.18 r) Tune Threshold for Maximum Recall (3 point)\nUsing the optimal K value identified in part (q), find the threshold that maximizes cross-validation Recall, following the specifications below:\n\nEvaluate all possible threshold values with a step size of 0.05.\nUse the cross-validation settings from part (f).\n\nThen: - Print the best cross-validation recall. - Report the threshold value that achieves this recall.\nNote: This task is very similar to part (h), but it’s important for the next part.\n\n\nB.1.19 s) Evaluate Precision-Optimized Model on Test Set (2 points)\nUsing the tuned classifier and threshold from parts (q) and (r), evaluate the model on the test set. Report the following metrics:\n\nF1 Score\n\nAccuracy\n\nPrecision\n\nRecall\n\nAUC\n\n\n\nB.1.20 t) Final Reflection: Comparing Tuning Strategies (3 points)\nYou have now tuned your KNN classifier using three different strategies:\n\nSequential tuning of K and threshold based on F1 score (parts g–h)\nJoint tuning of K and threshold using F1 score (part k)\nTuning based on multiple metrics, selecting the K with the highest recall while maintaining precision ≥ 75% (parts p–r)\n\nReflect on the following:\n\nWhich tuning strategy led to the best overall performance on the test set, based on the metrics you care about most?\nWhich strategy would you choose in a real-world application, and why?\nWhat are the trade-offs between tuning for F1 score versus prioritizing precision or recall individually?\n\nNote: This is an open-ended question. As long as your reasoning makes sense, you will receive full credit.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment2_sp25.html#tuning-a-knn-regressor-on-bank-loan-data-32-points",
    "href": "Assignment2_sp25.html#tuning-a-knn-regressor-on-bank-loan-data-32-points",
    "title": "Appendix B — Assignment 2",
    "section": "B.2 Tuning a KNN Regressor on Bank Loan Data (32 points)",
    "text": "B.2 Tuning a KNN Regressor on Bank Loan Data (32 points)\nIn this question, you will use bank_loan_train_data.csv to tune (the model hyperparameters) and train the model. Each row is a loan and the each column represents some financial information as follows:\n\nmoney_made_inv: Indicates the amount of money made by the bank on the loan. This is the regression response.\nout_prncp_inv: Remaining outstanding principal for portion of total amount funded by investors\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\nint_rate: Interest Rate on the loan\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60\nmort_acc: The number of mortgage accounts\napplication_type_Individual: 1 if the loan is an individual application or a joint application with two co-borrowers\ntot_cur_bal: Total current balance of all accounts\npub_rec: Number of derogatory public records\n\nAs indicated above, money_made_inv is the response and all the remaining columns are predictors. You will tune and train a K-Nearest Neighbors (KNN) regressor throughout this question.\n\nB.2.1 a) Split, Scale, and Tune a KNN Regressor (15 point)\nCreate the training and test datasets using the following specifications:\n\nUse an 80%-20% split.\nSet random_state=1 for reproducibility.\n\nThen, scale all the predictors, as KNN is sensitive to the scale of input features.\nNext, you will tune a KNN Regressor by searching for the optimal hyperparameters using three search approaches: Grid Search, Random Search, and Bayesian Search.\n\nB.2.1.1 Cross-Validation Setting\nYou should use 5-fold cross-validation, with the following specifications:\n\nThe data should be shuffled before splitting\n\nUse random_state=1 to ensure reproducibility\n\n\n\nB.2.1.2 Hyperparameters to Tune:\nYou will tune the following hyperparameters for the KNN Regressor:\nYou will tune the following hyperparameters for the K-Nearest Neighbors Regressor, using Minkowski as the distance metric:\n\nn_neighbors: Number of nearest neighbors\n\nTune over the range: np.arange(1, 25, 1)\n\np: Power parameter for the Minkowski distance\n\nUse values: np.arange(1, 4, 1)\np = 1 corresponds to Manhattan distance\n\np = 2 corresponds to Euclidean distance\n\nNote: Set the distance metric to \"minkowski\"\n\nweights: Weight function used in prediction\nYou must consider the following 5 types of weights:\n\n'uniform': All neighbors are weighted equally\n\n'distance': Weight is inversely proportional to distance\n\nCustom weight functions:\n\n∝1distance2\\propto \\frac{1}{\\text{distance}^2}\n∝1distance3\\propto \\frac{1}{\\text{distance}^3}\n∝1distance4\\propto \\frac{1}{\\text{distance}^4}\n\n\n\nFor each search method (Grid Search, Random Search, Bayesian Search), report the following:\n\nbest_params_: The best combination of hyperparameters\n\nbest_score_: Cross-validated RMSE on the training set\n\nTest RMSE obtained from the best model\n\nExecution time for the search process\n\nHint:\nDefine three custom weight functions as shown below:\ndef dist_power_2(distance):\n    return 1 / (1e-10 + distance**2)\n\ndef dist_power_3(distance):\n    return 1 / (1e-10 + distance**3)\n\ndef dist_power_4(distance):\n    return 1 / (1e-10 + distance**4)\nNote the small constant 1e-10 helps avoid division by zero and numerical instability.\n\n\n\nB.2.2 b) Compare Tuning Approaches (1 point)\nCompare the results from part (2a) in terms of execution time and model performance.\nBriefly discuss the main trade-offs among the three hyperparameter tuning approaches: Grid Search, Random Search, and Bayesian Search.\n\n\nB.2.3 c) Feature Selection and Hyperparameter Tuning with GridSearchCV (15 point)\nKNN performance can deteriorate significantly if irrelevant or noisy predictors are included. In this part, you will explore feature selection to improve model performance, followed by hyperparameter tuning using GridSearchCV (with refit=True).\nTry the following three different feature selection approaches:\n\nCorrelation-based filtering:\n\nSelect features with an absolute correlation of at least 0.1 with the target variable.\n\nLasso regression for feature selection:\n\nUse Lasso(alpha=50) to select important features based on non-zero coefficients.\n\nSelectKBest:\n\nUse SelectKBest with f_regression, selecting the top 4 features.\n\n\nFor each approach, perform hyperparameter tuning using GridSearchCV, and report:\n\nThe best score (cross-validated RMSE) on the training set\nThe test RMSE from the best model\nThe best hyperparameters\n\n\n\nB.2.4 d) Compare Feature Selection Approaches (1 point)\nCreate a DataFrame that summarizes the model performance from each feature selection method, including:\n\nTraining RMSE\nTest RMSE\n\nBe sure to also include the results from the model trained without any feature selection for comparison.\nThen, briefly explain what you learned from this experiment.\nFor example: Did feature selection improve performance? Which method worked best?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment3_sp25.html",
    "href": "Assignment3_sp25.html",
    "title": "Appendix C — Assignment 3",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment3_sp25.html#instructions",
    "href": "Assignment3_sp25.html#instructions",
    "title": "Appendix C — Assignment 3",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Friday, 2th May 2025 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (2 pts). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file. If your issue doesn’t seem genuine, you will lose points.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment3_sp25.html#regression-problem---miami-housing",
    "href": "Assignment3_sp25.html#regression-problem---miami-housing",
    "title": "Appendix C — Assignment 3",
    "section": "C.1 Regression Problem - Miami housing",
    "text": "C.1 Regression Problem - Miami housing\n\nC.1.1 a) Data preparation\nRead the data miami-housing.csv. Check the description of the variables here. Split the data into 60% train and 40% test. Use random_state = 45. The response is SALE_PRC, and the rest of the columns are predictors, except PARCELNO. Print the shape of the predictors dataframe of the train data.\n(2 points)\n\n\nC.1.2 b) Baseline Decision Tree Model\nTrain a Decision Tree Regressor to predict SALE_PRC using all available predictors.\n\nUse random_state=45 and keep all other hyperparameters at their default values.\nAfter training the model, evaluate and report the following on both the training and test sets:\n\nMean Absolute Error (MAE)\nR² Score\n\n\n(3 points)\n\n\nC.1.3 c) Tune the Decision Tree Model\nTune the hyperparameters of the Decision Tree Regressor developed in the previous question and evaluate its performance.\nYour goal is to achieve a test set MAE (Mean Absolute Error) below $68,000.\n\nYou must display the optimal hyperparameter values obtained from the tuning process.\nCompute and report the test MAE and R² Score using the tuned model.\n\nHints: 1. BayesSearchCV() with max_depth and max_features can often complete in under a minute. 2. You may use any hyperparameter tuning method (e.g., GridSearchCV, RandomizedSearchCV, BayesSearchCV). 3. You are free to choose which hyperparameters to tune and define your own search space.\n(9 points)\n\n\nC.1.4 d) Bagged Decision Trees with Out-of-Bag Evaluation\nTrain a Bagging Regressor using Decision Trees as base estimators to predict SALE_PRC.\n\nUse only the n_estimators hyperparameter for tuning; keep all other parameters at their default values.\nIncrease the number of trees (n_estimators) until the out-of-bag (OOB) MAE stabilizes.\nReport the final OOB MAE, test MAE, and R² Score, and ensure that the OOB MAE is less than $48,000.\n\n(4 points)\n\n\nC.1.5 e) Bagged Decision Trees Without Bootstrapping\nTrain a Bagging Regressor using Decision Trees, but this time disable bootstrapping by setting bootstrap=False.\n\nUse the same n_estimators value as in the previous question.\nCompute and report the following on the test set:\n\nMean Absolute Error (MAE)\nR² Score\n\n\nExplain why the test MAE in this case is:\n\nMuch higher than the MAE obtained when bootstrapping was enabled (previous question).\nLower than the MAE obtained from a single untuned decision tree (as in Question 1(b)).\n\n\n💡 Hint: Consider the impact of bootstrap sampling on variance reduction and the benefits of aggregation in ensemble methods.\n\n(2 point for code, 3 + 3 points for reasoning)\n\n\nC.1.6 f) Bagged Decision Trees with Feature Bootstrapping Only\nTrain a Bagging Regressor using Decision Trees, with the following configuration: - Disable sample bootstrapping by setting bootstrap=False - Enable feature bootstrapping by setting bootstrap_features=True\nUse the same number of estimators (n_estimators) as in the previous bagging experiments.\n\nCompute and report the following on the test set:\n\nMean Absolute Error (MAE)\nR² Score\n\n\nExplain why the test MAE obtained in this setting is much lower than the one in the previous question, where neither bootstrapping samples nor features was used.\n(2 point for code, 3 points for reasoning)\n\n\nC.1.7 g) Tuning a Bagged Tree Model\n\nC.1.7.1 i) Approaches\nThere are two common approaches for tuning a bagged tree model:\n\nOut-of-Bag (OOB) Prediction\nKK-fold Cross-Validation using GridSearchCV\n\nWhat is the advantage of each approach over the other? Specifically:\n\nWhat is the advantage of the out-of-bag approach compared to KK-fold cross-validation?\nWhat is the advantage of KK-fold cross-validation compared to the out-of-bag approach?\n\n(3 + 3 points)\n\n\nC.1.7.2 ii) Tuning the hyperparameters\nTune the hyperparameters of the bagged tree model developed in 1(d). You may use either of the approaches mentioned in the previous question. Show the optimal values of the hyperparameters obtained. Compute the MAE and R² Score on test data with the tuned model. Your test MAE must be less than the test MAE ontained in the previous question.\nIt is up to you to pick the hyperparameters and their values in the grid.\nHint:\nGridSearchCV() may work better than BayesSearchCV() in this case.\n(9 points)\n\n\n\nC.1.8 h) Random Forest\n\nC.1.8.1 i) Tuning a Random Forest Model\nTrain and tune a Random Forest Regressor to predict SALE_PRC.\n\nSelect hyperparameters and define your own tuning grid.\nUse any tuning approach (e.g., Out-of-Bag (OOB) evaluation or KK-fold cross-validation).\nReport the following performance metrics on the test set:\n\nMean Absolute Error (MAE)\nR² Score\n\n\n\n✅ Your goal is to achieve a test MAE below $46,000.\n\nHint:\nThe OOB approach is efficient and can complete in under a minute.\n(9 points)\n\n\nC.1.8.2 ii) Feature Importance\nAfter fitting the tuned Random Forest Regressor, extract and display the feature importances.\n\nPrint the predictors in decreasing order of importance based on the trained model.\nThis helps identify which features contribute most to predicting SALE_PRC.\n\n(4 points)\n\n\nC.1.8.3 iii) Feature Selection\nDrop the least important predictor identified in the previous step, and re-train the tuned Random Forest model.\n\nCompute the test MAE and R² Score after dropping the feature.\nYou may need to adjust the max_features hyperparameter to reflect the reduced number of predictors.\nCompare the new test MAE with the previous one.\n\n\n❓ Did the test MAE decrease after removing the least important feature?\n\n(4 points)\n\n\nC.1.8.4 iv) Random Forest vs. Bagging: max_features\nThe max_features hyperparameter is available in both RandomForestRegressor() and BaggingRegressor().\nDoes max_features have the same meaning in both models?\nIf not, explain the difference in how it is interpreted and applied.\n\n💡 Hint: Refer to the scikit-learn documentation for both estimators to understand how max_features affects feature selection during training.\n\n(1 + 3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment3_sp25.html#classification---term-deposit",
    "href": "Assignment3_sp25.html#classification---term-deposit",
    "title": "Appendix C — Assignment 3",
    "section": "C.2 Classification - Term deposit",
    "text": "C.2 Classification - Term deposit\nThe data for this question is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls, where bank clients were called to subscribe for a term deposit.\nThere is a train data - train.csv, which you will use to develop a model. There is a test data - test.csv, which you will use to test your model. Each dataset has the following attributes about the clients called in the marketing campaign:\n\nage: Age of the client\neducation: Education level of the client\nday: Day of the month the call is made\nmonth: Month of the call\ny: did the client subscribe to a term deposit?\nduration: Call duration, in seconds. This attribute highly affects the output target (e.g., if duration=0 then y=‘no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for inference purposes and should be discarded if the intention is to have a realistic predictive model.\n\n(Raw data source: Source. Do not use the raw data source for this assignment. It is just for reference.)\n\nC.2.1 a) Data Preparation\nBegin by examining the distribution of the target variable in both the training and test sets. This will help you assess whether there is any significant class imbalance.\nNext, consider the two available approaches for hyperparameter tuning:\n\nCross-validation (CV)\nOut-of-bag (OOB) evaluation\n\n\nC.2.1.1 ❓ Which method do you prefer for this dataset, and why?\nDiscuss your choice based on:\n\nThe size of the dataset\nThe class imbalance in the target variable\nThe reliability and interpretability of each method\nWhether you need stratified sampling to preserve class distribution during evaluation\n\n(2 points)\n\n\n\nC.2.2 b) Random Forest for Term Deposit Subscription Prediction\nDevelop and tune a Random Forest Classifier to predict whether a client will subscribe to a term deposit using the following predictors:\n\nage\neducation\nday\nmonth\n\nThe model must satisfy the following performance criteria:\n\nC.2.2.1 ✅ Requirements:\n\nMinimum overall classification accuracy of 75%, across both train.csv and test.csv.\nMinimum recall of 60%, across both train.csv and test.csv.\n\nYou must:\n\nPrint the accuracy and recall for both datasets (train.csv and test.csv).\nUse cross-validation on the training data to optimize the model hyperparameters.\nSelect a threshold probability for classification and apply it consistently across both datasets.\n\n\n\nC.2.2.2 ⚠️ Important Notes:\n\nDo not use duration as a predictor. Its value is determined after the marketing call ends, so using it would leak information about the outcome.\nYou are free to choose any decision threshold for classification, but the same threshold must be used consistently for both training and test evaluation.\nUse cross-validation to tune hyperparameters such as max_features, max_depth, and max_leaf_nodes.\n- You may use StratifiedKFold or any appropriate CV method that respects class imbalance.\nAfter tuning the model, plot cross-validated accuracy and recall across a range of threshold values (e.g., 0.1 to 0.9). Use this plot to select a threshold that satisfies the required trade-off between accuracy and recall.\nEvaluate the final tuned model (with the chosen threshold) on the test dataset. Do not use the test data to guide any part of the tuning or threshold selection.\n\n\n\nC.2.2.3 💡 Hints:\n\nRestrict the search space to:\n\nmax_depth ≤ 25\n\nmax_leaf_nodes ≤ 45\nThese limits encourage generalization and help balance recall and accuracy.\n\nConsider using cross-validation scores to compute predicted probabilities when plotting recall/accuracy curves.\n\n\n\nC.2.2.4 📝 Scoring Breakdown (22 points total):\n\n8 points – Hyperparameter tuning via cross-validation\n\n5 points – Plotting accuracy and recall across thresholds\n\n5 points – Threshold selection based on the plot\n\n4 points – Reporting accuracy and recall on both datasets",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment3_sp25.html#predictor-transformations-in-trees",
    "href": "Assignment3_sp25.html#predictor-transformations-in-trees",
    "title": "Appendix C — Assignment 3",
    "section": "C.3 Predictor Transformations in Trees",
    "text": "C.3 Predictor Transformations in Trees\nCan a non-linear monotonic transformation of predictors (such as log(), sqrt(), etc.) be useful in improving the accuracy of decision tree models?\nProvide a brief explanation based on your understanding of how decision trees split data and handle predictor scales.\n(4 points for answer)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix D — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Datasets, assignment and project files</span>"
    ]
  },
  {
    "objectID": "bagging.html#oob-sample-and-oob-score-in-bagging",
    "href": "bagging.html#oob-sample-and-oob-score-in-bagging",
    "title": "6  Bagging",
    "section": "6.5 OOB Sample and OOB Score in Bagging",
    "text": "6.5 OOB Sample and OOB Score in Bagging\nWhen training a Bagging ensemble, such as BaggingClassifier or BaggingRegressor, each base learner is trained on a bootstrap sample—a random sample with replacement from the original dataset.\n\n6.5.1 What is an OOB Sample?\nFor each base learner, the data points not selected in the bootstrap sample form its Out-of-Bag (OOB) sample. On average, about 1/3 of the original data points are not included in each bootstrap sample. These unused samples are called OOB samples.\n\n\n6.5.2 What is OOB Score?\nEach base learner can be evaluated on its corresponding OOB sample—i.e., the instances it did not see during training. This provides a built-in validation mechanism without needing an explicit validation set or cross-validation.\nThe OOB score is the average performance (e.g., accuracy for classifiers, R² for regressors) of the ensemble evaluated on OOB samples.\n\n🔧 Note: By default, the oob_score option is turned off in scikit-learn. You must explicitly set oob_score=True to enable it, as shown below.\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbagging_clf = BaggingClassifier(\n    base_estimator=DecisionTreeClassifier(),\n    n_estimators=100,\n    oob_score=True,\n    random_state=42\n)\nbagging_clf.fit(X_train, y_train)\n\n# Access the OOB score\nprint(f\"OOB Score: {bagging_clf.oob_score_:.4f}\")\n\nn_estimators = [ 10, 15, 20, 25,30, 35, 40, 45, 50]\nrmse_scores = []\nr2_scores = []\noob_scores = []\noob_rmse_scores = []\nfor n in n_estimators:\n    bagging_reg = BaggingRegressor(estimator=tree_reg, n_estimators=n, oob_score=True, random_state=1,\n                        n_jobs=-1).fit(X_train_final, y_train)\n    y_pred_bagging = bagging_reg.predict(X_test_final)\n    rmse_scores.append(np.sqrt(np.mean((y_test - y_pred_bagging) ** 2)))\n    r2_scores.append(r2_score(y_test, y_pred_bagging))\n    oob_scores.append(bagging_reg.oob_score_)\n    oob_rmse_scores.append(np.sqrt(np.mean((y_train - bagging_reg.oob_prediction_) ** 2)))\n\n# plot the RMSE and R^2 scores against the number of estimators\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(n_estimators, rmse_scores, marker='o')\nplt.plot(n_estimators, oob_rmse_scores, marker='o')\nplt.legend(['RMSE', 'OOB RMSE'])\nplt.title('RMSE vs Number of Estimators')\nplt.xlabel('Number of Estimators')\nplt.ylabel('RMSE')\nplt.xticks(n_estimators)\nplt.grid()\nplt.subplot(1, 2, 2)\nplt.plot(n_estimators, r2_scores, marker='o')\nplt.plot(n_estimators, oob_scores, marker='o')\nplt.legend(['R^2 Score', 'OOB R^2 Score'])\nplt.title('R^2 Score vs Number of Estimators')\nplt.xlabel('Number of Estimators')\nplt.ylabel('R^2 Score')\nplt.xticks(n_estimators)\nplt.grid()\nplt.tight_layout()\nplt.show()\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\n\n\n\n\n\n\n\n\n\nQuick Takeaway\n\nOOB estimates become more reliable as the number of trees grows, with OOB scores closely tracking the test performance after around 30 estimators.\nOOB RMSE is consistently higher and OOB R² is consistently lower than their test counterparts when the ensemble is small, highlighting the instability of OOB estimates with few trees.\n\n\n# get the number of estimators that gives the best OOB RMSE score\nbest_oob_rmse_index = np.argmin(oob_rmse_scores)\nbest_oob_rmse_n_estimators = n_estimators[best_oob_rmse_index]\nbest_oob_rmse_value = oob_rmse_scores[best_oob_rmse_index]\nprint(\"Best number of estimators for OOB RMSE:\", best_oob_rmse_n_estimators)\nprint(\"Best OOB RMSE value:\", round(best_oob_rmse_value, 2))\n\nBest number of estimators for OOB RMSE: 30\nBest OOB RMSE value: 3539.1",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  }
]