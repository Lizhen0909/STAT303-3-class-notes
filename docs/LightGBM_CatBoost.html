<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; LightGBM and CatBoost – Data Science III with python (Class notes)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./smarter_hyper_tuning.html" rel="next">
<link href="./XGBoost.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8da5b4427184b79ecddefad3d342027e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regression_tree_sp25.html">Tree based models</a></li><li class="breadcrumb-item"><a href="./LightGBM_CatBoost.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">LightGBM and CatBoost</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="https://statistics.northwestern.edu/" class="sidebar-logo-link">
      <img src="./NU_Stat_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science III with python (Class notes)</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bias &amp; Variance; KNN</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./KNN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">KNN</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Bias_variance_code.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bias-variance tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Hyperparameter tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Basic Hyperparameter tuning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Tree based models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression_tree_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Regression trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Classification _Tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bagging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bagging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./random_forest.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Random Forests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./adaboost.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Adaptive Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Gradient_Boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Gradient Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./XGBoost.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">XGBoost</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LightGBM_CatBoost.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">LightGBM and CatBoost</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./smarter_hyper_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Smarter Hyperparameter Tuning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./voting_stacking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Advanced Ensemble Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Assignment1_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Assignment 1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Assignment2_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Assignment 2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Assignment3_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Assignment 3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Assignment4_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Assignment 4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Datasets, assignment and project files</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#what-they-share-with-xgboost" id="toc-what-they-share-with-xgboost" class="nav-link active" data-scroll-target="#what-they-share-with-xgboost"><span class="header-section-number">11.1</span> What They Share with XGBoost</a></li>
  <li><a href="#lightgbm" id="toc-lightgbm" class="nav-link" data-scroll-target="#lightgbm"><span class="header-section-number">11.2</span> LightGBM</a>
  <ul>
  <li><a href="#what-is-lightgbm" id="toc-what-is-lightgbm" class="nav-link" data-scroll-target="#what-is-lightgbm"><span class="header-section-number">11.2.1</span> What is LightGBM?</a></li>
  <li><a href="#what-makes-lightgbm-lighting-fast" id="toc-what-makes-lightgbm-lighting-fast" class="nav-link" data-scroll-target="#what-makes-lightgbm-lighting-fast"><span class="header-section-number">11.2.2</span> What Makes LightGBM Lighting Fast?</a>
  <ul class="collapse">
  <li><a href="#leaf-wise-tree-growth" id="toc-leaf-wise-tree-growth" class="nav-link" data-scroll-target="#leaf-wise-tree-growth"><span class="header-section-number">11.2.2.1</span> Leaf-Wise Tree Growth</a></li>
  <li><a href="#goss-gradient-based-one-side-sampling" id="toc-goss-gradient-based-one-side-sampling" class="nav-link" data-scroll-target="#goss-gradient-based-one-side-sampling"><span class="header-section-number">11.2.2.2</span> GOSS (Gradient-based One-Side Sampling)</a></li>
  <li><a href="#exclusive-feature-bundling-efb" id="toc-exclusive-feature-bundling-efb" class="nav-link" data-scroll-target="#exclusive-feature-bundling-efb"><span class="header-section-number">11.2.2.3</span> Exclusive Feature Bundling (EFB)</a></li>
  </ul></li>
  <li><a href="#using-lightgbm" id="toc-using-lightgbm" class="nav-link" data-scroll-target="#using-lightgbm"><span class="header-section-number">11.2.3</span> Using LightGBM</a>
  <ul class="collapse">
  <li><a href="#core-lightgbm-hyperparameters" id="toc-core-lightgbm-hyperparameters" class="nav-link" data-scroll-target="#core-lightgbm-hyperparameters"><span class="header-section-number">11.2.3.1</span> Core LightGBM Hyperparameters</a></li>
  <li><a href="#building-a-baseline-model-using-lightgbms-native-categorical-feature-support" id="toc-building-a-baseline-model-using-lightgbms-native-categorical-feature-support" class="nav-link" data-scroll-target="#building-a-baseline-model-using-lightgbms-native-categorical-feature-support"><span class="header-section-number">11.2.3.2</span> Building a Baseline Model Using LightGBM’s Native Categorical Feature Support</a></li>
  <li><a href="#enabling-goss-and-efb-in-lightgbm" id="toc-enabling-goss-and-efb-in-lightgbm" class="nav-link" data-scroll-target="#enabling-goss-and-efb-in-lightgbm"><span class="header-section-number">11.2.3.3</span> Enabling GOSS and EFB in LightGBM</a></li>
  <li><a href="#tuning-top_rate-and-other_rate-in-goss" id="toc-tuning-top_rate-and-other_rate-in-goss" class="nav-link" data-scroll-target="#tuning-top_rate-and-other_rate-in-goss"><span class="header-section-number">11.2.3.4</span> Tuning <code>top_rate</code> and <code>other_rate</code> in GOSS</a></li>
  <li><a href="#optimizing-lightgbm-with-bayessearchcv" id="toc-optimizing-lightgbm-with-bayessearchcv" class="nav-link" data-scroll-target="#optimizing-lightgbm-with-bayessearchcv"><span class="header-section-number">11.2.3.5</span> Optimizing LightGBM with <code>BayesSearchCV</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#catboost" id="toc-catboost" class="nav-link" data-scroll-target="#catboost"><span class="header-section-number">11.3</span> CatBoost</a>
  <ul>
  <li><a href="#what-is-catboost" id="toc-what-is-catboost" class="nav-link" data-scroll-target="#what-is-catboost"><span class="header-section-number">11.3.1</span> What is CatBoost?</a></li>
  <li><a href="#what-makes-catboost-unique" id="toc-what-makes-catboost-unique" class="nav-link" data-scroll-target="#what-makes-catboost-unique"><span class="header-section-number">11.3.2</span> What Makes CatBoost Unique?</a>
  <ul class="collapse">
  <li><a href="#symmetric-oblivious-trees" id="toc-symmetric-oblivious-trees" class="nav-link" data-scroll-target="#symmetric-oblivious-trees"><span class="header-section-number">11.3.2.1</span> Symmetric (Oblivious) Trees</a></li>
  <li><a href="#advanced-categorical-feature-handling" id="toc-advanced-categorical-feature-handling" class="nav-link" data-scroll-target="#advanced-categorical-feature-handling"><span class="header-section-number">11.3.2.2</span> Advanced Categorical Feature Handling</a></li>
  <li><a href="#ordered-boosting-vs.-standard-boosting" id="toc-ordered-boosting-vs.-standard-boosting" class="nav-link" data-scroll-target="#ordered-boosting-vs.-standard-boosting"><span class="header-section-number">11.3.2.3</span> Ordered Boosting (vs.&nbsp;Standard Boosting)</a></li>
  <li><a href="#handling-of-text-and-embedding-features" id="toc-handling-of-text-and-embedding-features" class="nav-link" data-scroll-target="#handling-of-text-and-embedding-features"><span class="header-section-number">11.3.2.4</span> Handling of Text and Embedding Features</a></li>
  <li><a href="#ease-of-use-and-defaults" id="toc-ease-of-use-and-defaults" class="nav-link" data-scroll-target="#ease-of-use-and-defaults"><span class="header-section-number">11.3.2.5</span> Ease of Use and Defaults</a></li>
  </ul></li>
  <li><a href="#using-catboost" id="toc-using-catboost" class="nav-link" data-scroll-target="#using-catboost"><span class="header-section-number">11.3.3</span> Using CatBoost</a></li>
  <li><a href="#installation" id="toc-installation" class="nav-link" data-scroll-target="#installation"><span class="header-section-number">11.3.4</span> Installation</a></li>
  <li><a href="#catboost-for-regression" id="toc-catboost-for-regression" class="nav-link" data-scroll-target="#catboost-for-regression"><span class="header-section-number">11.3.5</span> CatBoost for Regression</a></li>
  <li><a href="#tuning-catboostregressor" id="toc-tuning-catboostregressor" class="nav-link" data-scroll-target="#tuning-catboostregressor"><span class="header-section-number">11.3.6</span> Tuning <code>CatBoostRegressor</code></a>
  <ul class="collapse">
  <li><a href="#hyperparameters-not-used-in-catboost" id="toc-hyperparameters-not-used-in-catboost" class="nav-link" data-scroll-target="#hyperparameters-not-used-in-catboost"><span class="header-section-number">11.3.6.1</span> ❌ Hyperparameters <strong>not used</strong> in CatBoost:</a></li>
  <li><a href="#unique-hyperparameters-in-catboost" id="toc-unique-hyperparameters-in-catboost" class="nav-link" data-scroll-target="#unique-hyperparameters-in-catboost"><span class="header-section-number">11.3.6.2</span> ✅ Unique Hyperparameters in CatBoost</a></li>
  </ul></li>
  <li><a href="#when-to-use-catboost-over-xgboost" id="toc-when-to-use-catboost-over-xgboost" class="nav-link" data-scroll-target="#when-to-use-catboost-over-xgboost"><span class="header-section-number">11.3.7</span> When to Use <strong>CatBoost</strong> Over <strong>XGBoost</strong></a></li>
  </ul></li>
  <li><a href="#handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost" id="toc-handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost" class="nav-link" data-scroll-target="#handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost"><span class="header-section-number">11.4</span> Handling Imbalanced Classification: XGBoost vs.&nbsp;LightGBM vs.&nbsp;CatBoost</a></li>
  <li><a href="#summary-xgboost-vs.-lightgbm-vs.-catboost" id="toc-summary-xgboost-vs.-lightgbm-vs.-catboost" class="nav-link" data-scroll-target="#summary-xgboost-vs.-lightgbm-vs.-catboost"><span class="header-section-number">11.5</span> Summary: XGBoost vs.&nbsp;LightGBM vs.&nbsp;CatBoost</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">11.6</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regression_tree_sp25.html">Tree based models</a></li><li class="breadcrumb-item"><a href="./LightGBM_CatBoost.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">LightGBM and CatBoost</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">LightGBM and CatBoost</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="7170aa24" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display" data-execution_count="6">
<img src="./Datasets/xgboost_lightgbm_catboost.png">
</div>
</div>
<p>Gradient boosting is one of the most powerful techniques for <strong>structured/tabular data</strong>, often serving as the <strong>go-to choice for tabular data tasks in machine learning competitions</strong>.</p>
<p>In the previous chapter, we explored <strong>XGBoost</strong> in detail—covering its <strong>optimization objective</strong>, <strong>regularization techniques</strong>, <strong>split finding algorithms</strong>, and its role as a cornerstone in modern <strong>tabular modeling</strong>.</p>
<p>While <strong>XGBoost</strong> is highly effective, other libraries have introduced innovations to address challenges like scalability and categorical feature handling. In this chapter, we focus on two <strong>advanced gradient boosting libraries</strong> and their <strong>key innovations</strong>:</p>
<ul>
<li><p><strong>LightGBM</strong>: Developed by Microsoft, <strong>LightGBM</strong> is optimized for <strong>speed and scalability</strong>. It introduces <strong>Gradient-based One-Side Sampling (GOSS)</strong>, which prioritizes instances with larger gradients for faster training, and <strong>Exclusive Feature Bundling (EFB)</strong>, which reduces memory usage by grouping mutually exclusive features. <strong>LightGBM</strong> also supports <strong>categorical features</strong> efficiently, making it ideal for <strong>large datasets and high-dimensional features</strong>.</p></li>
<li><p><strong>CatBoost</strong>: Created by Yandex, <strong>CatBoost</strong> excels in its <strong>optimized support for categorical features</strong> through advanced target-based encoding. It uses <strong>ordered boosting</strong> to prevent prediction shift and reduce overfitting, often performing well with <strong>minimal tuning</strong> on datasets rich in <strong>categorical variables</strong>.</p></li>
</ul>
<section id="what-they-share-with-xgboost" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="what-they-share-with-xgboost"><span class="header-section-number">11.1</span> What They Share with XGBoost</h2>
<p>While <strong>LightGBM</strong> and <strong>CatBoost</strong> introduce unique innovations, they build on the same foundational principles as <strong>XGBoost</strong>:</p>
<ul>
<li>They use a <strong>similar objective function structure</strong> (loss plus regularization) to balance model fit and complexity.</li>
<li>They apply a <strong>second-order Taylor approximation</strong> for efficient optimization of the loss function.</li>
<li>They support <strong>histogram-based split-finding algorithms</strong> to speed up training, with <strong>LightGBM</strong> particularly optimized for this approach.</li>
<li>They support <strong>parallel tree building</strong>, significantly accelerating training compared to traditional gradient boosting.</li>
<li>They provide <strong>native support for categorical encoding</strong>, reducing the need for preprocessing (e.g., one-hot encoding), with <strong>XGBoost</strong> introducing this feature starting in version 1.5.0.</li>
</ul>
</section>
<section id="lightgbm" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="lightgbm"><span class="header-section-number">11.2</span> LightGBM</h2>
<section id="what-is-lightgbm" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="what-is-lightgbm"><span class="header-section-number">11.2.1</span> What is LightGBM?</h3>
<p><strong>LightGBM</strong> (Light Gradient Boosting Machine) is a high-performance gradient boosting framework developed by Microsoft in 2017. Designed for <strong>speed and scalability</strong>, it is typically faster than <strong>XGBoost</strong> due to its optimized algorithms, with accuracy that is generally comparable but may require tuning. LightGBM excels in:</p>
<ul>
<li><strong>Handling large-scale datasets</strong> with many rows and features</li>
<li><strong>Achieving high speed and memory efficiency</strong> through innovations like <strong>Gradient-based One-Side Sampling (GOSS)</strong>, <strong>Exclusive Feature Bundling (EFB)</strong>, and <strong>histogram-based splitting</strong></li>
</ul>
<p>Like <strong>XGBoost</strong> and <strong>CatBoost</strong>, it supports <strong>native categorical encoding</strong> and <strong>parallel tree building</strong>, enhancing its efficiency for tabular data tasks. See the <a href="https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf">LightGBM paper</a> for details on its algorithmic innovations and performance benchmarks.</p>
</section>
<section id="what-makes-lightgbm-lighting-fast" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="what-makes-lightgbm-lighting-fast"><span class="header-section-number">11.2.2</span> What Makes LightGBM Lighting Fast?</h3>
<p>LightGBM often outperforms XGBoost in <strong>training speed</strong> and <strong>memory efficiency</strong>, thanks to several key innovations:</p>
<section id="leaf-wise-tree-growth" class="level4" data-number="11.2.2.1">
<h4 data-number="11.2.2.1" class="anchored" data-anchor-id="leaf-wise-tree-growth"><span class="header-section-number">11.2.2.1</span> Leaf-Wise Tree Growth</h4>
<ul>
<li>LightGBM splits the <strong>leaf with the largest potential loss reduction</strong>, unlike XGBoost’s <strong>level-wise</strong> approach.</li>
<li>This leads to <strong>lower loss per tree</strong>, making learning more efficient — though it may <strong>overfit</strong> without proper regularization.</li>
<li>Main controls:
<ul>
<li><code>num_leaves</code>: primary control for tree complexity</li>
<li><code>max_depth</code>: optional constraint to prevent overfitting</li>
</ul></li>
</ul>
</section>
<section id="goss-gradient-based-one-side-sampling" class="level4" data-number="11.2.2.2">
<h4 data-number="11.2.2.2" class="anchored" data-anchor-id="goss-gradient-based-one-side-sampling"><span class="header-section-number">11.2.2.2</span> GOSS (Gradient-based One-Side Sampling)</h4>
<ul>
<li>GOSS improves speed by:
<ul>
<li><strong>Retaining all instances with large gradients</strong> (i.e., high error)</li>
<li><strong>Randomly sampling those with small gradients</strong></li>
</ul></li>
<li>This reduces the dataset size while maintaining accurate split decisions.</li>
</ul>
<p>In gradient boosting, the tree is fit to the <strong>negative gradient</strong> of the loss:</p>
<p><span class="math display">\[
r_m = -\left[ \frac{\partial L(y_i, f(x_i))}{\partial f(x_i)} \right]_{f = f_{m-1}}
\]</span></p>
<p>Observations with larger gradients have more influence on reducing the loss — GOSS prioritizes those. This approach reduces the number of data points processed per iteration, speeding up training while preserving important information.</p>
</section>
<section id="exclusive-feature-bundling-efb" class="level4" data-number="11.2.2.3">
<h4 data-number="11.2.2.3" class="anchored" data-anchor-id="exclusive-feature-bundling-efb"><span class="header-section-number">11.2.2.3</span> Exclusive Feature Bundling (EFB)</h4>
<ul>
<li><strong>EFB</strong> reduces memory usage and accelerates training by bundling <strong>mutually exclusive</strong> features (i.e., features that rarely have non-zero values simultaneously) in <strong>high-dimensional sparse feature spaces</strong>.</li>
<li>This is particularly effective for datasets with <strong>many categorical variables</strong> or <strong>one-hot encoded features</strong>, avoiding the memory overhead of one-hot encoding and complementing LightGBM’s <strong>native categorical support</strong>.</li>
</ul>
<p><strong>Example</strong>:<br>
The table below shows two mutually exclusive features, <code>feature1</code> and <code>feature2</code>, bundled into a single <code>feature_bundle</code> by assigning distinct value ranges (e.g., 1–4 for <code>feature1</code>, 5–6 for <code>feature2</code>):</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><code>feature1</code></th>
<th><code>feature2</code></th>
<th><code>feature_bundle</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>2</td>
<td>6</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>5</td>
</tr>
<tr class="odd">
<td>0</td>
<td>2</td>
<td>6</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0</td>
<td>2</td>
</tr>
<tr class="even">
<td>3</td>
<td>0</td>
<td>3</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0</td>
<td>4</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Hyperparameter for EFB</strong>:
<ul>
<li><code>enable_bundle</code>: Enabled by default to activate automatic bundling.</li>
<li><code>max_conflict_rate</code>: Controls the maximum conflict rate for bundling (default: 0.0, no conflicts allowed); adjust (e.g., 0.1) to allow minor overlaps.</li>
</ul></li>
</ul>
<p>This approach reduces the number of features processed per iteration, speeding up training while preserving important information.</p>
<p>Combined with <strong>GOSS</strong>, <strong>EFB</strong> makes <strong>LightGBM</strong> especially well-suited for <strong>large-scale, sparse, tabular datasets</strong>, offering <strong>speed and scalability</strong> while maintaining comparable accuracy with proper tuning.</p>
</section>
</section>
<section id="using-lightgbm" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="using-lightgbm"><span class="header-section-number">11.2.3</span> Using LightGBM</h3>
<p>Although <strong>LightGBM is not part of Scikit-learn</strong>, it provides a <strong>Scikit-learn-compatible API</strong> through the <code>lightgbm.sklearn</code> module. This allows you to use LightGBM models seamlessly with Scikit-learn tools such as <code>Pipeline</code>, <code>GridSearchCV</code>, and <code>cross_val_score</code>.</p>
<p>The main classes are:</p>
<ul>
<li><a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html"><code>LGBMRegressor</code></a>: for regression tasks<br>
</li>
<li><a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html"><code>LGBMClassifier</code></a>: for classification tasks</li>
</ul>
<p>To install the package:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>pip install lightgbm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p><strong>Note:</strong> LightGBM is a separate library, not part of Scikit-learn, but it provides a <strong>Scikit-learn-compatible API</strong> via <code>LGBMClassifier</code> and <code>LGBMRegressor</code>.<br>
This makes it easy to integrate LightGBM models into Scikit-learn workflows such as <code>Pipeline</code>, <code>GridSearchCV</code>, and <code>cross_val_score</code>.</p>
</blockquote>
<section id="core-lightgbm-hyperparameters" class="level4" data-number="11.2.3.1">
<h4 data-number="11.2.3.1" class="anchored" data-anchor-id="core-lightgbm-hyperparameters"><span class="header-section-number">11.2.3.1</span> Core LightGBM Hyperparameters</h4>
<p><strong>Core Tree Structure</strong>:</p>
<ul>
<li><code>num_leaves</code>: Maximum number of leaves (terminal nodes) per tree.</li>
<li><code>min_data_in_leaf</code>: Minimum number of data points required in a leaf.</li>
<li><code>max_depth</code>: Maximum depth of a tree (used to control overfitting).</li>
</ul>
<p><strong>Learning Control and Regularization</strong>:</p>
<ul>
<li><code>learning_rate (η)</code>: Shrinks the contribution of each tree.</li>
<li><code>n_estimators</code>: Number of boosting rounds.</li>
<li><code>lambda_l1</code> / <code>lambda_l2</code>: L1 and L2 regularization on leaf weights.</li>
<li><code>min_gain_to_split</code>: Minimum loss reduction required to make a further split (structure regularization).</li>
</ul>
<p><strong>Data Handling</strong>:</p>
<ul>
<li><code>feature_fraction</code>: Fraction of features randomly sampled for each tree (a.k.a. <code>colsample_bytree</code> in XGBoost).</li>
<li><code>bagging_fraction</code>: Fraction of data randomly sampled for each iteration.</li>
<li><code>bagging_freq</code>: Frequency (in iterations) to perform bagging.</li>
<li><code>categorical_feature</code>: Specifies which features are categorical (enables native handling).</li>
</ul>
<p><strong>Speed vs.&nbsp;Accuracy Trade-offs</strong>:</p>
<ul>
<li><code>max_bin</code>: Number of bins used to bucket continuous features.</li>
<li><code>data_sample_strategy</code> : <code>bagging</code> or <code>goss</code></li>
<li><code>top_rate</code> <em>(<code>goss</code> only)</em>: Fraction of instances with the largest gradients to keep.</li>
<li><code>other_rate</code> <em>(<code>goss</code> only)</em>: Fraction of small-gradient instances to randomly sample. -<code>enable_bundle</code>: set this to true to spped up the training for sparse datasets</li>
</ul>
<p><strong>Optimization Control</strong>:</p>
<ul>
<li><code>boosting</code>: Type of boosting algorithm (<code>gbdt</code>, <code>dart</code>, <code>rf</code>, etc.).</li>
<li><code>early_stopping_rounds</code>: Stops training if the validation score doesn’t improve over a set number of rounds.</li>
</ul>
<p><strong>Imbalanced Data</strong></p>
<ul>
<li><code>scale_pos_weight</code>: Manually sets the weight for the positive class in binary classification.</li>
<li><code>is_unbalance</code>: Automatically adjusts class weights based on the training data distribution.</li>
</ul>
<blockquote class="blockquote">
<p>⚠️ These two options are <strong>mutually exclusive</strong> — use <strong>only one</strong>. If both are set, <code>scale_pos_weight</code> takes priority.</p>
</blockquote>
<p>For full details and advanced options, see the <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html">LightGBM Parameters Guide</a>.</p>
<div id="f819f995" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, GridSearchCV</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder, StandardScaler</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBRegressor, XGBClassifier</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> lightgbm <span class="im">as</span> lgb</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt <span class="im">import</span> BayesSearchCV</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt.space <span class="im">import</span> Real, Categorical, Integer</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt.plots <span class="im">import</span> plot_objective, plot_histogram, plot_convergence</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll continue to use the same datasets that we have been using throughout the course.</p>
<div id="a9036ef3" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>car <span class="op">=</span> pd.read_csv(<span class="st">'Datasets/car.csv'</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>car.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">brand</th>
<th data-quarto-table-cell-role="th">model</th>
<th data-quarto-table-cell-role="th">year</th>
<th data-quarto-table-cell-role="th">transmission</th>
<th data-quarto-table-cell-role="th">mileage</th>
<th data-quarto-table-cell-role="th">fuelType</th>
<th data-quarto-table-cell-role="th">tax</th>
<th data-quarto-table-cell-role="th">mpg</th>
<th data-quarto-table-cell-role="th">engineSize</th>
<th data-quarto-table-cell-role="th">price</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>vw</td>
<td>Beetle</td>
<td>2014</td>
<td>Manual</td>
<td>55457</td>
<td>Diesel</td>
<td>30</td>
<td>65.3266</td>
<td>1.6</td>
<td>7490</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>vauxhall</td>
<td>GTC</td>
<td>2017</td>
<td>Manual</td>
<td>15630</td>
<td>Petrol</td>
<td>145</td>
<td>47.2049</td>
<td>1.4</td>
<td>10998</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>merc</td>
<td>G Class</td>
<td>2012</td>
<td>Automatic</td>
<td>43000</td>
<td>Diesel</td>
<td>570</td>
<td>25.1172</td>
<td>3.0</td>
<td>44990</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>audi</td>
<td>RS5</td>
<td>2019</td>
<td>Automatic</td>
<td>10</td>
<td>Petrol</td>
<td>145</td>
<td>30.5593</td>
<td>2.9</td>
<td>51990</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>merc</td>
<td>X-CLASS</td>
<td>2018</td>
<td>Automatic</td>
<td>14000</td>
<td>Diesel</td>
<td>240</td>
<td>35.7168</td>
<td>2.3</td>
<td>28990</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="db6b5f99" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> car.drop(columns<span class="op">=</span>[<span class="st">'price'</span>])</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> car[<span class="st">'price'</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the categorical columns and put them in a list</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>categorical_feature <span class="op">=</span> X.select_dtypes(include<span class="op">=</span>[<span class="st">'object'</span>]).columns.tolist()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the numerical columns and put them in a list</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>numerical_feature <span class="op">=</span> X.select_dtypes(include<span class="op">=</span>[<span class="st">'int64'</span>, <span class="st">'float64'</span>]).columns.tolist()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the categorical columns to category type</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> categorical_feature:</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    X[col] <span class="op">=</span> X[col].astype(<span class="st">'category'</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="building-a-baseline-model-using-lightgbms-native-categorical-feature-support" class="level4" data-number="11.2.3.2">
<h4 data-number="11.2.3.2" class="anchored" data-anchor-id="building-a-baseline-model-using-lightgbms-native-categorical-feature-support"><span class="header-section-number">11.2.3.2</span> Building a Baseline Model Using LightGBM’s Native Categorical Feature Support</h4>
<p>LightGBM provides <strong>built-in support for handling categorical features</strong>, eliminating the need for manual encoding (like one-hot or ordinal encoding). By directly passing categorical column names or indices to the model, LightGBM can internally apply efficient encoding and optimized split finding for categorical variables.</p>
<p>In this section, we’ll use this native capability to <strong>quickly build a baseline model</strong>, taking advantage of LightGBM’s efficiency with structured data that includes categorical columns.</p>
<p>This baseline model serves as a <strong>starting point</strong> for comparison against more advanced tuning</p>
<div id="38736472" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== 1. Baseline Model =====</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">===== Baseline LightGBM Model ====="</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the LightGBM regressor</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lgb.LGBMRegressor(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with categorical features specified</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>model.fit(</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    X_train, </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    y_train,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    categorical_feature<span class="op">=</span>categorical_feature</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the test set</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate evaluation metrics</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> root_mean_squared_error(y_test, y_pred)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r2_score(y_test, y_pred)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Output results</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE: </span><span class="sc">{</span>rmse<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R²: </span><span class="sc">{</span>r2<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
===== Baseline LightGBM Model =====
Test RMSE: 3680.8999
Test R²: 0.9538
CPU times: total: 875 ms
Wall time: 82.6 ms</code></pre>
</div>
</div>
</section>
<section id="enabling-goss-and-efb-in-lightgbm" class="level4" data-number="11.2.3.3">
<h4 data-number="11.2.3.3" class="anchored" data-anchor-id="enabling-goss-and-efb-in-lightgbm"><span class="header-section-number">11.2.3.3</span> Enabling GOSS and EFB in LightGBM</h4>
<section id="goss-is-not-enabled-by-default." class="level5" data-number="11.2.3.3.1">
<h5 data-number="11.2.3.3.1" class="anchored" data-anchor-id="goss-is-not-enabled-by-default."><span class="header-section-number">11.2.3.3.1</span> GOSS is <strong>not enabled by default</strong>.</h5>
<p>The default boosting type is <code>gbdt</code> (traditional Gradient Boosting Decision Tree), which uses all data instances for each iteration without sampling based on gradients.</p>
<p>To use GOSS, you must explicitly set the <code>boosting_type</code> parameter to <code>goss</code> in the model configuration. When you do this, LightGBM uses GOSS with default values for its specific hyperparameters:</p>
<ul>
<li><code>top_rate</code>: 0.2 (keeps 20% of instances with large gradients)</li>
<li><code>other_rate</code>: 0.1 (randomly samples 10% of instances with small gradients)</li>
</ul>
</section>
<section id="efb-is-enabled-by-default" class="level5" data-number="11.2.3.3.2">
<h5 data-number="11.2.3.3.2" class="anchored" data-anchor-id="efb-is-enabled-by-default"><span class="header-section-number">11.2.3.3.2</span> EFB is <strong>enabled by default</strong></h5>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>enable_bundle <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This optimization reduces dimensionality by bundling mutually exclusive sparse features, such as those resulting from one-hot encoding.</p>
<p>⚠️ Note: In our car dataset, the data size is small and there are only a few categorical features, so these optimizations may not have a noticeable impact. However, for large-scale datasets with many categorical features, enabling GOSS and EFB is highly recommended to improve training efficiency and reduce memory usage.</p>
<div id="e1677750" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== 2. LightGBM with GOSS Sampling =====</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">===== LightGBM with GOSS Sampling ====="</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the LightGBM regressor with GOSS</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>model_goss <span class="op">=</span> lgb.LGBMRegressor(</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    boosting_type<span class="op">=</span><span class="st">'goss'</span>,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with categorical features specified</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>model_goss.fit(</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    X_train,</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    y_train,</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    categorical_feature<span class="op">=</span>categorical_feature</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the test set</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>y_pred_goss <span class="op">=</span> model_goss.predict(X_test)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate evaluation metrics</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>rmse_goss <span class="op">=</span> root_mean_squared_error(y_test, y_pred_goss)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>r2_goss <span class="op">=</span> r2_score(y_test, y_pred_goss)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Output results</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE (GOSS): </span><span class="sc">{</span>rmse_goss<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R² (GOSS): </span><span class="sc">{</span>r2_goss<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
===== LightGBM with GOSS Sampling =====
Test RMSE (GOSS): 3510.7726
Test R² (GOSS): 0.9580
CPU times: total: 766 ms
Wall time: 79.6 ms</code></pre>
</div>
</div>
</section>
</section>
<section id="tuning-top_rate-and-other_rate-in-goss" class="level4" data-number="11.2.3.4">
<h4 data-number="11.2.3.4" class="anchored" data-anchor-id="tuning-top_rate-and-other_rate-in-goss"><span class="header-section-number">11.2.3.4</span> Tuning <code>top_rate</code> and <code>other_rate</code> in GOSS</h4>
<p>Even with this small dataset, we observed a <strong>shorter execution time</strong> and a <strong>slight improvement in performance</strong> using GOSS.</p>
<p>The default settings are reasonable for many datasets. To leverage <strong>GOSS</strong> more effectively, optimize performance by tuning <strong>top_rate</strong> (e.g., 0.1, 0.2, 0.3, 0.4) and <strong>other_rate</strong> (e.g., 0.05, 0.1, 0.15, 0.2) using cross-validation, especially for large or noisy datasets.</p>
<blockquote class="blockquote">
<p>⚠️ <strong>Note:</strong> When using <code>boosting_type='goss'</code>, LightGBM requires that<br>
<strong><code>top_rate + other_rate ≤ 1.0</code></strong><br>
This constraint ensures that the combined sample used for training does not exceed the size of the full dataset.</p>
</blockquote>
<div id="09e449da" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tuning the top_rate and other_rate parameters</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the LightGBM regressor with GOSS</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>model_goss_tune <span class="op">=</span> lgb.LGBMRegressor(</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    boosting_type<span class="op">=</span><span class="st">'goss'</span>,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameter grid for tuning</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'top_rate'</span>: Real(<span class="fl">0.1</span>, <span class="fl">0.6</span>, prior<span class="op">=</span><span class="st">'uniform'</span>),</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'other_rate'</span>: Real(<span class="fl">0.1</span>, <span class="fl">0.4</span>, prior<span class="op">=</span><span class="st">'uniform'</span>),</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the BayesSearchCV object</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> BayesSearchCV(</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    model_goss_tune,</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    param_grid,</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>opt.fit(</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    X_train,</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    y_train,</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    categorical_feature<span class="op">=</span>categorical_feature</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="co"># the best parameters</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters found: "</span>, opt.best_params_)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the test set</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>y_pred_opt <span class="op">=</span> opt.predict(X_test)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate evaluation metrics</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>rmse_opt <span class="op">=</span> root_mean_squared_error(y_test, y_pred_opt)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>r2_opt <span class="op">=</span> r2_score(y_test, y_pred_opt)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Output results</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE (GOSS with tuning): </span><span class="sc">{</span>rmse_opt<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R² (GOSS with tuning): </span><span class="sc">{</span>r2_opt<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best parameters found:  OrderedDict({'other_rate': 0.33986603248215197, 'top_rate': 0.31901459322046166})
Test RMSE (GOSS with tuning): 3458.7664
Test R² (GOSS with tuning): 0.9592</code></pre>
</div>
</div>
</section>
<section id="optimizing-lightgbm-with-bayessearchcv" class="level4" data-number="11.2.3.5">
<h4 data-number="11.2.3.5" class="anchored" data-anchor-id="optimizing-lightgbm-with-bayessearchcv"><span class="header-section-number">11.2.3.5</span> Optimizing LightGBM with <code>BayesSearchCV</code></h4>
<p><code>BayesSearchCV</code> from <code>scikit-optimize</code> provides an efficient way to tune hyperparameters. Here’s how to set this up:</p>
<div id="35e60b3b" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== 2. Hyperparameter Tuning with Bayesian Optimization =====</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameter space for Bayesian optimization</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>param_space <span class="op">=</span> {</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'num_leaves'</span>: Integer(<span class="dv">20</span>, <span class="dv">100</span>),</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: Integer(<span class="dv">5</span>, <span class="dv">50</span>),</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_data_in_leaf'</span>: Integer(<span class="dv">1</span>, <span class="dv">100</span>),</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: Real(<span class="fl">0.01</span>, <span class="fl">0.5</span>, prior<span class="op">=</span><span class="st">'uniform'</span>),</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: Integer(<span class="dv">50</span>, <span class="dv">500</span>),</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'top_rate'</span>: Real(<span class="fl">0.1</span>, <span class="fl">0.6</span>, prior<span class="op">=</span><span class="st">'uniform'</span>),</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'other_rate'</span>: Real(<span class="fl">0.1</span>, <span class="fl">0.4</span>, prior<span class="op">=</span><span class="st">'uniform'</span>),</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the Bayesian search object</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>bayes_search <span class="op">=</span> BayesSearchCV(</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># using verbose=-1 to suppress warnings</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># using n_jobs=-1 to use all available cores</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># using random_state=42 for reproducibility</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>lgb.LGBMRegressor( categorical_feature<span class="op">=</span>categorical_feature, random_state<span class="op">=</span><span class="dv">42</span>, boosting_type<span class="op">=</span><span class="st">'goss'</span>, verbose<span class="op">=-</span><span class="dv">1</span>),</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define the parameter space for Bayesian optimization</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    search_spaces<span class="op">=</span>param_space,</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_root_mean_squared_error'</span>,</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the Bayesian search object to the training data</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>bayes_search.fit(X_train, y_train)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best parameters and score</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> bayes_search.best_params_</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> bayes_search.best_score_</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best Parameters: </span><span class="sc">{</span>best_params<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best Score: </span><span class="sc">{</span>best_score<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best model</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> bayes_search.best_estimator_</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test set</span></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>y_pred_bayes <span class="op">=</span> best_model.predict(X_test)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate RMSE and R2 score for the best model</span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>rmse_bayes <span class="op">=</span> root_mean_squared_error(y_test, y_pred_bayes)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>r2_bayes <span class="op">=</span> r2_score(y_test, y_pred_bayes)</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE (Bayesian Optimized): </span><span class="sc">{</span>rmse_bayes<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R2 Score (Bayesian Optimized): </span><span class="sc">{</span>r2_bayes<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best Parameters: OrderedDict({'learning_rate': 0.31777940485083805, 'max_depth': 5, 'min_data_in_leaf': 47, 'n_estimators': 369, 'num_leaves': 20, 'other_rate': 0.4, 'top_rate': 0.6})
Best Score: -3361.8218393725633
RMSE (Bayesian Optimized): 3071.418344800289
R2 Score (Bayesian Optimized): 0.9678447743461689
CPU times: total: 49.4 s
Wall time: 1min 35s</code></pre>
</div>
</div>
<p><strong>LightGBM</strong> achieved performance comparable to <strong>XGBoost</strong>. By leveraging <strong>GOSS</strong> (Gradient-based One-Side Sampling) for gradient sampling and <strong>EFB</strong> (Exclusive Feature Bundling) for feature reduction, it improved training speed slightly on large, sparse datasets. These optimizations, along with <strong>native categorical feature support</strong>, can also help reduce cross-validation tuning time by simplifying the feature space and accelerating learning.</p>
</section>
</section>
</section>
<section id="catboost" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="catboost"><span class="header-section-number">11.3</span> CatBoost</h2>
<section id="what-is-catboost" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="what-is-catboost"><span class="header-section-number">11.3.1</span> What is CatBoost?</h3>
<p><strong>CatBoost</strong> (short for <em>Categorical Boosting</em>) is a high-performance gradient boosting framework developed by <strong>Yandex</strong>. While several modern boosting frameworks support native categorical features, CatBoost uses <strong>optimized encoding strategies</strong>—such as <strong>ordered target statistics</strong>—that often lead to better performance with less risk of overfitting on <strong>categorical-heavy</strong> data.</p>
<p>In addition to its categorical handling, CatBoost includes features like <strong>ordered boosting</strong> and strong <strong>regularization</strong>, which help reduce overfitting. It typically requires <strong>less hyperparameter tuning</strong> than XGBoost or LightGBM, making it more <strong>user-friendly</strong>, especially on datasets with many categorical variables.</p>
</section>
<section id="what-makes-catboost-unique" class="level3" data-number="11.3.2">
<h3 data-number="11.3.2" class="anchored" data-anchor-id="what-makes-catboost-unique"><span class="header-section-number">11.3.2</span> What Makes CatBoost Unique?</h3>
<p>CatBoost introduces several <strong>key innovations</strong> that set it apart from other gradient boosting frameworks:</p>
<section id="symmetric-oblivious-trees" class="level4" data-number="11.3.2.1">
<h4 data-number="11.3.2.1" class="anchored" data-anchor-id="symmetric-oblivious-trees"><span class="header-section-number">11.3.2.1</span> Symmetric (Oblivious) Trees</h4>
<p>CatBoost builds <strong>symmetric (oblivious) decision trees</strong>, where the same feature and split threshold are used at each level of the tree across all nodes. This structure results in:</p>
<ul>
<li><strong>Robust to noise</strong></li>
<li><strong>Improved regularization</strong></li>
<li><strong>Faster inference times</strong></li>
</ul>
</section>
<section id="advanced-categorical-feature-handling" class="level4" data-number="11.3.2.2">
<h4 data-number="11.3.2.2" class="anchored" data-anchor-id="advanced-categorical-feature-handling"><span class="header-section-number">11.3.2.2</span> Advanced Categorical Feature Handling</h4>
<p>CatBoost can <strong>natively process categorical features</strong> using an approach based on <strong>ordered target statistics</strong>, which:</p>
<ul>
<li>Avoids target leakage during training</li>
<li>Typically outperforms traditional encodings like one-hot or label encoding</li>
</ul>
</section>
<section id="ordered-boosting-vs.-standard-boosting" class="level4" data-number="11.3.2.3">
<h4 data-number="11.3.2.3" class="anchored" data-anchor-id="ordered-boosting-vs.-standard-boosting"><span class="header-section-number">11.3.2.3</span> Ordered Boosting (vs.&nbsp;Standard Boosting)</h4>
<p>Traditional gradient boosting algorithms often suffer from <strong>prediction shift</strong>, a form of overfitting that occurs when the model uses the same data to compute residuals and to fit new trees.</p>
<p>CatBoost addresses this with <strong>ordered boosting</strong>, a permutation-driven strategy that builds each tree on one subset of data and computes residuals on another (unseen) subset.</p>
<p>Recall that gradient boosting fits trees on the gradient of the loss function:</p>
<p><span class="math display">\[
r_m = -\left[ \frac{\partial L(y_i, f(x_i))}{\partial f(x_i)} \right]_{f = f_{m-1}}
\]</span></p>
<p>In classic boosting, this gradient is calculated using the same training observations that were used to fit the model, which leads to target leakage.</p>
<p>In contrast, CatBoost:</p>
<ul>
<li>Shuffles the data at each iteration</li>
<li>Computes residuals for an observation <strong>only from prior observations</strong> in the permutation</li>
<li>Ensures that <strong>each gradient estimate is based on unseen data</strong></li>
</ul>
<p>This significantly improves the model’s <strong>generalizability</strong> and reduces overfitting, especially on <strong>small or noisy datasets</strong>.</p>
</section>
<section id="handling-of-text-and-embedding-features" class="level4" data-number="11.3.2.4">
<h4 data-number="11.3.2.4" class="anchored" data-anchor-id="handling-of-text-and-embedding-features"><span class="header-section-number">11.3.2.4</span> Handling of Text and Embedding Features</h4>
<p>CatBoost can process text features directly by converting them into numerical representations (e.g., using bag-of-words or embeddings) within the model, reducing the need for external preprocessing. It also supports integration with pre-trained embeddings, which is useful for natural language processing (NLP) tasks.</p>
<p>Together, these innovations make CatBoost a strong candidate for modeling <strong>high-dimensional, categorical, and imbalanced tabular data</strong>, even with minimal feature engineering or hyperparameter tuning.</p>
</section>
<section id="ease-of-use-and-defaults" class="level4" data-number="11.3.2.5">
<h4 data-number="11.3.2.5" class="anchored" data-anchor-id="ease-of-use-and-defaults"><span class="header-section-number">11.3.2.5</span> Ease of Use and Defaults</h4>
<p>CatBoost’s default hyperparameters are well-tuned for a wide range of problems, reducing the need for extensive tuning. For example, its learning rate, depth, and regularization parameters often yield strong performance out of the box. In the paper, the authors also showed that CatBoost outperforms XGBoost and LightGBM without tuning, i.e., with default hyperparameter settings.</p>
<p>Read the <a href="https://proceedings.neurips.cc/paper_files/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf">CatBoost paper</a> for more details.</p>
<p>Here is a good <a href="https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm">blog</a> listing the key features of CatBoost.</p>
</section>
</section>
<section id="using-catboost" class="level3" data-number="11.3.3">
<h3 data-number="11.3.3" class="anchored" data-anchor-id="using-catboost"><span class="header-section-number">11.3.3</span> Using CatBoost</h3>
<p>CatBoost provides a <strong>scikit-learn-compatible API</strong> through <code>CatBoostClassifier</code> and <code>CatBoostRegressor</code>, which makes it easy to integrate into pipelines and use with tools like <code>GridSearchCV</code>, <code>cross_val_score</code>, and <code>train_test_split</code>.</p>
</section>
<section id="installation" class="level3" data-number="11.3.4">
<h3 data-number="11.3.4" class="anchored" data-anchor-id="installation"><span class="header-section-number">11.3.4</span> Installation</h3>
<p>To install CatBoost, run:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>pip install catboost</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>💡 GPU users: CatBoost automatically detects and uses GPU if available. You can explicitly enable it with <code>task_type='GPU'</code>.</p>
</blockquote>
</section>
<section id="catboost-for-regression" class="level3" data-number="11.3.5">
<h3 data-number="11.3.5" class="anchored" data-anchor-id="catboost-for-regression"><span class="header-section-number">11.3.5</span> CatBoost for Regression</h3>
<p>Let us check the performance of <code>CatBoostRegressor()</code> without tuning, i.e., with default hyperparameter settings on our car dataset</p>
<p>The parameter <code>cat_features</code> will be used to specify the indices of the categorical predictors for target encoding.</p>
<div id="bf0a6c1d" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># build a catboostregressor model</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> catboost <span class="im">import</span> CatBoostRegressor</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the CatBoost regressor</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>model_cat <span class="op">=</span> CatBoostRegressor(</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    cat_features<span class="op">=</span>categorical_feature,</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    random_seed<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>model_cat.fit(X_train, y_train)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the test set</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>y_pred_cat <span class="op">=</span> model_cat.predict(X_test)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate evaluation metrics</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>rmse_cat <span class="op">=</span> root_mean_squared_error(y_test, y_pred_cat)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>r2_cat <span class="op">=</span> r2_score(y_test, y_pred_cat)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Output results</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE (CatBoost): </span><span class="sc">{</span>rmse_cat<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R² (CatBoost): </span><span class="sc">{</span>r2_cat<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test RMSE (CatBoost): 3307.2604
Test R² (CatBoost): 0.9627</code></pre>
</div>
</div>
<p>Even with default hyperparameter settings, CatBoost has outperformed both XGBoost and LightGBM in terms of test RMSE and R-squared.</p>
</section>
<section id="tuning-catboostregressor" class="level3" data-number="11.3.6">
<h3 data-number="11.3.6" class="anchored" data-anchor-id="tuning-catboostregressor"><span class="header-section-number">11.3.6</span> Tuning <code>CatBoostRegressor</code></h3>
<p>You can tune the hyperparameters of <code>CatBoostRegressor</code> using <strong>Optuna</strong> or other tuning strategies, just as you would for <code>XGBoost</code> or <code>LightGBM</code>. However, CatBoost has a <strong>distinct set of hyperparameters</strong>, reflecting its unique design choices.</p>
<section id="hyperparameters-not-used-in-catboost" class="level4" data-number="11.3.6.1">
<h4 data-number="11.3.6.1" class="anchored" data-anchor-id="hyperparameters-not-used-in-catboost"><span class="header-section-number">11.3.6.1</span> ❌ Hyperparameters <strong>not used</strong> in CatBoost:</h4>
<ul>
<li><code>reg_alpha</code>: CatBoost does <strong>not</strong> support L1 regularization on leaf weights; it uses only <strong>L2 regularization</strong> (<code>l2_leaf_reg</code>).</li>
<li><code>colsample_bytree</code>: CatBoost <strong>does not</strong> use this parameter; it uses <code>rsm</code> and handles feature selection differently.</li>
</ul>
<p>These parameters are common in XGBoost and LightGBM but are <strong>not part of CatBoost’s configuration</strong>.</p>
</section>
<section id="unique-hyperparameters-in-catboost" class="level4" data-number="11.3.6.2">
<h4 data-number="11.3.6.2" class="anchored" data-anchor-id="unique-hyperparameters-in-catboost"><span class="header-section-number">11.3.6.2</span> ✅ Unique Hyperparameters in CatBoost</h4>
<p>CatBoost introduces several hyperparameters related to <strong>categorical feature handling</strong> and <strong>ordered boosting</strong>:</p>
<ul>
<li><p><code>one_hot_max_size</code>: Threshold for switching between <strong>one-hot encoding</strong> and <strong>target encoding</strong> for categorical features.</p></li>
<li><p><code>boosting_type='Ordered'</code>: Ordered boosting is <strong>enabled by default</strong> in CatBoost to reduce overfitting and prevent prediction shift.</p></li>
<li><p><code>bootstrap_type='Bayesian'</code>: Default bootstrap method. Works well with ordered boosting.</p></li>
<li><p><code>bagging_temperature</code>: Works with <code>bootstrap_type='Bayesian'</code>.<br>
Controls how sharply bootstrap weights are distributed:</p>
<ul>
<li><strong>Low values</strong> (e.g., <code>0</code>): more uniform sampling (close to deterministic).</li>
<li><strong>High values</strong> (e.g., <code>1</code>, <code>5</code>, <code>10</code>): more aggressive sampling—some rows are weighted more heavily.</li>
</ul></li>
<li><p><code>random_strength</code>: Adds randomness to the <strong>split selection score</strong>, especially useful for regularizing categorical splits.</p></li>
<li><p><code>rsm</code>: Random selection rate for column sampling.</p></li>
</ul>
<p>These CatBoost-specific hyperparameters are important when fine-tuning with Cross-Validation, particularly for datasets with many <strong>categorical features</strong> or at risk of <strong>overfitting</strong>.</p>
<div id="a6403528" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> optuna <span class="im">import</span> create_study</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> catboost <span class="im">import</span> CatBoostRegressor, Pool</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create a validation set for early stopping</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>X_train, X_valid, y_train, y_valid <span class="op">=</span> train_test_split(X_train, y_train, test_size<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">#convert to Catboost pool</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>train_pool <span class="op">=</span> Pool(X_train, y_train, cat_features<span class="op">=</span>categorical_feature)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>valid_pool <span class="op">=</span> Pool(X_valid, y_valid, cat_features<span class="op">=</span>categorical_feature)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the objective function for Optuna</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(trial):</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define the hyperparameters to tune</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> {</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">'learning_rate'</span>: trial.suggest_float(<span class="st">'learning_rate'</span>, <span class="fl">0.01</span>, <span class="fl">0.3</span>),</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">'depth'</span>: trial.suggest_int(<span class="st">'depth'</span>, <span class="dv">4</span>, <span class="dv">10</span>),</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">'l2_leaf_reg'</span>: trial.suggest_float(<span class="st">'l2_leaf_reg'</span>, <span class="fl">1e-8</span>, <span class="fl">10.0</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">'min_data_in_leaf'</span>: trial.suggest_int(<span class="st">'min_data_in_leaf'</span>, <span class="dv">1</span>, <span class="dv">30</span>),</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        <span class="st">'bagging_temperature'</span>: trial.suggest_float(<span class="st">'bagging_temperature'</span>, <span class="fl">0.0</span>, <span class="fl">1.0</span>),</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">'random_strength'</span>: trial.suggest_float(<span class="st">'random_strength'</span>, <span class="fl">1e-8</span>, <span class="fl">10.0</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fixed parameters</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">'iterations'</span>: <span class="dv">3000</span>,  <span class="co"># Set to a high number, early stopping will determine the actual number</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">'verbose'</span>: <span class="va">False</span>,</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>        <span class="st">'random_seed'</span>: <span class="dv">42</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create and train the model with early stopping</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> CatBoostRegressor(<span class="op">**</span>params)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use early stopping to prevent overfitting</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    model.fit(</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>        train_pool,</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>        eval_set<span class="op">=</span>valid_pool,</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>        early_stopping_rounds<span class="op">=</span><span class="dv">20</span>,  <span class="co"># Stop if no improvement for 50 rounds</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate on validation set</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(valid_pool)</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>    val_rmse <span class="op">=</span> root_mean_squared_error(y_valid, y_pred)</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return negative RMSE (for maximization)</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>val_rmse</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and run the study</span></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study(direction<span class="op">=</span><span class="st">'maximize'</span>)</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>study.optimize(objective, n_trials<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[I 2025-05-16 06:53:24,620] A new study created in memory with name: no-name-2b89c789-86c1-45aa-9511-7e7990a5767a
[I 2025-05-16 06:53:36,438] Trial 0 finished with value: -2885.7311361568736 and parameters: {'learning_rate': 0.12726746487613003, 'depth': 10, 'l2_leaf_reg': 0.00172009052841597, 'min_data_in_leaf': 13, 'bagging_temperature': 0.21730485409197553, 'random_strength': 0.0002251704542179119}. Best is trial 0 with value: -2885.7311361568736.
[I 2025-05-16 06:54:07,518] Trial 1 finished with value: -2620.171198484414 and parameters: {'learning_rate': 0.05369228812155998, 'depth': 7, 'l2_leaf_reg': 2.5645654686725287e-07, 'min_data_in_leaf': 28, 'bagging_temperature': 0.03449070019194467, 'random_strength': 2.200162833623553}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:55:01,902] Trial 2 finished with value: -2696.8294539080816 and parameters: {'learning_rate': 0.02548811714993481, 'depth': 8, 'l2_leaf_reg': 0.0023811609906064252, 'min_data_in_leaf': 13, 'bagging_temperature': 0.9288401488970826, 'random_strength': 1.082500115732704}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:55:22,721] Trial 3 finished with value: -2724.528794214105 and parameters: {'learning_rate': 0.06451150035785631, 'depth': 9, 'l2_leaf_reg': 4.5131464056397556e-08, 'min_data_in_leaf': 30, 'bagging_temperature': 0.5960655866129079, 'random_strength': 9.28215317432208e-07}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:55:36,587] Trial 4 finished with value: -2878.820243617394 and parameters: {'learning_rate': 0.11527331094852836, 'depth': 4, 'l2_leaf_reg': 0.4475951626701213, 'min_data_in_leaf': 20, 'bagging_temperature': 0.7324000256379442, 'random_strength': 0.4338664457953787}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:55:49,775] Trial 5 finished with value: -2668.619984949151 and parameters: {'learning_rate': 0.1855528089325658, 'depth': 6, 'l2_leaf_reg': 1.8378952869939127e-05, 'min_data_in_leaf': 24, 'bagging_temperature': 0.5768414336451607, 'random_strength': 0.010522014874970544}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:55:57,119] Trial 6 finished with value: -2912.272947098299 and parameters: {'learning_rate': 0.2451855882147167, 'depth': 10, 'l2_leaf_reg': 1.7673885188960817e-06, 'min_data_in_leaf': 10, 'bagging_temperature': 0.4334470051053827, 'random_strength': 0.11406026772554442}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:56:13,622] Trial 7 finished with value: -2695.2648598625924 and parameters: {'learning_rate': 0.09755102190100745, 'depth': 10, 'l2_leaf_reg': 9.566125114415794e-08, 'min_data_in_leaf': 19, 'bagging_temperature': 0.9211592949414568, 'random_strength': 2.436347090461743e-08}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:56:26,903] Trial 8 finished with value: -2753.084945654914 and parameters: {'learning_rate': 0.18843128663283498, 'depth': 5, 'l2_leaf_reg': 5.835652831494221e-06, 'min_data_in_leaf': 6, 'bagging_temperature': 0.7987698449327993, 'random_strength': 0.06362728767715319}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:56:34,769] Trial 9 finished with value: -2921.4504143784643 and parameters: {'learning_rate': 0.22960449698497942, 'depth': 10, 'l2_leaf_reg': 1.1782309803914464e-08, 'min_data_in_leaf': 22, 'bagging_temperature': 0.7169193876429558, 'random_strength': 0.023405106337219345}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:56:45,103] Trial 10 finished with value: -2797.656433395867 and parameters: {'learning_rate': 0.29638954558703867, 'depth': 7, 'l2_leaf_reg': 7.121156648133439, 'min_data_in_leaf': 1, 'bagging_temperature': 0.028555585214749657, 'random_strength': 0.00013397670124843048}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:57:00,983] Trial 11 finished with value: -2987.804912052353 and parameters: {'learning_rate': 0.17465375823124038, 'depth': 6, 'l2_leaf_reg': 3.169840926062148e-05, 'min_data_in_leaf': 29, 'bagging_temperature': 0.40829527505593155, 'random_strength': 7.361593602808678}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:58:30,211] Trial 12 finished with value: -2656.269812196844 and parameters: {'learning_rate': 0.017073325011989847, 'depth': 7, 'l2_leaf_reg': 0.00018641156280634486, 'min_data_in_leaf': 25, 'bagging_temperature': 0.2504148253088835, 'random_strength': 0.0036050209677933906}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:59:27,791] Trial 13 finished with value: -2744.3027101379294 and parameters: {'learning_rate': 0.01817177055582332, 'depth': 7, 'l2_leaf_reg': 0.03706735720453296, 'min_data_in_leaf': 25, 'bagging_temperature': 0.002547331855073512, 'random_strength': 2.0356833250740667e-05}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 06:59:53,265] Trial 14 finished with value: -2697.6837955182573 and parameters: {'learning_rate': 0.06236194061749396, 'depth': 8, 'l2_leaf_reg': 5.295910383215519e-07, 'min_data_in_leaf': 27, 'bagging_temperature': 0.20340714193628523, 'random_strength': 0.0018522086502427496}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 07:00:21,452] Trial 15 finished with value: -2766.897166940931 and parameters: {'learning_rate': 0.061014851192040906, 'depth': 6, 'l2_leaf_reg': 0.00013014280192782276, 'min_data_in_leaf': 17, 'bagging_temperature': 0.21609021631792624, 'random_strength': 9.66330609620808}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 07:01:00,089] Trial 16 finished with value: -2731.9161533728666 and parameters: {'learning_rate': 0.027412623287634285, 'depth': 8, 'l2_leaf_reg': 0.0010697541231850665, 'min_data_in_leaf': 26, 'bagging_temperature': 0.1205465052440132, 'random_strength': 9.050362919853505e-06}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 07:01:13,872] Trial 17 finished with value: -2950.387762628911 and parameters: {'learning_rate': 0.06559997537771182, 'depth': 5, 'l2_leaf_reg': 0.03303703827909148, 'min_data_in_leaf': 22, 'bagging_temperature': 0.33823889215394143, 'random_strength': 0.004091235937143242}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 07:01:24,805] Trial 18 finished with value: -2799.2851339120025 and parameters: {'learning_rate': 0.08913764405155444, 'depth': 7, 'l2_leaf_reg': 4.0696494316792653e-07, 'min_data_in_leaf': 30, 'bagging_temperature': 0.3238078952128749, 'random_strength': 0.0012282635994702415}. Best is trial 1 with value: -2620.171198484414.
[I 2025-05-16 07:01:36,339] Trial 19 finished with value: -2772.5541221320145 and parameters: {'learning_rate': 0.14059069364937926, 'depth': 9, 'l2_leaf_reg': 9.039692246883738e-05, 'min_data_in_leaf': 23, 'bagging_temperature': 0.07579428007376465, 'random_strength': 0.5896255350389338}. Best is trial 1 with value: -2620.171198484414.</code></pre>
</div>
</div>
<div id="8e347a2b" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get best parameters and train final model with early stopping</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> study.best_params</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters:"</span>, best_params)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best trial</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>best_trial <span class="op">=</span> study.best_trial</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best trial:"</span>, best_trial)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best parameters: {'learning_rate': 0.05369228812155998, 'depth': 7, 'l2_leaf_reg': 2.5645654686725287e-07, 'min_data_in_leaf': 28, 'bagging_temperature': 0.03449070019194467, 'random_strength': 2.200162833623553}
Best trial: FrozenTrial(number=1, state=1, values=[-2620.171198484414], datetime_start=datetime.datetime(2025, 5, 16, 6, 53, 36, 439603), datetime_complete=datetime.datetime(2025, 5, 16, 6, 54, 7, 518257), params={'learning_rate': 0.05369228812155998, 'depth': 7, 'l2_leaf_reg': 2.5645654686725287e-07, 'min_data_in_leaf': 28, 'bagging_temperature': 0.03449070019194467, 'random_strength': 2.200162833623553}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.3, log=False, low=0.01, step=None), 'depth': IntDistribution(high=10, log=False, low=4, step=1), 'l2_leaf_reg': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'min_data_in_leaf': IntDistribution(high=30, log=False, low=1, step=1), 'bagging_temperature': FloatDistribution(high=1.0, log=False, low=0.0, step=None), 'random_strength': FloatDistribution(high=10.0, log=True, low=1e-08, step=None)}, trial_id=1, value=None)</code></pre>
</div>
</div>
<div id="700beb75" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use column indices instead of names</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>cat_feature_indices <span class="op">=</span> [X_train.columns.get_loc(col) <span class="cf">for</span> col <span class="kw">in</span> categorical_feature]</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Add iterations parameter back for final model</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>best_params[<span class="st">'iterations'</span>] <span class="op">=</span> <span class="dv">3000</span>  <span class="co"># High number, early stopping will be used</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># create a train+validation set for final model</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>train_val_pool <span class="op">=</span> Pool(</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    np.vstack((X_train, X_valid)),</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    np.concatenate((y_train, y_valid)),</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    cat_features<span class="op">=</span>cat_feature_indices</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a test pool</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>test_pool <span class="op">=</span> Pool(X_test, y_test, cat_features<span class="op">=</span>categorical_feature)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Train final model on combined train+validation data</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> CatBoostRegressor(<span class="op">**</span>best_params)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>final_model.fit(</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    train_val_pool,</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    eval_set<span class="op">=</span>test_pool,</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>    early_stopping_rounds<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Get actual number of trees used after early stopping</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>actual_iterations <span class="op">=</span> final_model.tree_count_</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Actual number of trees used: </span><span class="sc">{</span>actual_iterations<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate on test set</span></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> final_model.predict(X_test)</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>test_rmse <span class="op">=</span> root_mean_squared_error(y_test, y_pred_test)</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>test_r2 <span class="op">=</span> r2_score(y_test, y_pred_test)</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE: </span><span class="sc">{</span>test_rmse<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R²: </span><span class="sc">{</span>test_r2<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Actual number of trees used: 1021
Test RMSE: 3147.8477
Test R²: 0.9662</code></pre>
</div>
</div>
<div id="c35df952" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>fig1 <span class="op">=</span> optuna.visualization.plot_optimization_history(study)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>fig1.show()</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>fig2 <span class="op">=</span> optuna.visualization.plot_param_importances(study)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>fig2.show()</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot feature importance from the final model</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Get feature importance values</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>feature_importance <span class="op">=</span> final_model.get_feature_importance()</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Get sorted indices</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>sorted_idx <span class="op">=</span> np.argsort(feature_importance)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">7</span>))</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>plt.barh(<span class="bu">range</span>(<span class="bu">len</span>(sorted_idx)), feature_importance[sorted_idx])</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Use feature names if X is a DataFrame</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> X.columns <span class="cf">if</span> <span class="bu">hasattr</span>(X, <span class="st">'columns'</span>) <span class="cf">else</span> np.array(<span class="bu">range</span>(X.shape[<span class="dv">1</span>]))</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>plt.yticks(<span class="bu">range</span>(<span class="bu">len</span>(sorted_idx)), feature_names[sorted_idx])</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'CatBoost Feature Importance'</span>)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LightGBM_CatBoost_files/figure-html/cell-14-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Check the <a href="https://catboost.ai/en/docs/references/training-parameters/common">documentation</a> for hyperparameter tuning.</p>
<p>With proper hyperparameter tuning, CatBoost can achieve better performance than its default settings. However, even without tuning, CatBoost’s default configuration often outperforms the default settings of XGBoost and LightGBM, particularly on datasets with categorical features.</p>
</section>
</section>
<section id="when-to-use-catboost-over-xgboost" class="level3" data-number="11.3.7">
<h3 data-number="11.3.7" class="anchored" data-anchor-id="when-to-use-catboost-over-xgboost"><span class="header-section-number">11.3.7</span> When to Use <strong>CatBoost</strong> Over <strong>XGBoost</strong></h3>
<ul>
<li>When your dataset is “categorical-heavy**</li>
<li><strong>CatBoost</strong> tends to perform well <strong>out of the box</strong> with minimal hyperparameter tuning, making it more user-friendly for quick experimentation or deployment<br>
</li>
<li>CatBoost’s <strong>GPU implementation</strong> is optimized for handling categorical data efficiently, and can <strong>outperform XGBoost</strong> on datasets dominated by categorical variables<br>
&gt; While both libraries support GPU acceleration, CatBoost’s architecture is particularly well-suited for categorical-heavy tasks</li>
</ul>
</section>
</section>
<section id="handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost"><span class="header-section-number">11.4</span> Handling Imbalanced Classification: XGBoost vs.&nbsp;LightGBM vs.&nbsp;CatBoost</h2>
<p>Imbalanced classification occurs when one class significantly outnumbers the other (e.g., fraud detection, disease diagnosis). Each boosting library offers tools to address this issue:</p>
<p><strong>XGBoost</strong>:</p>
<ul>
<li><strong>Parameter</strong>: <code>scale_pos_weight</code>
<ul>
<li>Formula:<br>
<span class="math display">\[
\texttt{scale\_pos\_weight} = \frac{\text{Number of negative samples}}{\text{Number of positive samples}}
\]</span></li>
<li>Increases the gradient of the positive class during training.</li>
</ul></li>
<li><strong>Additional Strategies</strong>:
<ul>
<li>Use custom <code>eval_metric</code> (e.g., <code>"auc"</code>, <code>"aucpr"</code>, or <code>"logloss"</code>)</li>
<li>Apply early stopping on validation AUC</li>
</ul></li>
</ul>
<p><strong>LightGBM</strong>:</p>
<ul>
<li><strong>Parameter</strong>: <code>scale_pos_weight</code> (same as in XGBoost)</li>
<li><strong>Alternative</strong>: <code>is_unbalance = TRUE</code>
<ul>
<li>Automatically adjusts class weights based on distribution</li>
</ul></li>
<li><strong>Other Tips</strong>:
<ul>
<li>Use <code>metric = "auc"</code> or <code>"binary_logloss"</code> for better guidance during training</li>
<li>Resampling techniques also compatible</li>
</ul></li>
</ul>
<p><strong>CatBoost</strong>:</p>
<ul>
<li><strong>Parameter</strong>: <code>class_weights</code>
<ul>
<li>Accepts a numeric vector (e.g., <code>class_weights = c(1, 5)</code> for [negative, positive])</li>
<li>Directly modifies the loss function to emphasize minority class</li>
</ul></li>
<li><strong>Advantages</strong>:
<ul>
<li>More flexible than <code>scale_pos_weight</code></li>
<li>Works well with default settings</li>
</ul></li>
<li><strong>Other Tips</strong>:
<ul>
<li>Use <code>loss_function = "Logloss"</code> and <code>eval_metric = "AUC"</code> for binary classification</li>
</ul></li>
</ul>
<p>Below is the summary table:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 38%">
<col style="width: 23%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Library</th>
<th>Imbalance Handling Parameter</th>
<th>Default Support</th>
<th>Recommended Metric</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>XGBoost</td>
<td><code>scale_pos_weight</code></td>
<td>No</td>
<td><code>auc</code>, <code>aucpr</code></td>
</tr>
<tr class="even">
<td>LightGBM</td>
<td><code>scale_pos_weight</code>, <code>is_unbalance</code></td>
<td>Yes (with flag)</td>
<td><code>auc</code>, <code>binary_logloss</code></td>
</tr>
<tr class="odd">
<td>CatBoost</td>
<td><code>class_weights</code></td>
<td>Yes</td>
<td><code>Logloss</code>, <code>AUC</code></td>
</tr>
</tbody>
</table>
</section>
<section id="summary-xgboost-vs.-lightgbm-vs.-catboost" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="summary-xgboost-vs.-lightgbm-vs.-catboost"><span class="header-section-number">11.5</span> Summary: XGBoost vs.&nbsp;LightGBM vs.&nbsp;CatBoost</h2>
<p>Gradient boosting is a powerful ensemble technique, and XGBoost, LightGBM, and CatBoost are three of its most widely used implementations. Each has unique strengths and is well-suited to different use cases.</p>
<p><strong>XGBoost</strong>:</p>
<ul>
<li><strong>Strengths</strong>: Robust, well-documented, strong performance on structured/tabular data<br>
</li>
<li><strong>Split Finding</strong>: Level-wise tree growth<br>
</li>
<li><strong>Regularization</strong>: Explicit L1 and L2 regularization<br>
</li>
<li><strong>Flexibility</strong>: Highly customizable with many hyperparameters<br>
</li>
<li><strong>Best for</strong>: General-purpose tabular data, especially when you have time to tune parameters</li>
</ul>
<p><strong>LightGBM</strong>:</p>
<ul>
<li><strong>Strengths</strong>: Fast training, low memory usage, excellent scalability<br>
</li>
<li><strong>Split Finding</strong>: Leaf-wise tree growth with depth control<br>
</li>
<li><strong>Binning</strong>: Uses histogram-based algorithm with <code>max_bin</code> to speed up training<br>
</li>
<li><strong>Best for</strong>: Large-scale datasets, high-dimensional features, and when training speed matters</li>
</ul>
<p><strong>CatBoost</strong>:</p>
<ul>
<li><strong>Strengths</strong>: Handles categorical features natively, works well with minimal tuning<br>
</li>
<li><strong>Boosting Innovation</strong>: Uses <em>ordered boosting</em> to prevent prediction shift<br>
</li>
<li><strong>Categorical Encoding</strong>: Uses target-based encoding internally<br>
</li>
<li><strong>Best for</strong>: Datasets with many categorical variables or limited time for tuning</li>
</ul>
<p><strong>Final Thoughts</strong></p>
<p>All three libraries are powerful and battle-tested. Here’s a rough guideline:</p>
<ul>
<li><strong>Use XGBoost</strong> if you want control, flexibility, and a well-documented standard</li>
<li><strong>Use LightGBM</strong> when training speed and large data scalability are your top priorities</li>
<li><strong>Use CatBoost</strong> when working with many categorical features or seeking strong baseline results with minimal tuning</li>
</ul>
</section>
<section id="references" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="references"><span class="header-section-number">11.6</span> References</h2>
<ul>
<li><a href="https://papers.nips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf">LightGBM Paper (Original NIPS 2017)</a></li>
<li><a href="https://lightgbm.readthedocs.io/">LightGBM Official Website</a></li>
<li><a href="https://arxiv.org/abs/1810.11363">CatBoost Paper (arXiv)</a></li>
<li><a href="https://catboost.ai/">CatBoost Official Website</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./XGBoost.html" class="pagination-link" aria-label="XGBoost">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">XGBoost</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./smarter_hyper_tuning.html" class="pagination-link" aria-label="Smarter Hyperparameter Tuning">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Smarter Hyperparameter Tuning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>