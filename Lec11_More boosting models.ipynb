{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4b9feeda",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"More boosting models\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "    toc-depth: 4\n",
    "    jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c10c7",
   "metadata": {},
   "source": [
    "This chapter is not in syllabus. However, the boosting models here may help in the prediction problem and / or in your future data science projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f819f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\n",
    "recall_score, precision_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import VotingRegressor, VotingClassifier, StackingRegressor, StackingClassifier, GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import itertools as it\n",
    "import time as time\n",
    "import xgboost as xgb\n",
    "from pyearth import Earth\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c74843",
   "metadata": {},
   "source": [
    "We'll continue to use the same datasets that we have been using throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9036ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carID</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>year</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>tax</th>\n",
       "      <th>mpg</th>\n",
       "      <th>engineSize</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18473</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2020</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>11</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.3282</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15064</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2019</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>10813</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.0430</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18268</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2020</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>6</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.4379</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18480</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2017</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>18895</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>51.5140</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18492</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2015</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>62953</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>160</td>\n",
       "      <td>51.4903</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carID brand      model  year transmission  mileage fuelType  tax      mpg  \\\n",
       "0  18473   bmw   6 Series  2020    Semi-Auto       11   Diesel  145  53.3282   \n",
       "1  15064   bmw   6 Series  2019    Semi-Auto    10813   Diesel  145  53.0430   \n",
       "2  18268   bmw   6 Series  2020    Semi-Auto        6   Diesel  145  53.4379   \n",
       "3  18480   bmw   6 Series  2017    Semi-Auto    18895   Diesel  145  51.5140   \n",
       "4  18492   bmw   6 Series  2015    Automatic    62953   Diesel  160  51.4903   \n",
       "\n",
       "   engineSize  price  \n",
       "0         3.0  37980  \n",
       "1         3.0  33980  \n",
       "2         3.0  36850  \n",
       "3         3.0  25998  \n",
       "4         3.0  18990  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the same datasets as used for linear regression in STAT303-2, \n",
    "#so that we can compare the non-linear models with linear regression\n",
    "trainf = pd.read_csv('./Datasets/Car_features_train.csv')\n",
    "trainp = pd.read_csv('./Datasets/Car_prices_train.csv')\n",
    "testf = pd.read_csv('./Datasets/Car_features_test.csv')\n",
    "testp = pd.read_csv('./Datasets/Car_prices_test.csv')\n",
    "train = pd.merge(trainf,trainp)\n",
    "test = pd.merge(testf,testp)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6b5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['mileage','mpg','year','engineSize']]\n",
    "Xtest = test[['mileage','mpg','year','engineSize']]\n",
    "y = train['price']\n",
    "ytest = test['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a895b16",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282de73d",
   "metadata": {},
   "source": [
    "LightGBM is a gradient boosting decision tree algorithm, which outperforms XGBoost in terms of compuational speed, and provides comparable accuracy in general. The following two key features in LightGBM that make it faster than XGBoost: \n",
    "\n",
    "**1. Gradient-based One-Side Sampling** (GOSS): Recall, in gradient boosting, we fit trees on the gradient of the loss function *(refer the gradient boosting algorithm in section 10.10.2 of the book, [Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/)):*\n",
    "\n",
    "$$r_m = -\\bigg[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}  \\bigg]_{f = f_{m-1}}. $$\n",
    "\n",
    "Observations that correspond to relatively larger gradients contribute more to minimizing the loss function as compared to observations with smaller gradients. The alogrithm down samples the observations with small gradients, while selecting all the observations with large gradients. As observations with large gradients contribute the most to the reduction in loss function when considering a split, the accuracy of loss reduction estimate is maintained even with a reduced sample size. This leads to similar performance in terms of prediction accuracy while reducing computation speed due to reduction in sample size to fit trees.\n",
    "\n",
    "**2. Exclusive feature bundling** (EFB): This is useful when there are a lot of predictors, but the predictor space is sparse, i.e., most of the values are zero for several predictors, and the predictors rarely take non-zero values simultaneously. This can typically happen in case of a lot of dummy variables in the data. In such a case, the predictors are bundled to create a single predictor. \n",
    "\n",
    "In the example below you can see that feature1 and feature2 are mutually exclusive. In order to achieve non overlapping buckets we add bundle size of feature1 to feature2. This makes sure that non zero data points of bundled features (feature1 and feature2) reside in different buckets. In feature_bundle buckets 1 to 4 contains non zero instances of feature1 and buckets 5,6 contain non zero instances of feature2 *([Reference](https://towardsdatascience.com/what-makes-lightgbm-lightning-fast-a27cf0d9785e))*. \n",
    "\n",
    "| feature1 | feature2 | feature_bundle |\n",
    "| --- | --- | --- |\n",
    "| 0 | 2 | 6 |\n",
    "| 0 | 1 | 5 |\n",
    "| 0 | 2 | 6 |\n",
    "| 1 | 0 | 1 |\n",
    "| 2 | 0 | 2 |\n",
    "| 3 | 0 | 3 |\n",
    "| 4 | 0 | 4 |\n",
    "\n",
    "The paper for the algorithm can be accessed [here](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380a289",
   "metadata": {},
   "source": [
    "### LightGBM for regression\n",
    "Let us tune a lightGBM model for regression for our problem of predicting car price. We'll use the function [`LGBMRegressor`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html). For classification problems, [`LGBMClassifier`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html) can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4787ccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Optimal parameter values = {'subsample': 0.75, 'reg_lambda': 0, 'reg_alpha': 100, 'num_leaves': 31, 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "Optimal cross validation R-squared =  0.8935432951824455\n",
      "Time taken =  1  minutes\n"
     ]
    }
   ],
   "source": [
    "#K-fold cross validation to find optimal parameters for LightGBM regressor\n",
    "start_time = time.time()\n",
    "param_grid = {'max_depth': [4,6,8],\n",
    "              'num_leaves': [20, 31, 40],\n",
    "              'learning_rate': [0.01, 0.05, 0.1],\n",
    "               'reg_lambda':[0, 10, 100],\n",
    "                'n_estimators':[100, 500, 1000],\n",
    "                'reg_alpha': [0, 10, 100],\n",
    "                'subsample': [0.5, 0.75, 1.0],\n",
    "                'colsample_bytree': [0.5, 0.75, 1.0]}\n",
    "\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "optimal_params = RandomizedSearchCV(estimator=LGBMRegressor(random_state=1),                                                       \n",
    "                             param_distributions = param_grid, n_iter = 200,\n",
    "                             verbose = 1,\n",
    "                             n_jobs=-1,\n",
    "                             cv = cv)\n",
    "optimal_params.fit(X,y)\n",
    "print(\"Optimal parameter values =\", optimal_params.best_params_)\n",
    "print(\"Optimal cross validation R-squared = \",optimal_params.best_score_)\n",
    "print(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fabd5eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5400.723918176313"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RMSE based on the optimal parameter values of a LighGBM Regressor model\n",
    "np.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ef811",
   "metadata": {},
   "source": [
    "LightGBM model took 1 minute for a random search with 1000 fits as compared to 4 minutes for an XGBoost model with 1000 fits on the same data (as shown below). In terms of prediction accuracy, we observe that the accuracy of LightGBM on test *(unseen)* data is comparable to that of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "336194fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Optimal parameter values = {'subsample': 0.75, 'reg_lambda': 1, 'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.01, 'gamma': 100, 'colsample_bytree': 1.0}\n",
      "Optimal cross validation R-squared =  0.9002580404500382\n",
      "Time taken =  4  minutes\n"
     ]
    }
   ],
   "source": [
    "#K-fold cross validation to find optimal parameters for XGBoost\n",
    "start_time = time.time()\n",
    "param_grid = {'max_depth': [4,6,8],\n",
    "              'learning_rate': [0.01, 0.05, 0.1],\n",
    "               'reg_lambda':[0, 1, 10],\n",
    "                'n_estimators':[100, 500, 1000],\n",
    "                'gamma': [0, 10, 100],\n",
    "                'subsample': [0.5, 0.75, 1.0],\n",
    "                'colsample_bytree': [0.5, 0.75, 1.0]}\n",
    "\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "optimal_params = RandomizedSearchCV(estimator=xgb.XGBRegressor(random_state=1),                                                       \n",
    "                             param_distributions = param_grid, n_iter = 200,\n",
    "                             verbose = 1,\n",
    "                             n_jobs=-1,\n",
    "                             cv = cv)\n",
    "optimal_params.fit(X,y)\n",
    "print(\"Optimal parameter values =\", optimal_params.best_params_)\n",
    "print(\"Optimal cross validation R-squared = \",optimal_params.best_score_)\n",
    "print(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ebe7659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5497.553788113875"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RMSE based on the optimal parameter values\n",
    "np.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
