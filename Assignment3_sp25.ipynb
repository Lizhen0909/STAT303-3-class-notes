{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bd972c94",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Assignment 3\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a5837",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity. \n",
    "\n",
    "2. Write your code in the *Code* cells and your answer in the *Markdown* cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\n",
    "\n",
    "3. Use [Quarto](https://quarto.org/docs/output-formats/html-basics.html) to print the *.ipynb* file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: `quarto render filename.ipynb --to html`. Submit the HTML file.\n",
    "\n",
    "4. The assignment is worth 100 points, and is due on **Friday, 2th May 2025 at 11:59 pm**. \n",
    "\n",
    "5. **Five points are properly formatting the assignment**. The breakdown is as follows:\n",
    "- Must be an HTML file rendered using Quarto (2 pts). *If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file. If your issue doesn't seem genuine, you will lose points.* \n",
    "- There aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\n",
    "- Final answers of each question are written in Markdown cells (1 pt).\n",
    "- There is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b697b3",
   "metadata": {},
   "source": [
    "## 1) Regression Problem - Miami housing\n",
    "### 1a) Data preparation\n",
    "Read the data *miami-housing.csv*. Check the description of the variables [here](https://www.kaggle.com/datasets/deepcontractor/miami-housing-dataset). Split the data into 60% train and 40% test. Use `random_state = 45`. The response is `SALE_PRC`, and the rest of the columns are predictors, except `PARCELNO`. Print the shape of the predictors dataframe of the train data.\n",
    "\n",
    "*(2 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a34ecb",
   "metadata": {},
   "source": [
    "###  1b) Baseline Decision Tree Model\n",
    "\n",
    "Train a **Decision Tree Regressor** to predict `SALE_PRC` using all available predictors.\n",
    "\n",
    "- Use `random_state=45` and keep all other hyperparameters at their default values.\n",
    "- After training the model, evaluate and report the following on **both the training and test sets**:\n",
    "  - **Mean Absolute Error (MAE)**\n",
    "  - **R² Score**\n",
    "\n",
    "*(3 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65853b5",
   "metadata": {},
   "source": [
    "### 1c) Tune the Decision Tree Model\n",
    "\n",
    "Tune the hyperparameters of the **Decision Tree Regressor** developed in the previous question and evaluate its performance.\n",
    "\n",
    "Your goal is to achieve a **test set MAE (Mean Absolute Error) below $68,000**.\n",
    "\n",
    "- You must display the **optimal hyperparameter values** obtained from the tuning process.\n",
    "- Compute and report the **test MAE and R² Score** using the tuned model.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "1. `BayesSearchCV()` with `max_depth` and `max_features` can often complete in under a minute.\n",
    "2. You may use **any hyperparameter tuning method** (e.g., GridSearchCV, RandomizedSearchCV, BayesSearchCV).\n",
    "3. You are free to choose **which hyperparameters to tune** and define your own **search space**.\n",
    "\n",
    "*(9 points)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f19055",
   "metadata": {},
   "source": [
    "### 1d) Bagged Decision Trees with Out-of-Bag Evaluation\n",
    "\n",
    "Train a **Bagging Regressor** using Decision Trees as base estimators to predict `SALE_PRC`.\n",
    "\n",
    "- Enable **out-of-bag (OOB) evaluation** by setting `oob_score=True`.\n",
    "- Keep all other parameters at their default values.\n",
    "- Tune only the `n_estimators` hyperparameter: increase the number of trees until the **OOB MAE stabilizes**.\n",
    "- Ensure that the final **OOB MAE is less than \\$48,000**.\n",
    "- Report the final **OOB MAE**, **test MAE**, and **R² score**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d611e34a",
   "metadata": {},
   "source": [
    "### 1e) Bagged Decision Trees Without Bootstrapping\n",
    "\n",
    "Train a **Bagging Regressor** using Decision Trees, but this time **disable bootstrapping** by setting `bootstrap=False`.\n",
    "\n",
    "- Use the same `n_estimators` value as in the previous question.\n",
    "- Compute and report the following on the **test set**:\n",
    "  - **Mean Absolute Error (MAE)**\n",
    "  - **R² Score**\n",
    "\n",
    "Explain **why the test MAE** in this case is:\n",
    "\n",
    "- **Much higher** than the MAE obtained when bootstrapping was enabled (previous question).\n",
    "- **Lower** than the MAE obtained from a single untuned decision tree (as in Question 1(b)).\n",
    "\n",
    "> 💡 Hint: Consider the impact of bootstrap sampling on variance reduction and the benefits of aggregation in ensemble methods.\n",
    "\n",
    "\n",
    "*(2 point for code, 3 + 3 points for reasoning)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31eb6d",
   "metadata": {},
   "source": [
    "### 1f) Bagged Decision Trees with Feature Bootstrapping Only\n",
    "\n",
    "Train a **Bagging Regressor** using Decision Trees, with the following configuration:\n",
    "\n",
    "- **Disable sample bootstrapping** by setting `bootstrap=False`.  \n",
    "- **Enable feature bootstrapping** by setting `bootstrap_features=True`.  \n",
    "- **Keep the default setting** for `max_features` (i.e., do not modify it).\n",
    "\n",
    "Use the same number of estimators (`n_estimators`) as in the previous bagging experiments.\n",
    "\n",
    "- Compute and report the following on the **test set**:\n",
    "  \n",
    "  - **Mean Absolute Error (MAE)**\n",
    "  - **R² Score**\n",
    "\n",
    "Explain why the **test MAE** obtained in this setting is **much lower** than the one in the previous question, where neither bootstrapping samples nor features was used.\n",
    "\n",
    "\n",
    "*(2 point for code, 3 points for reasoning)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06644b7",
   "metadata": {},
   "source": [
    "### 1g) Tuning a Bagged Tree Model\n",
    "\n",
    "#### i) Approaches\n",
    "\n",
    "There are two common approaches for tuning a **bagged tree model**:\n",
    "\n",
    "1. **Out-of-Bag (OOB) Prediction**\n",
    "2. **$K$-fold Cross-Validation** using `GridSearchCV`\n",
    "\n",
    "What is the advantage of each approach over the other? Specifically:\n",
    "\n",
    "- What is the **advantage of the out-of-bag approach** compared to $K$-fold cross-validation?\n",
    "- What is the **advantage of $K$-fold cross-validation** compared to the out-of-bag approach?\n",
    "\n",
    "*(3 + 3 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75312e3c",
   "metadata": {},
   "source": [
    "#### ii) Tuning the Hyperparameters\n",
    "\n",
    "Tune the hyperparameters of the Bagging Regressor model developed in 1(d).  \n",
    "You may use any tuning approach of your choice (e.g., grid search, random search, bayes search).  \n",
    "It is up to you to select which hyperparameters to tune and define their candidate values.\n",
    "\n",
    "**Your tuned model must achieve a test MAE lower than the one obtained in Question 1f.**\n",
    "\n",
    "After tuning:\n",
    "\n",
    "- Report the optimal hyperparameter values found.\n",
    "- Compute and report the **test MAE** and **R² score** using the tuned model.\n",
    "\n",
    "*(9 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982732e",
   "metadata": {},
   "source": [
    "### 1h) Random Forest\n",
    "\n",
    "#### i) Tuning a Random Forest Model\n",
    "\n",
    "Train and tune a **Random Forest Regressor** to predict `SALE_PRC`.\n",
    "\n",
    "- Select hyperparameters and define your own tuning grid.\n",
    "- Use any tuning approach (e.g., Out-of-Bag (OOB) evaluation or $K$-fold cross-validation).\n",
    "- Report the following performance metrics on the **test set**:\n",
    "  - **Mean Absolute Error (MAE)**\n",
    "  - **R² Score**\n",
    "\n",
    "> ✅ Your goal is to achieve a **test MAE below $46,000**.\n",
    "\n",
    "\n",
    "*(9 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18e636",
   "metadata": {},
   "source": [
    "#### ii) Feature Importance\n",
    "\n",
    "After fitting the tuned **Random Forest Regressor**, extract and display the **feature importances**.\n",
    "\n",
    "- Print the predictors in **decreasing order of importance** based on the trained model.\n",
    "- This helps identify which features contribute most to predicting `SALE_PRC`.\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4211a1",
   "metadata": {},
   "source": [
    "#### iii) Random Forest vs. Bagging: `max_features`\n",
    "\n",
    "The `max_features` hyperparameter is available in both `RandomForestRegressor()` and `BaggingRegressor()`.\n",
    "\n",
    "Does `max_features` have the **same meaning** in both models?  \n",
    "If not, explain the **difference in how it is interpreted and applied**.\n",
    "\n",
    "> 💡 **Hint:** Refer to the scikit-learn documentation for both estimators to understand how `max_features` affects feature selection during training.\n",
    "\n",
    "*(1 + 3 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ae154",
   "metadata": {},
   "source": [
    "## 2) Classification - Term deposit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9e803",
   "metadata": {},
   "source": [
    "The data for this question is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls, where bank clients were called to subscribe for a term deposit. \n",
    "\n",
    "There is a train data - *train.csv*, which you will use to develop a model. There is a test data - *test.csv*, which you will use to test your model. Each dataset has the following attributes about the clients called in the marketing campaign:\n",
    "\n",
    "1. `age`: Age of the client\n",
    "\n",
    "2. `education`: Education level of the client \n",
    "\n",
    "3. `day`: Day of the month the call is made\n",
    "\n",
    "4. `month`: Month of the call \n",
    "\n",
    "5. `y`: did the client subscribe to a term deposit? \n",
    "\n",
    "6. `duration`: Call duration, in seconds. This attribute highly affects the output target (e.g., if `duration`=0 then `y`='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call `y` is obviously known. Thus, this input should only be included for inference purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "(Raw data source: [Source](https://archive.ics.uci.edu/ml/datasets/bank+marketing). Do not use the raw data source for this assignment. It is just for reference.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2b007",
   "metadata": {},
   "source": [
    "### a) Data Preparation\n",
    "\n",
    "Begin by examining the **distribution of the target variable** in both the training and test sets. This will help you assess whether there is any significant **class imbalance**.\n",
    "\n",
    "Next, consider the two available approaches for hyperparameter tuning:\n",
    "\n",
    "- **Cross-validation (CV)**\n",
    "- **Out-of-bag (OOB) evaluation**\n",
    "\n",
    "#### ❓ Which method do you prefer for this dataset, and why?\n",
    "\n",
    "Discuss your choice based on:\n",
    "\n",
    "- The **size of the dataset**\n",
    "- The **class imbalance** in the target variable\n",
    "- The **reliability and interpretability** of each method\n",
    "- Whether you need **stratified sampling** to preserve class distribution during evaluation\n",
    "\n",
    "*(2 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a40ee0",
   "metadata": {},
   "source": [
    "### b) Random Forest for Term Deposit Subscription Prediction\n",
    "\n",
    "Develop and tune a **Random Forest Classifier** to predict whether a client will subscribe to a term deposit using the following predictors:  \n",
    "\n",
    "- `age`\n",
    "- `education`\n",
    "- `day`\n",
    "- `month`\n",
    "\n",
    "The model must satisfy the following performance criteria:\n",
    "\n",
    "#### ✅ Requirements:\n",
    "\n",
    "1. **Minimum overall classification accuracy of 75%**, across both *train.csv* and *test.csv*.\n",
    "2. **Minimum recall of 60%**, across both *train.csv* and *test.csv*.\n",
    "\n",
    "You must:\n",
    "\n",
    "- Print the **accuracy** and **recall** for both datasets (*train.csv* and *test.csv*).\n",
    "- Use **cross-validation on the training data** to optimize the model hyperparameters.\n",
    "- Select a **threshold probability** for classification and apply it consistently across both datasets.\n",
    "\n",
    "\n",
    "#### ⚠️ Important Notes:\n",
    "\n",
    "i. **Do not use `duration`** as a predictor. Its value is determined after the marketing call ends, so using it would leak information about the outcome.\n",
    "\n",
    "ii. You are free to choose any **decision threshold** for classification, but the same threshold must be used consistently for both training and test evaluation.\n",
    "\n",
    "iii. Use **cross-validation** to tune hyperparameters such as `max_features`, `max_depth`, and `max_leaf_nodes`.  \n",
    "  - You may rely on the default cross-validation behavior, which uses **stratified folds by default for classification tasks** to account for class imbalance.\n",
    "\n",
    "iv. After tuning the model, **plot cross-validated accuracy and recall** across a range of threshold values (e.g., 0.1 to 0.9). Use this plot to select a threshold that satisfies the required trade-off between accuracy and recall.\n",
    "\n",
    "v. **Evaluate the final tuned model (with the chosen threshold)** on the test dataset. Do not use the test data to guide any part of the tuning or threshold selection.\n",
    "\n",
    "\n",
    "#### 💡 Hints:\n",
    "\n",
    "- Restrict the search space to:\n",
    "  - `max_depth` ≤ 25  \n",
    "  - `max_leaf_nodes` ≤ 45  \n",
    "  These limits encourage generalization and help balance recall and accuracy.\n",
    "  \n",
    "- Consider using cross-validation scores to compute predicted probabilities when plotting recall/accuracy curves.\n",
    "\n",
    "\n",
    "\n",
    "#### 📝 Scoring Breakdown *(22 points total)*:\n",
    "\n",
    "- **8 points** – Hyperparameter tuning via cross-validation  \n",
    "- **5 points** – Plotting accuracy and recall across thresholds  \n",
    "- **5 points** – Threshold selection based on the plot  \n",
    "- **4 points** – Reporting accuracy and recall on both datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867ae27",
   "metadata": {},
   "source": [
    "## 3) Predictor Transformations in Trees\n",
    "\n",
    "Can a **non-linear monotonic transformation** of predictors (such as `log()`, `sqrt()`, etc.) be useful in improving the accuracy of **decision tree models**?\n",
    "\n",
    "Provide a brief explanation based on your understanding of how decision trees split data and handle predictor scales.\n",
    "\n",
    "*(4 points for answer)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
