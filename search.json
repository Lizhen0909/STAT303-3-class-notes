[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science III with python (Class notes)",
    "section": "",
    "text": "Preface\nThese are class notes for the course STAT303-3. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThe course notes are currently being written, and will continue to being developed as the course progresses (just like the class notes last quarter). Please report any typos / mistakes / inconsistencies / issues with the class notes / class presentations in your comments here. Thank you!"
  },
  {
    "objectID": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "href": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "title": "1  Introduction to scikit-learn",
    "section": "1.1 Splitting data into train and test",
    "text": "1.1 Splitting data into train and test\nLet us create train and test datasets for developing a model to predict if a person has diabetes.\n\n# Creating training and test data\n    # 80-20 split, which is usual - 70-30 split is also fine, 90-10 is fine if the dataset is large\n    # random_state to set a random seed for the splitting - reproducible results\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)\n\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the complete dataset.\n\n#Proportion of 0s and 1s in the complete data\ny.value_counts()/y.shape\n\n0    0.651042\n1    0.348958\nName: Outcome, dtype: float64\n\n\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the train dataset.\n\n#Proportion of 0s and 1s in train data\ny_train.value_counts()/y_train.shape\n\n0    0.644951\n1    0.355049\nName: Outcome, dtype: float64\n\n\n\n#Proportion of 0s and 1s in test data\ny_test.value_counts()/y_test.shape\n\n0    0.675325\n1    0.324675\nName: Outcome, dtype: float64\n\n\nWe observe that the proportion of 0s and 1s in the train and test dataset are slightly different from that in the complete data. In order for these datasets to be more representative of the population, they should have a proportion of 0s and 1s similar to that in the complete dataset. This is especially critical in case of imbalanced datasets, where one class is represented by a significantly smaller number of instances than the other(s).\nWhen training a classification model on an imbalanced dataset, the model might not learn enough about the minority class, which can lead to poor generalization performance on new data. This happens because the model is biased towards the majority class, and it might even predict all instances as belonging to the majority class.\n\n1.1.1 Stratified splitting\nWe will use the argument stratify to obtain a proportion of 0s and 1s in the train and test datasets that is similar to the proportion in the complete `data.\n\n#Stratified train-test split\nX_train_stratified, X_test_stratified, y_train_stratified,\\\ny_test_stratified = train_test_split(X, y, test_size = 0.2, random_state = 45, stratify=y)\n\n\n#Proportion of 0s and 1s in train data with stratified split\ny_train_stratified.value_counts()/y_train.shape\n\n0    0.651466\n1    0.348534\nName: Outcome, dtype: float64\n\n\n\n#Proportion of 0s and 1s in test data with stratified split\ny_test_stratified.value_counts()/y_test.shape\n\n0    0.649351\n1    0.350649\nName: Outcome, dtype: float64\n\n\nThe proportion of the classes in the stratified split mimics the proportion in the complete dataset more closely.\nBy using stratified splitting, we ensure that both the train and test data sets have the same proportion of instances from each class, which means that the model will see enough instances from the minority class during training. This, in turn, helps the model learn to distinguish between the classes better, leading to better performance on new data.\nThus, stratified splitting helps to ensure that the model sees enough instances from each class during training, which can improve the model’s ability to generalize to new data, particularly in cases where one class is underrepresented in the dataset.\nLet us develop a logistic regression model for predicting if a person has diabetes."
  },
  {
    "objectID": "L1_Scikit-learn.html#scaling-data",
    "href": "L1_Scikit-learn.html#scaling-data",
    "title": "1  Introduction to scikit-learn",
    "section": "1.2 Scaling data",
    "text": "1.2 Scaling data\nIn certain models, it may be important to scale data for various reasons. In a logistic regression model, scaling can help with model convergence. Scikit-learn uses a method known as gradient-descent (not in scope of the syllabus of this course) to obtain a solution. In case the predictors have different orders of magnitude, the algorithm may fail to converge. In such cases, it is useful to standardize the predictors so that all of them are at the same scale.\n\n# With linear/logistic regression in scikit-learn, especially when the predictors have different orders \n# of magn., scaling is necessary. This is to enable the training algo. which we did not cover. (Gradient Descent)\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test) # Do NOT refit the scaler with the test data, just transform it."
  },
  {
    "objectID": "L1_Scikit-learn.html#fitting-a-model",
    "href": "L1_Scikit-learn.html#fitting-a-model",
    "title": "1  Introduction to scikit-learn",
    "section": "1.3 Fitting a model",
    "text": "1.3 Fitting a model\nLet us fit a logistic regression model for predicting if a person has diabetes. Let us try fitting a model with the un-scaled data.\n\n# Create a model object - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train, y_train)\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()\n\n\nNote that the model with the un-scaled predictors fails to converge. Check out the data X_train to see that this may be probably due to the predictors have different orders of magnitude. For example, the predictor DiabetesPedigreeFunction has values in [0.078, 2.42], while the predictor Insulin has values in [0, 800].\nLet us fit the model to the scaled data.\n\n# Create a model - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train_scaled, y_train)\n\nLogisticRegression()\n\n\nThe model converges to a solution with the scaled data!\nThe coefficients of the model can be returned with the coef_ attribute of the LogisticRegression() object. However, the output is not as well formatted as in the case of the statsmodels library since sklearn is developed primarily for the purpose of prediction, and not inference.\n\n# Use coef_ to return the coefficients - only log reg inference you can do with sklearn\nprint(logreg.coef_) \n\n[[ 0.32572891  1.20110566 -0.32046591  0.06849882 -0.21727131  0.72619528\n   0.40088897  0.29698818]]"
  },
  {
    "objectID": "L1_Scikit-learn.html#computing-performance-metrics",
    "href": "L1_Scikit-learn.html#computing-performance-metrics",
    "title": "1  Introduction to scikit-learn",
    "section": "1.4 Computing performance metrics",
    "text": "1.4 Computing performance metrics\n\n1.4.1 Accuracy\nLet us test the model prediction accuracy on the test data. We’ll demonstrate two different functions that can be used to compute model accuracy - accuracy_score(), and score().\nThe accuracy_score() function from the metrics module of the sklearn library is general, and can be used for any classification model. We’ll use it along with the predict() method of the LogisticRegression() object, which returns the predicted class based on a threshold probability of 0.5.\n\n# Get the predicted classes first\ny_pred = logreg.predict(X_test_scaled)\n\n# Use the predicted and true classes for accuracy\nprint(accuracy_score(y_pred, y_test)*100) \n\n73.37662337662337\n\n\nThe score() method of the LogisticRegression() object can be used to compute the accuracy only for a logistic regression model. Note that for a LinearRegression() object, the score() method will return the model \\(R\\)-squared.\n\n# Use .score with test predictors and response to get the accuracy\n# Implements the same thing under the hood\nprint(logreg.score(X_test_scaled, y_test)*100)  \n\n73.37662337662337\n\n\n\n\n1.4.2 ROC-AUC\nThe roc_curve() and auc() functions from the metrics module of the sklearn library can be used to compute the ROC-AUC, or the area under the ROC curve. Note that for computing ROC-AUC, we need the predicted probability, instead of the predicted class. Thus, we’ll use the predict_proba() method of the LogisticRegression() object, which returns the predicted probability for the observation to belong to each of the classes, instead of using the predict() method, which returns the predicted class based on threshold probability of 0.5.\n\n#Computing the predicted probability for the observation to belong to the positive class (y=1);\n#The 2nd column in the output of predict_proba() consists of the probability of the observation to \n#belong to the positive class (y=1)\ny_pred_prob = logreg.predict_proba(X_test_scaled)[:,1] \n\n#Using the predicted probability computed above to find ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test, y_pred_prob)\nprint(auc(fpr, tpr))# AUC of ROC\n\n0.7923076923076922\n\n\n\n\n1.4.3 Confusion matrix & precision-recall\nThe confusion_matrix(), precision_score(), and recall_score() functions from the metrics module of the sklearn library can be used to compute the confusion matrix, precision, and recall respectively.\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\n\n\n\n\nprint(\"Precision: \", precision_score(y_test, y_pred))\nprint(\"Recall: \", recall_score(y_test, y_pred))\n\nPrecision:  0.6046511627906976\nRecall:  0.52\n\n\nLet us compute the performance metrics if we develop the model using stratified splitting.\n\n# Developing the model with stratified splitting\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n#Computing the accuracy\ny_pred_stratified = logreg.predict(X_test_stratified_scaled)\nprint(\"Accuracy: \",accuracy_score(y_pred_stratified, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_stratified))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_stratified))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_stratified), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  78.57142857142857\nROC-AUC:  0.8505555555555556\nPrecision:  0.7692307692307693\nRecall:  0.5555555555555556\n\n\n\n\n\nThe model with the stratified train-test split has a better performance as compared to the other model on all the performance metrics!"
  },
  {
    "objectID": "L1_Scikit-learn.html#tuning-the-model-hyperparameters",
    "href": "L1_Scikit-learn.html#tuning-the-model-hyperparameters",
    "title": "1  Introduction to scikit-learn",
    "section": "1.5 Tuning the model hyperparameters",
    "text": "1.5 Tuning the model hyperparameters\nA hyperparameter (among others) that can be trained in a logistic regression model is the regularization parameter.\nWe may also wish to tune the decision threshold probability. Note that the decision threshold probability is not considered a hyperparameter of the model. Hyperparameters are model parameters that are set prior to training and cannot be directly adjusted by the model during training. Examples of hyperparameters in a logistic regression model include the regularization parameter, and the type of shrinkage penalty - lasso / ridge. These hyperparameters are typically optimized through a separate tuning process, such as cross-validation or grid search, before training the final model.\nThe performance metrics can be computed using a desired value of the threshold probability. Let us compute the performance metrics for a desired threshold probability of 0.3.\n\n# Performance metrics computation for a desired threshold probability of 0.3\ndesired_threshold = 0.3\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob > desired_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  75.32467532467533\nROC-AUC:  0.8505555555555556\nPrecision:  0.6111111111111112\nRecall:  0.8148148148148148\n\n\n\n\n\n\n1.5.1 Tuning decision threshold probability\nSuppose we wish to find the optimal decision threshold probability to maximize accuracy. Note that we cannot use the test dataset to optimize model hyperparameters, as that may lead to overfitting on the test data. We’ll use \\(K\\)-fold cross validation on train data to find the optimal decision threshold probability.\nWe’ll use the cross_val_predict() function from the model_selection module of sklearn to compute the \\(K\\)-fold cross validated predicted probabilities. Note that this function simplifies the task of manually creating the \\(K\\)-folds, training the model \\(K\\)-times, and computing the predicted probabilities on each of the \\(K\\)-folds. Thereafter, the predicted probabilities will be used to find the one the optimal threshold probability that maximizes the classification accuracy.\n\nhyperparam_vals = np.arange(0,1.01,0.01)\naccuracy_iter = []\n\npredicted_probability = cross_val_predict(LogisticRegression(), X_train_stratified_scaled, \n                                              y_train_stratified, cv = 5, method = 'predict_proba')\n\nfor threshold_prob in hyperparam_vals:\n    predicted_class = predicted_probability[:,1] > threshold_prob\n    predicted_class = predicted_class.astype(int)\n\n    #Computing the accuracy\n    accuracy = accuracy_score(predicted_class, y_train_stratified)*100\n    accuracy_iter.append(accuracy)\n\nLet us visualize the accuracy with change in decision threshold probability.\n\n# Accuracy vs decision threshold probability\nsns.scatterplot(x = hyperparam_vals, y = accuracy_iter)\nplt.xlabel('Decision threshold probability')\nplt.ylabel('Average 5-fold CV accuracy');\n\n\n\n\nThe optimal decision threshold probability is the one that maximizes the \\(K\\)-fold cross validation accuracy.\n\n# Optimal decision threshold probability\nhyperparam_vals[accuracy_iter.index(max(accuracy_iter))]\n\n0.46\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.46\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob > desired_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8505555555555556\nPrecision:  0.7804878048780488\nRecall:  0.5925925925925926\n\n\n\n\n\nModel performance on test data has improved with the optimal decision threshold probability.\n\n\n1.5.2 Tuning the regularization parameter\nThe LogisticRegression() method has a default L2 regularization penalty, which means ridge regression.C is \\(1/\\lambda\\), where \\(\\lambda\\) is the hyperparameter that is multiplied with the ridge penalty. C is 1 by default.\n\naccuracy_iter = []\nhyperparam_vals = 10**np.linspace(-3.5, 1)\n\nfor c_val in hyperparam_vals: # For each possible C value in your grid\n    logreg_model = LogisticRegression(C=c_val) # Create a model with the C value\n    \n    accuracy_iter.append(cross_val_score(logreg_model, X_train_stratified_scaled, y_train_stratified,\n                                      scoring='accuracy', cv=5)) # Find the cv results\n\n\nplt.plot(hyperparam_vals, np.mean(np.array(accuracy_iter), axis=1))\nplt.xlabel('C')\nplt.ylabel('Average 5-fold CV accuracy')\nplt.xscale('log')\nplt.show()\n\n\n\n\n\n# Optimal value of the regularization parameter 'C'\noptimal_C = hyperparam_vals[np.argmax(np.array(accuracy_iter).mean(axis=1))]\noptimal_C\n\n0.11787686347935879\n\n\n\n# Developing the model with stratified splitting and optimal 'C'\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg = LogisticRegression(C = optimal_C)\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n#Computing the accuracy\ny_pred_stratified = logreg.predict(X_test_stratified_scaled)\nprint(\"Accuracy: \",accuracy_score(y_pred_stratified, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_stratified))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_stratified))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_stratified), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  78.57142857142857\nROC-AUC:  0.8516666666666666\nPrecision:  0.7837837837837838\nRecall:  0.5370370370370371\n\n\n\n\n\n\n\n1.5.3 Tuning the decision threshold probability and the regularization parameter simultaneously\n\nthreshold_hyperparam_vals = np.arange(0,1.01,0.01)\nC_hyperparam_vals = 10**np.linspace(-3.5, 1)\naccuracy_iter = pd.DataFrame(columns = {'threshold', 'C', 'accuracy'})\niter_number = 0\n\nfor c_val in C_hyperparam_vals:\n    predicted_probability = cross_val_predict(LogisticRegression(C = c_val), X_train_stratified_scaled, \n                                                  y_train_stratified, cv = 5, method = 'predict_proba')\n\n    for threshold_prob in threshold_hyperparam_vals:\n        predicted_class = predicted_probability[:,1] > threshold_prob\n        predicted_class = predicted_class.astype(int)\n\n        #Computing the accuracy\n        accuracy = accuracy_score(predicted_class, y_train_stratified)*100\n        accuracy_iter.loc[iter_number, 'threshold'] = threshold_prob\n        accuracy_iter.loc[iter_number, 'C'] = c_val\n        accuracy_iter.loc[iter_number, 'accuracy'] = accuracy\n        iter_number = iter_number + 1\n\n\n# Parameters for highest accuracy\noptimal_C = accuracy_iter.sort_values(by = 'accuracy', ascending = False).iloc[0,:]['C']\noptimal_threshold = accuracy_iter.sort_values(by = 'accuracy', ascending = False).iloc[0, :]['threshold']\n\n#Optimal decision threshold probability\nprint(\"Optimal decision threshold = \", optimal_threshold)\n\n#Optimal C\nprint(\"Optimal C = \", optimal_C)\n\nOptimal decision threshold =  0.46\nOptimal C =  4.291934260128778\n\n\n\n# Developing the model with stratified splitting, optimal decision threshold probability, and optimal 'C'\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg = LogisticRegression(C = optimal_C)\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n# Performance metrics computation for the optimal threshold probability\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob > optimal_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8509259259259259\nPrecision:  0.7804878048780488\nRecall:  0.5925925925925926\n\n\n\n\n\nLater in the course, we’ll see the sklearn function GridSearchCV, which is used to optimize several model hyperparameters simultaneously with \\(K\\)-fold cross validation, while avoiding for loops."
  },
  {
    "objectID": "Lec2_Regression_splines.html#polynomial-regression-vs-regression-splines",
    "href": "Lec2_Regression_splines.html#polynomial-regression-vs-regression-splines",
    "title": "2  Regression splines",
    "section": "2.1 Polynomial regression vs Regression splines",
    "text": "2.1 Polynomial regression vs Regression splines\n\n2.1.1 Model of degree 1\n\nX = pd.DataFrame(train['mileage'])\nX_test = pd.DataFrame(test['mileage'])\ny = train['price']\nlr_model = LinearRegression()\nlr_model.fit(X, y);\n\n\n#Regression spline of degree 1\n\n#Creating basis functions for splines of degree 1\ntransformed_x = dmatrix(\"bs(mileage , knots=(33000,66000,100000), degree = 1, include_intercept = False)\",\n                        data = trainf,return_type = 'dataframe')\n\n\ntransformed_x.head()\n\n\n\n\n\n  \n    \n      \n      Intercept\n      bs(mileage, knots=(33000, 66000, 100000), degree=1, include_intercept=False)[0]\n      bs(mileage, knots=(33000, 66000, 100000), degree=1, include_intercept=False)[1]\n      bs(mileage, knots=(33000, 66000, 100000), degree=1, include_intercept=False)[2]\n      bs(mileage, knots=(33000, 66000, 100000), degree=1, include_intercept=False)[3]\n    \n  \n  \n    \n      0\n      1.0\n      0.000303\n      0.000000\n      0.0\n      0.0\n    \n    \n      1\n      1.0\n      0.327646\n      0.000000\n      0.0\n      0.0\n    \n    \n      2\n      1.0\n      0.000152\n      0.000000\n      0.0\n      0.0\n    \n    \n      3\n      1.0\n      0.572563\n      0.000000\n      0.0\n      0.0\n    \n    \n      4\n      1.0\n      0.092333\n      0.907667\n      0.0\n      0.0\n    \n  \n\n\n\n\nNote that the truncated power basis in the class presentation is conceptually simple to understand, it may run into numerical issues as powers of large numbers can lead to severe rounding errors. The bs() function generates the B-spline basis, which allows for efficient computation, especially in case of a large number of knots. All the basis function values are normalized to be in [0, 1] in the B-spline basis. Although we’ll use the B-spline basis functions to fit splines, details regarding the B-spline basis functions are not included in the syllabus.\nWe actually don’t need to separately generate basis functions, and then fit the model. We can do it in the same line of code using the statsmodels OLS method.\n\n# Regression spline model with linear splines\nreg_spline_model = smf.ols('price~bs(mileage, knots = (33000,66000,100000), degree = 1, include_intercept = False)', \n                           data = train).fit()\n\n\n#Visualizing polynomial model and the regression spline model of degree 1\n\nknots = [33000,66000,100000] #Knots for the spline\nd=1 #Degree of predictor in the model\n#Writing a function to visualize polynomial model and the regression spline model of degree d\ndef viz_models():\n    fig, axes = plt.subplots(1,2,figsize = (15,5))\n    plt.subplots_adjust(wspace=0.2)\n\n    #Visualizing the linear regression model\n    pred_price = lr_model.predict(X)\n    sns.scatterplot(ax = axes[0],x = 'mileage', y = 'price', data = train, color = 'orange')\n    sns.lineplot(ax = axes[0],x = train.mileage, y = pred_price, color = 'blue')\n    axes[0].set_title('Polynomial regression model of degree '+str(d))\n    \n    #Visualizing the regression splines model of degree 'd'    \n    axes[1].set_title('Regression splines model of degree '+ str(d))\n    sns.scatterplot(ax=axes[1],x = 'mileage', y = 'price', data = train, color = 'orange')\n    sns.lineplot(ax=axes[1],x = train.mileage, y = reg_spline_model.predict(), color = 'blue')\n    for i in range(3):\n        plt.axvline(knots[i], 0,100,color='red')\nviz_models()\n\n\n\n\nWe observe the regression splines model better fits the data as compared to the polynomial regression model. This is because regression splines of degree 1 fit piecewise polynomials, or linear models on sub-sections of the predictor, which helps better capture the trend. However, this added flexibility may also lead to overfitting. Hence, one must be careful to check for overfitting when using splines. Overfitting may be checked by k-fold cross validation or comparing test and train errors.\nThe red lines in the plot on the right denote the position of knots. Knots separate distinct splines.\nAlthough, we can separately generate the basis functions for test data, it may lead to incaccurate results if the distribution of the predictor values in test data is different from that in the train data. This is because the B-spline basis functions of train data are generated after normalizing the predictor values. If the basis functions of test data are generated independently, their values may be inaccurate, as they will depend on the domain space spanned by the test data.\n\n# Basis functions for test data - avoid generating basis functions separately for test data\n# as the test data normailization may be different from the train data normalization\ntest_x = dmatrix(\"bs(mileage , knots=(33000,66000,100000), degree = 1, include_intercept = False)\",data = test,\n                                                                                                  return_type = 'dataframe')\n\n\n#Function to compute RMSE (root mean squared error on train and test datasets)\ndef rmse():\n    #Error on train data for the linear regression model\n    print(\"RMSE on train data:\")\n    print(\"Linear regression:\", np.sqrt(mean_squared_error(lr_model.predict(X),train.price)))\n\n    #Error on train data for the regression spline model\n    print(\"Regression splines:\", np.sqrt(mean_squared_error(reg_spline_model.predict(X),train.price)))\n    \n    #Error on test data for the linear regression model\n    print(\"\\nRMSE on test data:\")\n    print(\"Linear regression:\",np.sqrt(mean_squared_error(lr_model.predict(X_test),test.price)))\n\n    #Error on test data for the regression spline model\n    print(\"Regression splines:\",np.sqrt(mean_squared_error(reg_spline_model.predict(X_test),test.price)))    \nrmse()\n\nRMSE on train data:\nLinear regression: 14403.250083261853\nRegression splines: 13859.640716531134\n\nRMSE on test data:\nLinear regression: 14370.94086395544\nRegression splines: 13770.118474361932\n\n\n\n\n2.1.2 Model of degree 2\nA higher degree model will lead to additional flexibility for both polynomial and regression splines models.\n\n#Including mileage squared as a predictor and developing the model\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)', data = train)\nlr_model = ols_object.fit()\n\n#Regression spline of degree 2\nreg_spline_model = smf.ols('price~bs(mileage, knots = (33000,66000,100000), degree = 2, include_intercept = False)', \n                           data = train).fit()\n\n\nd=2\nviz_models()\n\n\n\n\nUnlike polynomial regression, splines functions avoid imposing a global structure on the non-linear function of X. This provides a better local fit to the data.\n\nrmse()\n\nRMSE on train data:\nLinear regression: 14009.819556665143\nRegression splines: 13818.572654146721\n\nRMSE on test data:\nLinear regression: 13944.20691909441\nRegression splines: 13660.777953039395\n\n\n\n\n2.1.3 Model of degree 3\n\n#Including mileage cube squared as a predictor and developing the model\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)+I(mileage**3)', data = train)\nlr_model = ols_object.fit()\n\n#Regression spline of degree 3\nreg_spline_model = smf.ols('price~bs(mileage, knots = (33000,66000,100000), degree = 3, include_intercept = False)', \n                           data = train).fit()\n\n\nd=3\nviz_models()\n\n\n\n\nUnlike polynomial regression, splines functions avoid imposing a global structure on the non-linear function of X. This provides a better local fit to the data.\n\nrmse()\n\nRMSE on train data:\nLinear regression: 13891.962447594644\nRegression splines: 13822.70511947823\n\nRMSE on test data:\nLinear regression: 13789.708418357186\nRegression splines: 13683.776494331632"
  },
  {
    "objectID": "Lec2_Regression_splines.html#regression-splines-with-knots-at-uniform-quantiles-of-data",
    "href": "Lec2_Regression_splines.html#regression-splines-with-knots-at-uniform-quantiles-of-data",
    "title": "2  Regression splines",
    "section": "2.2 Regression splines with knots at uniform quantiles of data",
    "text": "2.2 Regression splines with knots at uniform quantiles of data\nIf degrees of freedom are provided instead of knots, the knots are by default chosen at uniform quantiles of data. For example if there are 7 degrees of freedom (including the intercept), then there will be 7-4 = 3 knots. These knots will be chosen at the 25th, 50th and 75th quantiles of the data.\n\n#Regression spline of degree 3\n\n#Regression spline of degree 3 with knots at uniform quantiles of data\nreg_spline_model = smf.ols('price~bs(mileage, df = 6, degree = 3, include_intercept = False)', \n                           data = train).fit()\n\n\nd=3\nunif_knots = pd.qcut(train.mileage,4,retbins=True)[1][1:4]\nknots=unif_knots\nviz_models()\n\n\n\n\nSplines can be unstable at the outer range of predictors. Note that splines are themselves piecewise polynomials with no constraints at the 2 extreme ends of the predictor space. Thus, they may become unstable at those 2 ends. In the right scatter plot, we can see that price has a decreasing trend with mileage. However on the extreme left of the plot, we see the trend reversing with regard to the model, which suggests potential overfitting. Also, from the domain knowledge about cars we know that there is no reason why price will reduce if the car is relatively new. Thus, there may be overfitting with cubic splines at / near the extreme points of the domain space. In the figure (on the right), the left-most spline may be overfitting.\nThis motivates us to introduce natural cubic splines (below), which help with the stability at extreme points by enforcing the spline to be linear at those points. We may also think about it as another kind of a “knot” being put at the two ends to make the spline stable at these points.\n\nrmse()\n\nRMSE on train data:\nLinear regression: 13891.962447594644\nRegression splines: 13781.79102252679\n\nRMSE on test data:\nLinear regression: 13789.708418357186\nRegression splines: 13605.726076704668"
  },
  {
    "objectID": "Lec2_Regression_splines.html#natural-cubic-splines",
    "href": "Lec2_Regression_splines.html#natural-cubic-splines",
    "title": "2  Regression splines",
    "section": "2.3 Natural cubic splines",
    "text": "2.3 Natural cubic splines\nPage 298: “A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This additional constraint means that natural splines generally produce more stable estimates at the boundaries.”\n\n#Natural cubic spline\n\n#Creating basis functions for the natural cubic spline\nreg_spline_model = smf.ols('price~cr(mileage, df = 4)', \n                           data = train).fit()\n\n\nd=3;\nunif_knots = pd.qcut(train.mileage,4,retbins=True)[1][1:4]\nknots=unif_knots\nviz_models()\n\n\n\n\nNote that the natural cubic spline is more stable than a cubic splines with knots at uniformly distributed quantiles.\n\nrmse()\n\nRMSE on train data:\nLinear regression: 13891.962447594644\nRegression splines: 13826.125469174143\n\nRMSE on test data:\nLinear regression: 13789.708418357186\nRegression splines: 13660.35327661836"
  },
  {
    "objectID": "Lec2_Regression_splines.html#generalized-additive-model-gam",
    "href": "Lec2_Regression_splines.html#generalized-additive-model-gam",
    "title": "2  Regression splines",
    "section": "2.4 Generalized additive model (GAM)",
    "text": "2.4 Generalized additive model (GAM)\nGAM allows for flexible nonlinearities in several variables, but retain the additive structure of linear models. In a GAM, non-linear basis functions of predictors can be used as predictors of a linear regression model. For example, \\[y = f_1(X_1) + f_2(X_2) + \\epsilon\\] is a GAM, where \\(f_1(.)\\) may be a cubic spline based on the predictor \\(X_1\\), and \\(f_2(.)\\) may be a step function based on the predictor \\(X_2\\).\n\n#GAM\n#GAM includes cubic splines for mileage. Other predictors are year, engineSize, mpg, mileage and their interactions\nmodel_gam = smf.ols('price~bs(mileage,df=6,degree = 3)+year*engineSize*mpg*mileage', data = train).fit()\n\npreds = model_gam.predict(test)\nnp.sqrt(mean_squared_error(preds,test.price))\n\n8393.773177637542\n\n\n\n#GAM\n#GAM includes cubic splines for mileage, year, engineSize, mpg, and interactions of all predictors\nmodel_gam = smf.ols('price~bs(mileage,df=6,degree = 3)+bs(mpg,df=6,degree = 3)+\\\nbs(engineSize,df=6,degree = 3)+year*engineSize*mpg*mileage', data = train).fit()\n\npreds = model_gam.predict(test)\nnp.sqrt(mean_squared_error(preds,test.price))\n\n7981.100853841914\n\n\n\nols_object = smf.ols(formula = 'price~(year+engineSize+mileage+mpg)**2+I(mileage**2)+I(mileage**3)', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.704 \n\n\n  Model:                   OLS         Adj. R-squared:        0.703 \n\n\n  Method:             Least Squares    F-statistic:           1308. \n\n\n  Date:             Sun, 09 Apr 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 20:48:35       Log-Likelihood:      -52157. \n\n\n  No. Observations:        4960        AIC:                1.043e+05\n\n\n  Df Residuals:            4950        BIC:                1.044e+05\n\n\n  Df Model:                   9                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                        coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept             -0.0009     0.000    -2.752  0.006    -0.002    -0.000\n\n\n  year                  -1.1470     0.664    -1.728  0.084    -2.448     0.154\n\n\n  engineSize             0.0052     0.000    17.419  0.000     0.005     0.006\n\n\n  mileage              -31.4751     2.621   -12.010  0.000   -36.613   -26.337\n\n\n  mpg                   -0.0201     0.002   -13.019  0.000    -0.023    -0.017\n\n\n  year:engineSize        9.5957     0.254    37.790  0.000     9.098    10.094\n\n\n  year:mileage           0.0154     0.001    11.816  0.000     0.013     0.018\n\n\n  year:mpg               0.0572     0.013     4.348  0.000     0.031     0.083\n\n\n  engineSize:mileage    -0.1453     0.008   -18.070  0.000    -0.161    -0.130\n\n\n  engineSize:mpg       -98.9062    11.832    -8.359  0.000  -122.102   -75.710\n\n\n  mileage:mpg            0.0011     0.000     2.432  0.015     0.000     0.002\n\n\n  I(mileage ** 2)     7.713e-06  3.75e-07    20.586  0.000  6.98e-06  8.45e-06\n\n\n  I(mileage ** 3)    -1.867e-11  1.43e-12   -13.077  0.000 -2.15e-11 -1.59e-11\n\n\n\n\n  Omnibus:       1830.457   Durbin-Watson:         0.634 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   34927.811\n\n\n  Skew:            1.276    Prob(JB):               0.00 \n\n\n  Kurtosis:       15.747    Cond. No.           2.50e+18 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.5e+18. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nnp.sqrt(mean_squared_error(model.predict(test),test.price))\n\n9026.775740000594\n\n\nNote the RMSE with GAM that includes regression splines for mileage is lesser than that of the linear regression model, indicating a better fit."
  },
  {
    "objectID": "Lec2_Regression_splines.html#mars-multivariate-adaptive-regression-splines",
    "href": "Lec2_Regression_splines.html#mars-multivariate-adaptive-regression-splines",
    "title": "2  Regression splines",
    "section": "2.5 MARS (Multivariate Adaptive Regression Splines)",
    "text": "2.5 MARS (Multivariate Adaptive Regression Splines)\n\nfrom pyearth import Earth\nX=pd.DataFrame(train['mileage'])\ny=pd.DataFrame(train['price'])\n\n\n2.5.1 MARS of degree 1\n\nmodel = Earth(max_terms=500, max_degree=1) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\nEarth(max_degree=1, max_terms=500)\n\n\n\nprint(model.summary())\n\nEarth Model\n-------------------------------------\nBasis Function  Pruned  Coefficient  \n-------------------------------------\n(Intercept)     No      -553155      \nh(x0-22141)     Yes     None         \nh(22141-x0)     Yes     None         \nh(x0-3354)      No      -6.23571     \nh(3354-x0)      Yes     None         \nh(x0-15413)     No      -36.9613     \nh(15413-x0)     No      38.167       \nh(x0-106800)    Yes     None         \nh(106800-x0)    No      0.221844     \nh(x0-500)       No      170.039      \nh(500-x0)       Yes     None         \nh(x0-741)       Yes     None         \nh(741-x0)       No      -54.5265     \nh(x0-375)       No      -126.804     \nh(375-x0)       Yes     None         \nh(x0-2456)      Yes     None         \nh(2456-x0)      No      7.04609      \n-------------------------------------\nMSE: 188429705.7549, GCV: 190035470.5664, RSQ: 0.2998, GRSQ: 0.2942\n\n\nModel equation: \\[-553155 -6.23(h(x0-3354)) -36.96(h(x0-15413) + .......... -7.04(h(2456-x0)\\]\n\npred = model.predict(test.mileage)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n13650.2113154515\n\n\n\nsns.scatterplot(x = 'mileage', y = 'price', data = train, color = 'orange')\nsns.lineplot(x = train.mileage, y = model.predict(train.mileage), color = 'blue')\n\n<AxesSubplot:xlabel='mileage', ylabel='price'>\n\n\n\n\n\n\n\n2.5.2 MARS of degree 2\n\nmodel = Earth(max_terms=500, max_degree=2) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\nprint(model.summary())\n\nEarth Model\n-----------------------------------------------\nBasis Function           Pruned  Coefficient   \n-----------------------------------------------\n(Intercept)              No      19369.7       \nh(x0-22141)              Yes     None          \nh(22141-x0)              Yes     None          \nh(x0-7531)*h(22141-x0)   No      3.74934e-05   \nh(7531-x0)*h(22141-x0)   No      -6.74252e-05  \nx0*h(x0-22141)           No      -8.0703e-06   \nh(x0-15012)              Yes     None          \nh(15012-x0)              No      1.79813       \nh(x0-26311)*h(x0-22141)  No      8.85097e-06   \nh(26311-x0)*h(x0-22141)  Yes     None          \n-----------------------------------------------\nMSE: 189264421.5682, GCV: 190298913.1652, RSQ: 0.2967, GRSQ: 0.2932\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\n\npred = model.predict(test.mileage)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n13590.995419204985\n\n\n\nsns.scatterplot(x = 'mileage', y = 'price', data = train, color = 'orange')\nsns.lineplot(x = train.mileage, y = model.predict(train.mileage), color = 'blue')\n\n<AxesSubplot:xlabel='mileage', ylabel='price'>\n\n\n\n\n\nMARS provides a better fit than the splines that we used above. This is because MARS tunes the positions of the knots and considers interactions (also with tuned knots) to improve the model fit. Tuning of knots may improve the fit of splines as well.\n\n\n2.5.3 MARS including categorical variables\n\n#A categorical variable can be turned to dummy variables to use the Earth package for fitting MARS model\ntrain_cat = pd.get_dummies(train)\ntest_cat = pd.get_dummies(test)\n\n\ntrain_cat.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      year\n      mileage\n      tax\n      mpg\n      engineSize\n      price\n      brand_audi\n      brand_bmw\n      brand_ford\n      ...\n      model_ i8\n      transmission_Automatic\n      transmission_Manual\n      transmission_Other\n      transmission_Semi-Auto\n      fuelType_Diesel\n      fuelType_Electric\n      fuelType_Hybrid\n      fuelType_Other\n      fuelType_Petrol\n    \n  \n  \n    \n      0\n      18473\n      2020\n      11\n      145\n      53.3282\n      3.0\n      37980\n      0\n      1\n      0\n      ...\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n    \n    \n      1\n      15064\n      2019\n      10813\n      145\n      53.0430\n      3.0\n      33980\n      0\n      1\n      0\n      ...\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n    \n    \n      2\n      18268\n      2020\n      6\n      145\n      53.4379\n      3.0\n      36850\n      0\n      1\n      0\n      ...\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n    \n    \n      3\n      18480\n      2017\n      18895\n      145\n      51.5140\n      3.0\n      25998\n      0\n      1\n      0\n      ...\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n    \n    \n      4\n      18492\n      2015\n      62953\n      160\n      51.4903\n      3.0\n      18990\n      0\n      1\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 115 columns\n\n\n\n\nX = train_cat[['mileage','mpg','engineSize','year','fuelType_Diesel','fuelType_Electric',\n               'fuelType_Hybrid','fuelType_Petrol']]\nXtest = test_cat[['mileage','mpg','engineSize','year','fuelType_Diesel','fuelType_Electric',\n                  'fuelType_Hybrid','fuelType_Petrol']]\n\n\nmodel = Earth(max_terms=500, max_degree=2) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\nprint(model.summary())\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\n\n\nEarth Model\n---------------------------------------------------------\nBasis Function                      Pruned  Coefficient  \n---------------------------------------------------------\n(Intercept)                         No      2.17604e+06  \nh(engineSize-5.5)                   No      9.80752e+06  \nh(5.5-engineSize)                   No      1.92817e+06  \nh(mileage-21050)                    No      18.687       \nh(21050-mileage)                    No      -177.871     \nh(mileage-21050)*h(5.5-engineSize)  Yes     None         \nh(21050-mileage)*h(5.5-engineSize)  No      -0.224909    \nyear                                No      4126.41      \nh(mpg-53.3495)                      No      344595       \nh(53.3495-mpg)                      Yes     None         \nfuelType_Hybrid*h(5.5-engineSize)   No      6124.34      \nh(mileage-21050)*year               No      -0.00930239  \nh(21050-mileage)*year               No      0.0886455    \nh(engineSize-5.5)*year              No      -4864.84     \nh(5.5-engineSize)*year              No      -952.92      \nh(mileage-1422)*h(53.3495-mpg)      No      -16.62       \nh(1422-mileage)*h(53.3495-mpg)      No      16.4306      \nfuelType_Hybrid                     No      -89090.6     \nh(mpg-21.1063)*h(53.3495-mpg)       Yes     None         \nh(21.1063-mpg)*h(53.3495-mpg)       No      -8815.99     \nh(mpg-23.4808)*h(5.5-engineSize)    No      -3649.97     \nh(23.4808-mpg)*h(5.5-engineSize)    Yes     None         \nh(mpg-20.5188)*year                 No      31.7341      \nh(20.5188-mpg)*year                 Yes     None         \nh(mpg-22.2566)*h(53.3495-mpg)       No      -52.2531     \nh(22.2566-mpg)*h(53.3495-mpg)       No      7916.19      \nh(mpg-22.6767)                      No      7.56432e+06  \nh(22.6767-mpg)                      Yes     None         \nh(mpg-23.9595)*h(mpg-22.6767)       Yes     None         \nh(23.9595-mpg)*h(mpg-22.6767)       No      -63225.4     \nh(mpg-21.4904)*h(22.6767-mpg)       No      -149055      \nh(21.4904-mpg)*h(22.6767-mpg)       Yes     None         \nh(mpg-21.1063)                      No      -887098      \nh(21.1063-mpg)                      Yes     None         \nh(mpg-29.5303)*h(mpg-22.6767)       No      -3028.87     \nh(29.5303-mpg)*h(mpg-22.6767)       Yes     None         \nh(mpg-28.0681)*h(5.5-engineSize)    No      3572.89      \nh(28.0681-mpg)*h(5.5-engineSize)    Yes     None         \nengineSize*h(5.5-engineSize)        No      -2952.65     \nh(mpg-25.3175)*h(mpg-21.1063)       No      -332551      \nh(25.3175-mpg)*h(mpg-21.1063)       No      324298       \nfuelType_Petrol*year                No      -1.37031     \nh(mpg-68.9279)*fuelType_Hybrid      No      -4087.9      \nh(68.9279-mpg)*fuelType_Hybrid      Yes     None         \nh(mpg-31.5043)*h(5.5-engineSize)    Yes     None         \nh(31.5043-mpg)*h(5.5-engineSize)    No      3691.82      \nh(mpg-32.7011)*h(5.5-engineSize)    Yes     None         \nh(32.7011-mpg)*h(5.5-engineSize)    No      -2262.78     \nh(mpg-44.9122)*h(mpg-22.6767)       No      335577       \nh(44.9122-mpg)*h(mpg-22.6767)       No      -335623      \nh(engineSize-5.5)*h(mpg-21.1063)    No      27815        \nh(5.5-engineSize)*h(mpg-21.1063)    Yes     None         \nh(mpg-78.1907)*fuelType_Hybrid      Yes     None         \nh(78.1907-mpg)*fuelType_Hybrid      No      2221.49      \nh(mpg-63.1632)*h(mpg-22.6767)       Yes     None         \nh(63.1632-mpg)*h(mpg-22.6767)       No      21.0093      \nfuelType_Hybrid*h(mpg-53.3495)      No      4121.91      \nh(mileage-22058)*h(53.3495-mpg)     No      16.6177      \nh(22058-mileage)*h(53.3495-mpg)     No      -16.6044     \nh(mpg-21.8985)                      Yes     None         \nh(21.8985-mpg)                      No      371659       \n---------------------------------------------------------\nMSE: 45859836.5623, GCV: 47884649.3622, RSQ: 0.8296, GRSQ: 0.8221\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\n\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n7499.709075454322\n\n\nLet us compare the RMSE of a MARS model with mileage, mpg, engineSize and year with a linear regression model having the same predictors.\n\nX = train[['mileage','mpg','engineSize','year']]\n\n\nmodel = Earth(max_terms=500, max_degree=2) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\nprint(model.summary())\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\n\n\nEarth Model\n----------------------------------------------------------\nBasis Function                      Pruned  Coefficient   \n----------------------------------------------------------\n(Intercept)                         No      -8.13682e+06  \nh(engineSize-5.5)                   No      9.53908e+06   \nh(5.5-engineSize)                   Yes     None          \nh(mileage-21050)                    No      23.4448       \nh(21050-mileage)                    No      -215.861      \nh(mileage-21050)*h(5.5-engineSize)  Yes     None          \nh(21050-mileage)*h(5.5-engineSize)  No      -0.278562     \nyear                                No      4125.85       \nh(mpg-53.3495)                      Yes     None          \nh(53.3495-mpg)                      Yes     None          \nh(mileage-21050)*year               No      -0.0116601    \nh(21050-mileage)*year               No      0.107624      \nh(mpg-53.2957)*h(5.5-engineSize)    No      -59801.3      \nh(53.2957-mpg)*h(5.5-engineSize)    No      59950.5       \nh(engineSize-5.5)*year              No      -4713.74      \nh(5.5-engineSize)*year              No      -755.742      \nh(mileage-1766)*h(53.3495-mpg)      No      -0.00337072   \nh(1766-mileage)*h(53.3495-mpg)      No      -0.144905     \nh(mpg-19.1277)*h(53.3495-mpg)       No      161.153       \nh(19.1277-mpg)*h(53.3495-mpg)       Yes     None          \nh(mpg-23.4808)*h(5.5-engineSize)    Yes     None          \nh(23.4808-mpg)*h(5.5-engineSize)    Yes     None          \nh(mpg-21.4971)*h(5.5-engineSize)    Yes     None          \nh(21.4971-mpg)*h(5.5-engineSize)    Yes     None          \nh(mpg-40.224)*h(5.5-engineSize)     Yes     None          \nh(40.224-mpg)*h(5.5-engineSize)     No      298.139       \nengineSize*h(5.5-engineSize)        No      -2553.17      \nh(mpg-22.2566)                      Yes     None          \nh(22.2566-mpg)                      No      29257.3       \nh(mpg-20.7712)*h(22.2566-mpg)       No      143796        \nh(20.7712-mpg)*h(22.2566-mpg)       No      -1249.17      \nh(mpg-21.4971)*h(22.2566-mpg)       No      -315486       \nh(21.4971-mpg)*h(22.2566-mpg)       Yes     None          \nh(mpg-27.0995)*h(mpg-22.2566)       No      3855.71       \nh(27.0995-mpg)*h(mpg-22.2566)       Yes     None          \nh(mpg-29.3902)*year                 No      6.05449       \nh(29.3902-mpg)*year                 No      -20.176       \nh(mpg-28.0681)*h(5.5-engineSize)    No      59901.6       \nh(28.0681-mpg)*h(5.5-engineSize)    No      -55502.2      \nh(mpg-23.2962)*h(mpg-22.2566)       No      -56126        \nh(23.2962-mpg)*h(mpg-22.2566)       No      73153.9       \nh(mpg-69.0719)*h(mpg-53.3495)       Yes     None          \nh(69.0719-mpg)*h(mpg-53.3495)       No      -124.847      \nh(engineSize-5.5)*h(22.2566-mpg)    No      -20955.8      \nh(5.5-engineSize)*h(22.2566-mpg)    No      -8336.23      \nh(mpg-23.9595)*h(mpg-22.2566)       No      -62983        \nh(23.9595-mpg)*h(mpg-22.2566)       Yes     None          \nh(mpg-23.6406)*h(mpg-22.2566)       No      115253        \nh(23.6406-mpg)*h(mpg-22.2566)       Yes     None          \nh(mpg-56.1908)                      Yes     None          \nh(56.1908-mpg)                      No      -2239.85      \nh(mpg-29.7993)*h(53.3495-mpg)       No      -139.61       \nh(29.7993-mpg)*h(53.3495-mpg)       No      788.756       \n----------------------------------------------------------\nMSE: 49704412.0771, GCV: 51526765.3943, RSQ: 0.8153, GRSQ: 0.8086\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\n\nXtest = test[['mileage','mpg','engineSize','year']]\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n7614.158359050244\n\n\n\nols_object = smf.ols(formula = 'price~(year+engineSize+mileage+mpg)**2', data = train)\nmodel = ols_object.fit()\npred = model.predict(test)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n8729.912066822455\n\n\nThe RMSE for the MARS model is lesser than that of the linear regression model, as expected."
  },
  {
    "objectID": "Lec3_RegressionTrees.html#building-a-regression-tree",
    "href": "Lec3_RegressionTrees.html#building-a-regression-tree",
    "title": "3  Regression trees",
    "section": "3.1 Building a regression tree",
    "text": "3.1 Building a regression tree\nDevelop a regression tree to predict car price based on mileage\n\nX = train['mileage']\ny = train['price']\n\n\n#Defining the object to build a regression tree\nmodel = DecisionTreeRegressor(random_state=1, max_depth=3) \n\n#Fitting the regression tree to the data\nmodel.fit(X.values.reshape(-1,1), y)\n\nDecisionTreeRegressor(max_depth=3, random_state=1)\n\n\n\n#Visualizing the regression tree\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('car_price_tree.png')\nImage(graph.create_png())\n\n\n\n\n\n#prediction on test data\npred=model.predict(test[['mileage']])\n\n\n#RMSE on test data\nnp.sqrt(mean_squared_error(test.price, pred))\n\n13764.798425410803\n\n\n\n#Visualizing the model fit\nXtest = np.linspace(min(X), max(X), 100)\npred_test = model.predict(Xtest.reshape(-1,1))\nsns.scatterplot(x = 'mileage', y = 'price', data = train, color = 'orange')\nsns.lineplot(x = Xtest, y = pred_test, color = 'blue')\n\n<AxesSubplot:xlabel='mileage', ylabel='price'>\n\n\n\n\n\nAll cars falling within the same terminal node have the same predicted price, which is seen as flat line segments in the above model curve.\nDevelop a regression tree to predict car price based on mileage, mpg, engineSize and year\n\nX = train[['mileage','mpg','year','engineSize']]\nmodel = DecisionTreeRegressor(random_state=1, max_depth=3) \nmodel.fit(X, y)\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage','mpg','year','engineSize'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('car_price_tree.png')\nImage(graph.create_png())"
  },
  {
    "objectID": "Lec3_RegressionTrees.html#optimizing-parameters-to-improve-the-regression-tree",
    "href": "Lec3_RegressionTrees.html#optimizing-parameters-to-improve-the-regression-tree",
    "title": "3  Regression trees",
    "section": "3.2 Optimizing parameters to improve the regression tree",
    "text": "3.2 Optimizing parameters to improve the regression tree\nLet us find the optimal depth of the tree and the number of terminal nods (leaves) by cross validation.\n\n3.2.1 Range of hyperparameter values\nFirst, we’ll find the minimum and maximum possible values of the depth and leaves, and then find the optimal value in that range.\n\nmodel = DecisionTreeRegressor(random_state=1) \nmodel.fit(X, y)\n\nprint(\"Maximum tree depth =\", model.get_depth())\n\nprint(\"Maximum leaves =\", model.get_n_leaves())\n\nMaximum tree depth = 29\nMaximum leaves = 4845\n\n\n\n\n3.2.2 Cross validation: Coarse grid\nWe’ll use the sklearn function GridSearchCV to find the optimal hyperparameter values over a grid of possible values. By default, GridSearchCV returns the optimal hyperparameter values based on the coefficient of determination \\(R^2\\). However, the scoring argument of the function can be used to find the optimal parameters based on several different criteria as mentioned in the scoring-parameter documentation.\n\n#Finding cross-validation error for trees \nparameters = {'max_depth':range(2,30, 3),'max_leaf_nodes':range(2,4900, 100)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv)\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \n\nFitting 5 folds for each of 490 candidates, totalling 2450 fits\n0.8433100904754441 {'max_depth': 11, 'max_leaf_nodes': 302}\n\n\nLet us find the optimal hyperparameters based on the mean squared error, instead of \\(R^2\\). Let us compute \\(R^2\\) as well during cross validation, as we can compute multiple performance metrics using the scoring argument. However, when computing multiple performance metrics, we will need to specify the performance metric used to find the optimal hyperparameters with the refit argument.\n\n#Finding cross-validation error for trees \nparameters = {'max_depth':range(2,30, 3),'max_leaf_nodes':range(2,4900, 100)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv,\n                    scoring=['neg_mean_squared_error', 'r2'], refit = 'neg_mean_squared_error')\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \n\nFitting 5 folds for each of 490 candidates, totalling 2450 fits\n-42064467.15261547 {'max_depth': 11, 'max_leaf_nodes': 302}\n\n\nNote that as the GridSearchCV function maximizes the performance metric to find the optimal hyperparameters, we are maximizing the negative mean squared error (neg_mean_squared_error), and the function returns the optimal negative mean squared error.\nLet us visualize the mean squared error based on the hyperparameter values. We’ll use the cross validation results stored in the cv_results_ attribute of the GridSearchCV fit() object.\n\n#Detailed results of k-fold cross validation\ncv_results = pd.DataFrame(model.cv_results_)\ncv_results.head()\n\n\nfig, axes = plt.subplots(1,2,figsize=(14,5))\nplt.subplots_adjust(wspace=0.2)\naxes[0].plot(cv_results.param_max_depth, np.sqrt(-cv_results.mean_test_neg_mean_squared_error), 'o')\naxes[0].set_ylim([6200, 7500])\naxes[0].set_xlabel('Depth')\naxes[0].set_ylabel('K-fold RMSE')\naxes[1].plot(cv_results.param_max_leaf_nodes, np.sqrt(-cv_results.mean_test_neg_mean_squared_error), 'o')\naxes[1].set_ylim([6200, 7500])\naxes[1].set_xlabel('Leaves')\naxes[1].set_ylabel('K-fold RMSE');\n\n\n\n\nWe observe that for a depth of around 8-14, and number of leaves within 1000, we get the lowest \\(K\\)-fold RMSE. So, we should do a finer search in that region to obtain more precise hyperparameter values.\n\n\n3.2.3 Cross validation: Finer grid\n\n#Finding cross-validation error for trees\nstart_time = tm.time()\nparameters = {'max_depth':range(8,15),'max_leaf_nodes':range(2,1000)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv)\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \nprint(\"Time taken =\", round((tm.time() - start_time)/60), \"minutes\")\n\nFitting 5 folds for each of 6986 candidates, totalling 34930 fits\n0.8465176078797111 {'max_depth': 10, 'max_leaf_nodes': 262}\nTime taken = 1 minutes\n\n\nFrom the above cross-validation, the optimal hyperparameter values are max_depth = 10 and max_leaf_nodes = 262.\n\n#Developing the tree based on optimal hyperparameters found by cross-validation\nmodel = DecisionTreeRegressor(random_state=1, max_depth=10,max_leaf_nodes=262) \nmodel.fit(X, y)\n\nDecisionTreeRegressor(max_depth=10, max_leaf_nodes=262, random_state=1)\n\n\n\n#RMSE on test data\nXtest = test[['mileage','mpg','year','engineSize']]\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n6921.0404660552895\n\n\nThe RMSE for the decision tree is lower than that of linear regression models and spline regression models (including MARS), with these four predictors. This may be probably due to car price having a highly non-linear association with the predictors.\nPredictor importance: The importance of a predictor is computed as the (normalized) total reduction of the criterion (SSE in case of regression trees) brought by that predictor.\nWarning: impurity-based feature importances can be misleading for high cardinality features (many unique values) Source: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor.feature_importances_\nWhy?\nBecause high cardinality predictors will tend to overfit. When the predictors have high cardinality, it means they form little groups (in the leaf nodes) and then the model “learns” the individuals, instead of “learning” the general trend. The higher the cardinality of the predictor, the more prone is the model to overfitting.\n\nmodel.feature_importances_\n\narray([0.04490344, 0.15882336, 0.29739951, 0.49887369])\n\n\nEngine size is the most important predictor, followed by year, which is followed by mpg, and mileage is the least important predictor."
  },
  {
    "objectID": "Lec3_RegressionTrees.html#cost-complexity-pruning",
    "href": "Lec3_RegressionTrees.html#cost-complexity-pruning",
    "title": "3  Regression trees",
    "section": "3.3 Cost complexity pruning",
    "text": "3.3 Cost complexity pruning\nWhile optimizing parameters above, we optimized them within a range that we thought was reasonable. While doing so, we restricted ourselves to considering only a subset of the unpruned tree. Thus, we could have missed out on finding the optimal tree (or the best model).\nWith cost complexity pruning, we first develop an unpruned tree without any restrictions. Then, using cross validation, we find the optimal value of the tuning parameter \\(\\alpha\\). All the non-terminal nodes for which \\(\\alpha_{eff}\\) is smaller that the optimal \\(\\alpha\\) will be pruned. You will need to check out the link below to understand this better.\nCheck out a detailed explanation of how cost complexity pruning is implemented in sklearn at: https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning\nHere are some informative visualizations that will help you understand what is happening in cost complexity pruning: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n\nmodel = DecisionTreeRegressor(random_state = 1)#model without any restrictions\npath= model.cost_complexity_pruning_path(X,y)# Compute the pruning path during Minimal Cost-Complexity Pruning.\n\n\nalphas=path['ccp_alphas']\n\n\nlen(alphas)\n\n4126\n\n\n\nstart_time = tm.time()\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\ntree = GridSearchCV(DecisionTreeRegressor(random_state=1), param_grid = {'ccp_alpha':alphas}, \n                     scoring = 'neg_mean_squared_error',n_jobs=-1,verbose=1,cv=cv)\ntree.fit(X, y)\nprint (tree.best_score_, tree.best_params_)\nprint(\"Time taken =\",round((tm.time()-start_time)/60), \"minutes\")\n\nFitting 5 folds for each of 4126 candidates, totalling 20630 fits\n-44150619.209031895 {'ccp_alpha': 143722.94076639024}\nTime taken = 2 minutes\n\n\nThe code took 2 minutes to run on a dataset of about 5000 observations and 4 predictors.\n\nmodel = DecisionTreeRegressor(ccp_alpha=143722.94076639024,random_state=1)\nmodel.fit(X, y)\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(test.price, pred))\n\n7306.592294294368\n\n\nThe RMSE for the decision tree with cost complexity pruning is lower than that of linear regression models and spline regression models (including MARS), with these four predictors. However, it is higher than the one obtained with tuning tree parameters using grid search (shown previously). Cost complexity pruning considers a completely unpruned tree unlike the ‘grid search’ method of searching over a grid of hyperparameters such as max_depth and max_leaf_nodes, and thus may seem to be more comprehensive than the ‘grid search’ approach. However, both the approaches may consider trees that are not considered by the other approach, and thus either one may provide a more accurate model. Depending on the grid of parameters chosen for cross validation, the grid search method may be more or less comprehensive than cost complexity pruning.\n\ngridcv_results = pd.DataFrame(tree.cv_results_)\ncv_error = -gridcv_results['mean_test_score']\n\n\n#Visualizing the 5-fold cross validation error vs alpha\nplt.plot(alphas,cv_error)\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('K-fold MSE');\n\n\n\n\n\n#Zooming in the above visualization to see the alpha where the 5-fold cross validation error is minimizing\nplt.plot(alphas[0:4093],cv_error[0:4093])\nplt.xlabel('alpha')\nplt.ylabel('K-fold MSE');\n\n\n\n\n\n3.3.1 Depth vs alpha; Node counts vs alpha\n\nstime = time.time()\ntrees=[]\nfor i in alphas:\n    tree = DecisionTreeRegressor(ccp_alpha=i,random_state=1)\n    tree.fit(X, train['price'])\n    trees.append(tree)\nprint(time.time()-stime)\n\n268.10325384140015\n\n\nThis code takes 4.5 minutes to run\n\nnode_counts = [clf.tree_.node_count for clf in trees]\ndepth = [clf.tree_.max_depth for clf in trees]\n\n\nfig, ax = plt.subplots(1, 2,figsize=(10,6))\nax[0].plot(alphas[0:4093], node_counts[0:4093], marker=\"o\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(alphas[0:4093], depth[0:4093], marker=\"o\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\n#fig.tight_layout()\n\nText(0.5, 1.0, 'Depth vs alpha')\n\n\n\n\n\n\n\n3.3.2 Train and test accuracies (R-squared) vs alpha\n\ntrain_scores = [clf.score(X, y) for clf in trees]\ntest_scores = [clf.score(Xtest, test.price) for clf in trees]\n\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(alphas[0:4093], train_scores[0:4093], marker=\"o\", label=\"train\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax.plot(alphas[0:4093], test_scores[0:4093], marker=\"o\", label=\"test\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax.legend()\nplt.show()"
  },
  {
    "objectID": "Lec4_ClassificationTree.html#building-a-classification-tree",
    "href": "Lec4_ClassificationTree.html#building-a-classification-tree",
    "title": "4  Classification trees",
    "section": "4.1 Building a classification tree",
    "text": "4.1 Building a classification tree\nDevelop a classification tree to predict if a person has diabetes.\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n#Defining the object to build a regression tree\nmodel = DecisionTreeClassifier(random_state=1, max_depth=3) \n\n#Fitting the regression tree to the data\nmodel.fit(X, y)\n\nDecisionTreeClassifier(max_depth=3, random_state=1)\n\n\n\n#Visualizing the regression tree\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =X.columns,precision=2)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n#graph.write_png('car_price_tree.png')\nImage(graph.create_png())\n\n\n\n\n\n# Performance metrics computation \n\n#Computing the accuracy\ny_pred = model.predict(Xtest)\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\ny_pred_prob = model.predict_proba(Xtest)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  73.37662337662337\nROC-AUC:  0.8349197955226512\nPrecision:  0.7777777777777778\nRecall:  0.45901639344262296"
  },
  {
    "objectID": "Lec4_ClassificationTree.html#optimizing-hyperparameters-to-optimize-performance",
    "href": "Lec4_ClassificationTree.html#optimizing-hyperparameters-to-optimize-performance",
    "title": "4  Classification trees",
    "section": "4.2 Optimizing hyperparameters to optimize performance",
    "text": "4.2 Optimizing hyperparameters to optimize performance\nIn case of diabetes, it is important to reduce FNR (False negative rate) or maximize recall. This is because if a person has diabetes, the consequences of predicting that they don’t have diabetes can be much worse than the other way round.\nLet us find the optimal depth of the tree and the number of terminal nods (leaves) that minimizes the FNR or maximizes recall.\nFind the maximum values of depth and number of leaves.\n\n#Defining the object to build a regression tree\nmodel = DecisionTreeClassifier(random_state=1) \n\n#Fitting the regression tree to the data\nmodel.fit(X, y)\n\nDecisionTreeClassifier(random_state=1)\n\n\n\n# Maximum number of leaves\nmodel.get_n_leaves()\n\n118\n\n\n\n# Maximum depth\nmodel.get_depth()\n\n14\n\n\n\n#Defining parameters and the range of values over which to optimize\nparam_grid = {    \n    'max_depth': range(2,14),\n    'max_leaf_nodes': range(2,118),\n    'max_features': range(3, 8)\n}\n\n\n#Grid search to optimize parameter values\n\nstart_time = time.time()\nskf = StratifiedKFold(n_splits=5)#The folds are made by preserving the percentage of samples for each class.\n\n#Minimizing FNR is equivalent to maximizing recall\ngrid_search = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, scoring=['precision','recall'], \n                           refit=\"recall\", cv=skf, n_jobs=-1, verbose = True)\ngrid_search.fit(X, y)\n\n# make the predictions\ny_pred = grid_search.predict(Xtest)\n\nprint('Train accuracy : %.3f'%grid_search.best_estimator_.score(X, y))\nprint('Test accuracy : %.3f'%grid_search.best_estimator_.score(Xtest, ytest))\nprint('Best accuracy Through Grid Search : %.3f'%grid_search.best_score_)\n\nprint('Best params for recall')\nprint(grid_search.best_params_)\n\nprint(\"Time taken =\", round((time.time() - start_time)), \"seconds\")\n\nFitting 5 folds for each of 6960 candidates, totalling 34800 fits\nTrain accuracy : 0.915\nTest accuracy : 0.747\nBest accuracy Through Grid Search : 0.624\nBest params for recall\n{'max_depth': 10, 'max_features': 4, 'max_leaf_nodes': 59}\nTime taken = 36 seconds"
  },
  {
    "objectID": "Lec4_ClassificationTree.html#optimizing-the-decision-threshold-probability",
    "href": "Lec4_ClassificationTree.html#optimizing-the-decision-threshold-probability",
    "title": "4  Classification trees",
    "section": "4.3 Optimizing the decision threshold probability",
    "text": "4.3 Optimizing the decision threshold probability\nNote that decision threshold probability is not tuned with GridSearchCV because GridSearchCV is a technique used for hyperparameter tuning in machine learning models, and the decision threshold probability is not a hyperparameter of the model.\nThe decision threshold is set to 0.5 by default during hyperparameter tuning with GridSearchCV.\nGridSearchCV is used to tune hyperparameters that control the internal settings of a machine learning model, such as learning rate, regularization strength, and maximum tree depth, among others. These hyperparameters affect the model’s internal behavior and performance. On the other hand, the decision threshold is an external parameter that is used to interpret the model’s output and make predictions based on the predicted probabilities.\nTo tune the decision threshold, one typically needs to manually adjust it after the model has been trained and evaluated using a specific set of hyperparameter values. This can be done using methods, which involve evaluating the model’s performance at different decision threshold values and selecting the one that best meets the desired trade-off between false positives and false negatives based on the specific problem requirements.\nAs the recall will always be 100% for a decision threshold probability of zero, we’ll find a decision threshold probability that balances recall with another performance metric such as precision, false positive rate, accuracy, etc. Below are a couple of examples that show we can balance recall with (1) precision or (2) false positive rate.\n\n4.3.1 Balancing recall with precision\nWe can find a threshold probability that balances recall with precision.\n\nmodel = DecisionTreeClassifier(random_state=1, max_depth = 5, max_leaf_nodes=6, max_features=8).fit(X, y)\n\n# Note that we are using the cross-validated predicted probabilities, instead of directly using the \n# predicted probabilities on train data, as the model may be overfitting on the train data, and \n# may lead to misleading results\ncross_val_ypred = cross_val_predict(DecisionTreeClassifier(random_state=1, max_depth = 5, \n                                                        max_leaf_nodes=6, max_features=8), X, \n                                              y, cv = 5, method = 'predict_proba')\n\np, r, thresholds = precision_recall_curve(y, cross_val_ypred[:,1])\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n# Thresholds with precision and recall\nnp.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\n\narray([[0.        , 0.33713355, 1.        ],\n       [0.03703704, 0.34453782, 0.99033816],\n       [0.05511811, 0.34812287, 0.98550725],\n       [0.08241758, 0.36413043, 0.97101449],\n       [0.08743169, 0.38235294, 0.94202899],\n       [0.11557789, 0.40511727, 0.9178744 ],\n       [0.125     , 0.43203883, 0.85990338],\n       [0.14230769, 0.43349754, 0.85024155],\n       [0.20833333, 0.48255814, 0.80193237],\n       [0.25490196, 0.51290323, 0.76811594],\n       [0.26086957, 0.5248227 , 0.71497585],\n       [0.26785714, 0.53136531, 0.69565217],\n       [0.28301887, 0.54724409, 0.67149758],\n       [0.36923077, 0.56504065, 0.67149758],\n       [0.51666667, 0.58008658, 0.647343  ],\n       [0.5483871 , 0.59241706, 0.60386473],\n       [0.59459459, 0.58974359, 0.55555556],\n       [0.64516129, 0.60335196, 0.52173913],\n       [0.65789474, 0.61077844, 0.49275362],\n       [0.67105263, 0.64052288, 0.47342995],\n       [0.68421053, 0.64347826, 0.35748792],\n       [0.68707483, 0.62365591, 0.28019324],\n       [0.69911504, 0.61290323, 0.18357488],\n       [0.728     , 0.61764706, 0.10144928],\n       [1.        , 0.        , 0.        ]])\n\n\nSuppose, we wish to have at least 80% recall, with the highest possible precision. Then, based on the precision-recall curve (or the table above), we should have a decision threshold probability of 0.2.\nLet’s assess the model’s performance on test data with a threshold probability of 0.2.\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.2\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob > desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  75.97402597402598\nROC-AUC:  0.825753569539926\nPrecision:  0.6538461538461539\nRecall:  0.8360655737704918\n\n\n\n\n\n\n\n4.3.2 Balancing recall with false positive rate\nSuppose we wish to balance recall with false positive rate. We can optimize the model to maximize ROC-AUC, and then choose a point on the ROC-curve that balances recall with the false positive rate.\n\n# Defining parameters and the range of values over which to optimize\nparam_grid = {    \n    'max_depth': range(2,14),\n    'max_leaf_nodes': range(2,118),\n    'max_features': range(3, 9)\n}\n\n\n#Grid search to optimize parameter values\n\nstart_time = time.time()\nskf = StratifiedKFold(n_splits=5)#The folds are made by preserving the percentage of samples for each class.\n\n#Minimizing FNR is equivalent to maximizing recall\ngrid_search = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, scoring=['precision','recall',\n                            'roc_auc'], refit=\"roc_auc\", cv=skf, n_jobs=-1, verbose = True)\ngrid_search.fit(X, y)\n\n# make the predictions\ny_pred = grid_search.predict(Xtest)\n\nprint('Best params for recall')\nprint(grid_search.best_params_)\n\nprint(\"Time taken =\", round((time.time() - start_time)), \"seconds\")\n\nFitting 5 folds for each of 8352 candidates, totalling 41760 fits\nBest params for recall\n{'max_depth': 4, 'max_features': 8, 'max_leaf_nodes': 9}\nTime taken = 61 seconds\n\n\n\nmodel = DecisionTreeClassifier(random_state=1, max_depth = 4, max_leaf_nodes=9, max_features=8).fit(X, y)\n\n\ncross_val_ypred = cross_val_predict(DecisionTreeClassifier(random_state=1, max_depth = 4, \n                                                           max_leaf_nodes=9, max_features=8), X, \n                                              y, cv = 5, method = 'predict_proba')\n\nfpr, tpr, auc_thresholds = roc_curve(y, cross_val_ypred[:,1])\nprint(auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot(fpr, tpr, 'o', color = 'blue')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(y, cross_val_ypred[:,1])\nplot_roc_curve(fpr, tpr)\n\n0.7642345903215468\n\n\n\n\n\n\n# Thresholds with TPR and FPR\nall_thresholds = np.concatenate([auc_thresholds.reshape(-1,1), tpr.reshape(-1,1), fpr.reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,1]>0.8,:]\n# As the values in 'recall_more_than_80' are arranged in increasing order of recall and decreasing threshold,\n# the first value will provide the maximum threshold probability for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the minimum possible FPR\nrecall_more_than_80[0]\n\narray([0.25      , 0.80676329, 0.36855037])\n\n\nSuppose, we wish to have at least 80% recall, with the lowest possible precision. Then, based on the ROC-AUC curve, we should have a decision threshold probability of 0.25.\nLet’s assess the model’s performance on test data with a threshold probability of 0.25.\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.25\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob > desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  75.32467532467533\nROC-AUC:  0.8421470121628767\nPrecision:  0.6385542168674698\nRecall:  0.8688524590163934"
  },
  {
    "objectID": "Lec4_ClassificationTree.html#cost-complexity-pruning",
    "href": "Lec4_ClassificationTree.html#cost-complexity-pruning",
    "title": "4  Classification trees",
    "section": "4.4 Cost complexity pruning",
    "text": "4.4 Cost complexity pruning\nJust as we did cost complexity pruning in a regression tree, we can do it to optimize the model for a classification tree.\n\nmodel = DecisionTreeClassifier(random_state = 1)#model without any restrictions\npath= model.cost_complexity_pruning_path(X,y)# Compute the pruning path during Minimal Cost-Complexity Pruning.\n\n\nalphas=path['ccp_alphas']\nlen(alphas)\n\n58\n\n\n\n#Grid search to optimize parameter values\n\nskf = StratifiedKFold(n_splits=5)\ngrid_search = GridSearchCV(DecisionTreeClassifier(random_state = 1), param_grid = {'ccp_alpha':alphas}, \n                                                  scoring=['precision','recall','accuracy'], \n                                                  refit=\"recall\", cv=skf, n_jobs=-1, verbose = True)\ngrid_search.fit(X, y)\n\n# make the predictions\ny_pred = grid_search.predict(Xtest)\n\nprint('Best params for recall')\nprint(grid_search.best_params_)\n\nFitting 5 folds for each of 58 candidates, totalling 290 fits\nBest params for recall\n{'ccp_alpha': 0.010561291712538737}\n\n\n\n# Model with the optimal value of 'ccp_alpha'\nmodel = DecisionTreeClassifier(ccp_alpha=0.01435396,random_state=1)\nmodel.fit(X, y)\n\nDecisionTreeClassifier(ccp_alpha=0.01435396, random_state=1)\n\n\nNow we can tune the decision threshold probability to balance recall with another performance metrics as shown earlier in Section 4.3."
  },
  {
    "objectID": "Lec4_Bagging.html#bagging-regression-trees",
    "href": "Lec4_Bagging.html#bagging-regression-trees",
    "title": "5  Bagging",
    "section": "5.1 Bagging regression trees",
    "text": "5.1 Bagging regression trees\nBag regression trees to develop a model to predict car price using the predictors mileage,mpg,year,and engineSize.\n\n#Bagging the results of 10 decision trees to predict car price\nmodel = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=1,\n                        n_jobs=-1).fit(X, y)\n\n\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5752.0779571060875\n\n\nThe RMSE has reduced a lot by averaging the predictions of 10 trees. The RMSE for a single tree model with optimized paramters was around 7000.\n\n5.1.1 Model accuracy vs number of trees\nHow does the model accuracy vary with the number of trees?\nAs we increase the number of trees, it will tend to reduce the variance of individual trees leading to a more accurate prediction.\n\n#Finding model accuracy vs number of trees\noob_rsquared={};test_rsquared={};oob_rmse={};test_rmse = {}\nfor i in np.linspace(10,400,40,dtype=int):\n    model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=i, random_state=1,\n                        n_jobs=-1,oob_score=True).fit(X, y)\n    oob_rsquared[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_rsquared[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n    oob_rmse[i]=np.sqrt(mean_squared_error(model.oob_prediction_,y))\n    test_rmse[i]=np.sqrt(mean_squared_error(model.predict(Xtest),ytest))\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1069: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\"Some inputs do not have OOB scores. \"\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1069: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\"Some inputs do not have OOB scores. \"\n\n\nAs we are bagging only 10 trees in the first iteration, some of the observations are selected in every bootstrapped sample, and thus they don’t have an out-of-bag error, which is producing the warning. For every observation to have an out-of-bag error, the number of trees must be sufficiently large.\nLet us visualize the out-of-bag (OOB) R-squared and R-squared on test data vs the number of trees.\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),label = 'Out of bag R-squared')\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),'o',color = 'blue')\nplt.plot(test_rsquared.keys(),test_rsquared.values(), label = 'Test data R-squared')\nplt.xlabel('Number of trees')\nplt.ylabel('Rsquared')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x26f886f1610>\n\n\n\n\n\nThe out-of-bag R-squared initially increases, and then stabilizes after a certain number of trees (around 150 in this case). Note that increasing the number of trees further will not lead to overfitting. However, increasing the number of trees will increase the computations. Thus, we don’t need to develop more trees once the R-squared stabilizes.\n\n#Visualizing out-of-bag RMSE and test data RMSE\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rmse.keys(),oob_rmse.values(),label = 'Out of bag RMSE')\nplt.plot(oob_rmse.keys(),oob_rmse.values(),'o',color = 'blue')\nplt.plot(test_rmse.keys(),test_rmse.values(), label = 'Test data RMSE')\nplt.xlabel('Number of trees')\nplt.ylabel('RMSE')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x13712485df0>\n\n\n\n\n\nA similar trend can be seen by plotting out-of-bag RMSE and test RMSE. Note that RMSE is proportional to R-squared. We only need to visualize one of RMSE or R-squared to find the optimal number of trees.\n\n#Bagging with 150 trees\nmodel = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=150, random_state=1,\n                        oob_score=True,n_jobs=-1).fit(X, y)\n\n\n#OOB R-squared\nmodel.oob_score_\n\n0.897561533100511\n\n\n\n#RMSE on test data\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(test.price, pred))\n\n5673.756466489405\n\n\n\n\n5.1.2 Optimizing bagging hyperparameters using grid search\nMore parameters of a bagged regression tree model can be optimized using the typical approach of k-fold cross validation over a grid of parameter values.\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'base_estimator': [DecisionTreeRegressor(random_state = 1),LinearRegression()],#Comparing bagging with a linear regression model as well\n          'n_estimators': [150,200,250],\n          'max_samples': [0.5,1.0],\n          'max_features': [0.5,1.0],\n          'bootstrap': [True, False],\n          'bootstrap_features': [True, False]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\nbagging_regressor_grid = GridSearchCV(BaggingRegressor(random_state=1, n_jobs=-1), \n                                      param_grid =params, cv=cv, n_jobs=-1, verbose=1)\nbagging_regressor_grid.fit(X, y)\n\nprint('Train R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X, y))\nprint('Test R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(Xtest, ytest))\nprint('Best R^2 Score Through Grid Search : %.3f'%bagging_regressor_grid.best_score_)\nprint('Best Parameters : ',bagging_regressor_grid.best_params_)\n\nFitting 5 folds for each of 96 candidates, totalling 480 fits\nTrain R^2 Score : 0.986\nTest R^2 Score : 0.885\nBest R^2 Score Through Grid Search : 0.893\nBest Parameters :  {'base_estimator': DecisionTreeRegressor(random_state=1), 'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200}\n\n\n\n#Model with optimal parameters\nmodel = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=200, random_state=1,\n                        oob_score=True,n_jobs=-1,bootstrap_features=False,bootstrap=True,\n                        max_features=1.0,max_samples=1.0).fit(X, y)\n\n\n#RMSE on test data\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5638.330995970236\n\n\nYou may also use the object bagging_regressor_grid to directly make the prediction.\n\nnp.sqrt(mean_squared_error(test.price, bagging_regressor_grid.predict(Xtest)))\n\n5638.330995970236"
  },
  {
    "objectID": "Lec4_Bagging.html#bagging-for-classification",
    "href": "Lec4_Bagging.html#bagging-for-classification",
    "title": "5  Bagging",
    "section": "5.2 Bagging for classification",
    "text": "5.2 Bagging for classification\nBag classification tree models to predict if a person has diabetes.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n#Bagging the results of 10 decision trees to predict car price\nmodel = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=150, random_state=1,\n                        n_jobs=-1).fit(X, y)\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.23\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob > desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  76.62337662337663\nROC-AUC:  0.8766084963863917\nPrecision:  0.6404494382022472\nRecall:  0.9344262295081968\n\n\n\n\n\nAs a result of bagging, we obtain a model (with a threshold probabiltiy cutoff of 0.23) that has a better performance on test data in terms of almost all the metrics - accuracy, precision (comparable performance), recall, and ROC-AUC, as compared the single tree classification model (with a threshold probability cutoff of 0.23). Note that we have not yet tuned the model using GridSearchCv here, which is shown towards the end of this chapter.\n\n5.2.1 Model accuracy vs number of trees\n\n#Finding model accuracy vs number of trees\noob_accuracy={};test_accuracy={};oob_rmse={};test_rmse = {}\nfor i in np.linspace(10,400,40,dtype=int):\n    model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=i, random_state=1,\n                        n_jobs=-1,oob_score=True).fit(X, y)\n    oob_accuracy[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_accuracy[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:640: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\"Some inputs do not have OOB scores. \"\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:644: RuntimeWarning: invalid value encountered in true_divide\n  oob_decision_function = (predictions /\n\n\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),label = 'Out of bag accuracy')\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),'o',color = 'blue')\nplt.plot(test_accuracy.keys(),test_accuracy.values(), label = 'Test data accuracy')\nplt.xlabel('Number of trees')\nplt.ylabel('Rsquared')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x223302e62e0>\n\n\n\n\n\n\n#ROC curve on training data\nypred = model.predict_proba(X)[:, 1]\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nprint(auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nplot_roc_curve(fpr, tpr)\n\n1.0\n\n\n\n\n\nNote that there is perfect separation in train data as ROC-AUC = 1. This shows that the model is probably overfitting. However, this also shows that, despite the reduced variance (as compared to a single tree), the bagged tree model is flexibly enough to perfectly separate the classes.\n\n#ROC curve on test data\nypred = model.predict_proba(Xtest)[:, 1]\nfpr, tpr, auc_thresholds = roc_curve(ytest, ypred)\nprint(\"ROC-AUC = \",auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(ytest, ypred)\nplot_roc_curve(fpr, tpr)\n\nROC-AUC =  0.8781949585757096\n\n\n\n\n\n\n\n5.2.2 Optimizing bagging hyperparameters using grid search\nMore parameters of a bagged classification tree model can be optimized using the typical approach of k-fold cross validation over a grid of parameter values.\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'base_estimator': [DecisionTreeClassifier(random_state = 1),LogisticRegression()],#Comparing bagging with a linear regression model as well\n          'n_estimators': [150,200,250],\n          'max_samples': [0.5,1.0],\n          'max_features': [0.5,1.0],\n          'bootstrap': [True, False],\n          'bootstrap_features': [True, False]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\nbagging_classifier_grid = GridSearchCV(BaggingClassifier(random_state=1, n_jobs=-1), \n                                      param_grid =params, cv=cv, n_jobs=-1, verbose=1,\n                                      scoring = ['precision', 'recall'], refit='recall')\nbagging_classifier_grid.fit(X, y)\n\nprint('Train accuracy : %.3f'%bagging_classifier_grid.best_estimator_.score(X, y))\nprint('Test accuracy : %.3f'%bagging_classifier_grid.best_estimator_.score(Xtest, ytest))\nprint('Best accuracy Through Grid Search : %.3f'%bagging_classifier_grid.best_score_)\nprint('Best Parameters : ',bagging_classifier_grid.best_params_)\n\nFitting 5 folds for each of 96 candidates, totalling 480 fits\nTrain accuracy : 1.000\nTest accuracy : 0.786\nBest accuracy Through Grid Search : 0.573\nBest Parameters :  {'base_estimator': DecisionTreeClassifier(random_state=1), 'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200}\n\n\n\n\n5.2.3 Tuning the decision threshold probability\nWe’ll find a decision threshold probability that balances recall with precision.\n\nmodel = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=1), n_estimators=200, \n                          random_state=1,max_features=1.0, oob_score=True,\n                        max_samples=1.0,n_jobs=-1,bootstrap=True,bootstrap_features=False).fit(X, y)\n\nAs the model is overfitting on the train data, it will not be a good idea to tune the decision threshold probability based on the precision-recall curve on train data, as shown in the figure below.\n\nypred = model.predict_proba(X)[:,1]\np, r, thresholds = precision_recall_curve(y, ypred)\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\nInstead, we should make the precision-recall curve using the out-of-bag predictions, as shown below. The method oob_decision_function_ provides the predicted probability.\n\nypred = model.oob_decision_function_[:,1]\np, r, thresholds = precision_recall_curve(y, ypred)\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n# Thresholds with precision and recall\nall_thresholds = np.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,2]>0.8,:]\n# As the values in 'recall_more_than_80' are arranged in decreasing order of recall and increasing threshold,\n# the last value will provide the maximum threshold probabiltiy for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the maximum possible precision\nrecall_more_than_80[recall_more_than_80.shape[0]-1]\n\narray([0.2804878 , 0.53205128, 0.80193237])\n\n\nSuppose, we wish to have at least 80% recall, with the highest possible precision. Then, based on the precision-recall curve, we should have a decision threshold probability of 0.28.\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.28\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob > desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.22077922077922\nROC-AUC:  0.8802221047065044\nPrecision:  0.6705882352941176\nRecall:  0.9344262295081968\n\n\n\n\n\nNote that this model has a better performance than the untuned bagged model earlier, and the single tree classification model, as expected."
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-for-regression",
    "href": "Lec6_RandomForest.html#random-forest-for-regression",
    "title": "6  Random Forest",
    "section": "6.1 Random Forest for regression",
    "text": "6.1 Random Forest for regression\nNow, let us visualize small trees with the random forest algorithm to see if a predictor dominates all the trees.\n\n#Averaging the results of 10 decision trees, while randomly considering sqrt(4)=2 predictors at each node\n#to split, to predict car price\nmodel = RandomForestRegressor(n_estimators=10, random_state=1,max_features=\"sqrt\",max_depth=3,\n                        n_jobs=-1).fit(X, y)\n\n\n#Change the index of model.estimators_[index] to visualize the 10 random forest trees, one at a time\ndot_data = StringIO()\nexport_graphviz(model.estimators_[4], out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage','mpg','year','engineSize'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n#graph.write_png('car_price_tree.png')\nImage(graph.create_png())\n\n\n\n\nAs two of the four predictors are randomly selected for splitting each node, engineSize no longer seems to dominate the trees. This will tend to reduce correlation among trees, thereby reducing the prediction variance, which in turn will tend to improve prediction accuracy.\n\n#Averaging the results of 10 decision trees, while randomly considering sqrt(4)=2 predictors at each node\n#to split, to predict car price\nmodel = RandomForestRegressor(n_estimators=10, random_state=1,max_features=\"sqrt\",\n                        n_jobs=-1).fit(X, y)\n\n\nmodel.feature_importances_\n\narray([0.16370584, 0.35425511, 0.18552673, 0.29651232])\n\n\nNote that the feature importance of engineSize is reduced in random forests (as compared to bagged trees), and it no longer dominates the trees.\n\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5856.022395768459\n\n\nThe RMSE is similar to that obtained by bagging. We will discuss the comparison later.\n\n6.1.1 Model accuracy vs number of trees\nHow does the model accuracy vary with the number of trees?\nAs we increase the number of trees, it will tend to reduce the variance of individual trees leading to a more accurate prediction.\n\n#Finding model accuracy vs number of trees\noob_rsquared={};test_rsquared={};oob_rmse={};test_rmse = {}\n\nfor i in np.linspace(10,400,40,dtype=int):\n    model = RandomForestRegressor(n_estimators=i, random_state=1,max_features=\"sqrt\",\n                        n_jobs=-1,oob_score=True).fit(X, y)\n    oob_rsquared[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_rsquared[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n    oob_rmse[i]=np.sqrt(mean_squared_error(model.oob_prediction_,y))\n    test_rmse[i]=np.sqrt(mean_squared_error(model.predict(Xtest),ytest))\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:833: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n  warn(\"Some inputs do not have OOB scores. \"\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:833: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n  warn(\"Some inputs do not have OOB scores. \"\n\n\nAs we are ensemble only 10 trees in the first iteration, some of the observations are selected in every bootstrapped sample, and thus they don’t have an out-of-bag error, which is producing the warning. For every observation to have an out-of-bag error, the number of trees must be sufficiently large.\nLet us visualize the out-of-bag (OOB) R-squared and R-squared on test data vs the number of trees.\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),label = 'Out of bag R-squared')\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),'o',color = 'blue')\nplt.plot(test_rsquared.keys(),test_rsquared.values(), label = 'Test data R-squared')\nplt.xlabel('Number of trees')\nplt.ylabel('Rsquared')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x17677ce6580>\n\n\n\n\n\nThe out-of-bag \\(R\\)-squared initially increases, and then stabilizes after a certain number of trees (around 200 in this case). Note that increasing the number of trees further will not lead to overfitting. However, increasing the number of trees will increase the computations. Thus, the number of trees developed should be the number beyond which the \\(R\\)-squared stabilizes.\n\n#Visualizing out-of-bag RMSE and test data RMSE\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rmse.keys(),oob_rmse.values(),label = 'Out of bag RMSE')\nplt.plot(oob_rmse.keys(),oob_rmse.values(),'o',color = 'blue')\nplt.plot(test_rmse.keys(),test_rmse.values(), label = 'Test data RMSE')\nplt.xlabel('Number of trees')\nplt.ylabel('RMSE')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x1767fff7460>\n\n\n\n\n\nA similar trend can be seen by plotting out-of-bag RMSE and test RMSE. Note that RMSE is proportional to R-squared. You only need to visualize one of RMSE or \\(R\\)-squared to find the optimal number of trees.\n\n#Bagging with 150 trees\nmodel = RandomForestRegressor(n_estimators=200, random_state=1,max_features=\"sqrt\",\n                        oob_score=True,n_jobs=-1).fit(X, y)\n\n\n#OOB R-squared\nmodel.oob_score_\n\n0.8998265006519903\n\n\n\n#RMSE on test data\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(test.price, pred))\n\n5647.195064555622"
  },
  {
    "objectID": "Lec6_RandomForest.html#tuning-random-forests",
    "href": "Lec6_RandomForest.html#tuning-random-forests",
    "title": "6  Random Forest",
    "section": "6.2 Tuning random forests",
    "text": "6.2 Tuning random forests\nThe Random forest object has options to set parameters such as depth, leaves, minimum number of observations in a leaf etc., for individual trees. These parameters are useful to prune a decion tree model consisting of a single tree, in order to avoid overfitting due to high variance of an unpruned tree.\nPruning individual trees in random forests is not likely to add much value, since averaging a sufficient number of unpruned trees reduces the variance of the trees, which enhances prediction accuracy. Pruning individual trees is unlikely to further reduce the prediction variance.\nHere is a comment from page 596 of the The Elements of Statistical Learning that supports the above statement: Segal (2004) demonstrates small gains in performance by controlling the depths of the individual trees grown in random forests. Our experience is that using full-grown trees seldom costs much, and results in one less tuning parameter.\nBelow we attempt to optimize parameters that prune individual trees. However, as expected, it does not result in a substantial increase in prediction accuracy.\n\n#Optimizing with OOB score takes half the time as compared to cross validation. \n#The number of models developed with OOB score tuning is one-fifth of the number of models developed with\n#5-fold cross validation\nstart_time = time.time()\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'n_estimators': [250,300,350],\n          'max_depth': [12,15,18],\n          'max_leaf_nodes':[1100,1200,1300],\n          'max_features': [1,2,3,4]}\n\nparam_list=list(it.product(*(params[Name] for Name in params)))\n\noob_score = [0]*len(param_list)\ni=0\nfor pr in param_list:\n    model = RandomForestRegressor(random_state=1,oob_score=True,verbose=False,n_estimators = pr[0],\n                                 max_depth=pr[1],\n                                  max_leaf_nodes=pr[2],max_features=pr[3],\n                                  n_jobs=-1).fit(X,y)\n    oob_score[i] = model.oob_score_\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"Best params = \", param_list[np.argmax(oob_score)])\nprint(\"Best score (R-squared) = \", np.max(oob_score))\n\ntime taken =  1.2196365237236022  minutes\nBest params =  (300, 15, 1200, 3)\nBest score (R-squared) =  0.9018153771851661\n\n\nThere is a very small increase in OOB \\(R\\)-squared after pruning the individual trees.\n\n#Model with optimal parameters\nmodel = RandomForestRegressor(n_estimators=300, random_state=1,max_leaf_nodes=1200,max_depth=15,\n                        oob_score=True,n_jobs=-1, max_features=3).fit(X, y)\n\n\n#RMSE on test data\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5654.435430858449\n\n\nOptimizing depth and leaves of individual trees didn’t improve the prediction accuracy of the model. Important parameters to optmize in random forests will be the number of trees (n_estimators), and number of predictors considered at each split (max_features). However, we’ll see in the assignment that sometimes indivdual pruning of trees may be useful.\n\n#Tuning only n_estimators and max_features produces similar results\nstart_time = time.time()\nparams = {'n_estimators': [250,300,350],\n          'max_features': [1,2,3,4]}\n\nparam_list=list(it.product(*(params[Name] for Name in params)))\n\noob_score = [0]*len(param_list)\ni=0\nfor pr in param_list:\n    model = RandomForestRegressor(random_state=1,oob_score=True,verbose=False,n_estimators = pr[0],\n                                 max_features=pr[1],      n_jobs=-1).fit(X,y)\n    oob_score[i] = model.oob_score_\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"Best params = \", param_list[np.argmax(oob_score)])\nprint(\"Best score (R-squared) = \", np.max(oob_score))\n\ntime taken =  0.12583016157150267  minutes\nBest params =  (300, 2)\nBest score (R-squared) =  0.9005106776136418\n\n\n\n#Model with optimal parameters\nmodel = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5642.45839697972"
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-for-classification",
    "href": "Lec6_RandomForest.html#random-forest-for-classification",
    "title": "6  Random Forest",
    "section": "6.3 Random forest for classification",
    "text": "6.3 Random forest for classification\nRandom forest model to predict if a person has diabetes.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n#Ensembling the results of 10 decision trees\nmodel = RandomForestClassifier(n_estimators=200, random_state=1,max_features=\"sqrt\",n_jobs=-1).fit(X, y)\n\n\n#Feature importance for Random forest\nnp.mean([tree.feature_importances_ for tree in model.estimators_],axis=0)\n\narray([0.08380406, 0.25403736, 0.09000104, 0.07151063, 0.07733353,\n       0.16976023, 0.12289303, 0.13066012])\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.23\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob > desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  72.72727272727273\nROC-AUC:  0.8744050766790058\nPrecision:  0.6021505376344086\nRecall:  0.9180327868852459\n\n\n\n\n\nThe model obtained above is similar to the one obtained by bagging. We’ll discuss the comparison later.\n\n6.3.1 Model accuracy vs number of trees\n\n#Finding model accuracy vs number of trees\noob_accuracy={};test_accuracy={};oob_precision={}; test_precision = {}\nfor i in np.linspace(50,500,45,dtype=int):\n    model = RandomForestClassifier(n_estimators=i, random_state=1,max_features=\"sqrt\",n_jobs=-1,oob_score=True).fit(X, y)\n    oob_accuracy[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_accuracy[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n    oob_pred = (model.oob_decision_function_[:,1]>=0.5).astype(int)     \n    oob_precision[i] = precision_score(y, oob_pred)\n    test_pred = model.predict(Xtest)\n    test_precision[i] = precision_score(ytest, test_pred)\n\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),label = 'Out of bag accuracy')\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),'o',color = 'blue')\nplt.plot(test_accuracy.keys(),test_accuracy.values(), label = 'Test data accuracy')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Classification accuracy')\nplt.legend();\n\n\n\n\nWe can also plot other metrics of interest such as out-of-bag precision vs number of trees.\n\n#Precision vs number of trees\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_precision.keys(),oob_precision.values(),label = 'Out of bag precision')\nplt.plot(oob_precision.keys(),oob_precision.values(),'o',color = 'blue')\nplt.plot(test_precision.keys(),test_precision.values(), label = 'Test data precision')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Precision')\nplt.legend();\n\n\n\n\n\n\n6.3.2 Tuning random forest parameters\nHere we tune the number of predictors to be considered at each node for the split to maximize recall.\n\nstart_time = time.time()\n\nparams = {'n_estimators': [500],\n          'max_features': range(1,9),\n         }\n\nparam_list=list(it.product(*(params[Name] for Name in list(params.keys()))))\noob_recall = [0]*len(param_list)\n\ni=0\nfor pr in param_list:\n    model = RandomForestClassifier(random_state=1,oob_score=True,verbose=False,n_estimators = pr[0],\n                                  max_features=pr[1], n_jobs=-1).fit(X,y)\n    \n    oob_pred = (model.oob_decision_function_[:,1]>=0.5).astype(int)     \n    oob_recall[i] = recall_score(y, oob_pred)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"max recall = \", np.max(oob_recall))\nprint(\"params= \", param_list[np.argmax(oob_recall)])\n\ntime taken =  0.08032723267873128  minutes\nmax recall =  0.5990338164251208\nparams=  (500, 8)\n\n\n\nmodel = RandomForestClassifier(random_state=1,n_jobs=-1,max_features=8,n_estimators=500).fit(X, y)\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.23\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob > desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  76.62337662337663\nROC-AUC:  0.8787237793054822\nPrecision:  0.6404494382022472\nRecall:  0.9344262295081968\n\n\n\n\n\n\nmodel.feature_importances_\n\narray([0.069273  , 0.31211579, 0.08492953, 0.05225877, 0.06179047,\n       0.17732674, 0.12342981, 0.1188759 ])"
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-vs-bagging",
    "href": "Lec6_RandomForest.html#random-forest-vs-bagging",
    "title": "6  Random Forest",
    "section": "6.4 Random forest vs Bagging",
    "text": "6.4 Random forest vs Bagging\nWe saw in the above examples that the performance of random forest was similar to that of bagged trees. This may happen in some cases including but not limited to:\n\nAll the predictors are more or less equally important, and the bagged trees are not higly correlated.\nOne of the predictors dominates the trees, resulting in highly correlated trees. However, each of the highly correlated trees have high prediction accuracy, leading to overall high prediction accuracy of the bagged trees despite the high correlation.\n\nWhen can random forests perform poorly: When the number of variables is large, but the fraction of relevant variables small, random forests are likely to perform poorly with small \\(m\\) (fraction of predictors considered for each split). At each split the chance can be small that the relevant variables will be selected. - Elements of Statistical Learning, page 596.\nHowever, in general, random forests are expected to decorrelate and improve the bagged trees.\nLet us consider a classification example.\n\ndata = pd.read_csv('Heart.csv')\ndata.dropna(inplace = True)\ndata.head()\n\n\n\n\n\n  \n    \n      \n      Age\n      Sex\n      ChestPain\n      RestBP\n      Chol\n      Fbs\n      RestECG\n      MaxHR\n      ExAng\n      Oldpeak\n      Slope\n      Ca\n      Thal\n      AHD\n    \n  \n  \n    \n      0\n      63\n      1\n      typical\n      145\n      233\n      1\n      2\n      150\n      0\n      2.3\n      3\n      0.0\n      fixed\n      No\n    \n    \n      1\n      67\n      1\n      asymptomatic\n      160\n      286\n      0\n      2\n      108\n      1\n      1.5\n      2\n      3.0\n      normal\n      Yes\n    \n    \n      2\n      67\n      1\n      asymptomatic\n      120\n      229\n      0\n      2\n      129\n      1\n      2.6\n      2\n      2.0\n      reversable\n      Yes\n    \n    \n      3\n      37\n      1\n      nonanginal\n      130\n      250\n      0\n      0\n      187\n      0\n      3.5\n      3\n      0.0\n      normal\n      No\n    \n    \n      4\n      41\n      0\n      nontypical\n      130\n      204\n      0\n      2\n      172\n      0\n      1.4\n      1\n      0.0\n      normal\n      No\n    \n  \n\n\n\n\nIn the above dataset, we wish to predict if a person has acquired heart disease (AHD = ‘Yes’), based on their symptoms.\n\n#Response variable\ny = pd.get_dummies(data['AHD'])['Yes']\n\n#Creating a dataframe for predictors with dummy varibles replacing the categorical variables\nX = data.drop(columns = ['AHD','ChestPain','Thal'])\nX = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\nX.head()\n\n\n\n\n\n  \n    \n      \n      Age\n      Sex\n      RestBP\n      Chol\n      Fbs\n      RestECG\n      MaxHR\n      ExAng\n      Oldpeak\n      Slope\n      Ca\n      asymptomatic\n      nonanginal\n      nontypical\n      typical\n      fixed\n      normal\n      reversable\n    \n  \n  \n    \n      0\n      63\n      1\n      145\n      233\n      1\n      2\n      150\n      0\n      2.3\n      3\n      0.0\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n    \n    \n      1\n      67\n      1\n      160\n      286\n      0\n      2\n      108\n      1\n      1.5\n      2\n      3.0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      2\n      67\n      1\n      120\n      229\n      0\n      2\n      129\n      1\n      2.6\n      2\n      2.0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      3\n      37\n      1\n      130\n      250\n      0\n      0\n      187\n      0\n      3.5\n      3\n      0.0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n    \n    \n      4\n      41\n      0\n      130\n      204\n      0\n      2\n      172\n      0\n      1.4\n      1\n      0.0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n    \n  \n\n\n\n\n\nX.shape\n\n(297, 18)\n\n\n\n#Creating train and test datasets\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)"
  },
  {
    "objectID": "Lec6_RandomForest.html#tuning-random-forest",
    "href": "Lec6_RandomForest.html#tuning-random-forest",
    "title": "6  Random Forest",
    "section": "Tuning random forest",
    "text": "Tuning random forest\n\n#Tuning the random forest parameters\nstart_time = time.time()\n\noob_score = {}\n\ni=0\nfor pr in range(1,19):\n    model = RandomForestClassifier(random_state=1,oob_score=True,verbose=False,n_estimators = 500,\n                                  max_features=pr, n_jobs=-1).fit(X,y)\n    oob_score[i] = model.oob_score_\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"max accuracy = \", np.max(list(oob_score.values())))\nprint(\"Best value of max_features= \", np.argmax(list(oob_score.values()))+1)\n\ntime taken =  0.21557459433873494  minutes\nmax accuracy =  0.8249158249158249\nBest value of max_features=  3\n\n\n\nsns.scatterplot(x = oob_score.keys(),y = oob_score.values())\nplt.xlabel('Max features')\nplt.ylabel('Classification accuracy')\n\nText(0, 0.5, 'Classification accuracy')\n\n\n\n\n\nNote that as the value of max_features is increasing, the accuracy is decreasing. This is probably due to the trees getting correlated as we consider more predictors for each split.\n\n#Finding model accuracy vs number of trees\noob_accuracy={};test_accuracy={};\noob_accuracy2={};test_accuacy2={};\n\nfor i in np.linspace(100,500,40,dtype=int):\n    #Bagging\n    model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=i, random_state=1,\n                        n_jobs=-1,oob_score=True).fit(Xtrain, ytrain)\n    oob_accuracy[i]=model.oob_score_  #Returns the out-of-bag classification accuracy of the model\n    test_accuracy[i]=model.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n    \n    #Random forest\n    model2 = RandomForestClassifier(n_estimators=i, random_state=1,max_features=3,\n                        n_jobs=-1,oob_score=True).fit(Xtrain, ytrain)\n    oob_accuracy2[i]=model2.oob_score_  #Returns the out-of-bag classification accuracy of the model\n    test_accuacy2[i]=model2.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n   \n\n\n#Feature importance for bagging\nnp.mean([tree.feature_importances_ for tree in model.estimators_],axis=0)\n\narray([0.04381883, 0.05913479, 0.08585651, 0.07165678, 0.00302965,\n       0.00903484, 0.05890448, 0.01223421, 0.072461  , 0.01337919,\n       0.17495662, 0.18224651, 0.00527156, 0.00953965, 0.00396654,\n       0.00163193, 0.09955286, 0.09332406])\n\n\nNote that no predictor is too important to consider. That’s why a small value of three for max_features is likely to decorrelate trees without compromising the quality of predictions.\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),label = 'Bagging OOB')\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),'o',color = 'blue')\nplt.plot(test_accuracy.keys(),test_accuracy.values(), label = 'Bagging test accuracy')\n\nplt.plot(oob_accuracy2.keys(),oob_accuracy2.values(),label = 'RF OOB')\nplt.plot(oob_accuracy2.keys(),oob_accuracy2.values(),'o',color = 'green')\nplt.plot(test_accuacy2.keys(),test_accuacy2.values(), label = 'RF test accuracy')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Classification accuracy')\nplt.legend(bbox_to_anchor=(0, -0.15, 1, 0), loc=2, ncol=2, mode=\"expand\", borderaxespad=0)\n\n<matplotlib.legend.Legend at 0x187bd2a2640>\n\n\n\n\n\nIn the above example we observe that random forest does improve over bagged trees in terms of classifcation accuracy. Unlike the previous two examples, the optimal value of max_features for random forests is much smaller than the total number of availabe predictors, thereby making the random forest model much different than the bagged tree model."
  },
  {
    "objectID": "Assignment A.html#instructions",
    "href": "Assignment A.html#instructions",
    "title": "Appendix A — Assignment A",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Thursday, 13th April 2023 at 11:59 pm.\nFour points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (1 pt). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file. If your issue doesn’t seem genuine, you will lose points.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)"
  },
  {
    "objectID": "Assignment A.html#bias-variance-trade-off",
    "href": "Assignment A.html#bias-variance-trade-off",
    "title": "Appendix A — Assignment A",
    "section": "A.1 Bias-variance trade-off",
    "text": "A.1 Bias-variance trade-off\nThroughout the course, the conceptual clarity about bias and variance will help you tune the models for optimal performance and enable you to compare different models in terms of bias and variance. In this question, you will perform simulations to understand and visualize bias-variance trade-off as in Fig. 2.12 of the book (page 36).\nAssume that the response yy is a function of the predictors x1x_1 and x2x_2 and includes a random error ϵ\\epsilon, as follows:\ny=f(x1,x2)+ϵ,(A.1)\ny = f(x_1, x_2) + \\epsilon, \\qquad\n \\qquad(A.1)\nwhere the function f(.)f(.) is the Bukin function, x1∼U[−15,−5],x2∼U[−3,3]x_1 \\sim U[-15, -5], x_2 \\sim U[-3, 3], and ϵ∼N(0,σ2);σ=10\\epsilon \\sim N(0, \\sigma^2); \\sigma = 10. Here UU refers to Uniform distribution, and NN refers to normal distribution. Use NumPy to simulate values from these distributions.\nYou will code an algorithm (described below) to compute the expected squared bias, expected variance, var(ϵ\\epsilon) and expected test MSE of the following 7 linear regression models having the predictors as:\n\nx1x_1 and x2x_2\nAll the predictors in the above model, and all polynomial combinations of x1x_1, and x2x_2 of degree 2, which will be x12,x22x_1^2, x_2^2, and x1x2x_1x_2\nAll the predictors in the above model, and all polynomial combinations of x1x_1, and x2x_2 of degree 3, which will be x13,x23,x12x2x_1^3, x_2^3, x_1^2x_2, and x1x22x_1x_2^2\nAll the predictors in the above model, and all polynomial combinations of x1x_1, and x2x_2 of degree 4\nAll the predictors in the above model, and all polynomial combinations of x1x_1, and x2x_2 of degree 5\nAll the predictors in the above model, and all polynomial combinations of x1x_1, and x2x_2 of degree 6\nAll the predictors in the above model, and all polynomial combinations of x1x_1, and x2x_2 of degree 7\n\nAs you can see the models are arranged in increasing order of flexibility / complexity. This corresponds to the horizontal axis of Fig. 2.12 in the book.\nUse the following algorithm to compute the expected squared bias, expected variance, var(ϵ\\epsilon) and expected test MSE of the 7 linear regression models above:\nI. Define the Bukin function that accepts x1x_1 and x2x_2 as parameters and returns the Bukin function value (f(x1,x2)f(x_1, x_2)).\n(2 points)\nII. Repeat steps III - VII for all degrees dd in {1,2,...,7}\\{1, 2, ..., 7\\}\n(2 points)\nIII. Considering a model of degree dd, simulate the following test and train datasets.\nA. Simulate test data\n\nSet a seed of 100. Use the code: np.random.seed(100), where np refers to the numpy library\nSimulate 100 values of x1x_1 from U[−15,−5]U[-15, -5].\nSimulate 100 values of x2x_2 from U[−3,3]U[-3, 3].\nCompute the Bukin function value f(x1,x2)f(x_1, x_2) for the simulated values of x1x_1 and x2x_2.\nUse the function PolynomialFeatures from the preprocessing module of the sklearn library to create all polynomial combinations of x1x_1, and x2x_2 up to degree dd.\n\n(4 points)\nB. Simulate 100 train data sets, where each train data is simulated as follows:\n\nSet a seed of i for simualting the ith train data. Use the code: np.random.seed(i), where np refers to the numpy library.\nSimulate 100 values of x1x_1 from U[−15,−5]U[-15, -5]\nSimulate 100 values of x2x_2 from U[−3,3]U[-3, 3]\nCompute the Bukin function value f(x1,x2)f(x_1, x_2) for the simulated values of x1x_1 and x2x_2\nSimulate the response yy using the above set of simulated values with Equation A.1\nUse the function PolynomialFeatures from the preprocessing module of the sklearn library to create all polynomial combinations of x1x_1, and x2x_2 up to degree dd.\n\n(6 points)\nIV. For each train data in III(B), develop a linear regression model using the LinearRegression() function from the linear_model module of the sklearn library.\n(2 points)\nV. Note that the squared bias at a test point x1_test,x2_testx_{1\\_test}, x_{2\\_test} is:\n[Bias(f̂(x1_test,x2_test))]2=[E(f̂(x1_test,x2_test))−f(x1_test,x2_test)]2,(A.2)\n[Bias(\\hat{f}(x_{1\\_test}, x_{2\\_test}))]^2 = [E(\\hat{f}(x_{1\\_test}, x_{2\\_test})) - f(x_{1\\_test}, x_{2\\_test})]^2, \\qquad\n \\qquad(A.2)\nwhere E(f̂(x1_test,x2_test))E(\\hat{f}(x_{1\\_test}, x_{2\\_test})) is the mean prediction of the 100 trained models at x1_test,x2_testx_{1\\_test}, x_{2\\_test}.\nCompute the overall expected squared bias as the average squared bias at all the test data points, as in the equation below:\n[Bias(f̂(.))]2=1100Σi=1100[Bias(f̂(x1i_test,x2i_test))]2,(A.3)\n[Bias(\\hat{f}(.))]^2 = \\frac{1}{100}\\Sigma_{i=1}^{100} \\big[Bias(\\hat{f}(x_{1i\\_test}, x_{2i\\_test}))\\big]^2, \\qquad\n \\qquad(A.3)\n(8 points)\nVI. Note that the variance at a test point x1_test,x2_testx_{1\\_test}, x_{2\\_test} is Var(f̂(x1_test,x2_test))Var(\\hat{f}(x_{1\\_test}, x_{2\\_test})). Compute the overall expected variance as the average variance at all the test data points, as in the equation below:\nVar(f̂(.))=1100Σi=1100Var(f̂(x1i_test,x2i_test))(A.4)\nVar(\\hat{f}(.)) = \\frac{1}{100}\\Sigma_{i=1}^{100} Var(\\hat{f}(x_{1i\\_test}, x_{2i\\_test})) \\qquad\n \\qquad(A.4)\n(6 points)\nVII. Compute the overall expected test mean squared error as the sum of the expected squared bias (Equation A.3), expected variance (Equation A.4), and error variance (σ2\\sigma^2):\nMSE=[Bias(f̂(.))]2+Var(f̂(.))+σ2,(A.5)\nMSE = [Bias(\\hat{f}(.))]^2 + Var(\\hat{f}(.)) + \\sigma^2, \\qquad\n \\qquad(A.5)\n(4 points)\nVIII. Plot the overall expected squared bias, overall expected variance, and overall expected test MSE (as obtained from Equation A.3, Equation A.4, and Equation A.5 respectively) against the degree dd (or flexibility / complexity) of the model . Your plot should look like one of the plots in Fig. 2.12 of the book.\n(3 points)\nIX. What is the degree of the optimal model, i.e., the degree that provides the best bias-variance trade-off?\n(2 points)\nNote: While coding the algorithm, comment it well so that it is easy to give partial credit in case of mistakes. Include the numerals of the algorithm (such as II(B), V, VI, etc.) in your comments so that it is easy to check your algorithm for completeness."
  },
  {
    "objectID": "Assignment A.html#tuning-a-classification-model-with-sklearn",
    "href": "Assignment A.html#tuning-a-classification-model-with-sklearn",
    "title": "Appendix A — Assignment A",
    "section": "A.2 Tuning a classification model with sklearn",
    "text": "A.2 Tuning a classification model with sklearn\n\nData\nRead the data classification_data.csv. The description of the columns is as follows:\n\nhi_int_prncp_pd: Indicates if a high percentage of the repayments made went to interest rather than principal. Target variable.\nout_prncp_inv: Remaining outstanding principal for portion of total amount funded by investors\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\nint_rate: Interest rate on the loan\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60.\n\nYou will develop and tune a logistic regression model to predict hi_int_prncp_pd based on the rest of the columns (predictors) as per the instructions below.\n\n\nA.2.1 Train-test split\nUse the function train_test_split from the model_selection module of the sklearn library to split the data into 75% train and 25% test. Stratify the split based on the response. Use random_state as 45. Print the proportion of 0s and 1s in both the train and test datasets.\n(4 points)\n\n\nA.2.2 Scaling predictors\nScale the predictors to avoid convergence errors when fitting the logistic regression model.\nNote that last quarter, we were focusing on inference (along with prediction), so we avoided scaling. It is a bit inconvenient to interpret odds with scaled predictors. However, avoiding scaling may lead to convergence errors as some of you saw in your course projects. So, it is a good practice to scale, especially when your focus is prediction.\n(3 points)\n\n\nA.2.3 Tuning the degree\nUse the functions:\n\ncross_val_score from the model_selection module of the sklearn library to tune the degree of the logistic regression model for maximizing the stratified 5-fold prediction accuracy. Consider degrees from 1 to 6.\nPolynomialFeatures from the preprocessing module of the sklearn library to create all polynomial combinations of the predictors up to degree dd.\n\nWhat is the optimal degree?\n(4 points)\nNotes:\n\nA model of degree dd will consist of polynomial transformations and interactions of predictors up to degree dd. For example, a model of degree 2 will consist of the square of each predictor and all 2-factor interactions of the predictors.\nYou may use the newton-cg solver to avoid convergence issues.\nUse the default C value at this point, you will tune it later.\n\n\n\nA.2.4 Test accuracy with optimal degree\nFor the optimal degree identified in the previous question, compute the test accuracy.\n(4 points)\n\n\nA.2.5 Tuning C\nWith the optimal degree identified in the previous question, find the optimal regularization parameter C. Again use the cross_val_score function.\n(3 points)\n\n\nA.2.6 Test accuracy with optimal degree and C\nFor the optimal degree and optimal C identified in the previous questions, compute the test accuracy.\n(3 points)\n\n\nA.2.7 Tuning decision threshold probability\nWith the optimal degree and optimal C identified in the previous questions, find the optimal decision threshold probability to maximize accuracy. Use the cross_val_predict function.\n(4 points)\n\n\nA.2.8 Test accuracy for optimal degree, C, and threshold probability\nFor the optimal degree, optimal C, and optimal decision threshold probabilities identified in the previous questions, compute the test accuracy.\n(4 points)\n\n\nA.2.9 Simultaneous optimization of multiple parameters\nIn the above tuning approach we optimized the hyperparameters and the decision threshold probability sequentially. This is a greedy approach, which doesn’t consider all combinations of hyperparameters and decision threshold probabilities, and thus may fail to find the optimal combination of values that maximize accuracy. Thus, tune both the model hyperparameters - degree and C, and the decision threshold probability simultaneously considering all value combinations. This will take more time, but is likely to provide more accurate optimal parameter values.\n(6 points)\n\n\nA.2.10 Test accuracy with optimal parameters obtained simultaneously\nFor the optimal degree, optimal C, and optimal decision threshold probabilities identified in the previous question, compute the test accuracy.\n(4 points)\n\n\nA.2.11 Optimizing parameters for multiple performance metrics\nFind the optimal C and degree to maximize recall while having a precision of more than 75%. Use the function cross_validate from the model_selection module of the sklearn library.\nNote: cross_validate function is very similar to cross_val_score, the only difference is you can use multiple metrics with the scoring input, as you need in this question.\n(8 points)\n\n\nA.2.12 Performance metrics computation\nFor the optimal degree and C identified in the previous question, compute the following performance metrics on test data. Use sklearn functions, manual computation is not allowed.\n\nPrecision\nRecall\nAccuracy\nROC-AUC\nShow the confusion matrix\n\n(10 points)"
  },
  {
    "objectID": "Assignment B.html#instructions",
    "href": "Assignment B.html#instructions",
    "title": "Appendix B — Assignment B",
    "section": "Instructions",
    "text": "Instructions\n\nYou may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Sunday, 23rd April 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (2 pts). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file. If your issue doesn’t seem genuine, you will lose points.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)\n\n\nFor all questions on cross-validation, you must use sklearn functions."
  },
  {
    "objectID": "Assignment B.html#degrees-of-freedom",
    "href": "Assignment B.html#degrees-of-freedom",
    "title": "Appendix B — Assignment B",
    "section": "B.1 Degrees of freedom",
    "text": "B.1 Degrees of freedom\nFind the number of degrees of freedom of the following models. Exclude the intercept when counting the degrees of freedom. You may either show your calculation, or explain briefly how you are computing the degrees of freedom.\n\nB.1.1 Quadratic spline\nA model with one predictor, where the predictor is transformed into a quadratic spline with 5 knots\n(2 points)\n\n\nB.1.2 Natural cubic splines\nA model with one predictor, where the predictor is transformed into a natural cubic spline with 4 knots\n(2 points)\n\n\nB.1.3 Generalized additive model\nA model with four predictors, where the transformations of the respective predictors are (i) cubic spline transformation with 3 knots, (ii) log transformation, (iii) linear spline transformation with 2 knots, (iv) polynomial transformation of degree 4.\n(4 points)"
  },
  {
    "objectID": "Assignment B.html#number-of-knots",
    "href": "Assignment B.html#number-of-knots",
    "title": "Appendix B — Assignment B",
    "section": "B.2 Number of knots",
    "text": "B.2 Number of knots\nFind the number of knots in the following spline transformations, if each of the transformations corresponds to 7 degrees of freedom (excluding the intercept).\n\nB.2.1 Cubic splines\nCubic spline transformation\n(1 point)\n\n\nB.2.2 Natural cubic splines\nNatural cubic spline transformation\n(1 point)\n\n\nB.2.3 Degree 4 spline\nSpline transformation of degree 4\n(1 point)"
  },
  {
    "objectID": "Assignment B.html#regression-problem",
    "href": "Assignment B.html#regression-problem",
    "title": "Appendix B — Assignment B",
    "section": "B.3 Regression problem",
    "text": "B.3 Regression problem\nRead the file investment_clean_data.csv. This data is a cleaned version of the file train.csv in last quarter’s regression prediction problem. Refer to the link for description of variables. It required some effort to get a RMSE of less than 650 with linear regression. In this question, we’ll use MARS / natural cubic splines to get a RMSE of less than 350 with relatively less effort. Use mean squared error as the performance metric in cross validation.\n\nB.3.1 Data preparation\nPrepare the data for modeling as follows:\n\nUse the Pandas function get_dummies() to convert all the categorical predictors to dummy variables.\nUsing the sklearn function train_test_split, split the data into 20% test and 80% train. Use random_state = 45.\n\nNote:\nA. The function get_dummies() can be used over the entire DataFrame. Don’t convert the categorical variables individually.\nB. The MARS model does not accept categorical predictors, which is why the conversion is done.\nC. The response is money_made_inv\n(2 points)\n\n\nB.3.2 Optimal MARS degree\nUse 55-fold cross validation to find the optimal degree of the MARS model to predict money_made_inv based on all the predictors in the dataset.\nHint: Start from degree 1, and keep going until it doesn’t benefit.\n(4 points)\n\n\nB.3.3 Fitting MARS model\nWith the optimal degree identified in the previous question, fit a MARS model. Print the model summary. What is the degree of freedom of the model (excluding the intercept)?\n(1 + 1 + 2 points)\n\n\nB.3.4 Interpreting MARS basis functions\nBased on the model summary in the previous question, answer the following question. Holding all other predictors constant, what will be the mean increase in money_made_inv for a unit increase in out_prncp_inv, given that out_prncp_inv is in [500, 600], term = 36 (months), loan_amnt = 1000, and int_rate = 0.1?\nFirst, write the basis functions being used to answer the question, and then substitute the values.\nAlso, which basis function is non-zero for the smallest domain space of out_prncp_inv? Also, specify the domain space in which it is non-zero.\n(3 + 2 points)\n\n\nB.3.5 Feature importance\nFind the relative importance of each predictor in the MARS model developed in B.3.3. You may choose any criterion for finding feature importance based on the MARS documentation. Print a DataFrame with 2 columns - one column consisting of predictors arranged in descending order of relative importance, and the second column quantifying their relative importance. Exclude predictors rejected by the model developed in B.3.3.\nNote the forward pass and backward passes of the algorithm perform feature selection without manual intervention.\n(4 points)\n\n\nB.3.6 Prediction\nUsing the model developed in B.3.3, compute the RMSE on test data.\n(2 points)\n\n\nNon-trivial train data\nLet us call the part of the dataset where out_prncp_inv = 0 as a trivial subset of data. For this subset, we can directly predict the response without developing a model (recall the EDA last quarter). For all the questions below, fit / tune the model only on the non-trivial part of the train data. However, when making predictions, and computing RMSE, consider the entire test data. Combine the predictions of the model on the non-trivial subset of test data with the predictions on the trivial subset of test data to make predictions on the entire test data.\n\n\nB.3.7 Prediction with non-trivial train data\nFind the optimal degree of the MARS model based on the non-trivial train data, fit the model, and re-compute the RMSE on test data.\nNote: You should get a lesser RMSE as compared to what you got in B.3.6.\n(4 points)\n\n\nB.3.8 Reducing model variance\nThe MARS model is highly flexible, which makes it a low bias-high variance model. However, high prediction variance increases the expected mean squared error on test data (see equation 2.7 on page 34 of the book). How can you reduce the prediction variance of the model without increasing the bias? Check slide 12 of the bias-variance presentation. The MARS model, in general, corresponds to case B. You can see that by averaging the predictions of multiple models, you will reduce prediction variance without increasing the bias.\nTake 10 samples of train data of the same size as the train data, with replacement. For each sample, fit a MARS model with the optimal degree identified earlier. Use the ithi^{th} model, say f̂i\\hat{f}_i to make prediction fî(𝐱test)\\hat{f_i}(\\mathbf{x}_{test}) on each test data point 𝐱test\\mathbf{x}_{test} (Note that predictions will be made using the model on the non-trivial test data, and without the model on the trivial test data). Compute the average prediction on each test data point based on the 10 models as follows:\nf̂(𝐱test)=110Σ1=110fî(𝐱test)\\hat{f}(\\mathbf{x}_{test}) = \\frac{1}{10}\\Sigma_{1=1}^{10} \\hat{f_i}(\\mathbf{x}_{test})\nConsider f̂(𝐱test)\\hat{f}(\\mathbf{x}_{test}) as the prediction at the test data point 𝐱test\\mathbf{x}_{test}. Compute the RMSE based on this model, which is the average prediction of 10 models. You should get a lesser RMSE as compared to the previous question (B.3.7).\nNote: For ease in grading, use the Pandas DataFrame method sample to take samples with replacement, and put random_state for the ith sample as i, where i goes from 0 to 9.\n(6 points)\n\n\nB.3.9 Generalized additive model (GAM)\nDevelop a Generalized linear model f̂GLM(.)\\hat{f}_{GLM}(.) to predict money_made_inv as follows:\nf̂GLM(𝐱)=β̂0+Σi=14β̂ifi(𝐱),\\hat{f}_{GLM}(\\mathbf{x}) = \\hat{\\beta}_0 + \\Sigma_{i=1}^{4} \\hat{\\beta}_i{f}_i(\\mathbf{x}),\nwhere fi(𝐱){f}_i(\\mathbf{x}) is a MARS model of degree ii.\nPrint the estimated beta coefficients (β̂0,β̂1,β̂2,β̂3,β̂4\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\beta}_3, \\hat{\\beta}_4) of the developed model.\nNote: The model is developed on the non-trivial train data\n(8 points)\n\n\nB.3.10 Prediction with GAM\nUse the GAM developed in the previous question to compute RMSE on test data.\nNote: Predictions will be made using the model on the non-trivial test data, and without the model on the trivial test data\n(5 points)\n\n\nB.3.11 Reducing GAM prediction variance\nAs we reduced the variance of the MARS model in B.3.8, follow the same approach to reduce the variance of the GAM developed in B.3.9, and compute the RMSE on test data.\nNote: You should get a lesser RMSE as compared to what you got in B.3.10.\n(8 points)\n\n\nB.3.12 Natural cubic splines\nEven though MARS is efficient and highly flexible, natural cubic splines work very well too, if tuned properly.\nConsider the predictors identified in the model summary of the MARS model printed in B.3.3. For each predictor, create natural cubic splines basis functions with dd degrees of freedom. Include all-order interactions (i.e., 2-factor, 3-factor, 4-factor interactions, and so on) of all the basis functions. Use the sklearn function cross_val_score() to find and report the optimal degrees of freedom for the natural cubic spline of each predictor.\nConsider degrees of freedom from 3 to 6 for the natural cubic spline transformation of each predictor.\n(8 points)\n\n\nB.3.13 Fitting the natural cubic splines model\nWith the optimal degrees of freedom identified in the previous question, fit a model to predict money_made_inv, where the basis functions correspond to the natural cubic splines of each predictor, and all-factor interactions of the basis functions. Compute the RMSE on test data.\nNote: Predictions will be made using the model on the non-trivial test data, and without the model on the trivial test data\n(4 points)"
  },
  {
    "objectID": "Assignment B.html#gam-for-classification",
    "href": "Assignment B.html#gam-for-classification",
    "title": "Appendix B — Assignment B",
    "section": "B.4 GAM for classification",
    "text": "B.4 GAM for classification\nThe data for this question is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls, where bank clients were called to subscribe for a term deposit.\nThere is one train data - train.csv, which you will use to develop a model. There are two test datasets - test1.csv and test2.csv, which you will use to test your model. Each dataset has the following attributes about the clients called in the marketing campaign:\n\nage: Age of the client\neducation: Education level of the client\nday: Day of the month the call is made\nmonth: Month of the call\ny: did the client subscribe to a term deposit?\nduration: Call duration, in seconds. This attribute highly affects the output target (e.g., if duration=0 then y=‘no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for inference purposes and should be discarded if the intention is to have a realistic predictive model.\n\n(Raw data source: Source. Do not use the raw data source for this assignment. It is just for reference.)\nDevelop a generalized additive model (GAM) to predict the probability of a client subscribing to a term deposit based on age, education, day and month. The model must have:\n(a) Minimum overall classification accuracy of 75% among the classification accuracies on train.csv, test1.csv and test2.csv.\n(b) Minimum recall of 55% among the recall on train.csv, test1.csv and test2.csv.\nPrint the accuracy and recall for all the three datasets - train.csv, test1.csv and test2.csv.\nNote that:\n\nYou cannot use duration as a predictor. The predictor is not useful for prediction because its value is determined after the marketing call ends. However, after the call ends, we already know whether the client responded positively or negatively.\nOne way to develop the model satisfying constrains (a) and (b) is to use spline transformations for age and day, and interacting month with all the predictors (including the spline transformations)\nYou may assume that the distribution of the predictors is the same in all the three datasets. Thus, you may create B-spline basis functions independently for the train and test datasets.\nUse cross-validation on train data to optimize the model hyperparameters, and the decision threshold probability. Then, use the optimal hyperparameters to fit the model on train data. Then, evaluate its accuracy and recall on all the three datasets. Note that the test datasets must only be used to evaluate performance metrics, and not optimize any hyperparameters or decision threshold probability.\n\n(20 points: 10 points for cross validation, 5 points for obtaining and showing the optimal values of the hyperparameters and decision threshold probability, 2 points for fitting the model with the optimal hyperparameters, and 3 points for printing the accuracy & recall on each of the three datasets)"
  },
  {
    "objectID": "Stratified splitting.html#stratified-splitting-with-respect-to-response",
    "href": "Stratified splitting.html#stratified-splitting-with-respect-to-response",
    "title": "Appendix C — Stratified splitting (classification problem)",
    "section": "C.1 Stratified splitting with respect to response",
    "text": "C.1 Stratified splitting with respect to response\nQ: When splitting data into train and test for developing and assessing a classification model, it is recommended to stratify the split with respect to the response. Why?\nA: The main advantage of stratified splitting is that it can help ensure that the training and testing sets have similar distributions of the target variable, which can lead to more accurate and reliable model performance estimates.\nIn many real-world datasets, the target variable may be imbalanced, meaning that one class is more prevalent than the other(s). For example, in a medical dataset, the majority of patients may not have a particular disease, while only a small fraction may have the disease. If a random split is used to divide the dataset into training and testing sets, there is a risk that the testing set may not have enough samples from the minority class, which can lead to biased model performance estimates.\nStratified splitting addresses this issue by ensuring that both the training and testing sets have similar proportions of the target variable. This can lead to more accurate model performance estimates, especially for imbalanced datasets, by ensuring that the testing set contains enough samples from each class to make reliable predictions.\nAnother advantage of stratified splitting is that it can help ensure that the model is not overfitting to a particular class. If a random split is used and one class is overrepresented in the training set, the model may learn to predict that class well but perform poorly on the other class(es). Stratified splitting can help ensure that the model is exposed to a representative sample of all classes during training, which can improve its generalization performance on new, unseen data.\nIn summary, the advantages of stratified splitting are that it can lead to more accurate and reliable model performance estimates, especially for imbalanced datasets, and can help prevent overfitting to a particular class."
  },
  {
    "objectID": "Stratified splitting.html#stratified-splitting-with-respect-to-response-and-categorical-predictors",
    "href": "Stratified splitting.html#stratified-splitting-with-respect-to-response-and-categorical-predictors",
    "title": "Appendix C — Stratified splitting (classification problem)",
    "section": "C.2 Stratified splitting with respect to response and categorical predictors",
    "text": "C.2 Stratified splitting with respect to response and categorical predictors\nQ: Will it be better to stratify the split with respect to the response as well as categorical predictors, instead of only the response? In that case, the train and test datasets will be even more representative of the complete data.\nA: It is not recommended to stratify with respect to both the response and categorical predictors simultaneously, while splitting a dataset into train and test, because doing so may result in the test data being very similar to train data, thereby defeating the purpose of assessing the model on unseen data. This kind of a stratified splitting will tend to make the relationships between the response and predictors in train data also appear in test data, which will result in the performance on test data being very similar to that in train data. Thus, in this case, the ability of the model to generalize to new, unseen data won’t be assessed by test data.\nTherefore, it is generally recommended to only stratify the response variable when splitting the data for model training, and to use random sampling for the predictor variables. This helps to ensure that the model is able to capture the underlying relationships between the predictor variables and the response variable, while still being able to generalize well to new, unseen data.\nIn the extreme scenario, when there are no continuous predictors, and there are enough observations for stratification with respect to the response and the categorical predictors, the train and test datasets may turn out to be exactly the same. Example 1 below illustrates this scenario."
  },
  {
    "objectID": "Stratified splitting.html#example-1",
    "href": "Stratified splitting.html#example-1",
    "title": "Appendix C — Stratified splitting (classification problem)",
    "section": "C.3 Example 1",
    "text": "C.3 Example 1\nThe example below shows that the train and test data can be exactly the same if we stratify the split with respect to response and the categorical predictors.\n\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom itertools import product\nsns.set(font_scale=1.35)\n\nLet us simulate a dataset with 8 observations, two categorical predictors x1 and x2 and the the binary response y.\n\n#Setting a seed for reproducible results\nnp.random.seed(9)\n\n# 8 observations\nn = 8\n\n#Simulating the categorical predictors\nx1 = pd.Series(np.random.randint(0,2,n), name = 'x1')\nx2 = pd.Series(np.random.randint(0,2,n), name = 'x2')\n\n#Simulating the response\npr = (x1==1)*0.7+(x2==0)*0.3# + (x3*0.1>0.1)*0.1\ny = pd.Series(1*(np.random.uniform(size = n) < pr), name = 'y')\n\n#Defining the predictor object 'X'\nX = pd.concat([x1, x2], axis = 1)\n\n#Stratified splitting with respect to the response and predictors to create 50% train and test datasets\nX_train_stratified, X_test_stratified, y_train_stratified,\\\ny_test_stratified = train_test_split(X, y, test_size = 0.5, random_state = 45, stratify=data[['x1', 'x2', 'y']])\n\n#Train and test data resulting from the above stratified splitting\ndata_train = pd.concat([X_train_stratified, y_train_stratified], axis = 1)\ndata_test = pd.concat([X_test_stratified, y_test_stratified], axis = 1)\n\nLet us check the train and test datasets created with stratified splitting with respect to both the predictors and the response.\n\ndata_train\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      y\n    \n  \n  \n    \n      2\n      0\n      0\n      1\n    \n    \n      7\n      0\n      1\n      0\n    \n    \n      3\n      1\n      0\n      1\n    \n    \n      1\n      0\n      1\n      0\n    \n  \n\n\n\n\n\ndata_test\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      y\n    \n  \n  \n    \n      4\n      0\n      1\n      0\n    \n    \n      6\n      1\n      0\n      1\n    \n    \n      0\n      0\n      1\n      0\n    \n    \n      5\n      0\n      0\n      1\n    \n  \n\n\n\n\nNote that the train and test datasets are exactly the same! Stratified splitting tends to have the same proportion of observations corresponding to each strata in both the train and test datasets, where each strata is a unique combination of values of x1, x2, and y. This will tend to make the train and test datasets quite similar!"
  },
  {
    "objectID": "Stratified splitting.html#example-2-simulation-results",
    "href": "Stratified splitting.html#example-2-simulation-results",
    "title": "Appendix C — Stratified splitting (classification problem)",
    "section": "C.4 Example 2: Simulation results",
    "text": "C.4 Example 2: Simulation results\nThe example below shows that train and test set performance will tend to be quite similar if we stratify the datasets with respect to the predictors and the response.\nWe’ll simulate a dataset consisting of 1000 observations, 2 categorical predictors x1 and x2, a continuous predictor x3, and a binary response y.\n\n#Setting a seed for reproducible results\nnp.random.seed(99)\n\n# 1000 Observations\nn = 1000\n\n#Simulating categorical predictors x1 and x2\nx1 = pd.Series(np.random.randint(0,2,n), name = 'x1')\nx2 = pd.Series(np.random.randint(0,2,n), name = 'x2')\n\n#Simulating continuous predictor x3\nx3 = pd.Series(np.random.normal(0,1,n), name = 'x3')\n\n#Simulating the response\npr = (x1==1)*0.7+(x2==0)*0.3 + (x3*0.1>0.1)*0.1\ny = pd.Series(1*(np.random.uniform(size = n) < pr), name = 'y')\n\n#Defining the predictor object 'X'\nX = pd.concat([x1, x2, x3], axis = 1)\n\nWe’ll comparing model performance metrics when the data is split into train and test by performing stratified splitting\n\nOnly with respect to the response\nWith respect to the response and categorical predictors\n\nWe’ll perform 1000 simulations, where the data is split using a different seed in each simulation.\n\n#Creating an empty dataframe to store simulation results of 1000 simulations\naccuracy_iter = pd.DataFrame(columns = {'train_y_stratified','test_y_stratified',\n                                        'train_y_CatPredictors_stratified','test_y_CatPredictors_stratified'})\n\n\n# Comparing model performance metrics when the data is split into train and test by performing stratified splitting\n# (1) only with respect to the response\n# (2) with respect to the response and categorical predictors\n\n# Stratified splitting is performed 1000 times and the results are compared\nfor i in np.arange(1,1000):\n \n    #--------Case 1-------------------#\n    # Stratified splitting with respect to response only to create train and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = i, stratify=y)\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Model accuracy on train and test data, with stratification only on response while splitting \n    # the complete data into train and test\n    accuracy_iter.loc[(i-1), 'train_y_stratified'] = model.score(X_train, y_train)\n    accuracy_iter.loc[(i-1), 'test_y_stratified'] = model.score(X_test, y_test)\n        \n    #--------Case 2-------------------#\n    # Stratified splitting with respect to response and categorical predictors to create train \n    # and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = i, \n                                                        stratify=pd.concat([x1, x2, y], axis = 1))\n    model.fit(X_train, y_train)\n\n    # Model accuracy on train and test data, with stratification on response and predictors while \n    # splitting the complete data into train and test\n    accuracy_iter.loc[(i-1), 'train_y_CatPredictors_stratified'] = model.score(X_train, y_train)\n    accuracy_iter.loc[(i-1), 'test_y_CatPredictors_stratified'] = model.score(X_test, y_test)\n    \n# Converting accuracy to numeric\naccuracy_iter = accuracy_iter.apply(lambda x:x.astype(float), axis = 1)\n\n\nDistribution of train and test accuracies\nThe table below shows the distribution of train and test accuracies when the data is split into train and test by performing stratified splitting:\n\nOnly with respect to the response (see train_y_stratified and test_y_stratified)\nWith respect to the response and categorical predictors (see train_y_CatPredictors_stratified and test_y_CatPredictors_stratified)\n\n\naccuracy_iter.describe()\n\n\n\n\n\n  \n    \n      \n      train_y_stratified\n      test_y_stratified\n      train_y_CatPredictors_stratified\n      test_y_CatPredictors_stratified\n    \n  \n  \n    \n      count\n      999.000000\n      999.000000\n      9.990000e+02\n      9.990000e+02\n    \n    \n      mean\n      0.834962\n      0.835150\n      8.350000e-01\n      8.350000e-01\n    \n    \n      std\n      0.005833\n      0.023333\n      8.552999e-15\n      8.552999e-15\n    \n    \n      min\n      0.812500\n      0.755000\n      8.350000e-01\n      8.350000e-01\n    \n    \n      25%\n      0.831250\n      0.820000\n      8.350000e-01\n      8.350000e-01\n    \n    \n      50%\n      0.835000\n      0.835000\n      8.350000e-01\n      8.350000e-01\n    \n    \n      75%\n      0.838750\n      0.850000\n      8.350000e-01\n      8.350000e-01\n    \n    \n      max\n      0.855000\n      0.925000\n      8.350000e-01\n      8.350000e-01\n    \n  \n\n\n\n\nLet us visualize the distribution of these accuracies.\n\n\nC.4.1 Stratified splitting only with respect to the response\n\nsns.histplot(data=accuracy_iter, x=\"train_y_stratified\", color=\"red\", label=\"Train accuracy\", kde=True)\nsns.histplot(data=accuracy_iter, x=\"test_y_stratified\", color=\"skyblue\", label=\"Test accuracy\", kde=True);\nplt.legend()\nplt.xlabel('Accuracy')\n\nText(0.5, 0, 'Accuracy')\n\n\n\n\n\nNote the variability in train and test accuracies when the data is stratified only with respect to the response. The train accuracy varies between 81.2% and 85.5%, while the test accuracy varies between 75.5% and 92.5%.\n\n\nC.4.2 Stratified splitting with respect to the response and categorical predictors\n\nsns.histplot(data=accuracy_iter, x=\"train_y_CatPredictors_stratified\", color=\"red\", label=\"Train accuracy\", kde=True)\nsns.histplot(data=accuracy_iter, x=\"test_y_CatPredictors_stratified\", color=\"skyblue\", label=\"Test accuracy\", kde=True);\nplt.legend()\nplt.xlabel('Accuracy')\n\nText(0.5, 0, 'Accuracy')\n\n\n\n\n\nThe train and test accuracies are between 85% and 85.5% for all the simulations. As a results of stratifying the splitting with respect to both the response and the categorical predictors, the train and test datasets are almost the same because the datasets are engineered to be quite similar, thereby making the test dataset inappropriate for assessing accuracy on unseen data. Thus, it is recommended to stratify the splitting only with respect to the response."
  },
  {
    "objectID": "Tuning a hyperparameter.html#tuning-c-a.2.5",
    "href": "Tuning a hyperparameter.html#tuning-c-a.2.5",
    "title": "Appendix D — Tuning a hyperparameter",
    "section": "D.1 Tuning C (A.2.5)",
    "text": "D.1 Tuning C (A.2.5)\nWith the optimal degree identified in the previous question, find the optimal regularization parameter C. Again use the cross_val_score function.\n(4 points)\nWe are tuning C for the optimal degree of 5 identified in one of the previous questions.\n\n\nCode\npoly = PolynomialFeatures(degree = 5)\nX_train_poly = poly.fit_transform(X_train_scaled)"
  },
  {
    "objectID": "Tuning a hyperparameter.html#what-should-be-the-minimum-value-of-c-to-consider",
    "href": "Tuning a hyperparameter.html#what-should-be-the-minimum-value-of-c-to-consider",
    "title": "Appendix D — Tuning a hyperparameter",
    "section": "D.2 What should be the minimum value of C to consider?",
    "text": "D.2 What should be the minimum value of C to consider?\n\nAs C is the regularization parameter, it cannot be negative.\nAs C = 1/lambda, a value of C = 0 will mean infinite regularization, which corresponds to an intercept-only model. Also, as the LogisticRegression() function computes the value of lambda as 1/C, it throws a division by zero error if C = 0. Thus, we should consider C>0.\nEven if C is positive, but very small, the value of lambda will be too high, which gives rise to numerical errors. Thus, we need to find the minimum value of C that is large enough to avoid numerical errors when fitting the model to the given standardized dataset.\n\nWe start with an extremely low value of C = 1e-10, and check if the model converges. It doesn’t converge! If it had converged, we will consider even lower values of C. However, in this case, it fails to converge indicating the possibility that this value of C is too small to avoid numerical errors. We keep increasing the order of C until we don’t see convergence errors. With the code below, we find that for values of C starting from 1e-6, the algorithm successfully converges. Thus, the minimum value of C that we consider will be 1e-6.\nIn the plot below, we also see that as we increase C starting from C = 1e-6, the model coefficients change, which may potentially change model fit and accuracy. Thus, we should consider increasing values of C starting from C = 1e-6.\n\n\nCode\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nmodel = LogisticRegression(solver = 'newton-cg', C = 1e-6, max_iter=100).fit(X_train_poly, y_train)\nmodel2 = LogisticRegression(solver = 'newton-cg', C = 2*1e-6).fit(X_train_poly, y_train)\n\n# Visualizing the model coefficients with changing values of 'C'\nplt.plot(range(126), model.coef_[0,:], color = 'blue', label = \"C = 1e-6\")\nplt.plot(range(126), model2.coef_[0,:], color = 'orange', label = \"C = 2*1e-6\");\nplt.xlabel('Model coefficients')\nplt.legend();"
  },
  {
    "objectID": "Tuning a hyperparameter.html#what-should-be-the-maximum-value-of-c-to-consider",
    "href": "Tuning a hyperparameter.html#what-should-be-the-maximum-value-of-c-to-consider",
    "title": "Appendix D — Tuning a hyperparameter",
    "section": "D.3 What should be the maximum value of C to consider?",
    "text": "D.3 What should be the maximum value of C to consider?\nAs C tends to infinity, the regularization tends to disappear. Let us consider values of C starting from C = 1e10. The algorithm converges, and we obtain a plot as shown below.\nHowever, do we need to start from values as high as 1e10?\nNo, if we check the coefficients for C = 1e9, they appear to be the same as the coefficients for C = 1e10 (see plot below). Thus, we need to identify the maximum value of C below which the coefficients tend to change when the value of C decreases further.\n\n\nCode\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nmodel = LogisticRegression(solver = 'newton-cg', C = 1e10).fit(X_train_poly, y_train)\nmodel2 = LogisticRegression(solver = 'newton-cg', C = 1e9).fit(X_train_poly, y_train)\nplt.plot(range(126), model.coef_[0,:], color = 'blue', label = \"C = 1e10\")\nplt.plot(range(126), model2.coef_[0,:], color = 'orange', label = \"C = 1e9\")\nplt.legend();\n\n\n\n\n\nThere doesn’t seem to be a difference even between C = 1e5 and C = 1e4 - both the values are still practically infinity. Let us reduce C further.\n\n\nCode\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nmodel = LogisticRegression(solver = 'newton-cg', C = 1e5).fit(X_train_poly, y_train)\nmodel2 = LogisticRegression(solver = 'newton-cg', C = 1e4).fit(X_train_poly, y_train)\nplt.plot(range(126), model.coef_[0,:], color = 'blue', label = \"C = 1e5\")\nplt.plot(range(126), model2.coef_[0,:], color = 'orange', label = \"C = 1e4\")\nplt.legend();\n\n\n\n\n\nLet us consider C = 1e3. We get a convergence error. As the solution is found due to algorithms such as gradient descent, the algorithm may just need more steps or more iterations to converge to a solution. Thus, we can try increasing the max_iter value to see if it helps the model converge.\n\n\nCode\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nmodel = LogisticRegression(solver = 'newton-cg', C = 1e4).fit(X_train_poly, y_train)\nmodel2 = LogisticRegression(solver = 'newton-cg', C = 1e3).fit(X_train_poly, y_train)\nplt.plot(range(126), model.coef_[0,:], color = 'blue', label = \"C = 1e4\")\nplt.plot(range(126), model2.coef_[0,:], color = 'orange', label = \"C = 1e3\")\nplt.legend();\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:202: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n  warnings.warn(\"newton-cg failed to converge. Increase the \"\n\n\n\n\n\nIncreasing the max_iter value does take more time for the code to execute, but it helps the algorithm converge to a solution (see below). However, we see that the coefficients are very similar for the two values of C. Thus, we can decrease C further.\n\n\nCode\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nmodel = LogisticRegression(solver = 'newton-cg', C = 1e4).fit(X_train_poly, y_train)\nmodel2 = LogisticRegression(solver = 'newton-cg', C = 1e3, max_iter=1000).fit(X_train_poly, y_train)\nplt.plot(range(126), model.coef_[0,:], color = 'blue', label = \"C = 1e4\")\nplt.plot(range(126), model2.coef_[0,:], color = 'orange', label = \"C = 1e3\")\nplt.legend();\n\n\n\n\n\nEven for C = 1e3 and C = 1e2, we have similar coefficients. Let us reduce C further.\n\n\nCode\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nmodel = LogisticRegression(solver = 'newton-cg', C = 1e3, max_iter=1000).fit(X_train_poly, y_train)\nmodel2 = LogisticRegression(solver = 'newton-cg', C = 1e2, max_iter=1000).fit(X_train_poly, y_train)\nplt.plot(range(126), model.coef_[0,:], color = 'blue', label = \"C = 1e3\")\nplt.plot(range(126), model2.coef_[0,:], color = 'orange', label = \"C = 1e2\")\nplt.legend();\n\n\n\n\n\nAs we decrease C from C = 1e2, we observe that the coefficients start changing. Thus, the maximum value of C that we should consider is C = 1e2, as this value is practically infinity, and higher values will not be useful for consideration.\n\n\nCode\nsns.set(font_scale=1.25)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nmodel = LogisticRegression(solver = 'newton-cg', C = 1e2, max_iter=1000).fit(X_train_poly, y_train)\nmodel2 = LogisticRegression(solver = 'newton-cg', C = 1e1, max_iter=1000).fit(X_train_poly, y_train)\nplt.plot(range(126), model.coef_[0,:], color = 'blue', label = \"C = 1e2\")\nplt.plot(range(126), model2.coef_[0,:], color = 'orange', label = \"C = 1e1\")\nplt.legend();"
  },
  {
    "objectID": "Tuning a hyperparameter.html#grid-search-coarse-grid",
    "href": "Tuning a hyperparameter.html#grid-search-coarse-grid",
    "title": "Appendix D — Tuning a hyperparameter",
    "section": "D.4 Grid search: Coarse grid",
    "text": "D.4 Grid search: Coarse grid\nLet us consider 50 values of C between the minimum and maximum values identified above. We’ll consider values of C equidistant in the log scale, so that we consider values of all orders (such as 1e-5, 1e-4, etc.). Also, we saw earlier that the model coefficients change as the order of values of C changes from 1e-6 to 1e-5. Thus, we should consider values of C equidistant in logscale, instead of the linear scale.\n\n\nCode\nstart_time = tm.time()\nhyperparam_vals = np.logspace(-6,2)\naccuracy_iter = []\nfor c_val in hyperparam_vals:\n    poly = PolynomialFeatures(degree = 5)\n    X_train_poly = poly.fit_transform(X_train_scaled)\n    accuracy_iter.append(cross_val_score(LogisticRegression(solver = 'newton-cg', C = c_val, max_iter=1000),\n                                         X_train_poly, \n                                                  y_train, cv = 5, scoring='accuracy'))\nprint(\"Time taken = \", (tm.time() - start_time)/60, \"minutes\")\n\n\nTime taken =  1.9628210226694742 minutes\n\n\nNext, we plot the 5-fold accuracy with increasing C.\n\n\nCode\n#K-fold accuracy vs C\nacc_vector = np.array(accuracy_iter).mean(axis=1)\nplt.plot(10**np.linspace(-6, 2), acc_vector)\nplt.xscale(\"log\")\nplt.xlabel('C')\nplt.ylabel('K-fold accuracy');\n\n\n\n\n\n\n\nCode\nhyperparam_vals[np.argmax(np.array(accuracy_iter).mean(axis=1))]\n\n\n10.481131341546853\n\n\nWe observe that the accuracy is the maximum when C is more than 0.1. Thus, we’ll zoom-in and search for the optimal value in the domain 0.1 < C < 100 to obtain a more precise estimate of optimal C."
  },
  {
    "objectID": "Tuning a hyperparameter.html#grid-search-finer-grid",
    "href": "Tuning a hyperparameter.html#grid-search-finer-grid",
    "title": "Appendix D — Tuning a hyperparameter",
    "section": "D.5 Grid search: Finer grid",
    "text": "D.5 Grid search: Finer grid\n\n\nCode\nstart_time = tm.time()\nhyperparam_vals = np.logspace(-1,2)\naccuracy_iter2 = []\nfor c_val in hyperparam_vals:\n    poly = PolynomialFeatures(degree = 5)\n    X_train_poly = poly.fit_transform(X_train_scaled)\n    accuracy_iter2.append(cross_val_score(LogisticRegression(solver = 'newton-cg', C = c_val, max_iter=1000),\n                                         X_train_poly, \n                                                  y_train, cv = 5, scoring='accuracy'))\nprint(\"Time taken = \", (tm.time() - start_time)/60, \"minutes\")\n\n\nTime taken =  4.3717537442843115 minutes\n\n\n\n\nCode\n#K-fold accuracy vs C\nacc_vector = np.array(accuracy_iter2).mean(axis=1)\nplt.plot(10**np.linspace(-1, 2), acc_vector)\nplt.xscale(\"log\")\n\n\n\n\n\n\n\nCode\nhyperparam_vals[np.argmax(np.array(accuracy_iter2).mean(axis=1))]\n\n\n1.6768329368110082\n\n\nFrom the above plot, all values of C in [1, 100] seem to be optimal, and can be chosen as the optimal C!\nIndeed, for any value of C in [1, 100], we get a similar test accuracy.\n\n\nCode\nlogreg = LogisticRegression(solver = 'newton-cg', C =1, max_iter=1000)\nlogreg.fit(X_train_poly, y_train)\nX_test_poly = poly.fit_transform(X_test_scaled)\ny_pred = logreg.predict(X_test_poly)\n\nprint(accuracy_score(y_pred, y_test)*100) \n\n\n91.6955017301038\n\n\n\n\nCode\nlogreg = LogisticRegression(solver = 'newton-cg', C =100, max_iter=1000)\nlogreg.fit(X_train_poly, y_train)\nX_test_poly = poly.fit_transform(X_test_scaled)\ny_pred = logreg.predict(X_test_poly)\n\nprint(accuracy_score(y_pred, y_test)*100) \n\n\n91.62629757785467"
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix E — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here"
  }
]