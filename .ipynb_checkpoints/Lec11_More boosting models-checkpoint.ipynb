{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4b9feeda",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"More boosting models\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "    toc-depth: 4\n",
    "    jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f819f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\n",
    "recall_score, precision_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import VotingRegressor, VotingClassifier, StackingRegressor, StackingClassifier, GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import itertools as it\n",
    "import time as time\n",
    "import xgboost as xgb\n",
    "from pyearth import Earth\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c74843",
   "metadata": {},
   "source": [
    "We'll continue to use the same datasets that we have been using throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9036ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carID</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>year</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>tax</th>\n",
       "      <th>mpg</th>\n",
       "      <th>engineSize</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18473</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2020</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>11</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.3282</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15064</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2019</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>10813</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.0430</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18268</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2020</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>6</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.4379</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18480</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2017</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>18895</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>51.5140</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18492</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2015</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>62953</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>160</td>\n",
       "      <td>51.4903</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carID brand      model  year transmission  mileage fuelType  tax      mpg  engineSize  price\n",
       "0  18473   bmw   6 Series  2020    Semi-Auto       11   Diesel  145  53.3282         3.0  37980\n",
       "1  15064   bmw   6 Series  2019    Semi-Auto    10813   Diesel  145  53.0430         3.0  33980\n",
       "2  18268   bmw   6 Series  2020    Semi-Auto        6   Diesel  145  53.4379         3.0  36850\n",
       "3  18480   bmw   6 Series  2017    Semi-Auto    18895   Diesel  145  51.5140         3.0  25998\n",
       "4  18492   bmw   6 Series  2015    Automatic    62953   Diesel  160  51.4903         3.0  18990"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the same datasets as used for linear regression in STAT303-2, \n",
    "#so that we can compare the non-linear models with linear regression\n",
    "trainf = pd.read_csv('./Datasets/Car_features_train.csv')\n",
    "trainp = pd.read_csv('./Datasets/Car_prices_train.csv')\n",
    "testf = pd.read_csv('./Datasets/Car_features_test.csv')\n",
    "testp = pd.read_csv('./Datasets/Car_prices_test.csv')\n",
    "train = pd.merge(trainf,trainp)\n",
    "test = pd.merge(testf,testp)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6b5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['mileage','mpg','year','engineSize']]\n",
    "Xtest = test[['mileage','mpg','year','engineSize']]\n",
    "y = train['price']\n",
    "ytest = test['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a895b16",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282de73d",
   "metadata": {},
   "source": [
    "LightGBM is a gradient boosting decision tree algorithm developed by Microsoft in 2017. LightGBM outperforms XGBoost in terms of compuational speed, and provides comparable accuracy in general. The following two key features in LightGBM that make it faster than XGBoost: \n",
    "\n",
    "**1. Gradient-based One-Side Sampling** (GOSS): Recall, in gradient boosting, we fit trees on the gradient of the loss function *(refer the gradient boosting algorithm in section 10.10.2 of the book, [Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/)):*\n",
    "\n",
    "$$r_m = -\\bigg[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}  \\bigg]_{f = f_{m-1}}. $$\n",
    "\n",
    "Observations that correspond to relatively larger gradients contribute more to minimizing the loss function as compared to observations with smaller gradients. The algorithm down-samples the observations with small gradients, while selecting all the observations with large gradients. As observations with large gradients contribute the most to the reduction in loss function when considering a split, the accuracy of loss reduction estimate is maintained even with a reduced sample size. This leads to similar performance in terms of prediction accuracy while reducing computation speed due to reduction in sample size to fit trees.\n",
    "\n",
    "**2. Exclusive feature bundling** (EFB): This is useful when there are a lot of predictors, but the predictor space is sparse, i.e., most of the values are zero for several predictors, and the predictors rarely take non-zero values simultaneously. This can typically happen in case of a lot of dummy variables in the data. In such a case, the predictors are bundled to create a single predictor. \n",
    "\n",
    "In the example below you can see that feature1 and feature2 are mutually exclusive. In order to achieve non overlapping buckets we add bundle size of feature1 to feature2. This makes sure that non zero data points of bundled features (feature1 and feature2) reside in different buckets. In feature_bundle buckets 1 to 4 contains non zero instances of feature1 and buckets 5,6 contain non zero instances of feature2 *([Reference](https://towardsdatascience.com/what-makes-lightgbm-lightning-fast-a27cf0d9785e))*. \n",
    "\n",
    "| feature1 | feature2 | feature_bundle |\n",
    "| --- | --- | --- |\n",
    "| 0 | 2 | 6 |\n",
    "| 0 | 1 | 5 |\n",
    "| 0 | 2 | 6 |\n",
    "| 1 | 0 | 1 |\n",
    "| 2 | 0 | 2 |\n",
    "| 3 | 0 | 3 |\n",
    "| 4 | 0 | 4 |\n",
    "\n",
    "Read the [LightGBM paper](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380a289",
   "metadata": {},
   "source": [
    "### LightGBM for regression\n",
    "Let us tune a lightGBM model for regression for our problem of predicting car price. We'll use the function [`LGBMRegressor`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html). For classification problems, [`LGBMClassifier`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html) can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4787ccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Optimal parameter values = {'subsample': 0.75, 'reg_lambda': 0, 'reg_alpha': 100, 'num_leaves': 31, 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "Optimal cross validation R-squared =  0.8935432951824455\n",
      "Time taken =  1  minutes\n"
     ]
    }
   ],
   "source": [
    "#K-fold cross validation to find optimal parameters for LightGBM regressor\n",
    "start_time = time.time()\n",
    "param_grid = {'max_depth': [4,6,8],\n",
    "              'num_leaves': [20, 31, 40],\n",
    "              'learning_rate': [0.01, 0.05, 0.1],\n",
    "               'reg_lambda':[0, 10, 100],\n",
    "                'n_estimators':[100, 500, 1000],\n",
    "                'reg_alpha': [0, 10, 100],\n",
    "                'subsample': [0.5, 0.75, 1.0],\n",
    "                'colsample_bytree': [0.5, 0.75, 1.0]}\n",
    "\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "optimal_params = RandomizedSearchCV(estimator=LGBMRegressor(random_state=1),                                                       \n",
    "                             param_distributions = param_grid, n_iter = 200,\n",
    "                             verbose = 1,\n",
    "                             n_jobs=-1,\n",
    "                             cv = cv)\n",
    "optimal_params.fit(X,y)\n",
    "print(\"Optimal parameter values =\", optimal_params.best_params_)\n",
    "print(\"Optimal cross validation R-squared = \",optimal_params.best_score_)\n",
    "print(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fabd5eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5400.723918176313"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RMSE based on the optimal parameter values of a LighGBM Regressor model\n",
    "np.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ef811",
   "metadata": {},
   "source": [
    "### LightGBM vs XGBoost\n",
    "\n",
    "LightGBM model took 1 minute for a random search with 1000 fits as compared to 4 minutes for an XGBoost model with 1000 fits on the same data (as shown below). In terms of prediction accuracy, we observe that the accuracy of LightGBM on test *(unseen)* data is comparable to that of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "336194fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Optimal parameter values = {'subsample': 0.75, 'reg_lambda': 1, 'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.01, 'gamma': 100, 'colsample_bytree': 1.0}\n",
      "Optimal cross validation R-squared =  0.9002580404500382\n",
      "Time taken =  4  minutes\n"
     ]
    }
   ],
   "source": [
    "#K-fold cross validation to find optimal parameters for XGBoost\n",
    "start_time = time.time()\n",
    "param_grid = {'max_depth': [4,6,8],\n",
    "              'learning_rate': [0.01, 0.05, 0.1],\n",
    "               'reg_lambda':[0, 1, 10],\n",
    "                'n_estimators':[100, 500, 1000],\n",
    "                'gamma': [0, 10, 100],\n",
    "                'subsample': [0.5, 0.75, 1.0],\n",
    "                'colsample_bytree': [0.5, 0.75, 1.0]}\n",
    "\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "optimal_params = RandomizedSearchCV(estimator=xgb.XGBRegressor(random_state=1),                                                       \n",
    "                             param_distributions = param_grid, n_iter = 200,\n",
    "                             verbose = 1,\n",
    "                             n_jobs=-1,\n",
    "                             cv = cv)\n",
    "optimal_params.fit(X,y)\n",
    "print(\"Optimal parameter values =\", optimal_params.best_params_)\n",
    "print(\"Optimal cross validation R-squared = \",optimal_params.best_score_)\n",
    "print(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ebe7659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5497.553788113875"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RMSE based on the optimal parameter values\n",
    "np.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b16144",
   "metadata": {},
   "source": [
    "## CatBoost\n",
    "\n",
    "CatBoost is a gradient boosting algorithm developed by Yandex *(Russian Google)* in 2017. Like LightGBM, CatBoost is also faster than XGBoost in training. However, unlike LightGBM, the authors have claimed that it outperforms both LightGBM and XGBoost in terms of prediction accuracy as well. \n",
    "\n",
    "The key feature of CatBoost that address the issue with the gradient boosting procedure is the idea of ordered boosting. Classic boosting algorithms are prone to overfitting on small/noisy datasets due to a problem known as prediction shift. Recall, in gradient boosting, we fit trees on the gradient of the loss function *(refer the gradient boosting algorithm in section 10.10.2 of the book, [Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/)):*\n",
    "\n",
    "$$r_m = -\\bigg[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}  \\bigg]_{f = f_{m-1}}. $$\n",
    "\n",
    "When calculating the gradient estimate of an observation, these algorithms use the same observations that the model was built with, thus having no chances of experiencing unseen data. CatBoost, on the other hand, uses the concept of ordered boosting, a permutation-driven approach to train model on a subset of data while calculating residuals on another subset, thus preventing \"target leakage\" and overfitting. The residuals of an observation are computed based on a model developed on the previous observations, where the observations are randomly shuffled at each iteration, i.e., for each tree.\n",
    "\n",
    "Thus, the gradient of the loss function is based on test *(unseen)* data, instead of the data on which the model has been trained, which improves the generalizability of the model, and avoids overfitting on train data.\n",
    "\n",
    "The authors have also shown that CatBoost performs better than XGBoost and LightGBM without tuning, i.e., with default hyperparameter settings.\n",
    "\n",
    "Read the [CatBoost paper](https://proceedings.neurips.cc/paper_files/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf) for more details.\n",
    "\n",
    "Here is a good [blog](https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm) listing the key features of CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c009a2",
   "metadata": {},
   "source": [
    "### CatBoost for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d987907",
   "metadata": {},
   "source": [
    "We'll use the function [CatBoostRegressor](https://catboost.ai/en/docs/concepts/python-reference_catboostregressor) for regression. For classification problems [CatBoostClassifier](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier) can be used.\n",
    "\n",
    "Let us check the performance of `CatBoostRegressor()` without tuning, i.e., with default hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat = CatBoostRegressor().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce4b9894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5288.82153844634"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(model_cat.predict(Xtest),ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2895c618",
   "metadata": {},
   "source": [
    "Even with default hyperparameter settings, CatBoost has outperformed both XGBoost and LightGBM in terms of RMSE on test data for our example of predicting car prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317248f",
   "metadata": {},
   "source": [
    "### CatBoost vs XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5e92a",
   "metadata": {},
   "source": [
    "Let us see the performance of XGBoost with default hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4cfa0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6821.745153860935"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = xgb.XGBRFRegressor().fit(X, y)\n",
    "np.sqrt(mean_squared_error(model_xgb.predict(Xtest),ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c63ec5a",
   "metadata": {},
   "source": [
    "XGBoost performance deteriorates showing that hyperparameter tuning is more important in XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ddf45",
   "metadata": {},
   "source": [
    "Let us see the performance of LightGBM with default hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9031b444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5494.0777923513515"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgbm = LGBMRegressor().fit(X, y)\n",
    "np.sqrt(mean_squared_error(model_lgbm.predict(Xtest),ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d72501",
   "metadata": {},
   "source": [
    "LightGBM's default hyperparameter settings also seem to be more robust as compared to those of XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c009cc0",
   "metadata": {},
   "source": [
    "### Tuning `CatBoostRegressor`\n",
    "\n",
    "The CatBoost hyperparameters can be tuned just like the XGBoost hyperparameters. However, there is some difference in the hyperparameters of both the packages. For example, `reg_alpha` *(the L1 penalization on weights of leaves)* and `colsample_bytree` *(subsample ratio of columns when constructing each tree)* hyperparameters are not there in CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6403528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan 0.86528582        nan        nan 0.84104458 0.79227627\n",
      "        nan        nan 0.84395413 0.89887462        nan        nan\n",
      "        nan 0.90260407        nan        nan        nan        nan\n",
      "        nan        nan 0.86545114        nan 0.84894322        nan\n",
      " 0.8913253         nan        nan 0.90681372 0.90270419 0.84033192\n",
      "        nan        nan        nan        nan        nan 0.89897627\n",
      "        nan 0.75750273 0.63799634 0.82429155 0.8541958         nan\n",
      " 0.85795537 0.84778687        nan 0.82552044 0.88776603        nan\n",
      " 0.87183014        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.868381   0.88627774\n",
      "        nan        nan        nan        nan 0.88302748        nan\n",
      "        nan        nan 0.87173927 0.90364659        nan 0.68716329\n",
      " 0.86810108        nan 0.90387934 0.86198    0.79482791        nan\n",
      " 0.867492          nan        nan        nan 0.8681382         nan\n",
      "        nan        nan        nan        nan        nan 0.82487077\n",
      " 0.54242665        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.88919641        nan        nan        nan\n",
      " 0.85336326        nan 0.8619873  0.83934649 0.90477081 0.79750609\n",
      " 0.86543518        nan        nan        nan        nan 0.80115517\n",
      "        nan        nan        nan        nan        nan 0.75434919\n",
      " 0.60871141        nan        nan 0.79028956 0.66728925 0.89361737\n",
      "        nan        nan        nan        nan        nan 0.89080628\n",
      "        nan 0.75063605        nan 0.90090587        nan        nan\n",
      " 0.82573579 0.90680318 0.85290443        nan        nan 0.89928321\n",
      "        nan        nan        nan 0.86285405 0.8978184         nan\n",
      "        nan 0.84783232        nan        nan        nan        nan\n",
      " 0.86225177        nan        nan 0.8621329         nan        nan\n",
      " 0.54359637        nan        nan 0.8994749  0.84800071        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63020005        nan 0.87308398        nan 0.86614844        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.68846678 0.8406747         nan        nan        nan        nan\n",
      " 0.88741464 0.86148835]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameter values = {'subsample': 0.75, 'reg_lambda': 0, 'num_leaves': 31, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.05}\n",
      "Optimal cross validation R-squared =  0.9068137174802073\n",
      "Time taken =  2  minutes\n"
     ]
    }
   ],
   "source": [
    "#K-fold cross validation to find optimal parameters for CatBoost regressor\n",
    "start_time = time.time()\n",
    "param_grid = {'max_depth': [4,6,8],\n",
    "              'num_leaves': [20, 31, 40],\n",
    "              'learning_rate': [0.01, 0.05, 0.1],\n",
    "               'reg_lambda':[0, 10, 100],\n",
    "                'n_estimators':[100, 500, 1000],\n",
    "                'subsample': [0.5, 0.75, 1.0]}\n",
    "\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "optimal_params = RandomizedSearchCV(estimator=CatBoostRegressor(random_state=1, verbose=False),                                                       \n",
    "                             param_distributions = param_grid, n_iter = 200,\n",
    "                             verbose = 1,random_state = 1,\n",
    "                             n_jobs=-1,\n",
    "                             cv = cv)\n",
    "optimal_params.fit(X,y)\n",
    "print(\"Optimal parameter values =\", optimal_params.best_params_)\n",
    "print(\"Optimal cross validation R-squared = \",optimal_params.best_score_)\n",
    "print(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daa32e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5254.902079026533"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RMSE based on the optimal parameter values\n",
    "np.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77739800",
   "metadata": {},
   "source": [
    "It takes 2 minutes to tune CatBoost, which is higher than LightGBM and lesser than XGBoost. CatBoost falls in between LightGBM and XGBoost in terms of speed. However, it is likely to be more accurate than XGBoost and LighGBM, and likely to require lesser tuning as compared to XGBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
