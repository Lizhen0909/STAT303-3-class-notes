{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cfb58366",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Assignment A\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d50fa",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity. \n",
    "\n",
    "2. Do not write your name on the assignment.\n",
    "\n",
    "3. Write your code in the *Code* cells and your answer in the *Markdown* cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\n",
    "\n",
    "4. Use [Quarto](https://quarto.org/docs/output-formats/html-basics.html) to print the *.ipynb* file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: `quarto render filename.ipynb --to html`. Submit the HTML file.\n",
    "\n",
    "5. The assignment is worth 100 points, and is due on **Thursday, 13th April 2023 at 11:59 pm**. \n",
    "\n",
    "6. **Four points are properly formatting the assignment**. The breakdown is as follows:\n",
    "- Must be an HTML file rendered using Quarto (1 pt). *If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file. If your issue doesn't seem genuine, you will lose points.* \n",
    "- There aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\n",
    "- Final answers of each question are written in Markdown cells (1 pt).\n",
    "- There is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400070a0",
   "metadata": {},
   "source": [
    "##  Bias-variance trade-off\n",
    "\n",
    "Throughout the course, the conceptual clarity about bias and variance will help you tune the models for optimal performance and enable you to compare different models in terms of bias and variance. In this question, you will perform simulations to understand and visualize bias-variance trade-off as in Fig. 2.12 of the [book](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf) (page 36)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85370d0",
   "metadata": {},
   "source": [
    "Assume that the response $y$ is a function of the predictors $x_1$ and $x_2$ and includes a random error $\\epsilon$, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467cba12",
   "metadata": {},
   "source": [
    "$$\n",
    "y = f(x_1, x_2) + \\epsilon, \\qquad\n",
    "$$  {#eq-function}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfee69d",
   "metadata": {},
   "source": [
    "where the function $f(.)$ is the [Bukin function](https://www.sfu.ca/~ssurjano/bukin6.html), $x_1 \\sim U[-15, -5], x_2 \\sim U[-3, 3]$, and $\\epsilon \\sim N(0, \\sigma^2); \\sigma = 10$. *Here $U$ refers to Uniform distribution, and $N$ refers to normal distribution.* Use NumPy to simulate values from these distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97faed8d",
   "metadata": {},
   "source": [
    "You will code an algorithm (described below) to compute the expected squared bias, expected variance, var($\\epsilon$) and expected test MSE of the following 7 linear regression models having the predictors as:\n",
    "\n",
    "1. $x_1$ and $x_2$\n",
    "\n",
    "2. All the predictors in the above model, and all polynomial combinations of $x_1$, and $x_2$ of degree 2, which will be $x_1^2, x_2^2$, and $x_1x_2$\n",
    "\n",
    "3. All the predictors in the above model, and all polynomial combinations of $x_1$, and $x_2$ of degree 3, which will be $x_1^3, x_2^3, x_1^2x_2$, and $x_1x_2^2$\n",
    "\n",
    "4. All the predictors in the above model, and all polynomial combinations of $x_1$, and $x_2$ of degree 4\n",
    "\n",
    "5. All the predictors in the above model, and all polynomial combinations of $x_1$, and $x_2$ of degree 5\n",
    "\n",
    "6. All the predictors in the above model, and all polynomial combinations of $x_1$, and $x_2$ of degree 6\n",
    "\n",
    "7. All the predictors in the above model, and all polynomial combinations of $x_1$, and $x_2$ of degree 7\n",
    "\n",
    "As you can see the models are arranged in increasing order of flexibility / complexity. This corresponds to the horizontal axis of Fig. 2.12 in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6ec4f",
   "metadata": {},
   "source": [
    "Use the following **algorithm** to compute the expected squared bias, expected variance, var($\\epsilon$) and expected test MSE of the 7 linear regression models above:\n",
    "\n",
    "**I. Define the Bukin function** that accepts $x_1$ and $x_2$ as parameters and returns the Bukin function value ($f(x_1, x_2)$).\n",
    "\n",
    "*(2 points)*\n",
    "\n",
    "**II.** Repeat steps **III - VII** for all degrees $d$ in $\\{1, 2, ..., 7\\}$\n",
    "\n",
    "*(2 points)*\n",
    "\n",
    "**III.** Considering a model of **degree $d$**, simulate the following test and train datasets.\n",
    "\n",
    "   A. **Simulate test data**\n",
    "   \n",
    "   1. Set a seed of 100. Use the code: `np.random.seed(100)`, where `np` refers to the numpy library\n",
    "\n",
    "   2. Simulate 100 values of $x_1$ from $U[-15, -5]$.\n",
    "\n",
    "   3. Simulate 100 values of $x_2$ from $U[-3, 3]$. \n",
    "\n",
    "   4. Compute the Bukin function value $f(x_1, x_2)$ for the simulated values of $x_1$ and $x_2$.\n",
    "\n",
    "   5. Use the function `PolynomialFeatures` from the `preprocessing` module of the `sklearn` library to create all polynomial combinations of $x_1$, and $x_2$ up to degree $d$.\n",
    "   \n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bed5a",
   "metadata": {},
   "source": [
    "B. **Simulate 100 train data sets**, where each train data is simulated as follows:\n",
    "\n",
    "   1. Set a seed of *i* for simualting the *ith* train data. Use the code: `np.random.seed(i)`, where `np` refers to the numpy library.\n",
    "\n",
    "   2. Simulate 100 values of $x_1$ from $U[-15, -5]$\n",
    "\n",
    "   3. Simulate 100 values of $x_2$ from $U[-3, 3]$ \n",
    "\n",
    "   4. Compute the Bukin function value $f(x_1, x_2)$ for the simulated values of $x_1$ and $x_2$\n",
    "\n",
    "   5.  Simulate the response $y$ using the above set of simulated values with @eq-function\n",
    "    \n",
    "   6. Use the function `PolynomialFeatures` from the `preprocessing` module of the `sklearn` library to create all polynomial combinations of $x_1$, and $x_2$ up to degree $d$.\n",
    "   \n",
    "*(6 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eaeca0",
   "metadata": {},
   "source": [
    "**IV.** For each train data in III(B), develop a linear regression model using the `LinearRegression()` function from the `linear_model` module of the `sklearn` library.\n",
    "\n",
    "*(2 points)*\n",
    "\n",
    "**V.** Note that the squared bias at a test point $x_{1\\_test}, x_{2\\_test}$ is:\n",
    "\n",
    "$$\n",
    "[Bias(\\hat{f}(x_{1\\_test}, x_{2\\_test}))]^2 = [E(\\hat{f}(x_{1\\_test}, x_{2\\_test})) - f(x_{1\\_test}, x_{2\\_test})]^2, \\qquad\n",
    "$$ {#eq-bias}\n",
    "\n",
    "where $E(\\hat{f}(x_{1\\_test}, x_{2\\_test}))$ is the mean prediction of the 100 trained models at $x_{1\\_test}, x_{2\\_test}$. \n",
    "\n",
    "Compute the overall expected squared bias as the average squared bias at all the test data points, as in the equation below:\n",
    "\n",
    "$$\n",
    "[Bias(\\hat{f}(.))]^2 = \\frac{1}{100}\\Sigma_{i=1}^{100} \\big[Bias(\\hat{f}(x_{1i\\_test}, x_{2i\\_test}))\\big]^2, \\qquad\n",
    "$$ {#eq-bias_overall}\n",
    "\n",
    "*(8 points)*\n",
    "\n",
    "**VI.** Note that the variance at a test point $x_{1\\_test}, x_{2\\_test}$ is $Var(\\hat{f}(x_{1\\_test}, x_{2\\_test}))$. Compute the overall expected variance as the average variance at all the test data points, as in the equation below:\n",
    "\n",
    "$$\n",
    "Var(\\hat{f}(.)) = \\frac{1}{100}\\Sigma_{i=1}^{100} Var(\\hat{f}(x_{1i\\_test}, x_{2i\\_test})) \\qquad\n",
    "$$ {#eq-variance}\n",
    "\n",
    "*(6 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163abbf",
   "metadata": {},
   "source": [
    "**VII.** Compute the overall expected test mean squared error as the sum of the expected squared bias (@eq-bias_overall), expected variance (@eq-variance), and error variance ($\\sigma^2$):\n",
    "\n",
    "$$\n",
    "MSE = [Bias(\\hat{f}(.))]^2 + Var(\\hat{f}(.)) + \\sigma^2, \\qquad\n",
    "$$ {#eq-mse}\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe83a7",
   "metadata": {},
   "source": [
    "**VIII.** Plot the overall expected squared bias, overall expected variance, and overall expected test MSE *(as obtained from @eq-bias_overall, @eq-variance, and @eq-mse respectively)* against the degree $d$ (or flexibility / complexity) of the model . Your plot should look like one of the plots in Fig. 2.12 of the book.\n",
    "\n",
    "*(3 points)*\n",
    "\n",
    "**IX.** What is the degree of the optimal model, i.e., the degree that provides the best **bias-variance trade-off**?\n",
    "\n",
    "*(2 points)*\n",
    "\n",
    "\n",
    "*Note: While coding the algorithm, comment it well so that it is easy to give partial credit in case of mistakes. Include the numerals of the algorithm (such as II(B), V, VI, etc.) in your comments so that it is easy to check your algorithm for completeness.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f5b14",
   "metadata": {},
   "source": [
    "## Tuning a classification model with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a43f4b",
   "metadata": {},
   "source": [
    "###  Data {-}\n",
    "Read the data *classification_data.csv*. The description of the columns is as follows:\n",
    "\n",
    "1. `hi_int_prncp_pd`: Indicates if a high percentage of the repayments made went to interest rather than principal. **Target variable.**\n",
    "\n",
    "2. `out_prncp_inv`: Remaining outstanding principal for portion of total amount funded by investors\n",
    "\n",
    "3. `loan_amnt`: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n",
    "\n",
    "4. `int_rate`: Interest rate on the loan\n",
    "\n",
    "5. `term`: The number of payments on the loan. Values are in months and can be either 36 or 60.\n",
    "\n",
    "You will develop and tune a logistic regression model to predict `hi_int_prncp_pd` based on the rest of the columns (predictors) as per the instructions below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82554302",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "Use the function `train_test_split` from the `model_selection` module of the `sklearn` library to split the data into 75% train and 25% test. Stratify the split based on the response. Use `random_state` as 45. Print the proportion of 0s and 1s in both the train and test datasets.\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6dcdf",
   "metadata": {},
   "source": [
    "### Scaling predictors\n",
    "Scale the predictors to avoid convergence errors when fitting the logistic regression model.\n",
    "\n",
    "*Note that last quarter, we were focusing on inference (along with prediction), so we avoided scaling. It is a bit inconvenient to interpret odds with scaled predictors. However, avoiding scaling may lead to convergence errors as some of you saw in your course projects. So, it is a good practice to scale, especially when your focus is prediction.*\n",
    "\n",
    "*(3 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0695799",
   "metadata": {},
   "source": [
    "### Tuning the degree\n",
    "Use the functions:\n",
    "\n",
    "1. `cross_val_score` from the `model_selection` module of the `sklearn` library to tune the degree of the logistic regression model for maximizing the stratified 5-fold prediction accuracy. Consider degrees from 1 to 6. \n",
    "\n",
    "2. `PolynomialFeatures` from the `preprocessing` module of the `sklearn` library to create all polynomial combinations of the predictors up to degree $d$.\n",
    "\n",
    "What is the optimal degree?\n",
    "\n",
    "*(4 points)*\n",
    "\n",
    "**Notes:** \n",
    "\n",
    "- A model of degree $d$ will consist of polynomial transformations and interactions of predictors up to degree $d$. For example, a model of degree 2 will consist of the square of each predictor and all 2-factor interactions of the predictors.\n",
    "\n",
    "- You may use the `newton-cg` solver to avoid convergence issues.\n",
    "\n",
    "- Use the default `C` value at this point, you will tune it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4051621",
   "metadata": {},
   "source": [
    "### Test accuracy with optimal degree\n",
    "For the optimal degree identified in the previous question, compute the test accuracy.\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1316504",
   "metadata": {},
   "source": [
    "### Tuning `C`\n",
    "With the optimal degree identified in the previous question, find the optimal regularization parameter `C`. Again use the `cross_val_score` function.\n",
    "\n",
    "*(3 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765e51f",
   "metadata": {},
   "source": [
    "### Test accuracy with optimal degree and `C`\n",
    "For the optimal degree and optimal `C` identified in the previous questions, compute the test accuracy.\n",
    "\n",
    "*(3 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7961716",
   "metadata": {},
   "source": [
    "### Tuning decision threshold probability\n",
    "With the optimal degree and optimal `C` identified in the previous questions, find the optimal decision threshold probability to maximize accuracy. Use the `cross_val_predict` function.\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79ced0",
   "metadata": {},
   "source": [
    "### Test accuracy for optimal degree, `C`, and threshold probability\n",
    "For the optimal degree, optimal `C`, and optimal decision threshold probabilities identified in the previous questions, compute the test accuracy.\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c911504",
   "metadata": {},
   "source": [
    "### Simultaneous optimization of multiple parameters\n",
    "In the above tuning approach we optimized the hyperparameters and the decision threshold probability sequentially. This is a greedy approach, which doesn't consider all combinations of hyperparameters and decision threshold probabilities, and thus may fail to find the optimal combination of values that maximize accuracy. Thus, tune both the model hyperparameters - degree and `C`, and the decision threshold probability simultaneously considering all value combinations. This will take more time, but is likely to provide more accurate optimal parameter values.\n",
    "\n",
    "*(6 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f2edf",
   "metadata": {},
   "source": [
    "### Test accuracy with optimal parameters obtained simultaneously\n",
    "For the optimal degree, optimal `C`, and optimal decision threshold probabilities identified in the previous question, compute the test accuracy.\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d783e",
   "metadata": {},
   "source": [
    "### Optimizing parameters for multiple performance metrics\n",
    "Find the optimal `C` and degree to maximize recall while having a precision of more than 75%. Use the function `cross_validate` from the `model_selection` module of the `sklearn` library.\n",
    "\n",
    "*Note: `cross_validate` function is very similar to `cross_val_score`, the only difference is you can use multiple metrics with the scoring input, as you need in this question.*\n",
    "\n",
    "*(8 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab37e617",
   "metadata": {},
   "source": [
    "### Performance metrics computation\n",
    "For the optimal degree and `C` identified in the previous question, compute the following performance metrics on test data. Use `sklearn` functions, manual computation is not allowed.\n",
    "\n",
    "1. Precision\n",
    "\n",
    "2. Recall\n",
    "\n",
    "3. Accuracy\n",
    "\n",
    "4. ROC-AUC\n",
    "\n",
    "5. Show the confusion matrix\n",
    "\n",
    "*(10 points)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
