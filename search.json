[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science III with python (Class notes)",
    "section": "",
    "text": "These are class notes for the course STAT303-2. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThe course notes are currently being written, and will continue to being developed as the course progresses (just like the course textbook last quarter). Please report any typos / mistakes / inconsistencies / issues with the class notes / class presentations in your comments here. Thank you!"
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html",
    "href": "Lec1_SimpleLinearRegression.html",
    "title": "1  Simple Linear Regression",
    "section": "",
    "text": "Read section 3.1 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately."
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html#simple-linear-regression",
    "href": "Lec1_SimpleLinearRegression.html#simple-linear-regression",
    "title": "1  Simple Linear Regression",
    "section": "1.1 Simple Linear Regression",
    "text": "1.1 Simple Linear Regression\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nDevelop a simple linear regression model that predicts car price based on engine size. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~engineSize', data = train)\n\n\n#Using the fit() function of the 'ols' class to fit the model\nmodel = ols_object.fit()\n\n\n#Printing model summary which contains among other things, the model coefficients\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.390 \n\n\n  Model:                   OLS         Adj. R-squared:        0.390 \n\n\n  Method:             Least Squares    F-statistic:           3177. \n\n\n  Date:             Thu, 19 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 16:44:04       Log-Likelihood:      -53949. \n\n\n  No. Observations:        4960        AIC:                1.079e+05\n\n\n  Df Residuals:            4958        BIC:                1.079e+05\n\n\n  Df Model:                   1                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -4122.0357   522.260    -7.893  0.000 -5145.896 -3098.176\n\n\n  engineSize  1.299e+04   230.450    56.361  0.000  1.25e+04  1.34e+04\n\n\n\n\n  Omnibus:       1271.986   Durbin-Watson:         0.517\n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   6490.719\n\n\n  Skew:            1.137    Prob(JB):               0.00\n\n\n  Kurtosis:        8.122    Cond. No.               7.64\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe model equation is: car price = -4122.0357 + 12990 * engineSize\nVisualize the regression line\n\nsns.regplot(x = 'engineSize', y = 'price', data = train, color = 'orange',line_kws={\"color\": \"red\"})\nplt.xlim(-1,7)\n#Note that some of the engineSize values are 0. They are incorrect, and should ideally be imputed before developing the model.\n\n(-1.0, 7.0)\n\n\n\n\n\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\n\n#Using the predict() function associated with the 'model' object to make predictions of car price on test (unknown) data\npred_price = model.predict(testf)#Note that the predict() function finds the predictor 'engineSize' in the testf dataframe, and plugs its values in the regression equation for prediction.\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.scatterplot(x = testp.price, y = pred_price)\n#In case of a perfect prediction, all the points must lie on the line x = y.\nsns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='orange') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\n\nText(0, 0.5, 'Predicted price')\n\n\n\n\n\nThe prediction doesn’t look too good. This is because we are just using one predictor - engine size. We can probably improve the model by adding more predictors when we learn multiple linear regression.\nWhat is the RMSE of the predicted car price?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n12995.1064515487\n\n\nThe root mean squared error in predicting car price is around $13k.\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n12810.109175214136\n\n\nThe residual standard error on the training data is close to the RMSE on the test data. This shows that the performance of the model on unknown data is comparable to its performance on known data. This implies that the model is not overfitting, which is good! In case we overfit a model on the training data, its performance on unknown data is likely to be worse than that on the training data.\nFind the confidence and prediction intervals of the predicted car price\n\n#Using the get_prediction() function associated with the 'model' object to get the intervals\nintervals = model.get_prediction(testf)\n\n\n#The function requires specifying alpha (probability of Type 1 error) instead of the confidence level to get the intervals\nintervals.summary_frame(alpha=0.05)\n\n\n\n\n\n  \n    \n      \n      mean\n      mean_se\n      mean_ci_lower\n      mean_ci_upper\n      obs_ci_lower\n      obs_ci_upper\n    \n  \n  \n    \n      0\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      1\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      2\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      3\n      8866.245277\n      316.580850\n      8245.606701\n      9486.883853\n      -16254.905974\n      33987.396528\n    \n    \n      4\n      47831.088340\n      468.949360\n      46911.740050\n      48750.436631\n      22700.782946\n      72961.393735\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2667\n      47831.088340\n      468.949360\n      46911.740050\n      48750.436631\n      22700.782946\n      72961.393735\n    \n    \n      2668\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      2669\n      8866.245277\n      316.580850\n      8245.606701\n      9486.883853\n      -16254.905974\n      33987.396528\n    \n    \n      2670\n      21854.526298\n      184.135754\n      21493.538727\n      22215.513869\n      -3261.551421\n      46970.604017\n    \n    \n      2671\n      21854.526298\n      184.135754\n      21493.538727\n      22215.513869\n      -3261.551421\n      46970.604017\n    \n  \n\n2672 rows × 6 columns\n\n\n\nShow the regression line predicting car price based on engine size for test data. Also show the confidence and prediction intervals for the car price.\n\ninterval_table = intervals.summary_frame(alpha=0.05)\n\n\nsns.scatterplot(x = testf.engineSize, y = pred_price,color = 'orange', s = 10)\nsns.lineplot(x = testf.engineSize, y = pred_price, color = 'red')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_lower, color = 'blue')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_upper, color = 'blue',label='_nolegend_')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_lower, color = 'green')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_upper, color = 'green')\nplt.legend(labels=[\"Regression line\",\"Confidence interval\", \"Prediction interval\"])\n\n<matplotlib.legend.Legend at 0x26a3a32c550>"
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html",
    "href": "Lec2_MultipleLinearRegression.html",
    "title": "2  Multiple Linear Regression",
    "section": "",
    "text": "Read section 3.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately."
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html#multiple-linear-regression",
    "href": "Lec2_MultipleLinearRegression.html#multiple-linear-regression",
    "title": "2  Multiple Linear Regression",
    "section": "2.1 Multiple Linear Regression",
    "text": "2.1 Multiple Linear Regression\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nDevelop a multiple linear regression model that predicts car price based on engine size, year, mileage, and mpg. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.660 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           2410. \n\n\n  Date:             Tue, 27 Dec 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:07:25       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4955        BIC:                1.050e+05\n\n\n  Df Model:                   4                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.661e+06  1.49e+05   -24.593  0.000 -3.95e+06 -3.37e+06\n\n\n  year        1817.7366    73.751    24.647  0.000  1673.151  1962.322\n\n\n  mileage       -0.1474     0.009   -16.817  0.000    -0.165    -0.130\n\n\n  mpg          -79.3126     9.338    -8.493  0.000   -97.620   -61.006\n\n\n  engineSize  1.218e+04   189.969    64.107  0.000  1.18e+04  1.26e+04\n\n\n\n\n  Omnibus:       2450.973   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31060.548\n\n\n  Skew:            2.045    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.557    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe model equation is: estimated car price = -3.661e6 + 1818 * year -0.15 * mileage - 79.31 * mpg + 12180 * engineSize\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\n\n#Using the predict() function associated with the 'model' object to make predictions of car price on test (unknown) data\npred_price = model.predict(testf)#Note that the predict() function finds the predictor 'engineSize' in the testf dataframe, and plugs its values in the regression equation for prediction.\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.scatterplot(x = testp.price, y = pred_price)\n#In case of a perfect prediction, all the points must lie on the line x = y.\nsns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='orange') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\n\nText(0, 0.5, 'Predicted price')\n\n\n\n\n\nThe prediction looks better as compared to the one with simple linear regression. This is because we have four predictors to help explain the variation in car price, instead of just one in the case of simple linear regression. Also, all the predictors have a significant relationship with price as evident from their p-values. Thus, all four of them are contributing in explaining the variation. Note the higher values of R2 as compared to the one in the case of simple linear regression.\nWhat is the RMSE of the predicted car price?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9956.82497993548\n\n\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n9563.74782917604\n\n\n\nsns.scatterplot(x = model.fittedvalues, y=model.resid,color = 'orange')\nsns.lineplot(x = [pred_price.min(),pred_price.max()],y = [0,0],color = 'blue')\nplt.xlabel('Predicted price')\nplt.ylabel('Residual')\n\nText(0, 0.5, 'Residual')\n\n\n\n\n\nWill the explained variation (R-squared) in car price always increase if we add a variable?\nShould we keep on adding variables as long as the explained variation (R-squared) is increasing?\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nnp.random.seed(1)\ntrain['rand_col'] = np.random.rand(train.shape[0])\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize+rand_col', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.661 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           1928. \n\n\n  Date:             Tue, 27 Dec 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:07:38       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4954        BIC:                1.050e+05\n\n\n  Df Model:                   5                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.662e+06  1.49e+05   -24.600  0.000 -3.95e+06 -3.37e+06\n\n\n  year        1818.1672    73.753    24.652  0.000  1673.578  1962.756\n\n\n  mileage       -0.1474     0.009   -16.809  0.000    -0.165    -0.130\n\n\n  mpg          -79.2837     9.338    -8.490  0.000   -97.591   -60.976\n\n\n  engineSize  1.218e+04   189.972    64.109  0.000  1.18e+04  1.26e+04\n\n\n  rand_col     451.1226   471.897     0.956  0.339  -474.004  1376.249\n\n\n\n\n  Omnibus:       2451.728   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31040.331\n\n\n  Skew:            2.046    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.552    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nAdding a variable with random values to the model (rand_col) increased the explained variation (R-squared). This is because the model has one more parameter to tune to reduce the residual squared error (RSS). However, the p-value of rand_col suggests that its coefficient is zero. Thus, using the model with rand_col may give poorer performance on unknown data, as compared to the model without rand_col. This implies that it is not a good idea to blindly add variables in the model to increase R-squared."
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix A — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here"
  }
]