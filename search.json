[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science III with python (Class notes)",
    "section": "",
    "text": "Preface\nThese are class notes for the course STAT303-3. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThe course notes are currently being written, and will continue to being developed as the course progresses (just like the class notes last quarter). Please report any typos / mistakes / inconsistencies / issues with the class notes / class presentations in your comments here. Thank you!"
  },
  {
    "objectID": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "href": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "title": "1  Introduction to scikit-learn",
    "section": "1.1 Splitting data into train and test",
    "text": "1.1 Splitting data into train and test\nLet us create train and test datasets for developing a model to predict if a person has diabetes.\n\n# Creating training and test data\n    # 80-20 split, which is usual - 70-30 split is also fine, 90-10 is fine if the dataset is large\n    # random_state to set a random seed for the splitting - reproducible results\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)\n\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the complete dataset.\n\n#Proportion of 0s and 1s in the complete data\ny.value_counts()/y.shape\n\n0    0.651042\n1    0.348958\nName: Outcome, dtype: float64\n\n\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the train dataset.\n\n#Proportion of 0s and 1s in train data\ny_train.value_counts()/y_train.shape\n\n0    0.644951\n1    0.355049\nName: Outcome, dtype: float64\n\n\n\n#Proportion of 0s and 1s in test data\ny_test.value_counts()/y_test.shape\n\n0    0.675325\n1    0.324675\nName: Outcome, dtype: float64\n\n\nWe observe that the proportion of 0s and 1s in the train and test dataset are slightly different from that in the complete data. In order for these datasets to be more representative of the population, they should have a proportion of 0s and 1s similar to that in the complete dataset. This is especially critical in case of imbalanced datasets, where one class is represented by a significantly smaller number of instances than the other(s).\nWhen training a classification model on an imbalanced dataset, the model might not learn enough about the minority class, which can lead to poor generalization performance on new data. This happens because the model is biased towards the majority class, and it might even predict all instances as belonging to the majority class.\n\n1.1.1 Stratified splitting\nWe will use the argument stratify to obtain a proportion of 0s and 1s in the train and test datasets that is similar to the proportion in the complete `data.\n\n#Stratified train-test split\nX_train_stratified, X_test_stratified, y_train_stratified,\\\ny_test_stratified = train_test_split(X, y, test_size = 0.2, random_state = 45, stratify=y)\n\n\n#Proportion of 0s and 1s in train data with stratified split\ny_train_stratified.value_counts()/y_train.shape\n\n0    0.651466\n1    0.348534\nName: Outcome, dtype: float64\n\n\n\n#Proportion of 0s and 1s in test data with stratified split\ny_test_stratified.value_counts()/y_test.shape\n\n0    0.649351\n1    0.350649\nName: Outcome, dtype: float64\n\n\nThe proportion of the classes in the stratified split mimics the proportion in the complete dataset more closely.\nBy using stratified splitting, we ensure that both the train and test data sets have the same proportion of instances from each class, which means that the model will see enough instances from the minority class during training. This, in turn, helps the model learn to distinguish between the classes better, leading to better performance on new data.\nThus, stratified splitting helps to ensure that the model sees enough instances from each class during training, which can improve the model’s ability to generalize to new data, particularly in cases where one class is underrepresented in the dataset.\nLet us develop a logistic regression model for predicting if a person has diabetes."
  },
  {
    "objectID": "L1_Scikit-learn.html#scaling-data",
    "href": "L1_Scikit-learn.html#scaling-data",
    "title": "1  Introduction to scikit-learn",
    "section": "1.2 Scaling data",
    "text": "1.2 Scaling data\nIn certain models, it may be important to scale data for various reasons. In a logistic regression model, scaling can help with model convergence. Scikit-learn uses a method known as gradient-descent (not in scope of the syllabus of this course) to obtain a solution. In case the predictors have different orders of magnitude, the algorithm may fail to converge. In such cases, it is useful to standardize the predictors so that all of them are at the same scale.\n\n# With linear/logistic regression in scikit-learn, especially when the predictors have different orders \n# of magn., scaling is necessary. This is to enable the training algo. which we did not cover. (Gradient Descent)\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test) # Do NOT refit the scaler with the test data, just transform it."
  },
  {
    "objectID": "L1_Scikit-learn.html#fitting-a-model",
    "href": "L1_Scikit-learn.html#fitting-a-model",
    "title": "1  Introduction to scikit-learn",
    "section": "1.3 Fitting a model",
    "text": "1.3 Fitting a model\nLet us fit a logistic regression model for predicting if a person has diabetes. Let us try fitting a model with the un-scaled data.\n\n# Create a model object - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train, y_train)\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()\n\n\nNote that the model with the un-scaled predictors fails to converge. Check out the data X_train to see that this may be probably due to the predictors have different orders of magnitude. For example, the predictor DiabetesPedigreeFunction has values in [0.078, 2.42], while the predictor Insulin has values in [0, 800].\nLet us fit the model to the scaled data.\n\n# Create a model - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train_scaled, y_train)\n\nLogisticRegression()\n\n\nThe model converges to a solution with the scaled data!\nThe coefficients of the model can be returned with the coef_ attribute of the LogisticRegression() object. However, the output is not as well formatted as in the case of the statsmodels library since sklearn is developed primarily for the purpose of prediction, and not inference.\n\n# Use coef_ to return the coefficients - only log reg inference you can do with sklearn\nprint(logreg.coef_) \n\n[[ 0.32572891  1.20110566 -0.32046591  0.06849882 -0.21727131  0.72619528\n   0.40088897  0.29698818]]"
  },
  {
    "objectID": "L1_Scikit-learn.html#computing-performance-metrics",
    "href": "L1_Scikit-learn.html#computing-performance-metrics",
    "title": "1  Introduction to scikit-learn",
    "section": "1.4 Computing performance metrics",
    "text": "1.4 Computing performance metrics\n\n1.4.1 Accuracy\nLet us test the model prediction accuracy on the test data. We’ll demonstrate two different functions that can be used to compute model accuracy - accuracy_score(), and score().\nThe accuracy_score() function from the metrics module of the sklearn library is general, and can be used for any classification model. We’ll use it along with the predict() method of the LogisticRegression() object, which returns the predicted class based on a threshold probability of 0.5.\n\n# Get the predicted classes first\ny_pred = logreg.predict(X_test_scaled)\n\n# Use the predicted and true classes for accuracy\nprint(accuracy_score(y_pred, y_test)*100) \n\n73.37662337662337\n\n\nThe score() method of the LogisticRegression() object can be used to compute the accuracy only for a logistic regression model. Note that for a LinearRegression() object, the score() method will return the model \\(R\\)-squared.\n\n# Use .score with test predictors and response to get the accuracy\n# Implements the same thing under the hood\nprint(logreg.score(X_test_scaled, y_test)*100)  \n\n73.37662337662337\n\n\n\n\n1.4.2 ROC-AUC\nThe roc_curve() and auc() functions from the metrics module of the sklearn library can be used to compute the ROC-AUC, or the area under the ROC curve. Note that for computing ROC-AUC, we need the predicted probability, instead of the predicted class. Thus, we’ll use the predict_proba() method of the LogisticRegression() object, which returns the predicted probability for the observation to belong to each of the classes, instead of using the predict() method, which returns the predicted class based on threshold probability of 0.5.\n\n#Computing the predicted probability for the observation to belong to the positive class (y=1);\n#The 2nd column in the output of predict_proba() consists of the probability of the observation to \n#belong to the positive class (y=1)\ny_pred_prob = logreg.predict_proba(X_test_scaled)[:,1] \n\n#Using the predicted probability computed above to find ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test, y_pred_prob)\nprint(auc(fpr, tpr))# AUC of ROC\n\n0.7923076923076922\n\n\n\n\n1.4.3 Confusion matrix & precision-recall\nThe confusion_matrix(), precision_score(), and recall_score() functions from the metrics module of the sklearn library can be used to compute the confusion matrix, precision, and recall respectively.\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\n\n\n\n\nprint(\"Precision: \", precision_score(y_test, y_pred))\nprint(\"Recall: \", recall_score(y_test, y_pred))\n\nPrecision:  0.6046511627906976\nRecall:  0.52\n\n\nLet us compute the performance metrics if we develop the model using stratified splitting.\n\n# Developing the model with stratified splitting\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n#Computing the accuracy\ny_pred_stratified = logreg.predict(X_test_stratified_scaled)\nprint(\"Accuracy: \",accuracy_score(y_pred_stratified, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_stratified))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_stratified))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_stratified), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  78.57142857142857\nROC-AUC:  0.8505555555555556\nPrecision:  0.7692307692307693\nRecall:  0.5555555555555556\n\n\n\n\n\nThe model with the stratified train-test split has a better performance as compared to the other model on all the performance metrics!"
  },
  {
    "objectID": "L1_Scikit-learn.html#tuning-the-model-hyperparameters",
    "href": "L1_Scikit-learn.html#tuning-the-model-hyperparameters",
    "title": "1  Introduction to scikit-learn",
    "section": "1.5 Tuning the model hyperparameters",
    "text": "1.5 Tuning the model hyperparameters\nA hyperparameter (among others) that can be trained in a logistic regression model is the regularization parameter.\nWe may also wish to tune the decision threhsold probability. Note that the decision threshold probability is not considered a hyperparameter of the model. Hyperparameters are model parameters that are set prior to training and cannot be directly adjusted by the model during training. Examples of hyperparameters in a logistic regression model include the regularization parameter, and the type of shrinkage penalty - lasso / ridge. These hyperparameters are typically optimized through a separate tuning process, such as cross-validation or grid search, before training the final model.\nThe performance metrics can be computed using a desired value of the threshold probability. Let us compute the performance metrics for a desired threshold probability of 0.3.\n\n# Performance metrics computation for a desired threshold probability of 0.3\ndesired_threshold = 0.3\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob > desired_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  75.32467532467533\nROC-AUC:  0.8505555555555556\nPrecision:  0.6111111111111112\nRecall:  0.8148148148148148\n\n\n\n\n\n\n1.5.1 Tuning decision threshold probability\nSuppose we wish to find the optimal decision threshold probability to maximize accuracy. Note that we cannot use the test dataset to optimize model hyperparameters, as that may lead to overfitting on the test data. We’ll use \\(K\\)-fold cross validation on train data to find the optimal decision threshold probability.\nWe’ll use the cross_val_predict() function from the model_selection module of sklearn to compute the \\(K\\)-fold cross validated predicted probabilities. Note that this function simplifies the task of manually creating the \\(K\\)-folds, training the model \\(K\\)-times, and computing the predicted probabilities on each of the \\(K\\)-folds. Thereafter, the predicted probabilities will be used to find the one the optimal threshold probability that maximizes the classification accuracy.\n\nhyperparam_vals = np.arange(0,1.01,0.01)\naccuracy_iter = []\n\npredicted_probability = cross_val_predict(LogisticRegression(), X_train_stratified_scaled, \n                                              y_train_stratified, cv = 5, method = 'predict_proba')\n\nfor threshold_prob in hyperparam_vals:\n    predicted_class = predicted_probability[:,1] > threshold_prob\n    predicted_class = predicted_class.astype(int)\n\n    #Computing the accuracy\n    accuracy = accuracy_score(predicted_class, y_train_stratified)*100\n    accuracy_iter.append(accuracy)\n\nLet us visualize the accuracy with change in decision threshold probability.\n\n# Accuracy vs decision threshold probability\nsns.scatterplot(x = hyperparam_vals, y = accuracy_iter)\nplt.xlabel('Decision threshold probability')\nplt.ylabel('Average 5-fold CV accuracy');\n\n\n\n\nThe optimal decision threshold probability is the one that maximizes the \\(K\\)-fold cross validation accuracy.\n\n# Optimal decision threshold probability\nhyperparam_vals[accuracy_iter.index(max(accuracy_iter))]\n\n0.46\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.46\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob > desired_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8505555555555556\nPrecision:  0.7804878048780488\nRecall:  0.5925925925925926\n\n\n\n\n\nModel performance on test data has improved with the optimal decision threshold probability.\n\n\n1.5.2 Tuning the regularization parameter\nThe LogisticRegression() method has a default L2 regularization penalty, which means ridge regression.C is \\(1/\\lambda\\), where \\(\\lambda\\) is the hyperparameter that is multiplied with the ridge penalty. C is 1 by default.\n\naccuracy_iter = []\nhyperparam_vals = 10**np.linspace(-3.5, 1)\n\nfor c_val in hyperparam_vals: # For each possible C value in your grid\n    logreg_model = LogisticRegression(C=c_val) # Create a model with the C value\n    \n    accuracy_iter.append(cross_val_score(logreg_model, X_train_stratified_scaled, y_train_stratified,\n                                      scoring='accuracy', cv=5)) # Find the cv results\n\n\nplt.plot(hyperparam_vals, np.mean(np.array(accuracy_iter), axis=1))\nplt.xlabel('C')\nplt.ylabel('Average 5-fold CV accuracy')\nplt.xscale('log')\nplt.show()\n\n\n\n\n\n# Optimal value of the regularization parameter 'C'\noptimal_C = hyperparam_vals[np.argmax(np.array(accuracy_iter).mean(axis=1))]\noptimal_C\n\n0.11787686347935879\n\n\n\n# Developing the model with stratified splitting and optimal 'C'\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg = LogisticRegression(C = optimal_C)\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n#Computing the accuracy\ny_pred_stratified = logreg.predict(X_test_stratified_scaled)\nprint(\"Accuracy: \",accuracy_score(y_pred_stratified, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_stratified))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_stratified))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_stratified), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  78.57142857142857\nROC-AUC:  0.8516666666666666\nPrecision:  0.7837837837837838\nRecall:  0.5370370370370371\n\n\n\n\n\n\n\n1.5.3 Tuning the decision threshold probability and the regularization parameter simultaneously\n\nthreshold_hyperparam_vals = np.arange(0,1.01,0.01)\nC_hyperparam_vals = 10**np.linspace(-3.5, 1)\naccuracy_iter = []\n\nfor c_val in C_hyperparam_vals:\n    predicted_probability = cross_val_predict(LogisticRegression(C = c_val), X_train_stratified_scaled, \n                                                  y_train_stratified, cv = 5, method = 'predict_proba')\n\n    for threshold_prob in threshold_hyperparam_vals:\n        predicted_class = predicted_probability[:,1] > threshold_prob\n        predicted_class = predicted_class.astype(int)\n\n        #Computing the accuracy\n        accuracy = accuracy_score(predicted_class, y_train_stratified)*100\n        accuracy_iter.append(accuracy)\n\n\nmax_acc_iter = np.argmax(accuracy_iter)\n\n#Optimal decision threshold probability\noptimal_threshold = threshold_hyperparam_vals[max_acc_iter%len(threshold_hyperparam_vals)]\nprint(\"Optimal decision threshold = \", optimal_threshold)\n\n#Optimal C\noptimal_C = C_hyperparam_vals[int(max_acc_iter/len(threshold_hyperparam_vals))]\nprint(\"Optimal C = \", optimal_C)\n\nOptimal decision threshold =  0.46\nOptimal C =  2.2758459260747887\n\n\n\n# Developing the model with stratified splitting, optimal decision threshold probability, and optimal 'C'\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg = LogisticRegression(C = optimal_C)\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n# Performance metrics computation for the optimal threshold probability\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob > optimal_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8507407407407408\nPrecision:  0.7804878048780488\nRecall:  0.5925925925925926\n\n\n\n\n\nLater in the course, we’ll see the sklearn function GridSearchCV, which is used to optimize several model hyperparameters simultaneously with \\(K\\)-fold cross validation, while avoiding for loops."
  },
  {
    "objectID": "Lec2_Regression_splines.html#polynomial-regression-vs-regression-splines",
    "href": "Lec2_Regression_splines.html#polynomial-regression-vs-regression-splines",
    "title": "2  Regression splines",
    "section": "2.1 Polynomial regression vs Regression splines",
    "text": "2.1 Polynomial regression vs Regression splines\n\n2.1.1 Model of degree 1\n\nols_object = smf.ols(formula = 'price~mileage', data = train)\nlr_model = ols_object.fit()\n\n\n#Regression spline of degree 1\n\n#Creating basis functions for splines of degree 1\ntransformed_x = dmatrix(\"bs(mileage , knots=(33000,66000,100000), degree = 1, include_intercept = False)\",\n                        data = {'mileage':train['mileage']},return_type = 'dataframe')\n\n#Developing a linear regression model on the spline basis functions - this is the regression splines model\nreg_spline_model = sm.OLS(train['price'], transformed_x).fit()\n\n\n#Visualizing polynomial model and the regression spline model of degree 1\n\nknots = [33000,66000,100000] #Knots for the spline\nd=1 #Degree of predictor in the model\n#Writing a function to visualize polynomial model and the regression spline model of degree d\ndef viz_models():\n    fig, axes = plt.subplots(1,2,figsize = (15,5))\n    plt.subplots_adjust(wspace=0.2)\n\n    #Visualizing the linear regression model\n    pred_price = lr_model.predict(train)\n    sns.scatterplot(ax = axes[0],x = 'mileage', y = 'price', data = train, color = 'orange')\n    sns.lineplot(ax = axes[0],x = train.mileage, y = pred_price, color = 'blue')\n    axes[0].set_title('Polynomial regression model of degree '+str(d))\n    \n    #Visualizing the regression splines model of degree 'd'    \n    axes[1].set_title('Regression splines model of degree '+ str(d))\n    sns.scatterplot(ax=axes[1],x = 'mileage', y = 'price', data = train, color = 'orange')\n    sns.lineplot(ax=axes[1],x = train.mileage, y = reg_spline_model.predict(), color = 'blue')\n    for i in range(3):\n        plt.axvline(knots[i], 0,100,color='red')\nviz_models()\n\n\n\n\nWe observe the regression splines model better fits the data as compared to the polynomial regression model. This is because regression splines of degree 1 fit piecewise polynomials, or linear models on sub-sections of the predictor, which helps better capture the trend. However, this added flexibility may also lead to overfitting. Hence, one must be careful to check for overfitting when using splines. Overfitting may be checked by k-fold cross validation or comparing test and train errors.\nThe red lines in the plot on the right denote the position of knots. Knots separate distinct splines.\n\n#Creating basis functions for test data for prediction\ntest_x = dmatrix(\"bs(mileage , knots=(33000,66000,100000), degree = 1, include_intercept = False)\",data = {'mileage':test['mileage']},\n                                                                                                  return_type = 'dataframe')\n\n\n#Function to compute RMSE (root mean squared error on train and test datasets)\ndef rmse():\n    #Error on train data for the linear regression model\n    print(\"RMSE on train data:\")\n    print(\"Linear regression:\", np.sqrt(mean_squared_error(lr_model.predict(),train.price)))\n\n    #Error on train data for the regression spline model\n    print(\"Regression splines:\", np.sqrt(mean_squared_error(reg_spline_model.predict(),train.price)))\n    \n    #Error on test data for the linear regression model\n    print(\"\\nRMSE on test data:\")\n    print(\"Linear regression:\",np.sqrt(mean_squared_error(lr_model.predict(test),test.price)))\n\n    #Error on test data for the regression spline model\n    print(\"Regression splines:\",np.sqrt(mean_squared_error(reg_spline_model.predict(test_x),test.price)))    \nrmse()\n\nRMSE on train data:\nLinear regression: 14403.250083261853\nRegression splines: 13859.640716531134\n\nRMSE on test data:\nLinear regression: 14370.94086395544\nRegression splines: 13770.133025694666\n\n\n\n\n2.1.2 Model of degree 2\nA higher degree model will lead to additional flexibility for both polynomial and regression splines models.\n\n#Including mileage squared as a predictor and developing the model\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)', data = train)\nlr_model = ols_object.fit()\n\n\n#Regression spline of degree 2\n\n#Creating basis functions for splines of degree 2\ntransformed_x = dmatrix(\"bs(mileage , knots=(33000,66000,100000), degree = 2, include_intercept = False)\",\n                        data = {'mileage':train['mileage']},return_type = 'dataframe')\n\n#Developing a linear regression model on the spline basis functions - this is the regression splines model\nreg_spline_model = sm.OLS(train['price'], transformed_x).fit()\n\n\nd=2\nviz_models()\n\n\n\n\nUnlike polynomial regression, splines functions avoid imposing a global structure on the non-linear function of X. This provides a better local fit to the data.\n\n#Creating basis functions for test data for prediction\ntest_x = dmatrix(\"bs(mileage , knots=(33000,66000,100000), degree = 2, include_intercept = False)\",data = {'mileage':test['mileage']},\n                                                                                                  return_type = 'dataframe')\n\n\nrmse()\n\nRMSE on train data:\nLinear regression: 14403.250083261853\nRegression splines: 13859.640716531134\n\nRMSE on test data:\nLinear regression: 14370.94086395544\nRegression splines: 13770.133025694666\n\n\n\n\n2.1.3 Model of degree 3\n\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)+I(mileage**3)', data = train)\nlr_model = ols_object.fit()\n\n\n#Regression spline of degree 3\n\n#Creating basis functions for splines of degree 3\ntransformed_x = dmatrix(\"bs(mileage , knots=(20000,40000,80000), degree = 3, include_intercept = False)\",\n                        data = {'mileage':train['mileage']},return_type = 'dataframe')\n\n#Developing a linear regression model on the spline basis functions - this is the regression splines model\nreg_spline_model = sm.OLS(train['price'], transformed_x).fit()\n\n\ntransformed_x\n\n\n\n\n\n  \n    \n      \n      Intercept\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[0]\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[1]\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[2]\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[3]\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[4]\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[5]\n    \n  \n  \n    \n      0\n      1.0\n      0.001499\n      3.749187e-07\n      1.562637e-11\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      1\n      1.0\n      0.583162\n      3.001491e-01\n      1.975041e-02\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      2\n      1.0\n      0.000750\n      9.374336e-08\n      1.953296e-12\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      3\n      1.0\n      0.293446\n      6.009875e-01\n      1.053974e-01\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      4\n      1.0\n      0.000000\n      2.580169e-02\n      7.669068e-01\n      0.200988\n      0.006303\n      0.000000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4955\n      1.0\n      0.005441\n      4.824519e-01\n      5.016606e-01\n      0.010446\n      0.000000\n      0.000000\n    \n    \n      4956\n      1.0\n      0.206763\n      6.438755e-01\n      1.493551e-01\n      0.000006\n      0.000000\n      0.000000\n    \n    \n      4957\n      1.0\n      0.000000\n      0.000000e+00\n      2.783832e-01\n      0.496164\n      0.213126\n      0.012326\n    \n    \n      4958\n      1.0\n      0.198162\n      6.468919e-01\n      1.549344e-01\n      0.000012\n      0.000000\n      0.000000\n    \n    \n      4959\n      1.0\n      0.000000\n      2.233101e-01\n      7.229229e-01\n      0.053702\n      0.000065\n      0.000000\n    \n  \n\n4960 rows × 7 columns\n\n\n\n\nd=3\nknots=[20000,40000,80000]\nviz_models()\n\n\n\n\nUnlike polynomial regression, splines functions avoid imposing a global structure on the non-linear function of X. This provides a better local fit to the data.\n\n#Creating basis functions for test data for prediction\ntest_x = dmatrix(\"bs(mileage , knots=(20000,40000,80000), degree = 3, include_intercept = False)\",data = {'mileage':test['mileage']},\n                                                                                                  return_type = 'dataframe')\n\n\nrmse()\n\nRMSE on train data:\nLinear regression: 13891.962447594644\nRegression splines: 13792.371446327243\n\nRMSE on test data:\nLinear regression: 13789.708418357186\nRegression splines: 13651.288965905529"
  },
  {
    "objectID": "Lec2_Regression_splines.html#regression-splines-with-knots-at-uniform-quantiles-of-data",
    "href": "Lec2_Regression_splines.html#regression-splines-with-knots-at-uniform-quantiles-of-data",
    "title": "2  Regression splines",
    "section": "2.2 Regression splines with knots at uniform quantiles of data",
    "text": "2.2 Regression splines with knots at uniform quantiles of data\nIf degrees of freedom are provided instead of knots, the knots are by default chosen at uniform quantiles of data. For example if there are 7 degrees of freedom (including the intercept), then there will be 7-4 = 3 knots. These knots will be chosen at the 255h, 50th and 75th quantiles of the data.\n\n#Regression spline of degree 3\n\n#Creating basis functions for splines of degree 3\ntransformed_x = dmatrix(\"bs(mileage , df=6, degree = 3, include_intercept = False)\",\n                        data = {'mileage':train['mileage']},return_type = 'dataframe')\n\n#Developing a linear regression model on the spline basis functions - this is the regression splines model\nreg_spline_model = sm.OLS(train['price'], transformed_x).fit()\n\n\nd=3\nunif_knots = pd.qcut(train.mileage,4,retbins=True)[1][1:4]\nknots=unif_knots\nviz_models()\n\n\n\n\nSplines can be unstable at the outer range of predictors. In the figure (on the right), the left-most spline may be overfitting.\n\n#Creating basis functions for test data for prediction\ntest_x = dmatrix(\"bs(mileage , knots=\" +str(tuple(unif_knots)) + \", degree = 3, include_intercept = False)\",data = {'mileage':test['mileage']},\n                                                                                                  return_type = 'dataframe')\n\n\nrmse()\n\nRMSE on train data:\nLinear regression: 13891.962447594644\nRegression splines: 13781.79102252679\n\nRMSE on test data:\nLinear regression: 13789.708418357186\nRegression splines: 13676.271829882426"
  },
  {
    "objectID": "Lec2_Regression_splines.html#natural-cubic-splines",
    "href": "Lec2_Regression_splines.html#natural-cubic-splines",
    "title": "2  Regression splines",
    "section": "2.3 Natural cubic splines",
    "text": "2.3 Natural cubic splines\nPage 298: “A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This additional constraint means that natural splines generally produce more stable estimates at the boundaries.”\n\n#Natural cubic spline\n\n#Creating basis functions for the natural cubic spline\ntransformed_x = dmatrix(\"cr(mileage , df=4,constraints='center')\",\n                        data = {'mileage':train['mileage']},return_type = 'dataframe')\nreg_spline_model = sm.GLM(train['price'], transformed_x).fit()\n\n\nd=3;\nunif_knots = pd.qcut(train.mileage,4,retbins=True)[1][1:4]\nknots=unif_knots\nviz_models()\n\n\n\n\nNote that the natural cubic spline is more stable than a cubic splines with knots at uniformly distributed quantiles.\n\n#Creating basis functions for test data for prediction\ntest_x = dmatrix(\"cr(mileage , knots=\"+str(tuple(unif_knots))+\",constraints='center')\",data = {'mileage':test['mileage']},\n                                                                                                  return_type = 'dataframe')\n\n\nrmse()\n\nRMSE on train data:\nLinear regression: 13891.962447594644\nRegression splines: 13805.022189679756\n\nRMSE on test data:\nLinear regression: 13789.708418357186\nRegression splines: 13666.943224268975"
  },
  {
    "objectID": "Lec2_Regression_splines.html#generalized-additive-model-gam",
    "href": "Lec2_Regression_splines.html#generalized-additive-model-gam",
    "title": "2  Regression splines",
    "section": "2.4 Generalized additive model (GAM)",
    "text": "2.4 Generalized additive model (GAM)\nGAM allow for flexible nonlinearities in several variables, but retain the additive structure of linear models. In a GAM, non-linear basis functions of predictors can be used as predictors of a linear regression model. For example, \\[y = f_1(X_1) + f_2(X_2) + \\epsilon\\] is a GAM, where \\(f_1(.)\\) may be a cubic spline based on the predictor \\(X_1\\), and \\(f_2(.)\\) may be a step function based on the predictor \\(X_2\\).\n\nsns.distplot(train.year)\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n<AxesSubplot:xlabel='year', ylabel='Density'>\n\n\n\n\n\n\n#GAM\n#GAM includes cubic splines for mileage. Other predictors are year, engineSize, mpg, mileage and their interactions\nX_transformed = dmatrix('bs(mileage,df=6,degree = 3)+year*engineSize*mpg*mileage', \n                data = {'year':train['year'],'engineSize':train['engineSize'],'mpg':train['mpg'],'mileage':train['mileage']} ,\n                                                                            return_type = 'dataframe')\n# fit the model\nmodel_gam = sm.OLS(train['price'],X_transformed).fit()\n\n#Creating basis functions for test data for prediction\nX_test = dmatrix('bs(mileage,df=6,degree = 3, include_intercept = False)+year*engineSize*mpg*mileage', \n                data = {'year':test['year'],'engineSize':test['engineSize'],'mpg':test['mpg'],'mileage':test['mileage']} ,\n                                                                            return_type = 'dataframe')\n\npreds = model_gam.predict(X_test)\nnp.sqrt(mean_squared_error(preds,test.price))\n\n8434.756663328963\n\n\n\n#GAM\n#GAM includes cubic splines for mileage, year, engineSize, mpg, and interactions of all predictors\nX_transformed = dmatrix('bs(mileage,df=6,degree = 3)+bs(mpg,df=6,degree = 3)+bs(engineSize,df=6,degree = 3)+year*engineSize*mpg*mileage', \n                data = {'year':train['year'],'engineSize':train['engineSize'],'mpg':train['mpg'],'mileage':train['mileage']} ,\n                                                                            return_type = 'dataframe')\n# fit the model\nmodel_gam = sm.OLS(train['price'],X_transformed).fit()\n\n#Creating basis functions for test data for prediction\nX_test = dmatrix('bs(mileage,df=6,degree = 3, include_intercept = False)+bs(mpg,df=6,degree = 3)+bs(engineSize,df=6,degree = 3)+year*engineSize*mpg*mileage', \n                data = {'year':test['year'],'engineSize':test['engineSize'],'mpg':test['mpg'],'mileage':test['mileage']} ,\n                                                                            return_type = 'dataframe')\n\npreds = model_gam.predict(X_test)\nnp.sqrt(mean_squared_error(preds,test.price))\n\n7997.325718841729\n\n\n\nols_object = smf.ols(formula = 'price~(year+engineSize+mileage+mpg)**2+I(mileage**2)+I(mileage**3)', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.704 \n\n\n  Model:                   OLS         Adj. R-squared:        0.703 \n\n\n  Method:             Least Squares    F-statistic:           1308. \n\n\n  Date:             Sun, 27 Mar 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:08:50       Log-Likelihood:      -52157. \n\n\n  No. Observations:        4960        AIC:                1.043e+05\n\n\n  Df Residuals:            4950        BIC:                1.044e+05\n\n\n  Df Model:                   9                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                        coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept             -0.0009     0.000    -2.752  0.006    -0.002    -0.000\n\n\n  year                  -1.1470     0.664    -1.728  0.084    -2.448     0.154\n\n\n  engineSize             0.0052     0.000    17.419  0.000     0.005     0.006\n\n\n  mileage              -31.4751     2.621   -12.010  0.000   -36.613   -26.337\n\n\n  mpg                   -0.0201     0.002   -13.019  0.000    -0.023    -0.017\n\n\n  year:engineSize        9.5957     0.254    37.790  0.000     9.098    10.094\n\n\n  year:mileage           0.0154     0.001    11.816  0.000     0.013     0.018\n\n\n  year:mpg               0.0572     0.013     4.348  0.000     0.031     0.083\n\n\n  engineSize:mileage    -0.1453     0.008   -18.070  0.000    -0.161    -0.130\n\n\n  engineSize:mpg       -98.9062    11.832    -8.359  0.000  -122.102   -75.710\n\n\n  mileage:mpg            0.0011     0.000     2.432  0.015     0.000     0.002\n\n\n  I(mileage ** 2)     7.713e-06  3.75e-07    20.586  0.000  6.98e-06  8.45e-06\n\n\n  I(mileage ** 3)    -1.867e-11  1.43e-12   -13.077  0.000 -2.15e-11 -1.59e-11\n\n\n\n\n  Omnibus:       1830.457   Durbin-Watson:         0.634 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   34927.811\n\n\n  Skew:            1.276    Prob(JB):               0.00 \n\n\n  Kurtosis:       15.747    Cond. No.           2.50e+18 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.5e+18. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nnp.sqrt(mean_squared_error(model.predict(test),test.price))\n\n9026.775740000594\n\n\nNote the RMSE with GAM that includes regression splines for mileage is lesser than that of the linear regression model, indicating a better fit."
  },
  {
    "objectID": "Lec2_Regression_splines.html#mars-multivariate-adaptive-regression-splines",
    "href": "Lec2_Regression_splines.html#mars-multivariate-adaptive-regression-splines",
    "title": "2  Regression splines",
    "section": "2.5 MARS (Multivariate Adaptive Regression Splines)",
    "text": "2.5 MARS (Multivariate Adaptive Regression Splines)\n\nX=train['mileage']\ny=train['price']\n\n\nEarth()\n\nEarth()\n\n\n\nfrom pyearth import Earth\n\n\n2.5.1 MARS of degree 1\n\nmodel = Earth(max_terms=500, max_degree=1) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\nEarth(max_degree=1, max_terms=500)\n\n\n\nprint(model.summary())\n\nEarth Model\n-------------------------------------\nBasis Function  Pruned  Coefficient  \n-------------------------------------\n(Intercept)     No      -553155      \nh(x0-22141)     Yes     None         \nh(22141-x0)     Yes     None         \nh(x0-3354)      No      -6.23571     \nh(3354-x0)      Yes     None         \nh(x0-15413)     No      -36.9613     \nh(15413-x0)     No      38.167       \nh(x0-106800)    Yes     None         \nh(106800-x0)    No      0.221844     \nh(x0-500)       No      170.039      \nh(500-x0)       Yes     None         \nh(x0-741)       Yes     None         \nh(741-x0)       No      -54.5265     \nh(x0-375)       No      -126.804     \nh(375-x0)       Yes     None         \nh(x0-2456)      Yes     None         \nh(2456-x0)      No      7.04609      \n-------------------------------------\nMSE: 188429705.7549, GCV: 190035470.5664, RSQ: 0.2998, GRSQ: 0.2942\n\n\nModel equation: \\[-553155 -6.23(h(x0-3354)) -36.96(h(x0-15413) + .......... -7.04(h(2456-x0)\\]\n\npred = model.predict(test.mileage)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n13650.2113154515\n\n\n\nsns.scatterplot(x = 'mileage', y = 'price', data = train, color = 'orange')\nsns.lineplot(x = train.mileage, y = model.predict(train.mileage), color = 'blue')\n\n<AxesSubplot:xlabel='mileage', ylabel='price'>\n\n\n\n\n\n\n\n2.5.2 MARS of degree 2\n\nmodel = Earth(max_terms=500, max_degree=2) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\nprint(model.summary())\n\nEarth Model\n-----------------------------------------------\nBasis Function           Pruned  Coefficient   \n-----------------------------------------------\n(Intercept)              No      19369.7       \nh(x0-22141)              Yes     None          \nh(22141-x0)              Yes     None          \nh(x0-7531)*h(22141-x0)   No      3.74934e-05   \nh(7531-x0)*h(22141-x0)   No      -6.74252e-05  \nx0*h(x0-22141)           No      -8.0703e-06   \nh(x0-15012)              Yes     None          \nh(15012-x0)              No      1.79813       \nh(x0-26311)*h(x0-22141)  No      8.85097e-06   \nh(26311-x0)*h(x0-22141)  Yes     None          \n-----------------------------------------------\nMSE: 189264421.5682, GCV: 190298913.1652, RSQ: 0.2967, GRSQ: 0.2932\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\n\npred = model.predict(test.mileage)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n13590.995419204985\n\n\n\nsns.scatterplot(x = 'mileage', y = 'price', data = train, color = 'orange')\nsns.lineplot(x = train.mileage, y = model.predict(train.mileage), color = 'blue')\n\n<AxesSubplot:xlabel='mileage', ylabel='price'>\n\n\n\n\n\nMARS provides a better fit than the splines that we used above. This is because MARS tunes the positions of the knots and considers interactions (also with tuned knots) to improve the model fit. Tuning of knots may improve the fit of splines as well.\n\n\n2.5.3 MARS including categorical variables\n\n#A categorical variable can be turned to dummy variables to use the Earth package for fitting MARS model\ntrain_cat = pd.concat([train,pd.get_dummies(train.fuelType)],axis=1)\ntest_cat = pd.concat([test,pd.get_dummies(test.fuelType)],axis=1)\n\n\ntrain_cat.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n      Diesel\n      Electric\n      Hybrid\n      Other\n      Petrol\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n      1\n      0\n      0\n      0\n      0\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n      1\n      0\n      0\n      0\n      0\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n      1\n      0\n      0\n      0\n      0\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n      1\n      0\n      0\n      0\n      0\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\n\nX = train_cat[['mileage','mpg','engineSize','year','Diesel','Electric','Hybrid','Petrol']]\nXtest = test_cat[['mileage','mpg','engineSize','year','Diesel','Electric','Hybrid','Petrol']]\n\n\nmodel = Earth(max_terms=500, max_degree=2) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\nprint(model.summary())\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\n\n\nEarth Model\n---------------------------------------------------------\nBasis Function                      Pruned  Coefficient  \n---------------------------------------------------------\n(Intercept)                         No      2.17604e+06  \nh(engineSize-5.5)                   No      9.80752e+06  \nh(5.5-engineSize)                   No      1.92817e+06  \nh(mileage-21050)                    No      18.687       \nh(21050-mileage)                    No      -177.871     \nh(mileage-21050)*h(5.5-engineSize)  Yes     None         \nh(21050-mileage)*h(5.5-engineSize)  No      -0.224909    \nyear                                No      4126.41      \nh(mpg-53.3495)                      No      344595       \nh(53.3495-mpg)                      Yes     None         \nHybrid*h(5.5-engineSize)            No      6124.34      \nh(mileage-21050)*year               No      -0.00930239  \nh(21050-mileage)*year               No      0.0886455    \nh(engineSize-5.5)*year              No      -4864.84     \nh(5.5-engineSize)*year              No      -952.92      \nh(mileage-1422)*h(53.3495-mpg)      No      -16.62       \nh(1422-mileage)*h(53.3495-mpg)      No      16.4306      \nHybrid                              No      -89090.6     \nh(mpg-21.1063)*h(53.3495-mpg)       Yes     None         \nh(21.1063-mpg)*h(53.3495-mpg)       No      -8815.99     \nh(mpg-23.4808)*h(5.5-engineSize)    No      -3649.97     \nh(23.4808-mpg)*h(5.5-engineSize)    Yes     None         \nh(mpg-20.5188)*year                 No      31.7341      \nh(20.5188-mpg)*year                 Yes     None         \nh(mpg-22.2566)*h(53.3495-mpg)       No      -52.2531     \nh(22.2566-mpg)*h(53.3495-mpg)       No      7916.19      \nh(mpg-22.6767)                      No      7.56432e+06  \nh(22.6767-mpg)                      Yes     None         \nh(mpg-23.9595)*h(mpg-22.6767)       Yes     None         \nh(23.9595-mpg)*h(mpg-22.6767)       No      -63225.4     \nh(mpg-21.4904)*h(22.6767-mpg)       No      -149055      \nh(21.4904-mpg)*h(22.6767-mpg)       Yes     None         \nh(mpg-21.1063)                      No      -887098      \nh(21.1063-mpg)                      Yes     None         \nh(mpg-29.5303)*h(mpg-22.6767)       No      -3028.87     \nh(29.5303-mpg)*h(mpg-22.6767)       Yes     None         \nh(mpg-28.0681)*h(5.5-engineSize)    No      3572.89      \nh(28.0681-mpg)*h(5.5-engineSize)    Yes     None         \nengineSize*h(5.5-engineSize)        No      -2952.65     \nh(mpg-25.3175)*h(mpg-21.1063)       No      -332551      \nh(25.3175-mpg)*h(mpg-21.1063)       No      324298       \nPetrol*year                         No      -1.37031     \nh(mpg-68.9279)*Hybrid               No      -4087.9      \nh(68.9279-mpg)*Hybrid               Yes     None         \nh(mpg-31.5043)*h(5.5-engineSize)    Yes     None         \nh(31.5043-mpg)*h(5.5-engineSize)    No      3691.82      \nh(mpg-32.7011)*h(5.5-engineSize)    Yes     None         \nh(32.7011-mpg)*h(5.5-engineSize)    No      -2262.78     \nh(mpg-44.9122)*h(mpg-22.6767)       No      335577       \nh(44.9122-mpg)*h(mpg-22.6767)       No      -335623      \nh(engineSize-5.5)*h(mpg-21.1063)    No      27815        \nh(5.5-engineSize)*h(mpg-21.1063)    Yes     None         \nh(mpg-78.1907)*Hybrid               Yes     None         \nh(78.1907-mpg)*Hybrid               No      2221.49      \nh(mpg-63.1632)*h(mpg-22.6767)       Yes     None         \nh(63.1632-mpg)*h(mpg-22.6767)       No      21.0093      \nHybrid*h(mpg-53.3495)               No      4121.91      \nh(mileage-22058)*h(53.3495-mpg)     No      16.6177      \nh(22058-mileage)*h(53.3495-mpg)     No      -16.6044     \nh(mpg-21.8985)                      Yes     None         \nh(21.8985-mpg)                      No      371659       \n---------------------------------------------------------\nMSE: 45859836.5623, GCV: 47884649.3622, RSQ: 0.8296, GRSQ: 0.8221\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\n\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(pred,test2.price))\n\n7499.709075454322\n\n\nLet us compare the RMSE of a MARS model with mileage, mpg, engineSize and year with a linear regression model having the same predictors.\n\nX = train[['mileage','mpg','engineSize','year']]\n\n\nmodel = Earth(max_terms=500, max_degree=2) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\nprint(model.summary())\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\n\n\nEarth Model\n----------------------------------------------------------\nBasis Function                      Pruned  Coefficient   \n----------------------------------------------------------\n(Intercept)                         No      -8.13682e+06  \nh(engineSize-5.5)                   No      9.53908e+06   \nh(5.5-engineSize)                   Yes     None          \nh(mileage-21050)                    No      23.4448       \nh(21050-mileage)                    No      -215.861      \nh(mileage-21050)*h(5.5-engineSize)  Yes     None          \nh(21050-mileage)*h(5.5-engineSize)  No      -0.278562     \nyear                                No      4125.85       \nh(mpg-53.3495)                      Yes     None          \nh(53.3495-mpg)                      Yes     None          \nh(mileage-21050)*year               No      -0.0116601    \nh(21050-mileage)*year               No      0.107624      \nh(mpg-53.2957)*h(5.5-engineSize)    No      -59801.3      \nh(53.2957-mpg)*h(5.5-engineSize)    No      59950.5       \nh(engineSize-5.5)*year              No      -4713.74      \nh(5.5-engineSize)*year              No      -755.742      \nh(mileage-1766)*h(53.3495-mpg)      No      -0.00337072   \nh(1766-mileage)*h(53.3495-mpg)      No      -0.144905     \nh(mpg-19.1277)*h(53.3495-mpg)       No      161.153       \nh(19.1277-mpg)*h(53.3495-mpg)       Yes     None          \nh(mpg-23.4808)*h(5.5-engineSize)    Yes     None          \nh(23.4808-mpg)*h(5.5-engineSize)    Yes     None          \nh(mpg-21.4971)*h(5.5-engineSize)    Yes     None          \nh(21.4971-mpg)*h(5.5-engineSize)    Yes     None          \nh(mpg-40.224)*h(5.5-engineSize)     Yes     None          \nh(40.224-mpg)*h(5.5-engineSize)     No      298.139       \nengineSize*h(5.5-engineSize)        No      -2553.17      \nh(mpg-22.2566)                      Yes     None          \nh(22.2566-mpg)                      No      29257.3       \nh(mpg-20.7712)*h(22.2566-mpg)       No      143796        \nh(20.7712-mpg)*h(22.2566-mpg)       No      -1249.17      \nh(mpg-21.4971)*h(22.2566-mpg)       No      -315486       \nh(21.4971-mpg)*h(22.2566-mpg)       Yes     None          \nh(mpg-27.0995)*h(mpg-22.2566)       No      3855.71       \nh(27.0995-mpg)*h(mpg-22.2566)       Yes     None          \nh(mpg-29.3902)*year                 No      6.05449       \nh(29.3902-mpg)*year                 No      -20.176       \nh(mpg-28.0681)*h(5.5-engineSize)    No      59901.6       \nh(28.0681-mpg)*h(5.5-engineSize)    No      -55502.2      \nh(mpg-23.2962)*h(mpg-22.2566)       No      -56126        \nh(23.2962-mpg)*h(mpg-22.2566)       No      73153.9       \nh(mpg-69.0719)*h(mpg-53.3495)       Yes     None          \nh(69.0719-mpg)*h(mpg-53.3495)       No      -124.847      \nh(engineSize-5.5)*h(22.2566-mpg)    No      -20955.8      \nh(5.5-engineSize)*h(22.2566-mpg)    No      -8336.23      \nh(mpg-23.9595)*h(mpg-22.2566)       No      -62983        \nh(23.9595-mpg)*h(mpg-22.2566)       Yes     None          \nh(mpg-23.6406)*h(mpg-22.2566)       No      115253        \nh(23.6406-mpg)*h(mpg-22.2566)       Yes     None          \nh(mpg-56.1908)                      Yes     None          \nh(56.1908-mpg)                      No      -2239.85      \nh(mpg-29.7993)*h(53.3495-mpg)       No      -139.61       \nh(29.7993-mpg)*h(53.3495-mpg)       No      788.756       \n----------------------------------------------------------\nMSE: 49704412.0771, GCV: 51526765.3943, RSQ: 0.8153, GRSQ: 0.8086\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\n\nXtest = test[['mileage','mpg','engineSize','year']]\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n7614.158359050244\n\n\n\nols_object = smf.ols(formula = 'price~(year+engineSize+mileage+mpg)**2', data = train)\nmodel = ols_object.fit()\npred = model.predict(test)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n8729.912066822455\n\n\nThe RMSE for the MARS model is lesser than that of the linear regression model, as expected."
  },
  {
    "objectID": "Stratified splitting.html#stratified-splitting-with-respect-to-response",
    "href": "Stratified splitting.html#stratified-splitting-with-respect-to-response",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "A.1 Stratified splitting with respect to response",
    "text": "A.1 Stratified splitting with respect to response\nQ: When splitting data into train and test for developing and assessing a classification model, it is recommended to stratify the split with respect to the response. Why?\nA: The main advantage of stratified splitting is that it can help ensure that the training and testing sets have similar distributions of the target variable, which can lead to more accurate and reliable model performance estimates.\nIn many real-world datasets, the target variable may be imbalanced, meaning that one class is more prevalent than the other(s). For example, in a medical dataset, the majority of patients may not have a particular disease, while only a small fraction may have the disease. If a random split is used to divide the dataset into training and testing sets, there is a risk that the testing set may not have enough samples from the minority class, which can lead to biased model performance estimates.\nStratified splitting addresses this issue by ensuring that both the training and testing sets have similar proportions of the target variable. This can lead to more accurate model performance estimates, especially for imbalanced datasets, by ensuring that the testing set contains enough samples from each class to make reliable predictions.\nAnother advantage of stratified splitting is that it can help ensure that the model is not overfitting to a particular class. If a random split is used and one class is overrepresented in the training set, the model may learn to predict that class well but perform poorly on the other class(es). Stratified splitting can help ensure that the model is exposed to a representative sample of all classes during training, which can improve its generalization performance on new, unseen data.\nIn summary, the advantages of stratified splitting are that it can lead to more accurate and reliable model performance estimates, especially for imbalanced datasets, and can help prevent overfitting to a particular class."
  },
  {
    "objectID": "Stratified splitting.html#stratified-splitting-with-respect-to-response-and-categorical-predictors",
    "href": "Stratified splitting.html#stratified-splitting-with-respect-to-response-and-categorical-predictors",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "A.2 Stratified splitting with respect to response and categorical predictors",
    "text": "A.2 Stratified splitting with respect to response and categorical predictors\nQ: Will it be better to stratify the split with respect to the response as well as categorical predictors, instead of only the response? In that case, the train and test datasets will be even more representative of the complete data.\nA: It is not recommended to stratify with respect to both the response and categorical predictors simultaneously, while splitting a dataset into train and test, because doing so may result in the test data being very similar to train data, thereby defeating the purpose of assessing the model on unseen data. This kind of a stratified splitting will tend to make the relationships between the response and predictors in train data also appear in test data, which will result in the performance on test data being very similar to that in train data. Thus, in this case, the ability of the model to generalize to new, unseen data won’t be assessed by test data.\nTherefore, it is generally recommended to only stratify the response variable when splitting the data for model training, and to use random sampling for the predictor variables. This helps to ensure that the model is able to capture the underlying relationships between the predictor variables and the response variable, while still being able to generalize well to new, unseen data.\nIn the extreme scenario, when there are no continuous predictors, and there are enough observations for stratification with respect to the respone and the categorical predictors, the train and test datasets may turn out to be exactly the same. Example 1 below illustrates this scenario."
  },
  {
    "objectID": "Stratified splitting.html#example-1",
    "href": "Stratified splitting.html#example-1",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "A.3 Example 1",
    "text": "A.3 Example 1\nThe example below shows that the train and test data can be exactly the same if we stratify the split with respect to response and the categorical predictors.\n\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom itertools import product\nsns.set(font_scale=1.35)\n\nLet us simulate a dataset with 8 observations, two categorical predictors x1 and x2 and the the binary response y.\n\n#Setting a seed for reproducible results\nnp.random.seed(9)\n\n# 8 observations\nn = 8\n\n#Simulating the categorical predictors\nx1 = pd.Series(np.random.randint(0,2,n), name = 'x1')\nx2 = pd.Series(np.random.randint(0,2,n), name = 'x2')\n\n#Simulating the response\npr = (x1==1)*0.7+(x2==0)*0.3# + (x3*0.1>0.1)*0.1\ny = pd.Series(1*(np.random.uniform(size = n) < pr), name = 'y')\n\n#Defining the predictor object 'X'\nX = pd.concat([x1, x2], axis = 1)\n\n#Stratified splitting with respect to the response and predictors to create 50% train and test datasets\nX_train_stratified, X_test_stratified, y_train_stratified,\\\ny_test_stratified = train_test_split(X, y, test_size = 0.5, random_state = 45, stratify=data[['x1', 'x2', 'y']])\n\n#Train and test data resulting from the above stratified splitting\ndata_train = pd.concat([X_train_stratified, y_train_stratified], axis = 1)\ndata_test = pd.concat([X_test_stratified, y_test_stratified], axis = 1)\n\nLet us check the train and test datasets created with stratified splitting with respect to both the predictors and the response.\n\ndata_train\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      y\n    \n  \n  \n    \n      2\n      0\n      0\n      1\n    \n    \n      7\n      0\n      1\n      0\n    \n    \n      3\n      1\n      0\n      1\n    \n    \n      1\n      0\n      1\n      0\n    \n  \n\n\n\n\n\ndata_test\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      y\n    \n  \n  \n    \n      4\n      0\n      1\n      0\n    \n    \n      6\n      1\n      0\n      1\n    \n    \n      0\n      0\n      1\n      0\n    \n    \n      5\n      0\n      0\n      1\n    \n  \n\n\n\n\nNote that the train and test datasets are exactly the same! Stratified splitting tends to have the same proportion of observations corresponding to each strata in both the train and test datasets, where each strata is a unique combination of values of x1, x2, and y. This will tend to make the train and test datasets quite similar!"
  },
  {
    "objectID": "Stratified splitting.html#example-2-simulation-results",
    "href": "Stratified splitting.html#example-2-simulation-results",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "A.4 Example 2: Simulation results",
    "text": "A.4 Example 2: Simulation results\nThe example below shows that train and test set performance will tend to be quite similar if we stratify the datasets with respect to the predictors and the response.\nWe’ll simulate a dataset consisting of 1000 observations, 2 categorical predictors x1 and x2, a continuous predictor x3, and a binary response y.\n\n#Setting a seed for reproducible results\nnp.random.seed(99)\n\n# 1000 Observations\nn = 1000\n\n#Simulating categorical predictors x1 and x2\nx1 = pd.Series(np.random.randint(0,2,n), name = 'x1')\nx2 = pd.Series(np.random.randint(0,2,n), name = 'x2')\n\n#Simulating continuous predictor x3\nx3 = pd.Series(np.random.normal(0,1,n), name = 'x3')\n\n#Simulating the response\npr = (x1==1)*0.7+(x2==0)*0.3 + (x3*0.1>0.1)*0.1\ny = pd.Series(1*(np.random.uniform(size = n) < pr), name = 'y')\n\n#Defining the predictor object 'X'\nX = pd.concat([x1, x2, x3], axis = 1)\n\nWe’ll comparing model performance metrics when the data is split into train and test by performing stratified splitting\n\nOnly with repect to the response\nWith respect to the response and categorical predictors\n\nWe’ll perform 1000 simulations, where the data is split using a different seed in each simulation.\n\n#Creating an empty dataframe to store simulation results of 1000 simulations\naccuracy_iter = pd.DataFrame(columns = {'train_y_stratified','test_y_stratified',\n                                        'train_y_CatPredictors_stratified','test_y_CatPredictors_stratified'})\n\n\n# Comparing model performance metrics when the data is split into train and test by performing stratified splitting\n# (1) only with repect to the response\n# (2) with respect to the response and categorical predictors\n\n# Stratified splitting is performed 1000 times and the results are compared\nfor i in np.arange(1,1000):\n \n    #--------Case 1-------------------#\n    # Stratified splitting with respect to response only to create train and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = i, stratify=y)\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Model accuracy on train and test data, with stratification only on response while splitting \n    # the complete data into train and test\n    accuracy_iter.loc[(i-1), 'train_y_stratified'] = model.score(X_train, y_train)\n    accuracy_iter.loc[(i-1), 'test_y_stratified'] = model.score(X_test, y_test)\n        \n    #--------Case 2-------------------#\n    # Stratified splitting with respect to response and categorical predictors to create train \n    # and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = i, \n                                                        stratify=pd.concat([x1, x2, y], axis = 1))\n    model.fit(X_train, y_train)\n\n    # Model accuracy on train and test data, with stratification on response and predictors while \n    # splitting the complete data into train and test\n    accuracy_iter.loc[(i-1), 'train_y_CatPredictors_stratified'] = model.score(X_train, y_train)\n    accuracy_iter.loc[(i-1), 'test_y_CatPredictors_stratified'] = model.score(X_test, y_test)\n    \n# Converting accuracy to numeric\naccuracy_iter = accuracy_iter.apply(lambda x:x.astype(float), axis = 1)\n\n\nDistribution of train and test accuracies\nThe table below shows the distribution of train and test accuracies when the data is split into train and test by performing stratified splitting:\n\nOnly with repect to the response (see train_y_stratified and test_y_stratified)\nWith respect to the response and categorical predictors (see train_y_CatPredictors_stratified and test_y_CatPredictors_stratified)\n\n\naccuracy_iter.describe()\n\n\n\n\n\n  \n    \n      \n      train_y_stratified\n      test_y_stratified\n      train_y_CatPredictors_stratified\n      test_y_CatPredictors_stratified\n    \n  \n  \n    \n      count\n      999.000000\n      999.000000\n      9.990000e+02\n      9.990000e+02\n    \n    \n      mean\n      0.834962\n      0.835150\n      8.350000e-01\n      8.350000e-01\n    \n    \n      std\n      0.005833\n      0.023333\n      8.552999e-15\n      8.552999e-15\n    \n    \n      min\n      0.812500\n      0.755000\n      8.350000e-01\n      8.350000e-01\n    \n    \n      25%\n      0.831250\n      0.820000\n      8.350000e-01\n      8.350000e-01\n    \n    \n      50%\n      0.835000\n      0.835000\n      8.350000e-01\n      8.350000e-01\n    \n    \n      75%\n      0.838750\n      0.850000\n      8.350000e-01\n      8.350000e-01\n    \n    \n      max\n      0.855000\n      0.925000\n      8.350000e-01\n      8.350000e-01\n    \n  \n\n\n\n\nLet us visualize the distribution of these accuracies.\n\n\nA.4.1 Stratified splitting only with respect to the response\n\nsns.histplot(data=accuracy_iter, x=\"train_y_stratified\", color=\"red\", label=\"Train accuracy\", kde=True)\nsns.histplot(data=accuracy_iter, x=\"test_y_stratified\", color=\"skyblue\", label=\"Test accuracy\", kde=True);\nplt.legend()\nplt.xlabel('Accuracy')\n\nText(0.5, 0, 'Accuracy')\n\n\n\n\n\nNote the variability in train and test accuracies when the data is stratified only with respect to the response. The train accuracy varies betweem 81.2% and 85.5%, while the test accuracy varies between 75.5% and 92.5%.\n\n\nA.4.2 Stratified splitting with respect to the response and categorical predictors\n\nsns.histplot(data=accuracy_iter, x=\"train_y_CatPredictors_stratified\", color=\"red\", label=\"Train accuracy\", kde=True)\nsns.histplot(data=accuracy_iter, x=\"test_y_CatPredictors_stratified\", color=\"skyblue\", label=\"Test accuracy\", kde=True);\nplt.legend()\nplt.xlabel('Accuracy')\n\nText(0.5, 0, 'Accuracy')\n\n\n\n\n\nThe train and test accuracies are between 85% and 85.5% for all the simulations. As a results of startifying the splitting with respect to both the response and the categorical predictors, the train and test datasets are almost the same because the datasets are engineered to be quite similar, thereby making the test dataset inappropriate for assessing accuracy on unseen data. Thus, it is recommended to stratify the splitting only with respect to the response."
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix B — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here"
  }
]