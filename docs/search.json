[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science III with python (Class notes)",
    "section": "",
    "text": "Preface\nThis book serves as the course notes for STAT 303 Sec20, Spring 2025 at Northwestern University. To enhance your understanding of the material, you are expected to read the textbook before using these notes.\nIt is an evolving resource designed to support the course’s learning objectives. This edition builds upon the foundational work of Professor Arvind Krishna, whose contributions have provided a strong framework for this resource. We are deeply grateful for his efforts, which continue to shape the book’s development.\nThroughout the quarter, the content will be updated and refined in real time to enhance clarity, depth, and relevance. These modifications ensure alignment with current teaching objectives and methodologies.\nAs a living document, this book welcomes feedback, suggestions, and contributions from students, instructors, and the broader academic community. Your input helps improve its quality and effectiveness.\nThank you for being part of this journey—we hope this resource serves as a valuable guide in your learning.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Bias_variance_code.html",
    "href": "Bias_variance_code.html",
    "title": "1  Bias-variance tradeoff",
    "section": "",
    "text": "1.1 Simple model (Less flexible)\nRead section 2.2.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nIn this chapter, we will show that a flexible model is likely to have high variance and low bias, while a relatively less flexible model is likely to have a high bias and low variance.\nThe examples considered below are motivated from the examples shown in the documentation of the bias_variance_decomp() function from the mlxtend library. We will first manually compute the bias and variance for understanding of the concept. Later, we will show application of the bias_variance_decomp() function to estimate bias and variance.\nLet us consider a linear regression model as the less-flexible (or relatively simple) model.\nWe will first simulate the test dataset for which we will compute the bias and variance.\nnp.random.seed(101)\n\n# Simulating predictor values of test data\nxtest = np.random.uniform(-15, 10, 200)\n\n# Assuming the true mean response is square of the predictor value\nfxtest = xtest**2\n\n# Simulating noiseless test response \nytest = fxtest\n\n# We will find bias and variance using a linear regression model for prediction\nmodel = LinearRegression()\n# Visualizing the data and the true mean response\nsns.scatterplot(x = xtest, y = ytest)\nsns.lineplot(x = xtest, y = fxtest, color = 'grey', linewidth = 2)\n\n# Initializing objects to store predictions and mean squared error\n# of 100 models developed on 100 distinct training datasets samples\npred_test = []; mse_test = []\n\n# Iterating over each of the 100 models\nfor i in range(100):\n    np.random.seed(i)\n    \n    # Simulating the ith training data\n    x = np.random.uniform(-15, 10, 200)\n    fx = x**2\n    y = fx + np.random.normal(0, 10, 200)\n    \n    # Fitting the ith model on the ith training data\n    model.fit(x.reshape(-1,1), y)\n    \n    # Plotting the ith model\n    sns.lineplot(x = x, y = model.predict(x.reshape(-1,1)))\n    \n    # Storing the predictions of the ith model on test data\n    pred_test.append(model.predict(xtest.reshape(-1,1)))\n    \n    # Storing the mean squared error of the ith model on test data\n    mse_test.append(mean_squared_error(model.predict(xtest.reshape(-1,1)), ytest))\nThe above plots show that the 100 models seem to have low variance, but high bias. Note that the bias is low only around a couple of points (x = -10 & x = 5).\nLet us compute the average squared bias over all the test data points.\nmean_pred = np.array(pred_test).mean(axis = 0)\nsq_bias = ((mean_pred - fxtest)**2).mean()\nsq_bias\n\n2042.104126728109\nLet us compute the average variance over all the test data points.\nmean_var = np.array(pred_test).var(axis = 0).mean()\nmean_var\n\n28.37397844429763\nLet us compute the mean squared error over all the test data points.\nnp.array(mse_test).mean()\n\n2070.4781051724062\nNote that the mean squared error should be the same as the sum of squared bias and variance\nThe sum of squared bias and model variance is:\nsq_bias + mean_var\n\n2070.4781051724067\nNote that this is exactly the same as the mean squared error computed above as we are developing a finite number of models, and making predictions on a finite number of test data points.",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "Bias_variance_code.html#complex-model-more-flexible",
    "href": "Bias_variance_code.html#complex-model-more-flexible",
    "title": "1  Bias-variance tradeoff",
    "section": "1.2 Complex model (more flexible)",
    "text": "1.2 Complex model (more flexible)\nLet us consider a decion tree as the more flexible model.\n\nnp.random.seed(101)\nxtest = np.random.uniform(-15, 10, 200)\nfxtest = xtest**2\nytest = fxtest\nmodel = DecisionTreeRegressor()\n\n\nsns.scatterplot(x = xtest, y = ytest)\nsns.lineplot(x = xtest, y = fxtest, color = 'grey', linewidth = 2)\npred_test = []; mse_test = []\nfor i in range(100):\n    np.random.seed(i)\n    x = np.random.uniform(-15, 10, 200)\n    fx = x**2\n    y = fx + np.random.normal(0, 10, 200)\n    model.fit(x.reshape(-1,1), y)\n    sns.lineplot(x = x, y = model.predict(x.reshape(-1,1)))\n    pred_test.append(model.predict(xtest.reshape(-1,1)))\n    mse_test.append(mean_squared_error(model.predict(xtest.reshape(-1,1)), ytest))\n\n\n\n\n\n\n\n\nThe above plots show that the 100 models seem to have high variance, but low bias.\nLet us compute the average squared bias over all the test data points.\n\nmean_pred = np.array(pred_test).mean(axis = 0)\nsq_bias = ((mean_pred - fxtest)**2).mean()\nsq_bias\n\n1.3117561629333938\n\n\nLet us compute the average model variance over all the test data points.\n\nmean_var = np.array(pred_test).var(axis = 0).mean()\nmean_var\n\n102.5226748977198\n\n\nLet us compute the average mean squared error over all the test data points.\n\nnp.array(mse_test).mean()\n\n103.83443106065317\n\n\nNote that the above error is still the same as the sum of the squared bias, model variance and the irreducible error.\nNote that the relatively more flexible model has a higher variance, but lower bias as compared to the less flexible linear model. This will typically be the case, but may not be true in all scenarios. We will discuss one such scenario later.",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "KNN.html",
    "href": "KNN.html",
    "title": "2  KNN",
    "section": "",
    "text": "2.1 KNN for regression\nRead section 4.7.6 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\n\n# Split the dataset into features and target variable\nX = car.drop(columns=['price'])\ny = car['price']\n\n# split the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# extract the categorical columns and put them in a list\ncat_cols = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnum_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n# First transform categorical variables\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', num_cols),  # Just pass numerical features through\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n    ])\n\n# Create pipeline that scales all features together\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('scaler', StandardScaler()),  # Scale everything together\n    ('knn', KNeighborsRegressor(n_neighbors=5))\n])\n\n# Fit the pipeline to the training data\npipeline.fit(X_train, y_train)\n# Predict on the test data\ny_pred = pipeline.predict(X_test)\n# Calculate RMSE\nrmse = root_mean_squared_error(y_test, y_pred)\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"R² Score: {pipeline.score(X_test, y_test):.2f}\")\n\nRMSE: 4364.84\nR² Score: 0.94\n# show the features in the numerical transformer    \npipeline.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out()\nprint(\"numerical features in the pipeline:\", pipeline.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out())\n\n# show the features in the categorical transformer\npipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out()\nprint(\"categorical features in the pipeline:\", pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out())\n\nnumerical features in the pipeline: ['year' 'mileage' 'tax' 'mpg' 'engineSize']\ncategorical features in the pipeline: ['brand_audi' 'brand_bmw' 'brand_ford' 'brand_hyundi' 'brand_merc'\n 'brand_skoda' 'brand_toyota' 'brand_vauxhall' 'brand_vw'\n 'model_ 6 Series' 'model_ 7 Series' 'model_ 8 Series' 'model_ A7'\n 'model_ A8' 'model_ Agila' 'model_ Amarok' 'model_ Antara'\n 'model_ Arteon' 'model_ Avensis' 'model_ Beetle' 'model_ CC'\n 'model_ CLA Class' 'model_ CLK' 'model_ CLS Class' 'model_ Caddy'\n 'model_ Caddy Life' 'model_ Caddy Maxi Life' 'model_ California'\n 'model_ Camry' 'model_ Caravelle' 'model_ Combo Life' 'model_ Edge'\n 'model_ Eos' 'model_ Fusion' 'model_ G Class' 'model_ GL Class'\n 'model_ GLB Class' 'model_ GLS Class' 'model_ GT86' 'model_ GTC'\n 'model_ Galaxy' 'model_ Getz' 'model_ Grand C-MAX'\n 'model_ Grand Tourneo Connect' 'model_ Hilux' 'model_ I40' 'model_ I800'\n 'model_ IQ' 'model_ IX20' 'model_ IX35' 'model_ Jetta' 'model_ KA'\n 'model_ Kamiq' 'model_ Land Cruiser' 'model_ M Class' 'model_ M2'\n 'model_ M3' 'model_ M4' 'model_ M5' 'model_ M6' 'model_ Mustang'\n 'model_ PROACE VERSO' 'model_ Prius' 'model_ Puma' 'model_ Q8'\n 'model_ R8' 'model_ RS3' 'model_ RS4' 'model_ RS5' 'model_ RS6'\n 'model_ Rapid' 'model_ Roomster' 'model_ S Class' 'model_ S3' 'model_ S4'\n 'model_ SLK' 'model_ SQ5' 'model_ SQ7' 'model_ Santa Fe' 'model_ Scala'\n 'model_ Scirocco' 'model_ Shuttle' 'model_ Supra'\n 'model_ Tiguan Allspace' 'model_ Tourneo Connect' 'model_ Tourneo Custom'\n 'model_ V Class' 'model_ Verso' 'model_ Vivaro' 'model_ X-CLASS'\n 'model_ X4' 'model_ X6' 'model_ X7' 'model_ Yeti' 'model_ Z3' 'model_ Z4'\n 'model_ Zafira Tourer' 'model_ i3' 'model_ i8' 'transmission_Automatic'\n 'transmission_Manual' 'transmission_Other' 'transmission_Semi-Auto'\n 'fuelType_Diesel' 'fuelType_Electric' 'fuelType_Hybrid' 'fuelType_Other'\n 'fuelType_Petrol']",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "KNN.html#feature-scaling-in-knn",
    "href": "KNN.html#feature-scaling-in-knn",
    "title": "2  KNN",
    "section": "2.2 Feature Scaling in KNN",
    "text": "2.2 Feature Scaling in KNN\nFeature scaling is essential when using K-Nearest Neighbors (KNN) because the algorithm relies on calculating distances between data points. If features are measured on different scales (e.g., mileage in thousands and mpg in tens), the features with larger numeric ranges can dominate the distance calculations and distort the results.\nTo ensure that all features contribute equally, it’s important to standardize or normalize them before applying KNN. Common scaling techniques include:\n\nStandardization (zero mean, unit variance) using StandardScaler\nMin-max scaling to bring values into the [0, 1] range\n\nWithout scaling, KNN may produce biased or misleading predictions.\nThe example below illustrates how the same KNN model performs without feature scaling, highlighting the importance of preprocessing your data.\n\npreprocessor_no_scaling = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', num_cols),  # Pass numerical features through without scaling\n        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)  # Only one-hot encode categorical\n    ])\n\n# Create pipeline without any scaling\npipeline_no_scaling = Pipeline(steps=[\n    ('preprocessor', preprocessor_no_scaling),\n    ('knn', KNeighborsRegressor(n_neighbors=5))\n])\n\n# Fit the pipeline\npipeline_no_scaling.fit(X_train, y_train)\n\n# Evaluate\ny_pred_no_scaling = pipeline_no_scaling.predict(X_test)\n\nrmse_no_scaling = root_mean_squared_error(y_test, y_pred_no_scaling)\nprint(f\"RMSE without scaling: {rmse_no_scaling:.2f}\")\nprint(f\"R² Score without scaling: {pipeline_no_scaling.score(X_test, y_test):.2f}\")\n\nRMSE without scaling: 13758.38\nR² Score without scaling: 0.35",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "KNN.html#hyperparameters-in-knn",
    "href": "KNN.html#hyperparameters-in-knn",
    "title": "2  KNN",
    "section": "2.3 Hyperparameters in KNN",
    "text": "2.3 Hyperparameters in KNN\nThe most important hyperparameter in K-Nearest Neighbors (KNN) is k, which determines the number of neighbors considered when making predictions. Tuning k helps balance the model’s bias and variance:\n\nA small k (e.g., 1 or 3) can lead to low bias but high variance, making the model sensitive to noise in the training data.\nA large k results in higher bias but lower variance, producing smoother predictions that may underfit the data.\n\n\n2.3.1 Tuning k in KNN\nTo find the optimal value of k, it’s common to use cross-validation, which evaluates model performance on different subsets of the data. A popular tool for this is GridSearchCV, which automates the search process by testing multiple values of k using cross-validation behind the scenes. It selects the value of k that minimizes prediction error on unseen data—helping you achieve a good balance between underfitting and overfitting.\n\n# Create parameter grid for k values\nparam_grid = {\n    'knn__n_neighbors': list(range(1, 20))  # Test k values from 1 to 20\n}\n\n# Set up GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    cv=5,  # 5-fold cross-validation\n    scoring='neg_root_mean_squared_error',  # Optimize for RMSE\n    n_jobs=-1,  # Use all available cores\n    verbose=1\n)\n\n# Fit grid search\nprint(\"Tuning k parameter...\")\ngrid_search.fit(X_train, y_train)\n\n# Get best parameters and results\nbest_k = grid_search.best_params_['knn__n_neighbors']\nbest_score = -grid_search.best_score_  # Convert back from negative RMSE\n\nprint(f\"Best k: {best_k}\")\nprint(f\"Best CV RMSE: {best_score:.2f}\")\n\n# Evaluate on test set using best model\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\ntest_rmse = root_mean_squared_error(y_test, y_pred)\ntest_r2 = r2_score(y_test, y_pred)\n\nprint(f\"Test RMSE with k={best_k}: {test_rmse:.2f}\")\nprint(f\"Test R² Score with k={best_k}: {test_r2:.2f}\")\n\nTuning k parameter...\nFitting 5 folds for each of 19 candidates, totalling 95 fits\nBest k: 3\nBest CV RMSE: 4117.42\nTest RMSE with k=3: 4051.06\nTest R² Score with k=3: 0.94\n\n\n\n# Plot performance across different k values\ncv_results = grid_search.cv_results_\nk_values = param_grid['knn__n_neighbors']\nmean_rmse = -cv_results['mean_test_score'] \n\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, mean_rmse, marker='o')\nplt.xlabel('k (Number of Neighbors)')\nplt.ylabel('RMSE (Cross-Validation)')\nplt.title('KNN Performance for Different k Values')\nplt.grid(True)\nplt.xticks(k_values)\nplt.axvline(x=best_k, color='r', linestyle='--', label=f'Best k = {best_k}')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe distances and the indices of the nearest K observations to each test observation can be obtained using the kneighbors() method.\n\n# Get the KNN estimator from the pipeline\nknn_estimator = best_model.named_steps['knn']\n\n# Get indices of K-nearest neighbors for each test observation\nneighbor_indices = knn_estimator.kneighbors(best_model.named_steps['preprocessor'].transform(X_test), \n                                           return_distance=False)\n# neighbor_indices will contain the indices of the K nearest neighbors for each test observation\n# Note: The indices are relative to the training set, not the test set.\n# To get the actual neighbor observations, you can use these indices to index into the training set\n# For example, to get the actual neighbor observations for the first test observation:\nneighbors = X_train.iloc[neighbor_indices[0]]\nneighbors\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\n\n\n\n\n4580\nmerc\nV Class\n2010\nAutomatic\n259000\nDiesel\n540\n30.8345\n3.0\n\n\n5651\nmerc\nCLK\n2003\nAutomatic\n185000\nPetrol\n330\n18.0803\n4.3\n\n\n3961\nvw\nCaravelle\n2006\nManual\n178000\nDiesel\n325\n34.5738\n2.5\n\n\n\n\n\n\n\n\n\n2.3.2 Tuning Other KNN Hyperparameters\nIn addition to the number of neighbors (k), KNN has several other important hyperparameters that can significantly affect the model’s performance. Fine-tuning these settings helps you get the most out of the algorithm. Key hyperparameters include:\n\nweights: Determines how the neighbors contribute to the prediction.\n\n'uniform': All neighbors are weighted equally (default).\n\n'distance': Closer neighbors have more influence.\n\nChoosing 'distance' can improve performance, especially when data points are unevenly distributed.\n\nmetric: Defines the distance function used to measure similarity between data points.\n\n'minkowski' (default) is a general-purpose metric that includes both Euclidean and Manhattan distances.\n\nOther options include 'euclidean', 'manhattan', or even custom distance functions.\n\np: Used when metric='minkowski'.\n\np=2 gives Euclidean distance (standard for continuous features).\n\np=1 gives Manhattan distance (useful when features are sparse or grid-based).\n\nalgorithm: Controls the method used to compute nearest neighbors.\n\n'auto', 'ball_tree', 'kd_tree', or 'brute'.\n\nMost users can leave this as 'auto', which lets scikit-learn choose the best algorithm based on the data.\n\n\nThese hyperparameters can be tuned using GridSearchCV to find the combination that yields the best performance on validation data.\nThe model hyperparameters can be obtained using the get_params() method. Note that there are other hyperparameters to tune in addition to number of neighbors. However, the number of neighbours may be the most influential hyperparameter in most cases.\n\n# Get the best model parameters\nbest_model.get_params()\n\n{'memory': None,\n 'steps': [('preprocessor',\n   ColumnTransformer(transformers=[('num', 'passthrough',\n                                    ['year', 'mileage', 'tax', 'mpg',\n                                     'engineSize']),\n                                   ('cat',\n                                    OneHotEncoder(handle_unknown='ignore',\n                                                  sparse_output=False),\n                                    ['brand', 'model', 'transmission',\n                                     'fuelType'])])),\n  ('scaler', StandardScaler()),\n  ('knn', KNeighborsRegressor(n_neighbors=3))],\n 'transform_input': None,\n 'verbose': False,\n 'preprocessor': ColumnTransformer(transformers=[('num', 'passthrough',\n                                  ['year', 'mileage', 'tax', 'mpg',\n                                   'engineSize']),\n                                 ('cat',\n                                  OneHotEncoder(handle_unknown='ignore',\n                                                sparse_output=False),\n                                  ['brand', 'model', 'transmission',\n                                   'fuelType'])]),\n 'scaler': StandardScaler(),\n 'knn': KNeighborsRegressor(n_neighbors=3),\n 'preprocessor__force_int_remainder_cols': True,\n 'preprocessor__n_jobs': None,\n 'preprocessor__remainder': 'drop',\n 'preprocessor__sparse_threshold': 0.3,\n 'preprocessor__transformer_weights': None,\n 'preprocessor__transformers': [('num',\n   'passthrough',\n   ['year', 'mileage', 'tax', 'mpg', 'engineSize']),\n  ('cat',\n   OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n   ['brand', 'model', 'transmission', 'fuelType'])],\n 'preprocessor__verbose': False,\n 'preprocessor__verbose_feature_names_out': True,\n 'preprocessor__num': 'passthrough',\n 'preprocessor__cat': OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n 'preprocessor__cat__categories': 'auto',\n 'preprocessor__cat__drop': None,\n 'preprocessor__cat__dtype': numpy.float64,\n 'preprocessor__cat__feature_name_combiner': 'concat',\n 'preprocessor__cat__handle_unknown': 'ignore',\n 'preprocessor__cat__max_categories': None,\n 'preprocessor__cat__min_frequency': None,\n 'preprocessor__cat__sparse_output': False,\n 'scaler__copy': True,\n 'scaler__with_mean': True,\n 'scaler__with_std': True,\n 'knn__algorithm': 'auto',\n 'knn__leaf_size': 30,\n 'knn__metric': 'minkowski',\n 'knn__metric_params': None,\n 'knn__n_jobs': None,\n 'knn__n_neighbors': 3,\n 'knn__p': 2,\n 'knn__weights': 'uniform'}\n\n\n\n# Extended parameter grid\nparam_grid = {\n    'knn__n_neighbors': list(range(1, 20, 2)),  # Test odd k values from 1 to 19 (step=2 for efficiency)\n    'knn__weights': ['uniform', 'distance'],  # Uniform: equal weight; Distance: closer neighbors weigh more\n    'knn__metric': ['euclidean', 'manhattan', 'minkowski'],  # Common distance metrics\n    'knn__p': [1, 2]  # p=1 (Manhattan), p=2 (Euclidean) - only relevant for Minkowski\n}\n\n# Set up GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    cv=5,  # 5-fold cross-validation\n    scoring='neg_root_mean_squared_error',  # Optimize for RMSE\n    n_jobs=-1,  # Use all available cores\n    verbose=1\n)\n\n# Fit grid search\nprint(\"Tuning KNN hyperparameters...\")\ngrid_search.fit(X_train, y_train)\n\n# Get best parameters and results\nbest_params = grid_search.best_params_\nbest_score = -grid_search.best_score_  # Convert negative RMSE to positive\n\n# Display results\nprint(\"\\nBest Parameters:\")\nfor param, value in best_params.items():\n    print(f\"{param}: {value}\")\nprint(f\"Best CV RMSE: {best_score:.2f}\")\n\n# Evaluate on test set using best model\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\ntest_rmse = root_mean_squared_error(y_test, y_pred)  # Calculate RMSE\nprint(f\"Test RMSE: {test_rmse:.2f}\")\n\nTuning KNN hyperparameters...\nFitting 5 folds for each of 120 candidates, totalling 600 fits\n\nBest Parameters:\nknn__metric: euclidean\nknn__n_neighbors: 3\nknn__p: 1\nknn__weights: distance\nBest CV RMSE: 4001.34\nTest RMSE: 3826.94\n\n\nThe results for each cross-validation are stored in the cv_results_ attribute.\n\npd.DataFrame(grid_search.cv_results_).head()\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_knn__metric\nparam_knn__n_neighbors\nparam_knn__p\nparam_knn__weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.033124\n0.003042\n0.127347\n0.023598\neuclidean\n1\n1\nuniform\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4656.637196\n-3474.998033\n-4250.919748\n-4620.623046\n-4839.806784\n-4368.596961\n485.981480\n64\n\n\n1\n0.035615\n0.010407\n0.179835\n0.013115\neuclidean\n1\n1\ndistance\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4656.637196\n-3474.998033\n-4250.919748\n-4620.623046\n-4839.806784\n-4368.596961\n485.981480\n64\n\n\n2\n0.027877\n0.002536\n0.148597\n0.018612\neuclidean\n1\n2\nuniform\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4656.637196\n-3474.998033\n-4250.919748\n-4620.623046\n-4839.806784\n-4368.596961\n485.981480\n64\n\n\n3\n0.043631\n0.016927\n0.168392\n0.027444\neuclidean\n1\n2\ndistance\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4656.637196\n-3474.998033\n-4250.919748\n-4620.623046\n-4839.806784\n-4368.596961\n485.981480\n64\n\n\n4\n0.043071\n0.009615\n0.184532\n0.042681\neuclidean\n3\n1\nuniform\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4227.667178\n-3303.871045\n-3851.430697\n-4603.426146\n-4600.719641\n-4117.422942\n492.858432\n22\n\n\n\n\n\n\n\nThese results can be useful to see if other hyperparameter values are equally good.\n\npd.DataFrame(grid_search.cv_results_).sort_values(by = 'rank_test_score').head()\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_knn__metric\nparam_knn__n_neighbors\nparam_knn__p\nparam_knn__weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n87\n0.038193\n0.010690\n0.149261\n0.050225\nminkowski\n3\n2\ndistance\n{'knn__metric': 'minkowski', 'knn__n_neighbors...\n-4298.611714\n-3197.944286\n-3735.321059\n-4407.722340\n-4367.108381\n-4001.341556\n469.790238\n1\n\n\n5\n0.047902\n0.013181\n0.185623\n0.049865\neuclidean\n3\n1\ndistance\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4298.611714\n-3197.944286\n-3735.321059\n-4407.722340\n-4367.108381\n-4001.341556\n469.790238\n1\n\n\n7\n0.040595\n0.005817\n0.132290\n0.009807\neuclidean\n3\n2\ndistance\n{'knn__metric': 'euclidean', 'knn__n_neighbors...\n-4298.611714\n-3197.944286\n-3735.321059\n-4407.722340\n-4367.108381\n-4001.341556\n469.790238\n1\n\n\n51\n0.034996\n0.001900\n0.744842\n0.052065\nmanhattan\n5\n2\ndistance\n{'knn__metric': 'manhattan', 'knn__n_neighbors...\n-4090.438714\n-3258.873954\n-3680.152758\n-4846.570061\n-4192.419206\n-4013.690938\n531.510686\n4\n\n\n49\n0.031465\n0.004676\n0.718503\n0.057517\nmanhattan\n5\n1\ndistance\n{'knn__metric': 'manhattan', 'knn__n_neighbors...\n-4090.438714\n-3258.873954\n-3680.152758\n-4846.570061\n-4192.419206\n-4013.690938\n531.510686\n4\n\n\n\n\n\n\n\nThe results show that the next two best hyperparameter values yield the same performance as the printed one",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "KNN.html#hyperparameter-tuning",
    "href": "KNN.html#hyperparameter-tuning",
    "title": "2  KNN",
    "section": "2.4 Hyperparameter Tuning",
    "text": "2.4 Hyperparameter Tuning\nWe used GridSearchCV to tune the hyperparameters of our KNN model above. Given a relatively simple set of hyperparameters and a limited number of combinations, this approach was sufficient to reduce the RMSE.\nHowever, when the number of possible hyperparameter values grows large, GridSearchCV can become computationally expensive. In such cases, RandomizedSearchCV provides a more efficient alternative by sampling a fixed number of random combinations from the specified hyperparameter space. This makes it well-suited for scenarios with limited computational resources.\n\n2.4.0.1 RandomizedSearchCV\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n# Set up RandomizedSearchCV\n\n# Define parameter distributions for randomized search\nparam_distributions = {\n    'knn__n_neighbors': randint(1, 20),  # Random ints from 1 to 19\n    'knn__weights': ['uniform', 'distance'],\n    'knn__metric': ['euclidean', 'manhattan', 'minkowski'],\n    'knn__p': [1, 2]  # Only relevant for Minkowski\n}\n\n\n# Set up RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    estimator=pipeline,\n    param_distributions=param_distributions,\n    n_iter=30,  # Number of random combinations to try\n    cv=5,\n    scoring='neg_root_mean_squared_error',\n    n_jobs=-1,\n    random_state=42,\n    verbose=1\n)\n\n\n# Fit randomized search\nprint(\"Tuning KNN hyperparameters with RandomizedSearchCV...\")\nrandom_search.fit(X_train, y_train)\n\n# Best results\nbest_params = random_search.best_params_\nbest_score = -random_search.best_score_\n\n# Display results\nprint(\"\\nBest Parameters (RandomizedSearchCV):\")\nfor param, value in best_params.items():\n    print(f\"{param}: {value}\")\nprint(f\"Best CV RMSE: {best_score:.2f}\")\n\nTuning KNN hyperparameters with RandomizedSearchCV...\nFitting 5 folds for each of 30 candidates, totalling 150 fits\n\nBest Parameters (RandomizedSearchCV):\nknn__metric: manhattan\nknn__n_neighbors: 8\nknn__p: 2\nknn__weights: distance\nBest CV RMSE: 4005.70\n\n\n\n# Evaluate on test set\nbest_model = random_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\n# Calculate RMSE\ntest_rmse = root_mean_squared_error(y_test, y_pred)\nprint(f\"Test RMSE: {test_rmse:.2f}\")\n\nTest RMSE: 3811.89\n\n\nWhy might RandomizedSearchCV outperform GridSearchCV?\nAlthough GridSearchCV systematically evaluates all combinations of hyperparameter values from a predefined grid, it doesn’t guarantee the best performance. In some cases, RandomizedSearchCV can actually perform better. Here’s why:\n\nLimited Grid Resolution:\nGridSearchCV evaluates only the specific values you include in the grid. If the true optimal value lies between grid points, it may be missed entirely.\nBroader Exploration:\nRandomizedSearchCV samples from distributions (e.g., continuous or discrete ranges), allowing it to explore a wider range of hyperparameter values, including combinations not explicitly considered in a grid.\nIn this case,\n\nlist(range(1, 20, 2)) in GridSearchCV\nBut in RandomizedSearchCV, it samples from randint(1, 20)\n\nThe best n_neighbors happens to be 11, only RandomizedSearchCV can find it unless you explicitly included it in your grid.\nEfficiency in High Dimensions:\nIn high-dimensional search spaces, the number of combinations in a grid grows exponentially. RandomizedSearchCV remains efficient by sampling a fixed number of combinations, avoiding the “curse of dimensionality.”\nBetter Use of Time Budget:\nGiven the same computational budget, RandomizedSearchCV may cover more diverse regions of the search space and stumble upon better-performing configurations.\n\nIn summary, RandomizedSearchCV is not only faster but can also lead to better models—especially when the hyperparameter space is large, continuous, or contains irrelevant parameters.\n\n\n2.4.0.2 BayesSearchCV\nIn addition to these methods, BayesSearchCV, based on Bayesian optimization, provides a more intelligent approach to hyperparameter tuning. It models the performance landscape and selects hyperparameter combinations to evaluate based on past results, often requiring fewer evaluations to find optimal or near-optimal values. This makes BayesSearchCV a powerful option, especially when training models is costly.\n\n# Step 1: Install scikit-optimize if not already installed\n!pip install scikit-optimize\n\nCollecting scikit-optimize\n  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: joblib&gt;=0.11 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.4.2)\nCollecting pyaml&gt;=16.9 (from scikit-optimize)\n  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy&gt;=1.20.3 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.26.4)\nRequirement already satisfied: scipy&gt;=1.1.0 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.13.1)\nRequirement already satisfied: scikit-learn&gt;=1.0.0 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.6.1)\nRequirement already satisfied: packaging&gt;=21.3 in c:\\users\\lsi8012\\appdata\\roaming\\python\\python312\\site-packages (from scikit-optimize) (24.2)\nRequirement already satisfied: PyYAML in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from pyaml&gt;=16.9-&gt;scikit-optimize) (6.0.1)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn&gt;=1.0.0-&gt;scikit-optimize) (3.5.0)\nDownloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n   ---------------------------------------- 0.0/107.8 kB ? eta -:--:--\n   ------------------------------------- -- 102.4/107.8 kB 5.8 MB/s eta 0:00:01\n   ---------------------------------------- 107.8/107.8 kB 3.1 MB/s eta 0:00:00\nDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\nInstalling collected packages: pyaml, scikit-optimize\nSuccessfully installed pyaml-25.1.0 scikit-optimize-0.10.2\n\n\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Integer, Categorical\n\n\n# Step 3: Define search space for Bayesian optimization\nsearch_space = {\n    'knn__n_neighbors': Integer(1, 19),  # Odd values will be sampled if needed\n    'knn__weights': Categorical(['uniform', 'distance']),\n    'knn__metric': Categorical(['euclidean', 'manhattan', 'minkowski']),\n    'knn__p': Integer(1, 2)  # Used only when metric is minkowski\n}\n\n\n# Step 4: Set up BayesSearchCV\nbayes_search = BayesSearchCV(\n    estimator=pipeline,\n    search_spaces=search_space,\n    n_iter=30,  # Number of different combinations to try\n    scoring='neg_root_mean_squared_error',\n    cv=5,\n    n_jobs=-1,\n    verbose=1,\n    random_state=42\n)\n\n\n# Step 5: Fit BayesSearchCV\nprint(\"Tuning KNN hyperparameters with Bayesian Optimization...\")\nbayes_search.fit(X_train, y_train)\n\n# Get best parameters and best score\nbest_params = bayes_search.best_params_\nbest_score = -bayes_search.best_score_  # Convert negative RMSE to positive\n\n# Display results\nprint(\"\\nBest Parameters (Bayesian Optimization):\")\nfor param, value in best_params.items():\n    print(f\"{param}: {value}\")\nprint(f\"Best CV RMSE: {best_score:.2f}\")\n\nTuning KNN hyperparameters with Bayesian Optimization...\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 2, 'distance'] before, using random point ['euclidean', 12, 1, 'distance']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 2, 'distance'] before, using random point ['euclidean', 9, 2, 'distance']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 2, 'distance'] before, using random point ['manhattan', 5, 1, 'distance']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 1, 'distance'] before, using random point ['manhattan', 4, 2, 'uniform']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 1, 'distance'] before, using random point ['euclidean', 6, 1, 'uniform']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 1, 'distance'] before, using random point ['euclidean', 13, 1, 'distance']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 1, 'distance'] before, using random point ['euclidean', 6, 1, 'uniform']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['manhattan', 5, 1, 'distance'] before, using random point ['manhattan', 2, 2, 'uniform']\n  warnings.warn(\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n\nBest Parameters (Bayesian Optimization):\nknn__metric: manhattan\nknn__n_neighbors: 8\nknn__p: 2\nknn__weights: distance\nBest CV RMSE: 4005.70\n\n\n\n# Step 6: Evaluate on test set\nbest_model = bayes_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\n# Calculate RMSE on test set\ntest_rmse = root_mean_squared_error(y_test, y_pred)\nprint(f\"Test RMSE: {test_rmse:.2f}\")\n\nTest RMSE: 3811.89",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html",
    "href": "Hyperparameter tuning.html",
    "title": "3  Hyperparameter tuning",
    "section": "",
    "text": "3.1 GridSearchCV\nIn this chapter we’ll introduce several functions that help with tuning hyperparameters of a machine learning model.\nLet us read and pre-process data first. Then we’ll be ready to tune the model hyperparameters. We’ll use KNN as the model. Note that KNN has multiple hyperparameters to tune, such as number of neighbors, distance metric, weights of neighbours, etc.\nThe function is used to compute the cross-validated score (MSE, RMSE, accuracy, etc.) over a grid of hyperparameter values. This helps avoid nested for() loops if multiple hyperparameter values need to be tuned.\n# GridSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor() # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n    # the keys should be EXACTLY the same as the names of the model inputs\n    # the values should be an array or list of hyperparam values you want to try out\n    \n# 30 K values x 2 weight settings x 3 metric settings = 180 different combinations in this grid\ngrid = {'n_neighbors': np.arange(5, 151, 5), 'weights':['uniform', 'distance'], \n        'metric': ['manhattan', 'euclidean', 'chebyshev']}\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, use it if you \n# have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within the function\ngcv = GridSearchCV(model, grid, cv = kfold, scoring = 'neg_root_mean_squared_error', n_jobs = -1, verbose = 10)\n\n# Fit the models, and cross-validate\ngcv.fit(X_train_scaled, y_train)\n\nFitting 5 folds for each of 180 candidates, totalling 900 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(), n_jobs=-1,\n             param_grid={'metric': ['manhattan', 'euclidean', 'chebyshev'],\n                         'n_neighbors': array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n        70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125, 130,\n       135, 140, 145, 150]),\n                         'weights': ['uniform', 'distance']},\n             scoring='neg_root_mean_squared_error', verbose=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(), n_jobs=-1,\n             param_grid={'metric': ['manhattan', 'euclidean', 'chebyshev'],\n                         'n_neighbors': array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n        70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125, 130,\n       135, 140, 145, 150]),\n                         'weights': ['uniform', 'distance']},\n             scoring='neg_root_mean_squared_error', verbose=10)estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()\nThe optimal estimator based on cross-validation is:\ngcv.best_estimator_\n\nKNeighborsRegressor(metric='manhattan', n_neighbors=10, weights='distance')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor(metric='manhattan', n_neighbors=10, weights='distance')\nThe optimal hyperparameter values (based on those considered in the grid search) are:\ngcv.best_params_\n\n{'metric': 'manhattan', 'n_neighbors': 10, 'weights': 'distance'}\nThe cross-validated root mean squared error for the optimal hyperparameter values is:\n-gcv.best_score_\n\n5740.928686723918\nThe RMSE on test data for the optimal hyperparameter values is:\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n5747.466851437544\nNote that the error is further reduced as compared to the case when we tuned only one hyperparameter in the previous chatper. We must tune all the hyperparameters that can effect prediction accuracy, in order to get the most accurate model.\nThe results for each cross-validation are stored in the cv_results_ attribute.\npd.DataFrame(gcv.cv_results_).head()\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_metric\nparam_n_neighbors\nparam_weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.011169\n0.005060\n0.011768\n0.001716\nmanhattan\n5\nuniform\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6781.316742\n-5997.969637\n-6726.786770\n-6488.191029\n-6168.502006\n-6432.553237\n306.558600\n19\n\n\n1\n0.009175\n0.001934\n0.009973\n0.000631\nmanhattan\n5\ndistance\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6449.449369\n-5502.975790\n-6306.888303\n-5780.902979\n-5365.980081\n-5881.239304\n429.577113\n3\n\n\n2\n0.008976\n0.001092\n0.012168\n0.001323\nmanhattan\n10\nuniform\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6668.299079\n-6116.693116\n-6387.505084\n-6564.727623\n-6219.094608\n-6391.263902\n205.856097\n16\n\n\n3\n0.007979\n0.000001\n0.011970\n0.000892\nmanhattan\n10\ndistance\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6331.374493\n-5326.304310\n-5787.179591\n-5809.777811\n-5450.007229\n-5740.928687\n349.872624\n1\n\n\n4\n0.006781\n0.000748\n0.012367\n0.001017\nmanhattan\n15\nuniform\n{'metric': 'manhattan', 'n_neighbors': 15, 'we...\n-6871.063499\n-6412.214411\n-6544.343677\n-7008.348770\n-6488.345118\n-6664.863095\n232.385843\n33\nThese results can be useful to see if other hyperparameter values are almost equally good.\nFor example, the next two best optimal values of the hyperparameter correspond to neighbors being 15 and 5 respectively. As the test error has a high variance, the best hyperparameter values need not necessarily be actually optimal.\npd.DataFrame(gcv.cv_results_).sort_values(by = 'rank_test_score').head()\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_metric\nparam_n_neighbors\nparam_weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n3\n0.007979\n0.000001\n0.011970\n0.000892\nmanhattan\n10\ndistance\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6331.374493\n-5326.304310\n-5787.179591\n-5809.777811\n-5450.007229\n-5740.928687\n349.872624\n1\n\n\n5\n0.009374\n0.004829\n0.013564\n0.001850\nmanhattan\n15\ndistance\n{'metric': 'manhattan', 'n_neighbors': 15, 'we...\n-6384.403268\n-5427.978762\n-5742.606651\n-6041.135255\n-5563.240077\n-5831.872803\n344.192700\n2\n\n\n1\n0.009175\n0.001934\n0.009973\n0.000631\nmanhattan\n5\ndistance\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6449.449369\n-5502.975790\n-6306.888303\n-5780.902979\n-5365.980081\n-5881.239304\n429.577113\n3\n\n\n7\n0.007977\n0.001092\n0.017553\n0.002054\nmanhattan\n20\ndistance\n{'metric': 'manhattan', 'n_neighbors': 20, 'we...\n-6527.825519\n-5534.609170\n-5860.837805\n-6100.919269\n-5679.403544\n-5940.719061\n349.270714\n4\n\n\n9\n0.007777\n0.000748\n0.019349\n0.003374\nmanhattan\n25\ndistance\n{'metric': 'manhattan', 'n_neighbors': 25, 'we...\n-6620.272336\n-5620.462675\n-5976.406911\n-6181.847891\n-5786.081991\n-6037.014361\n346.791650\n5\nLet us compute the RMSE on test data based on the 2nd and 3rd best hyperparameter values.\nmodel = KNeighborsRegressor(n_neighbors=15, metric='manhattan', weights='distance').fit(X_train_scaled, y_train)\nmean_squared_error(model.predict(X_test_scaled), y_test, squared = False)\n\n5800.418957612656\nmodel = KNeighborsRegressor(n_neighbors=5, metric='manhattan', weights='distance').fit(X_train_scaled, y_train)\nmean_squared_error(model.predict(X_test_scaled), y_test, squared = False)\n\n5722.4859230146685\nWe can see that the RMSE corresponding to the 3rd best hyperparameter value is the least. Due to variance in test errors, it may be a good idea to consider the set of top few best hyperparameter values, instead of just considering the best one.",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#randomizedsearchcv",
    "href": "Hyperparameter tuning.html#randomizedsearchcv",
    "title": "3  Hyperparameter tuning",
    "section": "3.2 RandomizedSearchCV()",
    "text": "3.2 RandomizedSearchCV()\nIn case of many possible values of hyperparameters, it may be comptaionally very expensive to use GridSearchCV(). In such cases, RandomizedSearchCV() can be used to compute the cross-validated score on a randomly selected subset of hyperparameter values from the specified grid. The number of values can be fixed by the user, as per the available budget.\n\n# RandomizedSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor() # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n    # the keys should be EXACTLY the same as the names of the model inputs\n    # the values should be an array or list of hyperparam values, or distribution of hyperparameter values\n    \n    \ngrid = {'n_neighbors': range(1, 500), 'weights':['uniform', 'distance'], \n        'metric': ['minkowski'], 'p': uniform(loc=1, scale=10)} #We can specify a distribution \n                                                                #for continuous hyperparameter values\n\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, use it if you \n# have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within the function\ngcv = RandomizedSearchCV(model, param_distributions = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1, verbose = 10)\n\n# Fit the models, and cross-validate\ngcv.fit(X_train_scaled, y_train)\n\nFitting 5 folds for each of 180 candidates, totalling 900 fits\n\n\nRandomizedSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n                   estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n                   param_distributions={'metric': ['minkowski'],\n                                        'n_neighbors': range(1, 500),\n                                        'p': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000226D6E70700&gt;,\n                                        'weights': ['uniform', 'distance']},\n                   random_state=10, scoring='neg_root_mean_squared_error',\n                   verbose=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n                   estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n                   param_distributions={'metric': ['minkowski'],\n                                        'n_neighbors': range(1, 500),\n                                        'p': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000226D6E70700&gt;,\n                                        'weights': ['uniform', 'distance']},\n                   random_state=10, scoring='neg_root_mean_squared_error',\n                   verbose=10)estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()\n\n\n\ngcv.best_params_\n\n{'metric': 'minkowski',\n 'n_neighbors': 3,\n 'p': 1.252639454318171,\n 'weights': 'uniform'}\n\n\n\ngcv.best_score_\n\n-6239.171627183809\n\n\n\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n6176.533397589911\n\n\nNote that in this example, RandomizedSearchCV() helps search for optimal values of the hyperparameter \\(p\\) over a continuous domain space. In this dataset, \\(p = 1\\) seems to be the optimal value. However, if the optimal value was somewhere in the middle of a larger continuous domain space (instead of the boundary of the domain space), and there were several other hyperparameters, some of which were not influencing the response (effect sparsity), RandomizedSearchCV() is likely to be more effective in estimating the optimal value of the continuous hyperparameter.\nThe advantages of RandomizedSearchCV() over GridSearchCV() are:\n\nRandomizedSearchCV() fixes the computational cost in case of large number of hyperparameters / large number of levels of individual hyperparameters. If there are \\(n\\) hyper parameters, each with 3 levels, the number of all possible hyperparameter values will be \\(3^n\\). The computational cost increase exponentially with increase in number of hyperparameters.\nIn case of a hyperparameter having continuous values, the distribution of the hyperparameter can be specified in RandomizedSearchCV().\nIn case of effect sparsity of hyperparameters, i.e., if only a few hyperparameters significantly effect prediction accuracy, RandomizedSearchCV() is likely to consider more unique values of the influential hyperparameters as compared to GridSearchCV(), and is thus likely to provide more optimal hyperparameter values as compared to GridSearchCV(). The figure below shows effect sparsity where there are 2 hyperparameters, but only one of them is associated with the cross-validated score, Here, it is more likely that the optimal cross-validated score will be obtained by RandomizedSearchCV(), as it is evaluating the model on 9 unique values of the relevant hyperparameter, instead of just 3.",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#bayessearchcv",
    "href": "Hyperparameter tuning.html#bayessearchcv",
    "title": "3  Hyperparameter tuning",
    "section": "3.3 BayesSearchCV()",
    "text": "3.3 BayesSearchCV()\nUnlike the grid search and random search, which treat hyperparameter sets independently, the Bayesian optimization is an informed search method, meaning that it learns from previous iterations. The number of trials in this approach is determined by the user.\n\nThe function begins by computing the cross-validated score by randomly selecting a few hyperparameter values from the specified disttribution of hyperparameter values.\nBased on the data of hyperparameter values tested (predictors), and the cross-validated score (the response), a Gaussian process model is developed to estimate the cross-validated score & the uncertainty in the estimate in the entire space of the hyperparameter values\nA criterion that “explores” uncertain regions of the space of hyperparameter values (where it is difficult to predict cross-validated score), and “exploits” promising regions of the space are of hyperparameter values (where the cross-validated score is predicted to minimize) is used to suggest the next hyperparameter value that will potentially minimize the cross-validated score\nCross-validated score is computed at the suggested hyperparameter value, the Gaussian process model is updated, and the previous step is repeated, until a certain number of iterations specified by the user.\n\nTo summarize, instead of blindly testing the model for the specified hyperparameter values (as in GridSearchCV()), or randomly testing the model on certain hyperparameter values (as in RandomizedSearchCV()), BayesSearchCV() smartly tests the model for those hyperparameter values that are likely to reduce the cross-validated score. The algorithm becomes “smarter” as it “learns” more with increasing iterations.\nHere is a nice blog, if you wish to understand more about the Bayesian optimization procedure.\n\n# BayesSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor(metric = 'minkowski') # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n# the keys should be EXACTLY the same as the names of the model inputs\n# the values should be the distribution of hyperparameter values. Lists and NumPy arrays can\n# also be used\n    \ngrid = {'n_neighbors': Integer(1, 500), 'weights': Categorical(['uniform', 'distance']), \n       'p': Real(1, 10, prior = 'uniform')} \n\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, \n# use it if you have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within \n# the function\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n\n# Fit the models, and cross-validate\n\n# Sometimes the Gaussian process model predicting the cross-validated score suggests a \n# \"promising point\" (i.e., set of hyperparameter values) for cross-validation that it has \n# already suggested earlier. In such  a case a warning is raised, and the objective \n# function (i.e., the cross-validation score) is computed at a randomly selected point \n# (as in RandomizedSearchCV()). This feature helps the algorithm explore other regions of\n# the hyperparameter space, rather than only searching in the promising regions. Thus, it \n# balances exploration (of the hyperparameter space) with exploitation (of the promising \n# regions of the hyperparameter space)\n\nwarnings.filterwarnings(\"ignore\")\ngcv.fit(X_train_scaled, y_train)\nwarnings.resetwarnings()\n\nThe optimal hyperparameter values (based on Bayesian search) on the provided distribution of hyperparameter values are:\n\ngcv.best_params_\n\nOrderedDict([('n_neighbors', 9),\n             ('p', 1.0008321732366932),\n             ('weights', 'distance')])\n\n\nThe cross-validated root mean squared error for the optimal hyperparameter values is:\n\n-gcv.best_score_\n\n5756.172382596493\n\n\nThe RMSE on test data for the optimal hyperparameter values is:\n\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n5740.432278861367\n\n\n\n3.3.1 Diagonosis of cross-validated score optimization\nBelow are the partial dependence plots of the objective function (i.e., the cross-validated score). The cross-validated score predictions are based on the most recently updated model (i.e., the updated Gaussian Process model at the end of n_iter iterations specified by the user) that predicts the cross-validated score.\nCheck the plot_objective() documentation to interpret the plots.\n\nplot_objective(gcv.optimizer_results_[0],\n                   dimensions=[\"n_neighbors\", \"p\", \"weights\"], size = 3)\nplt.show();\n\n\n\n\n\n\n\n\nThe frequence of individual hyperparameter values considered can also be visualized as below.\n\nfig, ax = plt.subplots(1, 3, figsize = (10, 3))\nplt.subplots_adjust(wspace=0.4)\nplot_histogram(gcv.optimizer_results_[0], 0, ax = ax[0])\nplot_histogram(gcv.optimizer_results_[0], 1, ax = ax[1])\nplot_histogram(gcv.optimizer_results_[0], 2, ax = ax[2])\nplt.show()\n\n\n\n\n\n\n\n\nBelow is the plot showing the minimum cross-validated score computed obtained until ‘n’ hyperparameter values are considered for cross-validation.\n\nplot_convergence(gcv.optimizer_results_)\nplt.show()\n\n\n\n\n\n\n\n\nNote that the cross-validated error is close to the optmial value in the 53rd iteration itself.\nThe cross-validated error at the 53rd iteration is:\n\ngcv.optimizer_results_[0]['func_vals'][53]\n\n5831.87280274334\n\n\nThe hyperparameter values at the 53rd iterations are:\n\ngcv.optimizer_results_[0]['x_iters'][53]\n\n[15, 1.0, 'distance']\n\n\nNote that this is the 2nd most optimal hyperparameter value based on GridSearchCV().\nBelow is the plot showing the cross-validated score computed at each of the 180 hyperparameter values considered for cross-validation. The plot shows that the algorithm seems to explore new regions of the domain space, instead of just exploting the promising ones. There is a balance between exploration and exploitation for finding the optimal hyperparameter values that minimize the objective function (i.e., the function that models the cross-validated score).\n\nsns.lineplot(x = range(1, 181), y = gcv.optimizer_results_[0]['func_vals'])\nplt.xlabel('Iteration')\nplt.ylabel('Cross-validated score')\nplt.show();\n\n\n\n\n\n\n\n\nThe advantages of BayesSearchCV() over GridSearchCV() and RandomizedSearchCV() are:\n\nThe Bayesian Optimization approach gives the benefit that we can give a much larger range of possible values, since over time we identify and exploit the most promising regions and discard the not so promising ones. Plain grid-search would burn computational resources to explore all regions of the domain space with the same granularity, even the not promising ones. Since we search much more effectively in Bayesian search, we can search over a larger domain space.\nBayesSearch CV may help us identify the optimal hyperparameter value in fewer iterations if the Gaussian process model estimating the cross-validated score is relatively accurate. However, this is not certain. Grid and random search are completely uninformed by past evaluations, and as a result, often spend a significant amount of time evaluating “bad” hyperparameters.\nBayesSearch CV is more reliable in cases of a large search space, where random selection may miss sampling values from optimal regions of the search space.\n\nThe disadvantages of BayesSearchCV() over GridSearchCV() and RandomizedSearchCV() are:\n\nBayesSearchCV() has a cost of learning from past data, i.e., updating the model that predicts the cross-validated score after every iteration of evaluating the cross-validated score on a new hyperparameter value. This cost will continue to increase as more and more data is collected. There is no such cost in GridSearchCV() and RandomizedSearchCV() as there is no learning. This implies that each iteration of BayesSearchCV() will take a longer time than each iteration of GridSearchCV() / RandomizedSearchCV(). Thus, even if BayesSearchCV() finds the optimal hyperparameter value in fewer iterations, it may take more time than GridSearchCV() / RandomizedSearchCV() for the same.\nThe success of BayesSearchCV() depends on the predictions and associated uncertainty estimated by the Gaussian process (GP) model that predicts the cross-validated score. The GP model, although works well in general, may not be suitable for certain datasets, or may take a relatively large number of iterations to learn for certain datasets.\n\n\n\n3.3.2 Live monitoring of cross-validated score\nNote that it will be useful monitor the cross-validated score while the Bayesian Search CV code is running, and stop the code as soon as the desired accuracy is reached, or the optimal cross-validated score doesn’t seem to improve. The fit() method of the BayesSeaerchCV() object has a callback argument that can be used as follows:\n\nmodel = KNeighborsRegressor(metric = 'minkowski') # No inputs defined inside the model\ngrid = {'n_neighbors': Integer(1, 500), 'weights': Categorical(['uniform', 'distance']), \n       'p': Real(1, 10, prior = 'uniform')} \n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n\n\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\n\n\ngcv.fit(X_train_scaled, y_train, callback = monitor)\n\n['n_neighbors', 'p', 'weights'] = [9, 1.0008321732366932, 'distance'] 5756.172382596493\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'n_neighbors': Integer(low=1, high=500, prior='uniform', transform='normalize'),\n                             'p': Real(low=1, high=10, prior='uniform', transform='normalize'),\n                             'weights': Categorical(categories=('uniform', 'distance'), prior=None)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'n_neighbors': Integer(low=1, high=500, prior='uniform', transform='normalize'),\n                             'p': Real(low=1, high=10, prior='uniform', transform='normalize'),\n                             'weights': Categorical(categories=('uniform', 'distance'), prior=None)})estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#cross_validate",
    "href": "Hyperparameter tuning.html#cross_validate",
    "title": "3  Hyperparameter tuning",
    "section": "3.4 cross_validate()",
    "text": "3.4 cross_validate()\nWe have used cross_val_score() and cross_val_predict() so far.\nWhen can we use one over the other?\nThe function cross_validate() is similar to cross_val_score() except that it has the option to return multiple cross-validated metrics, instead of a single one.\nConsider the heart disease classification problem, where the response is target (whether the person has a heart disease or not).\n\ndata = pd.read_csv('Datasets/heart_disease_classification.csv')\ndata.head()\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\nLet us pre-process the data.\n\n# First, separate the response and the predictors\ny = data['target']\nX = data.drop('target', axis=1)\n\n\n# Separate the data (X,y) into training and test\n\n# Inputs:\n    # data\n    # train-test ratio\n    # random_state for reproducible code\n    \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20, stratify=y) # 80%-20% split\n\n# stratify=y makes sure the class 0 to class 1 ratio in the training and test sets are kept the same as the entire dataset.\n\n\nmodel = KNeighborsClassifier() \nsc = StandardScaler()\nsc.fit(X_train)\nX_train_scaled = sc.transform(X_train)\nX_test_scaled = sc.transform(X_test)\n\nSuppose we want to take recall above a certain threshold with the highest precision possible. cross_validate() computes the cross-validated score for multiple metrics - rest is the same as cross_val_score().\n\nKs = np.arange(10,200,10)\n\nscores = []\n\nfor K in Ks:\n    model = KNeighborsClassifier(n_neighbors=K) # Keeping distance uniform\n    scores.append(cross_validate(model, X_train_scaled, y_train, cv=5, scoring = ['accuracy','recall', 'precision']))\n\n\nscores\n\n# The output is now a list of dicts - easy to convert to a df\n\ndf_scores = pd.DataFrame(scores) # We need to handle test_recall and test_precision cols\n\ndf_scores['CV_recall'] = df_scores['test_recall'].apply(np.mean)\ndf_scores['CV_precision'] = df_scores['test_precision'].apply(np.mean)\ndf_scores['CV_accuracy'] = df_scores['test_accuracy'].apply(np.mean)\n\ndf_scores.index = Ks # We can set K values as indices for convenience\n\n\n#df_scores\n# What happens as K increases?\n    # Recall increases (not monotonically)\n    # Precision decreases (not monotonically)\n# Why?\n    # Check the class distribution in the data - more obs with class 1\n    # As K gets higher, the majority class overrules (visualized in the slides)\n    # More 1s means less FNs - higher recall\n    # More 1s means more FPs - lower precision\n# Would this be the case for any dataset?\n    # NO!! Depends on what the majority class is!\n\nSuppose we wish to have the maximum possible precision for at least 95% recall.\nThe optimal K will be:\n\ndf_scores.loc[df_scores['CV_recall'] &gt; 0.95, 'CV_precision'].idxmax()\n\n120\n\n\nThe cross-validated precision, recall and accuracy for the optimal K are:\n\ndf_scores.loc[120, ['CV_recall', 'CV_precision', 'CV_accuracy']]\n\nCV_recall       0.954701\nCV_precision    0.734607\nCV_accuracy     0.785374\nName: 120, dtype: object\n\n\n\nsns.lineplot(x = df_scores.index, y = df_scores.CV_precision, color = 'blue', label = 'precision')\nsns.lineplot(x = df_scores.index, y = df_scores.CV_recall, color = 'red', label = 'recall')\nsns.lineplot(x = df_scores.index, y = df_scores.CV_accuracy, color = 'green', label = 'accuracy')\nplt.ylabel('Metric')\nplt.xlabel('K')\nplt.show()",
    "crumbs": [
      "Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "regression_tree_sp25.html",
    "href": "regression_tree_sp25.html",
    "title": "4  Regression trees",
    "section": "",
    "text": "4.1 Native Support for Missing Values\nRead section 8.1.1 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nWe will use the same dataset as in the KNN model for regression trees.\nStarting with scikit-learn version 1.3, classical tree-based models in scikit-learn have added native support for missing values, which simplifies preprocessing and improves model robustness:\nThis means you no longer need to impute missing values manually before training these models.\nTo take advantage of this feature, first check your scikit-learn version:\nimport sklearn\nprint(sklearn.__version__)\n\n1.6.1\nIf your version is below 1.4.0, you can upgrade by running:\n# pip install --upgrade scikit-learn\n# Make a copy of the original dataset\ncar_missing = car.copy()\n\n# Randomly add missing values\n# Inject missing values into 10% of the 'mileage' column\ncar_missing.loc[car_missing.sample(frac=0.1, random_state=42).index, 'mileage'] = np.nan\n# Inject missing values into 10% of the 'fuelType' and 'engineSize' columns\n\ncar_missing.loc[car_missing.sample(frac=0.1, random_state=42).index, 'fuelType'] = np.nan\ncar_missing.loc[car_missing.sample(frac=0.1, random_state=42).index, 'engineSize'] = np.nan\ncar_missing.isna().sum()\n\nbrand             0\nmodel             0\nyear              0\ntransmission      0\nmileage         763\nfuelType        763\ntax               0\nmpg               0\nengineSize      763\nprice             0\ndtype: int64\n# Split the car_missing dataset into features and target\nX_missing = car_missing.drop(columns=['price'])\ny_missing = car_missing['price']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "regression_tree_sp25.html#native-support-for-missing-values",
    "href": "regression_tree_sp25.html#native-support-for-missing-values",
    "title": "4  Regression trees",
    "section": "",
    "text": "DecisionTreeClassifier supports missing values as of version 1.3.0\n\nRandomForestClassifier adds support in version 1.4.0\n\n\n\n\n\n\n\n\n\n\n4.1.1 Build a regression tree using mileage as the solo predictor\n\n# Use only 'mileage' as the feature\nX_mileage = X_missing[['mileage']]\ny_mileage = y_missing\n\n\n# Create a DecisionTreeRegressor model\nreg_tree = DecisionTreeRegressor(random_state=42)\n\n# Fit the model to the data\nreg_tree.fit(X_mileage, y_mileage)\n\nDecisionTreeRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(random_state=42) \n\n\n\n# Predict the target variable using the model\ny_pred = reg_tree.predict(X_mileage)\n\n# Calculate the RMSE and R² score\nrmse = np.sqrt(np.mean((y_missing - y_pred) ** 2))\nr2 = r2_score(y_missing, y_pred)\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"R²: {r2:.2f}\")\n\nRMSE: 8797.85\nR²: 0.71",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "regression_tree_sp25.html#building-regression-trees",
    "href": "regression_tree_sp25.html#building-regression-trees",
    "title": "4  Regression trees",
    "section": "4.2 Building regression trees",
    "text": "4.2 Building regression trees\n\nX = car.drop(columns=['price'])\ny = car['price']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n4.2.1 Using only mileage feature\n\n# Use only 'mileage' as the feature\nX_train_mileage = X_train[['mileage']]\nX_test_mileage = X_test[['mileage']]\n\n# Create a DecisionTreeRegressor model\nreg_tree = DecisionTreeRegressor(random_state=42, max_depth=3)\n\n# Fit the model to the training data\nreg_tree.fit(X_train_mileage, y_train)\n\n# Predict the target variable using the model\ny_pred = reg_tree.predict(X_test_mileage)\n\n# Calculate the RMSE and R² score\nrmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\nr2 = r2_score(y_test, y_pred)\nprint(f\"RMSE using only mileage predictor: {rmse:.2f}\")\nprint(f\"R² using only mileage predictor: {r2:.2f}\")\n\nRMSE using only mileage predictor: 14437.80\nR² using only mileage predictor: 0.29\n\n\nLet’s visualize the tree structure\n\n# Plot the tree\nplt.figure(figsize=(18, 6))\nplot_tree(reg_tree, feature_names=['mileage'], filled=True, rounded=True)\nplt.title(\"Regression Tree Using Mileage\")\nplt.show()\n\n\n\n\n\n\n\n\nLet’s visualize how mileage is used in the decision tree below:\n\n# Create evenly spaced mileage values within the range of training data\nXtest = np.linspace(X_train_mileage['mileage'].min(), X_train_mileage['mileage'].max(), 100).reshape(-1, 1)\n\n# Convert Xtest to a DataFrame with the correct column name\nXtest_df = pd.DataFrame(Xtest, columns=['mileage'])\n\n# Predict using the DataFrame instead of NumPy array\nytest_pred = reg_tree.predict(Xtest_df)\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=X_train_mileage['mileage'], y=y_train, color='orange', label='Training data')\n\n# Step plot to reflect piecewise constant predictions\nplt.step(Xtest_df['mileage'], ytest_pred, color='blue', label='Tree prediction', where='mid')\n\nplt.xlabel(\"Mileage\")\nplt.ylabel(\"Price\")\nplt.title(\"Decision Tree Regression: Mileage vs. Price\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAll cars falling within the same terminal node have the same predicted price, which is seen as flat line segments in the above model curve.\n\n\n4.2.2 Using mileage and brand as predictors\n\nX_train.head() \n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\n\n\n\n\n216\nvw\nScirocco\n2016\nManual\n41167\nDiesel\n20\n55.2654\n2.0\n\n\n4381\nmerc\nCLS Class\n2018\nSemi-Auto\n12078\nDiesel\n145\n47.7624\n2.9\n\n\n6891\nhyundi\nSanta Fe\n2019\nAutomatic\n623\nDiesel\n145\n43.0887\n2.2\n\n\n421\nhyundi\nIX35\n2014\nManual\n37095\nDiesel\n145\n53.4862\n1.7\n\n\n505\nford\nEdge\n2016\nSemi-Auto\n15727\nDiesel\n160\n49.0741\n2.0\n\n\n\n\n\n\n\n\n# Select features and target\nX_train_tree = X_train[['mileage', 'brand']]\n\nX_test_tree = X_test[['mileage', 'brand']]\n\n\n# One-hot encode the categorical variable 'brand'\nX_train_tree_encoded = pd.get_dummies(X_train_tree, columns=['brand'])\nX_test_tree_encoded = pd.get_dummies(X_test_tree, columns=['brand'])\n\n\nmodel = DecisionTreeRegressor(max_depth=3, random_state=42)\nmodel.fit(X_train_tree_encoded, y_train)\n\nDecisionTreeRegressor(max_depth=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(max_depth=3, random_state=42) \n\n\n\nplt.figure(figsize=(12, 6))\nplot_tree(model, feature_names=X_train_tree_encoded.columns, filled=True, rounded=True)\nplt.title(\"Decision Tree Using Mileage and Brand to Predict Price\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Predict the target variable using the model\ny_pred_tree = model.predict(X_test_tree_encoded)\n\n# Calculate the RMSE and R² score\nrmse_tree = np.sqrt(np.mean((y_test - y_pred_tree) ** 2))\nr2_tree = r2_score(y_test, y_pred_tree)\nprint(f\"RMSE using mileage and brand predictor: {rmse_tree:.2f}\")\nprint(f\"R² using mileage and brand predictor: {r2_tree:.2f}\")\n\n# Compare the performance of the two models\nprint(f\"RMSE using only mileage predictor: {rmse:.2f}\")\nprint(f\"RMSE using mileage and brand predictor: {rmse_tree:.2f}\")\n# The RMSE using mileage and brand predictor is lower than using only mileage predictor.\n# This indicates that adding the brand feature improves the model's performance\n\nRMSE using mileage and brand predictor: 12531.44\nR² using mileage and brand predictor: 0.46\nRMSE using only mileage predictor: 14437.80\nRMSE using mileage and brand predictor: 12531.44\n\n\n\n\n4.2.3 Using all predictors\nNow that we’ve explored a single predictor (mileage) and added a second predictor (brand), let’s take it a step further and use all available features to build a more robust model.\nWe’ll construct a pipeline that handles necessary preprocessing steps (e.g., categorical encoding) and fits a Decision Tree Regressor in a streamlined and reproducible way.\n\n# extract the categorical columns and put them in a list\ncategorical_feature = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumerical_feature = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n\n# Create a ColumnTransformer to handle encoding\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_feature)\n    ],\n    remainder='passthrough',  # Keep numerical feature (mileage) unchanged\n    force_int_remainder_cols=False\n)\n\n# Create the pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', DecisionTreeRegressor(max_depth=4, random_state=42))\n])\n\n# Usage:\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n\n# Calculate the RMSE and R² score\nrmse_pipeline = np.sqrt(np.mean((y_test - y_pred) ** 2))\nr2_pipeline = r2_score(y_test, y_pred)\nprint(f\"RMSE using pipeline: {rmse_pipeline:.2f}\")\nprint(f\"R² using pipeline: {r2_pipeline:.2f}\")\n\nRMSE using pipeline: 8186.34\nR² using pipeline: 0.77\n\n\n\n# let's visuzalize the decision tree\n\nplt.figure(figsize=(12, 6))\nplot_tree(pipeline.named_steps['regressor'], feature_names=pipeline.named_steps['preprocessor'].get_feature_names_out(), filled=True, rounded=True)\nplt.title(\"Decision Tree Using Pipeline\")\nplt.show()",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "regression_tree_sp25.html#key-hyperparameters-in-decision-tree",
    "href": "regression_tree_sp25.html#key-hyperparameters-in-decision-tree",
    "title": "4  Regression trees",
    "section": "4.3 Key Hyperparameters in Decision Tree",
    "text": "4.3 Key Hyperparameters in Decision Tree\nIn regression trees, model complexity is controlled by hyperparameters, tuning them is crucial for balancing underfitting and overfitting.\n\n4.3.1 Underfitting\n\nThe model is too simple to capture patterns in the data.\nHigh bias, low variance.\nOften caused by:\n\nShallow trees (max_depth is too small)\nToo strict constraints (min_samples_split or min_samples_leaf is too high)\n\n\n\n\n4.3.2 Overfitting\n\nThe model is too complex and learns noise from the training data.\nLow bias, high variance.\nOften caused by:\n\nDeep trees with many splits\nVery small min_samples_leaf or min_samples_split\n\n\nBelow are the most commonly used hyperparameters:\n\n\n4.3.3 max_depth\n\nControls the maximum depth of the tree.\nIf None, the tree will expand until all leaves are pure or contain fewer than min_samples_split samples.\nControls overfitting (deep trees → overfit, shallow trees → underfit)\nTypical values: 3 to 20 (start with lower values).\n\n\n\n4.3.4 min_samples_split\n\nThe minimum number of samples required to split an internal node.\nhigher values → simpler trees (reducing overfitting)\n\n\n\n4.3.5 min_samples_leaf\n\nThe minimum number of samples required to be at a leaf node.\nSetting this to a higher number can smooth the model by reducing variance.\n\n\n\n4.3.6 max_features\n\nNumber of features to consider when looking for the best split.\nCan be set to:\n\n\"auto\" or None: use all features\n\"sqrt\": use the square root of the number of features\n\"log2\": use log base 2\n\n\n\n# Define your parameter grid with pipeline step prefix\nparam_grid = {\n    'regressor__max_depth': [3, 5, 7, 10, None],\n    'regressor__min_samples_split': [2, 5, 10],\n    'regressor__min_samples_leaf': [1, 2, 4],\n    'regressor__max_features': ['sqrt', None]\n}\n\n# Create custom scorer for RMSE\nrmse_scorer = make_scorer(lambda y_true, y_pred: root_mean_squared_error(y_true, y_pred),\n                          greater_is_better=False)\n# Create GridSearchCV object\ngrid_search = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    scoring={\n        'RMSE': rmse_scorer,\n        'R2': 'r2'\n    },\n    refit='R2',\n    cv=5,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit the grid search to training data\ngrid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 270 candidates, totalling 1350 fits\n\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(force_int_remainder_cols=False,\n                                                          remainder='passthrough',\n                                                          transformers=[('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['brand',\n                                                                          'model',\n                                                                          'transmission',\n                                                                          'fuelType'])])),\n                                       ('regressor',\n                                        DecisionTreeRegressor(max_depth=4,\n                                                              random_state=42))]),\n             n_jobs=-1,\n             param_grid={'regressor__ccp_alpha': [0.001, 0.01, 0.1],\n                         'regressor__max_depth': [3, 5, 7, 10, None],\n                         'regressor__max_features': ['sqrt', None],\n                         'regressor__min_samples_leaf': [1, 2, 4],\n                         'regressor__min_samples_split': [2, 5, 10]},\n             refit='R2',\n             scoring={'R2': 'r2',\n                      'RMSE': make_scorer(&lt;lambda&gt;, greater_is_better=False, response_method='predict')},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(force_int_remainder_cols=False,\n                                                          remainder='passthrough',\n                                                          transformers=[('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['brand',\n                                                                          'model',\n                                                                          'transmission',\n                                                                          'fuelType'])])),\n                                       ('regressor',\n                                        DecisionTreeRegressor(max_depth=4,\n                                                              random_state=42))]),\n             n_jobs=-1,\n             param_grid={'regressor__ccp_alpha': [0.001, 0.01, 0.1],\n                         'regressor__max_depth': [3, 5, 7, 10, None],\n                         'regressor__max_features': ['sqrt', None],\n                         'regressor__min_samples_leaf': [1, 2, 4],\n                         'regressor__min_samples_split': [2, 5, 10]},\n             refit='R2',\n             scoring={'R2': 'r2',\n                      'RMSE': make_scorer(&lt;lambda&gt;, greater_is_better=False, response_method='predict')},\n             verbose=1) best_estimator_: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(force_int_remainder_cols=False,\n                                   remainder='passthrough',\n                                   transformers=[('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('regressor',\n                 DecisionTreeRegressor(ccp_alpha=0.001, min_samples_split=5,\n                                       random_state=42))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(force_int_remainder_cols=False, remainder='passthrough',\n                  transformers=[('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') remainder['year', 'mileage', 'tax', 'mpg', 'engineSize'] passthroughpassthrough DecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(ccp_alpha=0.001, min_samples_split=5, random_state=42) \n\n\nThe GridSearchCV setup evaluates both RMSE and R² during cross-validation.\n\nR² is used to select the best model and is also used to refit the model on the entire training set.\nRMSE is computed during the process for evaluation purposes, but it is not used to determine the best model.\n\nThis allows for more comprehensive model assessment while still optimizing based on a single selected metric.\n\n# Get best estimator and predictions\nbest_model = grid_search.best_estimator_\ny_pred_tuned = best_model.predict(X_test)\n\n# Calculate metrics for tuned model\nrmse_tuned = root_mean_squared_error(y_test, y_pred_tuned)\nr2_tuned = r2_score(y_test, y_pred_tuned)\n\n\nprint(\"\\n=== Best Parameters ===\")\nprint(grid_search.best_params_)\nprint(\"\\n=== Tuned Model Performance ===\")\nprint(f\"RMSE (Tuned): {rmse_tuned:.2f}\")\nprint(f\"R² (Tuned): {r2_tuned:.2f}\")\nprint(f\"Improvement in R²: {(r2_tuned - r2_pipeline):.2%}\")\n\n\n=== Best Parameters ===\n{'regressor__ccp_alpha': 0.001, 'regressor__max_depth': None, 'regressor__max_features': None, 'regressor__min_samples_leaf': 1, 'regressor__min_samples_split': 5}\n\n=== Tuned Model Performance ===\nRMSE (Tuned): 4726.17\nR² (Tuned): 0.92\nImprovement in R²: 15.23%\n\n\n\nprint(\"\\n=== Best Parameters ===\")\nprint(grid_search.best_params_)\nprint(\"\\n=== Tuned Model Performance ===\")\nprint(f\"RMSE (Tuned): {rmse_tuned:.2f}\")\nprint(f\"R² (Tuned): {r2_tuned:.2f}\")\nprint(f\"Improvement in R²: {(r2_tuned - r2_pipeline):.2%}\")\n\n\n=== Best Parameters ===\n{'regressor__ccp_alpha': 0.001, 'regressor__max_depth': None, 'regressor__max_features': None, 'regressor__min_samples_leaf': 1, 'regressor__min_samples_split': 5}\n\n=== Tuned Model Performance ===\nRMSE (Tuned): 4726.17\nR² (Tuned): 0.92\nImprovement in R²: 15.23%\n\n\nGridSearchCV improves the r squared from 0.77 to 0.92, increased by 15.23%, Let us visualize the mean squared error based on the hyperparameter values. We’ll use the cross validation results stored in the cv_results_ attribute of the GridSearchCV fit() object.\n\n#Detailed results of k-fold cross validation\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results.head()\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_regressor__ccp_alpha\nparam_regressor__max_depth\nparam_regressor__max_features\nparam_regressor__min_samples_leaf\nparam_regressor__min_samples_split\nparams\n...\nstd_test_RMSE\nrank_test_RMSE\nsplit0_test_R2\nsplit1_test_R2\nsplit2_test_R2\nsplit3_test_R2\nsplit4_test_R2\nmean_test_R2\nstd_test_R2\nrank_test_R2\n\n\n\n\n0\n0.075184\n0.013049\n0.009999\n0.001052\n0.001\n3\nsqrt\n1\n2\n{'regressor__ccp_alpha': 0.001, 'regressor__ma...\n...\n607.458245\n250\n0.42527\n0.37778\n0.538252\n0.37182\n0.364829\n0.415590\n0.064903\n250\n\n\n1\n0.078505\n0.014115\n0.010804\n0.001051\n0.001\n3\nsqrt\n1\n5\n{'regressor__ccp_alpha': 0.001, 'regressor__ma...\n...\n607.458245\n250\n0.42527\n0.37778\n0.538252\n0.37182\n0.364829\n0.415590\n0.064903\n250\n\n\n2\n0.077370\n0.011081\n0.010873\n0.000946\n0.001\n3\nsqrt\n1\n10\n{'regressor__ccp_alpha': 0.001, 'regressor__ma...\n...\n606.926403\n244\n0.42527\n0.37778\n0.538252\n0.37182\n0.365155\n0.415655\n0.064852\n244\n\n\n3\n0.036274\n0.036182\n0.010617\n0.000345\n0.001\n3\nsqrt\n2\n2\n{'regressor__ccp_alpha': 0.001, 'regressor__ma...\n...\n607.458245\n250\n0.42527\n0.37778\n0.538252\n0.37182\n0.364829\n0.415590\n0.064903\n250\n\n\n4\n0.018702\n0.003564\n0.012120\n0.004090\n0.001\n3\nsqrt\n2\n5\n{'regressor__ccp_alpha': 0.001, 'regressor__ma...\n...\n607.458245\n250\n0.42527\n0.37778\n0.538252\n0.37182\n0.364829\n0.415590\n0.064903\n250\n\n\n\n\n5 rows × 26 columns\n\n\n\n\n# Plotting the RMSE for different max_depth values\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=cv_results, x='param_regressor__max_depth', y=np.abs(cv_results['mean_test_RMSE']), marker='o')\nplt.xlabel('Max Depth')\nplt.ylabel('Mean Test RMSE')\nplt.title('RMSE vs Max Depth');\n\n\n\n\n\n\n\n\n\n\n4.3.7 Output feature importance\n\n# Get feature importances and names\nfeature_importances = best_model.named_steps['regressor'].feature_importances_\nfeature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n\n# Create DataFrame and select top 10\nfeature_importance_df = (\n    pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n    .sort_values(by='Importance', ascending=False)\n    .head(10)  # Keep only top 10 features\n)\n\n# Print top 10 features\nprint(\"=== Top 10 Feature Importances ===\")\nprint(feature_importance_df)\n\n# Plot top 10 features\nplt.figure(figsize=(12, 6))\nsns.barplot(data=feature_importance_df, x='Importance', y='Feature')\nplt.title('Top 10 Feature Importances from Decision Tree')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.tight_layout()\nplt.show()\n\n=== Top 10 Feature Importances ===\n                   Feature  Importance\n112  remainder__engineSize    0.437921\n109     remainder__mileage    0.173215\n108        remainder__year    0.149303\n111         remainder__mpg    0.086922\n98          cat__model_ i8    0.017971\n2          cat__brand_ford    0.013837\n92          cat__model_ X7    0.013477\n110         remainder__tax    0.011502\n60     cat__model_ Mustang    0.009517\n86     cat__model_ V Class    0.008147",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "regression_tree_sp25.html#cost-complexity-pruning-ccp_alpha",
    "href": "regression_tree_sp25.html#cost-complexity-pruning-ccp_alpha",
    "title": "4  Regression trees",
    "section": "4.4 Cost-Complexity Pruning (ccp_alpha)",
    "text": "4.4 Cost-Complexity Pruning (ccp_alpha)\nCost-complexity pruning is a post-pruning technique used to reduce the size of a decision tree by removing sections that provide little to no improvement in prediction accuracy. It helps prevent overfitting and improves model generalization.\n\n4.4.1 Key Idea\nEach subtree in a decision tree has an associated cost-complexity score:\n$ R_(T) = R(T) + |T| $\n\n$ R(T) $: Total training error of the tree ( T )\n$ |T| $: Number of leaf nodes in the tree\n\\(alpha\\) (ccp_alpha): Complexity parameter that penalizes tree size\n\nAs \\(alpha\\) increases, the tree is pruned more aggressively.\n\n\n4.4.2 Parameter: ccp_alpha in scikit-learn\n\nAvailable in DecisionTreeRegressor and DecisionTreeClassifier\nDefault: ccp_alpha = 0.0 (no pruning)\nIncreasing ccp_alpha encourages simpler trees by penalizing extra leaf nodes\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, mean_squared_error\nimport numpy as np\n\n# Define your parameter grid with pipeline step prefix\nparam_grid = {\n    'regressor__ccp_alpha': [0.0, 0.001, 0.01, 0.1]\n}\n\n# Create custom scorer for RMSE\nrmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n                          greater_is_better=False)\n\n# Create GridSearchCV object\ngrid_search_ccp = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    scoring={\n        'RMSE': rmse_scorer,\n        'R2': 'r2'\n    },\n    refit='R2',\n    cv=5,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit the grid search to training data\ngrid_search_ccp.fit(X_train, y_train)\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(force_int_remainder_cols=False,\n                                                          remainder='passthrough',\n                                                          transformers=[('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['brand',\n                                                                          'model',\n                                                                          'transmission',\n                                                                          'fuelType'])])),\n                                       ('regressor',\n                                        DecisionTreeRegressor(max_depth=4,\n                                                              random_state=42))]),\n             n_jobs=-1,\n             param_grid={'regressor__ccp_alpha': [0.0, 0.001, 0.01, 0.1]},\n             refit='R2',\n             scoring={'R2': 'r2',\n                      'RMSE': make_scorer(&lt;lambda&gt;, greater_is_better=False, response_method='predict')},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(force_int_remainder_cols=False,\n                                                          remainder='passthrough',\n                                                          transformers=[('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['brand',\n                                                                          'model',\n                                                                          'transmission',\n                                                                          'fuelType'])])),\n                                       ('regressor',\n                                        DecisionTreeRegressor(max_depth=4,\n                                                              random_state=42))]),\n             n_jobs=-1,\n             param_grid={'regressor__ccp_alpha': [0.0, 0.001, 0.01, 0.1]},\n             refit='R2',\n             scoring={'R2': 'r2',\n                      'RMSE': make_scorer(&lt;lambda&gt;, greater_is_better=False, response_method='predict')},\n             verbose=1) best_estimator_: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(force_int_remainder_cols=False,\n                                   remainder='passthrough',\n                                   transformers=[('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('regressor',\n                 DecisionTreeRegressor(max_depth=4, random_state=42))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(force_int_remainder_cols=False, remainder='passthrough',\n                  transformers=[('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') remainder['year', 'mileage', 'tax', 'mpg', 'engineSize'] passthroughpassthrough DecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(max_depth=4, random_state=42) \n\n\n\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\nX_train_encoded = encoder.fit_transform(X_train[categorical_feature])\nX_test_encoded = encoder.transform(X_test[categorical_feature])\n\n# Convert the encoded features back to DataFrame\nX_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_feature))\nX_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_feature))\n\n# Concatenate the encoded features with the original numerical features\nX_train_final = pd.concat([X_train_encoded_df, X_train[numerical_feature].reset_index(drop=True)], axis=1)\nX_test_final = pd.concat([X_test_encoded_df, X_test[numerical_feature].reset_index(drop=True)], axis=1)\n\n# Check the final shape of the training and testing sets\nprint(\"Training set shape:\", X_train_final.shape)\nprint(\"Testing set shape:\", X_test_final.shape)\n# Check the first few rows of the final training set\nX_train_final.head()\n# Check the first few rows of the final testing set\n\nTraining set shape: (6105, 113)\nTesting set shape: (1527, 113)\n\n\n\n\n\n\n\n\n\nbrand_audi\nbrand_bmw\nbrand_ford\nbrand_hyundi\nbrand_merc\nbrand_skoda\nbrand_toyota\nbrand_vauxhall\nbrand_vw\nmodel_ 6 Series\n...\nfuelType_Diesel\nfuelType_Electric\nfuelType_Hybrid\nfuelType_Other\nfuelType_Petrol\nyear\nmileage\ntax\nmpg\nengineSize\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n2016\n41167\n20\n55.2654\n2.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n2018\n12078\n145\n47.7624\n2.9\n\n\n2\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n2019\n623\n145\n43.0887\n2.2\n\n\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n2014\n37095\n145\n53.4862\n1.7\n\n\n4\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n2016\n15727\n160\n49.0741\n2.0\n\n\n\n\n5 rows × 113 columns\n\n\n\n\nmodel = DecisionTreeRegressor(random_state = 1)#model without any restrictions\npath= model.cost_complexity_pruning_path(X_train_final,y_train)# Compute the pruning path during Minimal Cost-Complexity Pruning.\n\n\n# Extract the effective alphas and the corresponding performance metrics\nccp_alphas = path.ccp_alphas\nimpurities = path.impurities\n# Create a DataFrame to store the results\nccp_results = pd.DataFrame({'ccp_alpha': ccp_alphas, 'impurity': impurities})\n# Fit the model for each alpha value and calculate the mean test score\nmean_test_scores = []\nfor alpha in ccp_alphas:\n    model = DecisionTreeRegressor(random_state=1, ccp_alpha=alpha)\n    model.fit(X_train_final, y_train)\n    y_pred = model.predict(X_test_final)\n    mean_test_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n# Add the mean test scores to the DataFrame\nccp_results['mean_test_score'] = mean_test_scores\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.plot(ccp_results['ccp_alpha'], ccp_results['mean_test_score'], marker='o')\nplt.xlabel('ccp_alpha')\nplt.ylabel('Mean Test RMSE')\nplt.title('Effect of ccp_alpha on Test RMSE')\nplt.xscale('log')\nplt.grid()",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "Classification _Tree.html",
    "href": "Classification _Tree.html",
    "title": "5  Classification trees",
    "section": "",
    "text": "5.1 Building a Classification Tree\nRead section 8.1.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nImport libraries\nWe will build a classification tree to predict whether a person has heart disease, using the default parameters of the decision tree classifier.\n# create a decision tree classifier\ntree = DecisionTreeClassifier(random_state=42)\n\n# fit the model to the training data\ntree.fit(X_train, y_train)\n\n# make predictions on the test data\ny_pred = tree.predict(X_test)\n\n# calculate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Test Accuracy: {accuracy:.2f}')\n\n# train accuracy\ny_train_pred = tree.predict(X_train)\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\nprint(f'Train Accuracy: {train_accuracy:.2f}')\n\nTest Accuracy: 0.75\nTrain Accuracy: 1.00\n# plot the decision tree\nplt.figure(figsize=(12, 8))\nplot_tree(tree, filled=True, feature_names=X.columns, class_names=['No Disease', 'Disease'])\nplt.title('Decision Tree for Heart Disease Classification');\n# get the number of leaves in the tree\nnum_leaves = tree.get_n_leaves()\nprint(f'Number of leaves: {num_leaves}')\n\n# get the depth of the tree\ntree_depth = tree.get_depth()\nprint(f'Depth of the tree: {tree_depth}')\n\nNumber of leaves: 41\nDepth of the tree: 9\nClearly, the model is overfitting, as indicated by a training accuracy of 100% and a much lower test accuracy of 75%.\nNext, we will explore different strategies to address and reduce overfitting.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Classification _Tree.html#pre-pruning-hyperparameters-tuning",
    "href": "Classification _Tree.html#pre-pruning-hyperparameters-tuning",
    "title": "5  Classification trees",
    "section": "5.2 Pre-pruning: Hyperparameters Tuning",
    "text": "5.2 Pre-pruning: Hyperparameters Tuning\nMaximum depth of tree (max_depth) - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\nMinimum samples for a node split (min_samples_split) - Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting. - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\nMinimum samples for a terminal node (min_samples_leaf) - Defines the minimum samples (or observations) required in a terminal node or leaf. - Used to control over-fitting similar to min_samples_split. - Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.\nMaximum number of terminal nodes (max_leaf_nodes) - The maximum number of terminal nodes or leaves in a tree.\n\n# hyperparameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\n# define the parameter grid\nparam_grid = {\n    'max_depth': list(range(1, 9)) + [None], \n    'min_samples_split': [2, 5, 10, 15, 20],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# create a grid search object\ngrid_search = GridSearchCV(estimator=tree, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n# fit the grid search to the training data\ngrid_search.fit(X_train, y_train)\n# print the best parameters\nprint(\"Best parameters found: \", grid_search.best_params_)\n\n# print the best score\nprint(\"Best score: \", grid_search.best_score_)\n# get the best estimator\nbest_tree = grid_search.best_estimator_\n\n# make predictions on the test data with the best estimator\ny_pred_best = best_tree.predict(X_test)\n# calculate the accuracy of the best estimator\nbest_accuracy = accuracy_score(y_test, y_pred_best)\nprint(f'Best Test Accuracy: {best_accuracy:.2f}')\n\nFitting 5 folds for each of 135 candidates, totalling 675 fits\nBest parameters found:  {'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 10}\nBest score:  0.7687074829931972\nBest Test Accuracy: 0.85\n\n\n\n# print out the best tree depth and number of leaves\nbest_num_leaves = best_tree.get_n_leaves()\nprint(f'Best Number of leaves: {best_num_leaves}')\n\nbest_tree_depth = best_tree.get_depth()\nprint(f'Best Depth of the tree: {best_tree_depth}')\n\nBest Number of leaves: 21\nBest Depth of the tree: 6\n\n\n\n# plot the best decision tree\nplt.figure(figsize=(12, 8))\nplot_tree(best_tree, filled=True, feature_names=X.columns, class_names=['No Disease', 'Disease'])\nplt.title('Best Decision Tree for Heart Disease Classification')\nplt.show()\n\n\n\n\n\n\n\n\n\n5.2.1 Gini or entropy\n\n# define the parameter grid\nparam_grid_metric = {\n    'max_depth': list(range(1, 9)) + [None], \n    'min_samples_split': [2, 5, 10, 15, 20],\n    'min_samples_leaf': [1, 2, 4],\n    # adding the criterion parameter to the grid search\n    'criterion': ['gini', 'entropy']\n}\n\n# create a grid search object\ngrid_search_metric = GridSearchCV(estimator=tree, param_grid=param_grid_metric, cv=5, n_jobs=-1, verbose=2)\n# fit the grid search to the training data\ngrid_search_metric.fit(X_train, y_train)\n# print the best parameters\nprint(\"Best parameters found: \", grid_search_metric.best_params_)\n\n# print the best score\nprint(\"Best score: \", grid_search_metric.best_score_)\n# get the best estimator\nbest_tree = grid_search_metric.best_estimator_\n\n# make predictions on the test data with the best estimator\ny_pred_best = best_tree.predict(X_test)\n# calculate the accuracy of the best estimator\nbest_accuracy = accuracy_score(y_test, y_pred_best)\nprint(f'Best Test Accuracy: {best_accuracy:.2f}')\n\nFitting 5 folds for each of 270 candidates, totalling 1350 fits\nBest parameters found:  {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 20}\nBest score:  0.7811224489795918\nBest Test Accuracy: 0.85\n\n\n\n# print out the best tree depth and number of leaves\nbest_num_leaves = best_tree.get_n_leaves()\nprint(f'Best Number of leaves: {best_num_leaves}')\n\nbest_tree_depth = best_tree.get_depth()\nprint(f'Best Depth of the tree: {best_tree_depth}')\n\nBest Number of leaves: 11\nBest Depth of the tree: 4\n\n\n\n# plot the best decision tree\nplt.figure(figsize=(12, 8))\nplot_tree(best_tree, filled=True, feature_names=X.columns, class_names=['No Disease', 'Disease'])\nplt.title('Best Decision Tree for Heart Disease Classification')\nplt.show()\n\n\n\n\n\n\n\n\nBoth criteria aim to minimize impurity in splits, in pratice, they often lead to comparable performance in decision trees, even though their mathematical forulations differ\n\nGini: Faster to compute (no logarithms) and often used for large datasets. so it is a good default. May produce slightly more complex trees.\nEntropy: Slower but aligns with information theory. Prefers splits that balance node sizes, leading to more interpretable trees.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Classification _Tree.html#post-pruning-cost-complexity-pruning",
    "href": "Classification _Tree.html#post-pruning-cost-complexity-pruning",
    "title": "5  Classification trees",
    "section": "5.3 Post-pruning: Cost complexity pruning",
    "text": "5.3 Post-pruning: Cost complexity pruning\nPost-pruning, on the other hand, allows the decision tree to grow to its full extent and then prunes it back to reduce complexity. This approach first builds a complete tree and then removes or collapses branches that don’t significantly contribute to the model’s performance. One common post-pruning technique is called Cost-Complexity Pruning.\n\n5.3.1 step 1: calculate the cost complexity pruning path\n\npath = tree.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n\n\n5.3.2 step 2: Create trees with different ccp_alpha values and evaluate their performance\n\n\n# We'll skip the last alpha which would produce a single-node tree\nalphas = ccp_alphas[:-1]\n\n# Create empty lists to store the results\ntrain_scores = []\ntest_scores = []\ncv_scores = []\nnode_counts = []\n\n# For each alpha value, fit a tree and evaluate\nfor alpha in alphas:\n    # Create and train the model\n    clf = DecisionTreeClassifier(ccp_alpha=alpha, random_state=42)\n    clf.fit(X_train, y_train)\n    \n    # Record scores\n    train_scores.append(accuracy_score(y_train, clf.predict(X_train)))\n    test_scores.append(accuracy_score(y_test, clf.predict(X_test)))\n    \n    # Cross-validation score for robustness\n    cv_score = cross_val_score(clf, X_train, y_train, cv=5).mean()\n    cv_scores.append(cv_score)\n    \n    # Record tree complexity\n    node_counts.append(clf.tree_.node_count)\n\n\n\n5.3.3 Step 3: Visualize the results\n\n# Step 3: Visualize the results\nfig, ax = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot accuracy vs alpha\nax[0, 0].plot(alphas, train_scores, marker='o', label='Train')\nax[0, 0].plot(alphas, test_scores, marker='o', label='Test')\nax[0, 0].plot(alphas, cv_scores, marker='o', label='Cross-validation')\nax[0, 0].set_xlabel('ccp_alpha')\nax[0, 0].set_ylabel('Accuracy')\nax[0, 0].set_title('Accuracy vs. ccp_alpha')\nax[0, 0].legend()\nax[0, 0].grid(True)\n\n# Plot number of nodes vs alpha\nax[0, 1].plot(alphas, node_counts, marker='o')\nax[0, 1].set_xlabel('ccp_alpha')\nax[0, 1].set_ylabel('Number of nodes')\nax[0, 1].set_title('Tree complexity vs. ccp_alpha')\nax[0, 1].grid(True)\n\n# Log scale for better visualization of small alpha values\nax[1, 0].plot(alphas, train_scores, marker='o', label='Train')\nax[1, 0].plot(alphas, test_scores, marker='o', label='Test')\nax[1, 0].plot(alphas, cv_scores, marker='o', label='Cross-validation')\nax[1, 0].set_xlabel('ccp_alpha (log scale)')\nax[1, 0].set_ylabel('Accuracy')\nax[1, 0].set_title('Accuracy vs. ccp_alpha (log scale)')\nax[1, 0].set_xscale('log')\nax[1, 0].legend()\nax[1, 0].grid(True)\n\n# Find best alpha based on test score\nbest_test_idx = np.argmax(test_scores)\nbest_test_alpha = alphas[best_test_idx]\nbest_test_acc = test_scores[best_test_idx]\n\n# Find best alpha based on CV score (more robust)\nbest_cv_idx = np.argmax(cv_scores)\nbest_cv_alpha = alphas[best_cv_idx]\nbest_cv_acc = cv_scores[best_cv_idx]\n\n# Plot highlighting best points\nax[1, 1].plot(alphas, test_scores, 'b-', marker='o', label='Test accuracy')\nax[1, 1].plot(alphas, cv_scores, 'g-', marker='o', label='CV accuracy')\nax[1, 1].axvline(x=best_test_alpha, color='blue', linestyle='--', alpha=0.5, \n                label=f'Best test alpha: {best_test_alpha:.6f}')\nax[1, 1].axvline(x=best_cv_alpha, color='green', linestyle='--', alpha=0.5,\n                label=f'Best CV alpha: {best_cv_alpha:.6f}')\nax[1, 1].set_xlabel('ccp_alpha')\nax[1, 1].set_ylabel('Accuracy')\nax[1, 1].set_title('Finding optimal ccp_alpha')\nax[1, 1].legend(loc='lower left')\nax[1, 1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Print the optimal alpha values and corresponding metrics\nprint(f\"Best alpha based on test score: {best_test_alpha:.6f} (Accuracy: {best_test_acc:.4f}, Nodes: {node_counts[best_test_idx]})\")\nprint(f\"Best alpha based on CV score: {best_cv_alpha:.6f} (Accuracy: {best_cv_acc:.4f}, Nodes: {node_counts[best_cv_idx]})\")\n\n\n\n\n\n\n\n\n\n\n5.3.4 Step 4: Create the final model with the optimal alpha\n\n# Using CV-based alpha as it's more robust against overfitting\nfinal_model = DecisionTreeClassifier(ccp_alpha=best_cv_alpha, random_state=42)\nfinal_model.fit(X_train, y_train)\n\n# Evaluate the final model\ntrain_acc = accuracy_score(y_train, final_model.predict(X_train))\ntest_acc = accuracy_score(y_test, final_model.predict(X_test))\n\nprint(f\"\\nFinal model performance:\")\nprint(f\"Training accuracy: {train_acc:.4f}\")\nprint(f\"Test accuracy: {test_acc:.4f}\")\nprint(f\"Tree nodes: {final_model.tree_.node_count}\")\nprint(f\"Tree depth: {final_model.get_depth()}\")\n\nBest alpha based on test score: 0.007969 (Accuracy: 0.8852, Nodes: 23)\nBest alpha based on CV score: 0.007969 (Accuracy: 0.7604, Nodes: 23)\n\nFinal model performance:\nTraining accuracy: 0.8967\nTest accuracy: 0.8852\nTree nodes: 23\nTree depth: 6\n\n\n\n# plot the final decision tree\nplt.figure(figsize=(12, 8))\nplot_tree(final_model, filled=True, feature_names=X.columns, class_names=['No Disease', 'Disease'])\nplt.title('Final Decision Tree for Heart Disease Classification')\nplt.show()\n\n\n\n\n\n\n\n\nPost-pruning can potentially create more optimal trees, as it considers the entire tree structure before making pruning decisions. However, it can be more computationally expensive.\nBoth approaches aim to find a balance between model complexity and performance, with the goal of creating a model that generalizes well to unseen data. The choice between pre-pruning and post-pruning (or a combination of both) often depends on the specific dataset, the problem at hand, and of course, computational resources available.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Classification _Tree.html#feature-importance-in-decision-trees",
    "href": "Classification _Tree.html#feature-importance-in-decision-trees",
    "title": "5  Classification trees",
    "section": "5.4 Feature Importance in Decision Trees",
    "text": "5.4 Feature Importance in Decision Trees\nDecision tree algorithms, such as Classification and Regression Trees (CART), compute feature importance scores based on how much each feature contributes to reducing the splitting criterion (e.g., Gini impurity or entropy).\nThis methodology extends naturally to ensemble models like Random Forests and Gradient Boosting, which average feature importance across all trees in the ensemble.\nOnce a model is trained, the relative importance of each feature can be accessed using the .feature_importances_ attribute. These scores indicate how valuable each feature was in constructing the decision rules that led to the model’s predictions.\n\nimportances  = final_model.feature_importances_\n\n# Create a DataFrame for easier visualization\nfeature_importance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': importances\n}).sort_values(by='Importance', ascending=False)\n\n# Display the DataFrame\nfeature_importance_df\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n2\ncp\n0.340102\n\n\n11\nca\n0.145273\n\n\n9\noldpeak\n0.139508\n\n\n8\nexang\n0.113871\n\n\n0\nage\n0.095885\n\n\n4\nchol\n0.056089\n\n\n10\nslope\n0.041242\n\n\n12\nthal\n0.035531\n\n\n1\nsex\n0.032499\n\n\n3\ntrestbps\n0.000000\n\n\n5\nfbs\n0.000000\n\n\n6\nrestecg\n0.000000\n\n\n7\nthalach\n0.000000\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\nplt.xlabel(\"Importance Score\")\nplt.title(\"Feature Importances\")\nplt.gca().invert_yaxis()  # Most important feature at the top\nplt.show()\n\n\n\n\n\n\n\n\n\n5.4.1 Do We Need Feature Selection or Regularization with Tree Models?\nWhen using tree-based models (e.g., Decision Trees, Random Forests, Gradient Boosting) and you have a small number of predictors, you typically don’t need to worry much about feature selection or regularization. Here’s why:\n\nTree models inherently ignore uninformative features. They only split on features that reduce impurity (e.g., Gini or entropy), so irrelevant features tend to receive zero or very low importance.\nEnsemble methods like Random Forest or Boosting average over many trees. Features that don’t help prediction are rarely used across the ensemble.\nIncluding unimportant features won’t significantly hurt model performance in small feature spaces. The model will usually ignore them during training.\nHowever, even in small feature sets, irrelevant features may slightly:\nIncrease model variance\nIncrease training time\nReduce interpretability\n\n\n\n5.4.2 Bottom Line\n\nIf the number of predictors is small, you can typically skip feature selection and regularization when using tree-based models — they will handle irrelevant features gracefully.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Classification _Tree.html#next-lecture",
    "href": "Classification _Tree.html#next-lecture",
    "title": "5  Classification trees",
    "section": "5.5 Next Lecture",
    "text": "5.5 Next Lecture\nA single decision tree is highly susceptible to overfitting, especially on noisy or complex datasets.\nIn this notebook, we explored two techniques to reduce variance and improve generalization:\n\nPre-pruning (e.g., setting max_depth, min_samples_split, etc)\nPost-pruning (e.g., cost-complexity pruning)\n\nIn the next lecture, we’ll introduce another powerful method to combat overfitting:\n👉 Bagging (Bootstrap Aggregating) — an ensemble technique that builds multiple trees and averages their predictions to reduce variance and improve robustness.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "bagging.html",
    "href": "bagging.html",
    "title": "6  Bagging",
    "section": "",
    "text": "6.1 Bagging: A Variance Reduction Technique\nRead section 8.2.1 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nBagging, short for Bootstrap Aggregating, is an effective way to reduce overfitting in decision trees. It works by training multiple decision trees—each on a different bootstrap sample of the data—and then aggregating their predictions.\nEach individual decision tree is a weak learner and prone to overfitting, but by combining many such trees, bagging produces a strong learner with lower variance and improved generalization performance.\nThe number of trees to include in the ensemble is specified by the n_estimators hyperparameter.\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.35)\n\n# import the decision tree regressor\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree, export_graphviz\nfrom sklearn.ensemble import BaggingRegressor,BaggingClassifier\n\n# split the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict, KFold\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.metrics import root_mean_squared_error, r2_score, make_scorer, accuracy_score",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#bagging-regression-trees",
    "href": "bagging.html#bagging-regression-trees",
    "title": "6  Bagging",
    "section": "6.2 Bagging Regression Trees",
    "text": "6.2 Bagging Regression Trees\nLet’s revisit the same dataset used for building a single regression tree and explore whether we can further improve its performance using bagging\n\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\ncar.head()\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\nvw\nBeetle\n2014\nManual\n55457\nDiesel\n30\n65.3266\n1.6\n7490\n\n\n1\nvauxhall\nGTC\n2017\nManual\n15630\nPetrol\n145\n47.2049\n1.4\n10998\n\n\n2\nmerc\nG Class\n2012\nAutomatic\n43000\nDiesel\n570\n25.1172\n3.0\n44990\n\n\n3\naudi\nRS5\n2019\nAutomatic\n10\nPetrol\n145\n30.5593\n2.9\n51990\n\n\n4\nmerc\nX-CLASS\n2018\nAutomatic\n14000\nDiesel\n240\n35.7168\n2.3\n28990\n\n\n\n\n\n\n\nSplit the predictors and target, then perform the train-test split\n\nX = car.drop(columns=['price'])\ny = car['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# extract the categorical columns and put them in a list\ncategorical_feature = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumerical_feature = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\nEncode categorical predictors\n\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\nX_train_encoded = encoder.fit_transform(X_train[categorical_feature])\nX_test_encoded = encoder.transform(X_test[categorical_feature])\n\n# Convert the encoded features back to DataFrame\nX_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_feature))\nX_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_feature))\n\n# Concatenate the encoded features with the original numerical features\nX_train_final = pd.concat([X_train_encoded_df, X_train[numerical_feature].reset_index(drop=True)], axis=1)\nX_test_final = pd.concat([X_test_encoded_df, X_test[numerical_feature].reset_index(drop=True)], axis=1)\n\nBy default, a single decision tree grows to its full depth, which often leads to overfitting as shown below\n\n# build a decision tree regressor using the default parameters\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(X_train_final, y_train)\ny_pred = tree_reg.predict(X_test_final)\nrmse = root_mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"Test RMSE: {rmse:.2f}, test R^2: {r2:.2f}\")\n\n# training rmse and r2\ny_train_pred = tree_reg.predict(X_train_final)\ntrain_rmse = root_mean_squared_error(y_train, y_train_pred)\ntrain_r2 = r2_score(y_train, y_train_pred)\nprint(f\"Train RMSE: {train_rmse:.2f}, train R^2: {train_r2:.2f}\")\n\n# print the depth of the tree\nprint(f\"Depth of the tree: {tree_reg.get_depth()}\")\n# print the number of leaves in the tree\nprint(f\"Number of leaves in the tree: {tree_reg.get_n_leaves()}\")\n\nTest RMSE: 6219.96, test R^2: 0.87\nTrain RMSE: 0.00, train R^2: 1.00\nDepth of the tree: 34\nNumber of leaves in the tree: 5925\n\n\nAs observed, the model achieves an RMSE of 0.00 and an R² of 100% on the training data with default parameters, indicating overfitting\nTo address this, we’ve previously explored pre-pruning and post-pruning techniques. Another effective approach is bagging, which helps reduce overfitting by lowering model variance.\nNext, we’ll explore how bagging can improve the performance of unpruned decision trees by reducing variance\n\n#Bagging the results of 10 decision trees with the default parameters to predict car price\nbagging_reg = BaggingRegressor(random_state=1, \n                        n_jobs=-1).fit(X_train_final, y_train)\n\n# make predictions on the test set\ny_pred_bagging = bagging_reg.predict(X_test_final)\n\n# calculate the RMSE and R^2 score\nrmse_bagging = root_mean_squared_error(y_test, y_pred_bagging)\nr2_bagging = r2_score(y_test, y_pred_bagging)\n\nprint(\"Test RMSE with Bagging unpruned trees:\", round(rmse_bagging, 2))\nprint(\"Test R^2 score with Bagging unpruned trees:\", round(r2_bagging, 2))\n\n# training RMSE and R^2 score\ny_pred_train_bagging = bagging_reg.predict(X_train_final)\n\n# calculate the RMSE and R^2 score\nrmse_train_bagging = root_mean_squared_error(y_train, y_pred_train_bagging)\nr2_train_bagging = r2_score(y_train, y_pred_train_bagging)\n\nprint(\"Train RMSE with Bagging unpruned trees:\", round(rmse_train_bagging, 2))\nprint(\"Train R^2 score with Bagging unpruned trees:\", round(r2_train_bagging, 2))\n\nTest RMSE with Bagging unpruned trees: 3758.1\nTest R^2 score with Bagging unpruned trees: 0.95\nTrain RMSE with Bagging unpruned trees: 1501.04\nTrain R^2 score with Bagging unpruned trees: 0.99\n\n\nWith the default settings, bagging unpruned trees improves performance, reducing the RMSE from 6219.96 to 3758.10 and increasing the R² score from 0.87 to 0.95.\nWhat about bagging pruned trees? Since pruning improves the performance of a single decision tree, does that mean bagging pruned trees will also outperform bagging unpruned trees? Let’s find out through implementation.\nBelow is the best model obtained by tuning the hyperparameters of a single decision tree.\n\n# fit the decision tree regressor\npruned_tree_reg = DecisionTreeRegressor(max_depth=None, min_samples_leaf=1, min_samples_split=5, random_state=42)\npruned_tree_reg.fit(X_train_final, y_train)\n\n# make predictions on the test set\ny_pred = pruned_tree_reg.predict(X_test_final)\n# calculate the RMSE and R^2 score\nrmse = root_mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# print the RMSE and R^2 score, keep the decimal points to 2\nprint(\"Test RMSE:\", round(rmse, 2))\nprint(\"Test R^2 score:\", round(r2, 2))\n\n#print the depth of the tree\nprint(f\"Depth of the tree: {pruned_tree_reg.get_depth()}\")\n#print the number of leaves in the tree\nprint(f\"Number of leaves in the tree: {pruned_tree_reg.get_n_leaves()}\")\n\nTest RMSE: 4726.17\nTest R^2 score: 0.92\nDepth of the tree: 33\nNumber of leaves in the tree: 2558\n\n\nNext, let’s apply bagging to these pruned trees using the default settings.\n\n#Bagging the results of 10 decision trees to predict car price\nbagging_reg = BaggingRegressor(estimator=pruned_tree_reg, random_state=1,\n                        n_jobs=-1).fit(X_train_final, y_train)\n\n# make predictions on the test set\ny_pred_bagging = bagging_reg.predict(X_test_final)\n\n# calculate the RMSE and R^2 score\nrmse_bagging = root_mean_squared_error(y_test, y_pred_bagging)\nr2_bagging = r2_score(y_test, y_pred_bagging)\n\nprint(\"Test RMSE with Bagging pruned trees:\", round(rmse_bagging, 2))\nprint(\"Test R^2 score with Bagging pruned trees:\", round(r2_bagging, 2))\n\n\n# training RMSE and R^2 score\ny_pred_train_bagging = bagging_reg.predict(X_train_final)\n\n# calculate the RMSE and R^2 score\nrmse_train_bagging = root_mean_squared_error(y_train, y_pred_train_bagging)\nr2_train_bagging = r2_score(y_train, y_pred_train_bagging)\n\nprint(\"Train RMSE with Bagging pruned trees:\", round(rmse_train_bagging, 2))\nprint(\"Train R^2 score with Bagging pruned trees:\", round(r2_train_bagging, 2))\n\nTest RMSE with Bagging pruned trees: 3806.7\nTest R^2 score with Bagging pruned trees: 0.95\nTrain RMSE with Bagging pruned trees: 1868.7\nTrain R^2 score with Bagging pruned trees: 0.99\n\n\nCompared to bagging the unpruned trees, the performance is slightly worse, with bagging pruned trees the RMSE is 3806.7, bagging the unpruned trees lead to RMSe 3758.1.\nWhy is bagging tuned trees worse than bagging untuned trees?\nIn the pruned tree, limiting the maximum depth reduces variance but increases bias, as reflected by the smaller depth and fewer leaves compared to the unpruned tree. Since bagging only reduces variance and does not affect bias, applying it to pruned trees—which have slightly higher bias—results in slightly worse performance than bagging unpruned trees\nThis suggests that when using bagging, we don’t necessarily need to tune the hyperparameters of the base decision tree—bagging itself effectively combats overfitting by reducing variance, much like hyperparameter tuning does.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#bagging-doesnt-reduce-bias",
    "href": "bagging.html#bagging-doesnt-reduce-bias",
    "title": "6  Bagging",
    "section": "6.3 Bagging Doesn’t Reduce Bias",
    "text": "6.3 Bagging Doesn’t Reduce Bias\nBagging high-variance models can effectively lower overall variance, as long as the individual models are not highly correlated. However, Bagging high-bias models will still produce a high-bias ensemble.\nTo demonstrate this, we first fit a shallow decision tree with max_depth=2, which severely underfits the data due to its limited capacity. Then, we apply bagging using 10 such shallow trees (default setting) as base estimators.\nSince each tree has high bias, the aggregated predictions from bagging still inherit that bias. In our results, both the single shallow tree and the bagged version yield similar (and poor) performance in terms of RMSE and R² on both the training and test sets.\nThis experiment shows that if your base model is too simple to capture the underlying patterns in the data, bagging will not help. To improve performance in such cases, we need to use more expressive base models or consider methods like boosting, which are better suited to reducing both bias and variance.\n\n# Single shallow decision tree (underfitting)\nshallow_tree_reg = DecisionTreeRegressor(max_depth=2, random_state=1)\nshallow_tree_reg.fit(X_train_final, y_train)\n\n# Predict and evaluate on test set\ny_pred_single = shallow_tree_reg.predict(X_test_final)\nrmse_single = root_mean_squared_error(y_test, y_pred_single)\nr2_single = r2_score(y_test, y_pred_single)\n\n# Predict and evaluate on training set\ny_pred_train_single = shallow_tree_reg.predict(X_train_final)\nrmse_train_single = root_mean_squared_error(y_train, y_pred_train_single)\nr2_train_single = r2_score(y_train, y_pred_train_single)\n\nprint(\"Single Shallow Tree - Test RMSE:\", round(rmse_single, 2))\nprint(\"Single Shallow Tree - Test R^2:\", round(r2_single, 2))\nprint(\"Single Shallow Tree - Train RMSE:\", round(rmse_train_single, 2))\nprint(\"Single Shallow Tree - Train R^2:\", round(r2_train_single, 2))\n\nSingle Shallow Tree - Test RMSE: 11084.97\nSingle Shallow Tree - Test R^2: 0.58\nSingle Shallow Tree - Train RMSE: 10314.42\nSingle Shallow Tree - Train R^2: 0.6\n\n\nLet’s bag these shallow trees\n\n\n# Bagging with 10 shallow trees\nbagging_shallow = BaggingRegressor(estimator=DecisionTreeRegressor(max_depth=2, random_state=1),\n                                    random_state=1,\n                                    n_jobs=-1)\nbagging_shallow.fit(X_train_final, y_train)\n\n# Predict and evaluate on test set\ny_pred_bagging = bagging_shallow.predict(X_test_final)\nrmse_bagging = root_mean_squared_error(y_test, y_pred_bagging)\nr2_bagging = r2_score(y_test, y_pred_bagging)\n\n# Predict and evaluate on training set\ny_pred_train_bagging = bagging_shallow.predict(X_train_final)\nrmse_train_bagging = root_mean_squared_error(y_train, y_pred_train_bagging)\nr2_train_bagging = r2_score(y_train, y_pred_train_bagging)\n\nprint(\"Bagged Shallow Trees - Test RMSE:\", round(rmse_bagging, 2))\nprint(\"Bagged Shallow Trees - Test R^2:\", round(r2_bagging, 2))\nprint(\"Bagged Shallow Trees - Train RMSE:\", round(rmse_train_bagging, 2))\nprint(\"Bagged Shallow Trees - Train R^2:\", round(r2_train_bagging, 2))\n\nBagged Shallow Trees - Test RMSE: 10894.92\nBagged Shallow Trees - Test R^2: 0.6\nBagged Shallow Trees - Train RMSE: 10114.07\nBagged Shallow Trees - Train R^2: 0.62\n\n\n✅ What you should observe:\n\nBoth models show low R² and high RMSE due to the shallow depth (max_depth=2).\nBagging cannot fix the high bias inherent in a shallow decision tree.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#model-performance-vs.-number-of-trees",
    "href": "bagging.html#model-performance-vs.-number-of-trees",
    "title": "6  Bagging",
    "section": "6.4 Model Performance vs. Number of Trees",
    "text": "6.4 Model Performance vs. Number of Trees\nTo better understand how the number of base estimators affects the performance of a bagging model, we evaluate the test RMSE and R² score across different numbers of trees.\nThis analysis helps us determine whether adding more trees continues to improve performance or if the model reaches a performance plateau.\n\n# explore how the number of estimators affects the performance of the model, output both oob and test scores\nn_estimators = [ 10, 15, 20, 25,30, 35, 40, 45, 50]\nrmse_scores = []\nr2_scores = []\n\n# iterate through the number of estimators and fit the model\nfor n in n_estimators:\n    bagging_reg = BaggingRegressor(estimator=pruned_tree_reg, n_estimators=n, random_state=1,\n                        n_jobs=-1).fit(X_train_final, y_train)\n    y_pred_bagging = bagging_reg.predict(X_test_final)\n    rmse_scores.append(np.sqrt(np.mean((y_test - y_pred_bagging) ** 2)))\n    r2_scores.append(r2_score(y_test, y_pred_bagging))\n\n# plot the RMSE and R^2 scores against the number of estimators\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(n_estimators, rmse_scores, marker='o')\nplt.title('RMSE vs Number of Estimators')\nplt.xlabel('Number of Estimators')\nplt.ylabel('RMSE')\nplt.xticks(n_estimators)\nplt.grid()\nplt.subplot(1, 2, 2)\nplt.plot(n_estimators, r2_scores, marker='o')\nplt.title('R^2 Score vs Number of Estimators')\nplt.xlabel('Number of Estimators')\nplt.ylabel('R^2 Score')\nplt.xticks(n_estimators)\nplt.grid()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nQuick Takeaway\n\nIncreasing the number of estimators initially improves performance, as seen from the decreasing RMSE and increasing R² scores.\nPerformance stabilizes around 30–35 estimators. Beyond this point, additional trees provide minimal gains.\nDue to the small and possibly noisy test set, performance may appear to peak and then decline slightly. However, this is not typical—under normal circumstances, performance levels off and forms a plateau.\n\n\n# get the number of estimators that gives the best RMSE score\nbest_rmse_index = np.argmin(rmse_scores)\nbest_rmse_n_estimators = n_estimators[best_rmse_index]\nbest_rmse_value = rmse_scores[best_rmse_index]\nprint(\"Best number of estimators for RMSE:\", best_rmse_n_estimators)\nprint(\"Best RMSE value:\", round(best_rmse_value, 2))\n\n# get the number of estimators that gives the best R^2 score\nbest_r2_index = np.argmax(r2_scores)\nbest_r2_n_estimators = n_estimators[best_r2_index]\nbest_r2_value = r2_scores[best_r2_index]\nprint(\"Best number of estimators for R^2 score:\", best_r2_n_estimators)\nprint(\"Best R^2 score:\", round(best_r2_value, 2))\n\nBest number of estimators for RMSE: 30\nBest RMSE value: 3508.31\nBest number of estimators for R^2 score: 30\nBest R^2 score: 0.96",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#oob-sample-and-oob-score-in-bagging",
    "href": "bagging.html#oob-sample-and-oob-score-in-bagging",
    "title": "6  Bagging",
    "section": "6.5 OOB Sample and OOB Score in Bagging",
    "text": "6.5 OOB Sample and OOB Score in Bagging\nWhen training a Bagging ensemble, such as BaggingClassifier or BaggingRegressor, each base learner is trained on a bootstrap sample—a random sample with replacement from the original dataset.\n\n6.5.1 What is an OOB Sample?\nFor each base learner, the data points not selected in the bootstrap sample form its Out-of-Bag (OOB) sample. On average, about 1/3 of the original data points are not included in each bootstrap sample. These unused samples are called OOB samples.\n\n\n6.5.2 What is OOB Score?\nEach base learner can be evaluated on its corresponding OOB sample—i.e., the instances it did not see during training. This provides a built-in validation mechanism without needing an explicit validation set or cross-validation.\nThe OOB score is the average performance (e.g., accuracy for classifiers, R² for regressors) of the ensemble evaluated on OOB samples.\n\n🔧 Note: By default, the oob_score option is turned off in scikit-learn. You must explicitly set oob_score=True to enable it, as shown below.\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbagging_clf = BaggingClassifier(\n    base_estimator=DecisionTreeClassifier(),\n    n_estimators=100,\n    oob_score=True,\n    random_state=42\n)\nbagging_clf.fit(X_train, y_train)\n\n# Access the OOB score\nprint(f\"OOB Score: {bagging_clf.oob_score_:.4f}\")\n\nn_estimators = [ 10, 15, 20, 25,30, 35, 40, 45, 50]\nrmse_scores = []\nr2_scores = []\noob_scores = []\noob_rmse_scores = []\nfor n in n_estimators:\n    bagging_reg = BaggingRegressor(estimator=tree_reg, n_estimators=n, oob_score=True, random_state=1,\n                        n_jobs=-1).fit(X_train_final, y_train)\n    y_pred_bagging = bagging_reg.predict(X_test_final)\n    rmse_scores.append(np.sqrt(np.mean((y_test - y_pred_bagging) ** 2)))\n    r2_scores.append(r2_score(y_test, y_pred_bagging))\n    oob_scores.append(bagging_reg.oob_score_)\n    oob_rmse_scores.append(np.sqrt(np.mean((y_train - bagging_reg.oob_prediction_) ** 2)))\n\n# plot the RMSE and R^2 scores against the number of estimators\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(n_estimators, rmse_scores, marker='o')\nplt.plot(n_estimators, oob_rmse_scores, marker='o')\nplt.legend(['RMSE', 'OOB RMSE'])\nplt.title('RMSE vs Number of Estimators')\nplt.xlabel('Number of Estimators')\nplt.ylabel('RMSE')\nplt.xticks(n_estimators)\nplt.grid()\nplt.subplot(1, 2, 2)\nplt.plot(n_estimators, r2_scores, marker='o')\nplt.plot(n_estimators, oob_scores, marker='o')\nplt.legend(['R^2 Score', 'OOB R^2 Score'])\nplt.title('R^2 Score vs Number of Estimators')\nplt.xlabel('Number of Estimators')\nplt.ylabel('R^2 Score')\nplt.xticks(n_estimators)\nplt.grid()\nplt.tight_layout()\nplt.show()\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\n\n\n\n\n\n\n\n\n\nQuick Takeaway\n\nOOB estimates become more reliable as the number of trees grows, with OOB scores closely tracking the test performance after around 30 estimators.\nOOB RMSE is consistently higher and OOB R² is consistently lower than their test counterparts when the ensemble is small, highlighting the instability of OOB estimates with few trees.\n\n\n# get the number of estimators that gives the best OOB RMSE score\nbest_oob_rmse_index = np.argmin(oob_rmse_scores)\nbest_oob_rmse_n_estimators = n_estimators[best_oob_rmse_index]\nbest_oob_rmse_value = oob_rmse_scores[best_oob_rmse_index]\nprint(\"Best number of estimators for OOB RMSE:\", best_oob_rmse_n_estimators)\nprint(\"Best OOB RMSE value:\", round(best_oob_rmse_value, 2))\n\nBest number of estimators for OOB RMSE: 30\nBest OOB RMSE value: 3539.1",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#bagging-hyperparameter-tuning",
    "href": "bagging.html#bagging-hyperparameter-tuning",
    "title": "6  Bagging",
    "section": "6.6 Bagging Hyperparameter Tuning",
    "text": "6.6 Bagging Hyperparameter Tuning\nTo further improve the performance of our bagging model, we can tune key hyperparameters such as:\n\nn_estimators: the number of base estimators in the ensemble,\nmax_features: the maximum number of features considered at each split,\nmax_samples: the size of each bootstrap sample used to train base estimators,\nbootstrap: whether sampling is performed with replacement (True) or without (False),\nbootstrap_features: whether features are sampled with replacement when selecting subsets of features for each estimator.\n\nBy systematically exploring different combinations of these parameters, we aim to identify the optimal settings that enhance predictive accuracy while maintaining good generalization.\nThere are two common approaches for tuning these hyperparameters: - Cross-validation, which provides a robust estimate of model performance, and\n- Out-of-Bag (OOB) score, which offers an efficient built-in alternative without needing a separate validation set.\n\n6.6.1 Tuning with Cross-Validation\nNext, let’s use GridSearchCV to tune these hyperparameters and identify the best combination for improved model performance.\n\n# hyperparameter tuning using GridSearchCV\nparam_grid = {\n    'n_estimators': [10, 20, 30, 40, 50],\n    'max_samples': [0.5, 0.75, 1.0],\n    'max_features': [0.5, 0.75, 1.0],\n    'bootstrap': [True, False],\n    'bootstrap_features': [True, False]\n}\n\nbagging_reg_grid = BaggingRegressor(random_state=42, n_jobs=-1)\ngrid_search = GridSearchCV(bagging_reg_grid, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train_final, y_train)\n\n# get the best parameters and the best score\nbest_params = grid_search.best_params_\nbest_score = np.sqrt(-grid_search.best_score_)\nprint(\"Best parameters:\", best_params)\nprint(\"Best RMSE cv score:\", round(best_score, 2))\n\n# make predictions on the test set using the best parameters\nbest_bagging_reg = grid_search.best_estimator_\ny_pred_best_bagging = best_bagging_reg.predict(X_test_final)\n\n# calculate the RMSE and R^2 score\nrmse_best_bagging = root_mean_squared_error(y_test, y_pred_best_bagging)\nr2_best_bagging = r2_score(y_test, y_pred_best_bagging)\nprint(\"Test RMSE with best Bagging model:\", round(rmse_best_bagging, 2))\nprint(\"Test R^2 score with best Bagging model:\", round(r2_best_bagging, 2))\n\n# training RMSE and R^2 score\ny_pred_train_best_bagging = best_bagging_reg.predict(X_train_final)\n\n# calculate the RMSE and R^2 score\nrmse_train_best_bagging = root_mean_squared_error(y_train, y_pred_train_best_bagging)\nr2_train_best_bagging = r2_score(y_train, y_pred_train_best_bagging)\nprint(\"Train RMSE with best Bagging model:\", round(rmse_train_best_bagging, 2))\nprint(\"Train R^2 score with best Bagging model:\", round(r2_train_best_bagging, 2))\n\nBest parameters: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.75, 'max_samples': 1.0, 'n_estimators': 50}\nBest RMSE cv score: 3288.03\nTest RMSE with best Bagging model: 3348.45\nTest R^2 score with best Bagging model: 0.96\nTrain RMSE with best Bagging model: 1411.13\nTrain R^2 score with best Bagging model: 0.99\n\n\nAfter simultaneously tuning multiple hyperparameters of the bagging model, including n_estimators, max_features, and max_samples, we achieved the best performance:\n\nTest RMSE with best Bagging model: 3348.45\n\nTest R² score with best Bagging model: 0.96\n\nThis demonstrates that careful tuning of bagging-specific parameters can lead to further improvements beyond using default or even optimized single decision trees.\n\n\n6.6.2 Tuning with Out-of-Bag (OOB) Score\nAs an alternative to cross-validation, we can use the Out-of-Bag (OOB) score to evaluate model performance during training.\nThis method is more efficient for large datasets, as it avoids the need to split data or run multiple folds.\nBy enabling oob_score=True, we can monitor performance on unseen data points (those not included in each bootstrap sample) and use this score to guide hyperparameter tuning.\n\nfrom sklearn.model_selection import ParameterGrid\n# tune the hyperparameters of the decision tree regressor using oob score\n# Hyperparameter grid\nparam_grid = {\n    'n_estimators': [10, 20, 30, 40, 50],\n    'max_samples': [0.5, 0.75, 1.0],\n    'max_features': [0.5, 0.75, 1.0],\n    'bootstrap': [True],  # Required for OOB\n    'bootstrap_features': [True, False]\n}\n\n# Track best parameters and OOB score\nbest_score = -np.inf\nbest_params = {}\n\n# Iterate over all hyperparameter combinations\nfor params in ParameterGrid(param_grid):\n    # Train model with current params and OOB score enabled\n    model = BaggingRegressor(\n        estimator=tree_reg,\n        **params,\n        oob_score=True,\n        random_state=42,\n        n_jobs=-1\n    )\n    model.fit(X_train_final, y_train)\n    \n    # Get OOB score (higher is better for R², lower for RMSE)\n    current_score = model.oob_score_\n    \n    # Update best params if current score is better\n    if current_score &gt; best_score:\n        best_score = current_score\n        best_params = params\n\n# Best model\nbest_model = BaggingRegressor(\n    estimator=tree_reg,\n    **best_params,\n    oob_score=True,\n    random_state=42,\n    n_jobs=-1\n)\nbest_model.fit(X_train_final, y_train)\n\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best OOB Score:\", best_score)\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:1315: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\n\n\nBest Hyperparameters: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 50}\nBest OOB Score: 0.9587800605892783\n\n\n\n# output the test RMSE and R^2 score with the best model\ny_pred_best_model = best_model.predict(X_test_final)\nrmse_best_model = root_mean_squared_error(y_test, y_pred_best_model)\nr2_best_model = r2_score(y_test, y_pred_best_model)\nprint(\"Test RMSE with best model:\", round(rmse_best_model, 2))\nprint(\"Test R^2 score with best model:\", round(r2_best_model, 2))\n\nTest RMSE with best model: 3418.55\nTest R^2 score with best model: 0.96\n\n\n\n\n6.6.3 Comparing Hyperparameter Tuning: Cross-Validation vs. OOB Score\n\n\n\n\n\n\n\n\nAspect\nCross-Validation (CV)\nOut-of-Bag (OOB) Score\n\n\n\n\nMechanism\nSplits training data into multiple folds to validate\nUses unused (out-of-bag) samples in each bootstrap\n\n\nRequires Manual Splits?\nYes\nNo — internal to bagging process\n\n\nEfficiency\nSlower, especially for large datasets\nFaster and more efficient for large datasets\n\n\nBias-Variance Tradeoff\nMore stable and less biased performance estimate\nSlightly more variable and can underestimate accuracy\n\n\nAvailability\nAvailable for all models\nOnly available when bootstrap=True in bagging\n\n\nIntegration in Sklearn\nBuilt-in via GridSearchCV\nMust be implemented manually for tuning\n\n\nFlexibility\nWorks with any model type\nOnly works with bagging-based models\n\n\nUse Case\nIdeal for robust model comparison\nGreat for quick tuning on large datasets\n\n\nScoring Access\n.best_score_ from GridSearchCV\n.oob_score_ from trained model\n\n\n\n\n6.6.3.1 ✅ Best Practices for Imbalanced Classification\n\nPrefer Cross-Validation, especially with:\n\nStratifiedKFold to maintain class distribution in each fold.\nCustom metrics (e.g., F1-score, ROC-AUC, balanced accuracy) using scoring=.\n\nBe cautious using OOB score:\n\nOOB score may be misleading for rare classes, especially when n_estimators is small.\nOnly use it for rough estimates or early tuning when computational efficiency is critical.\n\n\n#### ✅ Summary\n\nUse Cross-Validation when:\n\nYou want robust, model-agnostic performance evaluation.\nYou need precise comparisons between different model types or pipelines.\nYou work on imbalanced classification task\n\nUse OOB Score when:\n\nYou’re working with large datasets and want faster tuning.\nYour model is based on bagging (e.g., BaggingClassifier, RandomForestClassifier).\nYou want to avoid manual train/validation splits.\n\n\n\n⚠️ Note: Scikit-learn’s GridSearchCV does not use OOB score for tuning—even if oob_score=True is set. To use OOB for tuning, you must loop over hyperparameters manually and evaluate using .oob_score_.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "bagging.html#bagging-classification-trees",
    "href": "bagging.html#bagging-classification-trees",
    "title": "6  Bagging",
    "section": "6.7 Bagging Classification Trees",
    "text": "6.7 Bagging Classification Trees\nLet’s revisit the same dataset used for building a single classification tree.\nWhen using the default settings, the tree tends to overfit the data, as shown here.\nIn that notebook, we addressed the overfitting issue using both pre-pruning and post-pruning techniques.\nNow, we’ll explore an alternative approach—bagging—to reduce overfitting and improve model performance.\n\n# load the dataset\nheart_df  = pd.read_csv('datasets/heart_disease_classification.csv')\nprint(heart_df .shape)\nheart_df .head()\n\n(303, 14)\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\n\n# split the x and y data\nX_cls = heart_df.drop(columns=['target'])\ny_cls = heart_df.target\n\n# split the data into train and test sets, add _cls to the variable names\nX_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)\n\n\n# using tree bagging to fit the data\n\nbagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, random_state=0)\nbagging.fit(X_train_cls, y_train_cls)\n\ny_pred_train_cls = bagging.predict(X_train_cls)\ny_pred_cls = bagging.predict(X_test_cls)\n\n\n#print out the accuracy on test set and training set\nprint(\"Train Accuracy is \", accuracy_score(y_train_cls,y_pred_train_cls)*100)\nprint(\"Test Accuracy is \", accuracy_score(y_test_cls,y_pred_cls)*100)\n\nTrain Accuracy is  100.0\nTest Accuracy is  85.24590163934425",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "random_forest.html",
    "href": "random_forest.html",
    "title": "7  Random Forests",
    "section": "",
    "text": "7.1 Motivation: Bagging Revisited\nRead section 8.2.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nIn Bagging (Bootstrap Aggregating), we:\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\ncar.head()\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\nvw\nBeetle\n2014\nManual\n55457\nDiesel\n30\n65.3266\n1.6\n7490\n\n\n1\nvauxhall\nGTC\n2017\nManual\n15630\nPetrol\n145\n47.2049\n1.4\n10998\n\n\n2\nmerc\nG Class\n2012\nAutomatic\n43000\nDiesel\n570\n25.1172\n3.0\n44990\n\n\n3\naudi\nRS5\n2019\nAutomatic\n10\nPetrol\n145\n30.5593\n2.9\n51990\n\n\n4\nmerc\nX-CLASS\n2018\nAutomatic\n14000\nDiesel\n240\n35.7168\n2.3\n28990\nX = car.drop(columns=['price'])\ny = car['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# extract the categorical columns and put them in a list\ncategorical_feature = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumerical_feature = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', FunctionTransformer(), numerical_feature),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_feature)\n    ],\n    remainder='passthrough'\n)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "random_forest.html#motivation-bagging-revisited",
    "href": "random_forest.html#motivation-bagging-revisited",
    "title": "7  Random Forests",
    "section": "",
    "text": "Train many trees on different bootstrap samples of the training data.\nAggregate their predictions by averaging (regression) or voting (classification).\n\n\n✅ Bagging helps reduce variance  ⚠️ But if the trees are too similar (i.e., highly correlated), averaging won’t help as much.\n\n\n\n\n\n7.1.1 Let’s build a single decision tree and output its performance\n\n# build a single decsision tree regressor\nsingle_tree_regressor = DecisionTreeRegressor(random_state=0)\n\n# pipeline for the single decision tree regressor\nsingle_tree_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('tree', single_tree_regressor)\n])\n# fit the pipeline to the training data\nsingle_tree_pipeline.fit(X_train, y_train)\n# make predictions on the test data\ny_pred_single_tree = single_tree_pipeline.predict(X_test)\n# calculate the RMSE and R^2 score\nrmse_single_tree = root_mean_squared_error(y_test, y_pred_single_tree)\nr2_single_tree = r2_score(y_test, y_pred_single_tree)\nprint(f'Single Tree RMSE: {rmse_single_tree:.2f}')\nprint(f'Single Tree R^2: {r2_single_tree:.2f}')\n\n# calculate the RMSE and R^2 score for the training data\ny_pred_train_single_tree = single_tree_pipeline.predict(X_train)\nrmse_train_single_tree = root_mean_squared_error(y_train, y_pred_train_single_tree)\nr2_train_single_tree = r2_score(y_train, y_pred_train_single_tree)\nprint(f'Single Tree Train RMSE: {rmse_train_single_tree:.2f}')\nprint(f'Single Tree Train R^2: {r2_train_single_tree:.2f}')\n\nSingle Tree RMSE: 5073.81\nSingle Tree R^2: 0.91\nSingle Tree Train RMSE: 0.00\nSingle Tree Train R^2: 1.00\n\n\n\n# single tree depth\ntree_depth = single_tree_pipeline.named_steps['tree'].get_depth()\nprint(f\"Depth of the single decision tree: {tree_depth}\")\n\nDepth of the single decision tree: 34\n\n\n\n\n7.1.2 Let’s Build a Bagging Tree with Bootstrap Sampling to Reduce Variance\nBy default, bootstrap=True, meaning each training set is created by sampling with replacement from the original dataset.\n\n# bagging with bootstrap\nbagging_with_bootstrap_regressor = BaggingRegressor(\n    estimator=DecisionTreeRegressor(random_state=0),\n    n_estimators=50,\n    random_state=42\n)\n\n# create a pipeline with the preprocessor and the bagging regressor\nbootstrap_bagging_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('bagging', bagging_with_bootstrap_regressor)\n])\n\n# fit the pipeline to the training data\nbootstrap_bagging_pipeline.fit(X_train, y_train)\n# make predictions on the test data\ny_pred = bootstrap_bagging_pipeline.predict(X_test)\n# calculate the RMSE and R^2 score\nrmse = root_mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'RMSE using bootstraping: {rmse:.2f}')\nprint(f'R^2 using bootstraping: {r2:.2f}')\n# calculate the training rmse and r^2 score\ny_train_pred = bootstrap_bagging_pipeline.predict(X_train)\ntrain_rmse = root_mean_squared_error(y_train, y_train_pred)\ntrain_r2 = r2_score(y_train, y_train_pred)\nprint(f'Training RMSE using bootstraping: {train_rmse:.2f}')\nprint(f'Training R^2 using bootstraping: {train_r2:.2f}')\n\nRMSE using bootstraping: 3756.85\nR^2 using bootstraping: 0.95\nTraining RMSE using bootstraping: 1395.75\nTraining R^2 using bootstraping: 0.99\n\n\nThe test RMSE improved significantly from 5073 to 3756, and the R² score increased from 0.91 to 0.95 after applying bagging.\n\n\n7.1.3 Let’s Build a Bagging Tree Without Bootstrap Sampling\nNow, we’ll turn off bootstrap sampling (bootstrap=False) and observe how it affects the bagging model’s performance.\n\nbagging_without_bootstrap_regressor = BaggingRegressor(\n    estimator=DecisionTreeRegressor(random_state=0),\n    bootstrap=False,\n    n_estimators=50,\n    random_state=42\n)\n\n# create a pipeline with the preprocessor and the bagging regressor\nwithout_bootstrap_bagging_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('bagging', bagging_without_bootstrap_regressor)\n])\n\n# fit the pipeline to the training data\nwithout_bootstrap_bagging_pipeline.fit(X_train, y_train)\n# make predictions on the test data\ny_pred = without_bootstrap_bagging_pipeline.predict(X_test)\n\n# calculate the RMSE and R^2 score\nrmse = root_mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'RMSE without bootstrap sampling: {rmse:.2f}')\nprint(f'R^2 without bootstrap sampling: {r2:.2f}')\n\n# calculate the training rmse and r^2 score\ny_train_pred = without_bootstrap_bagging_pipeline.predict(X_train)\ntrain_rmse = root_mean_squared_error(y_train, y_train_pred)\ntrain_r2 = r2_score(y_train, y_train_pred)\nprint(f'Training RMSE without bootstrap sampling: {train_rmse:.2f}')\nprint(f'Training R^2 without bootstrap sampling: {train_r2:.2f}')\n\nRMSE: 4667.43\nR^2: 0.93\nTraining RMSE: 0.00\nTraining R^2: 1.00\n\n\nAs observed from the results, the performance of bagging without bootstrap sampling is worse (RMSE: 4667) compared to using bootstrap sampling (RMSE: 3756).\n\n\n7.1.4 ❓ Why Does Bagging Without Bootstrap Perform Worse?\nWhen bootstrap=True, each tree in the ensemble is trained on a different bootstrap sample — a random sample drawn with replacement from the training data. This process has two key effects:\n\nIt introduces diversity among the individual trees.\nIt reduces correlation between trees.\n\nThis diversity is the core strength of bagging: even though individual trees may overfit, their errors tend to cancel out when their predictions are averaged, leading to improved generalization.\n\n\n7.1.5 ❓ Why Can Bagging Without Bootstrap Still Show Slight Improvement?\nWhen bootstrap=False, all trees are trained on the same full dataset, removing the primary source of diversity in bagging. As a result, the variance reduction benefit from averaging is significantly weakened.\nHowever, even when trees are trained on the same data, slight variations can still arise due to internal randomness in how decision trees are constructed. For example:\n\nWhen multiple splits yield the same information gain, one split may be selected randomly.\nTies between split candidates can be broken differently.\nMinor numerical differences can occur due to floating-point operations.\n\nThese small variations cause the trees to differ slightly, allowing the ensemble to achieve some variance reduction, which can slightly improve generalization compared to a single decision tree.\n\n⚠️ However, this improvement is typically much smaller than the improvement achieved when using full bootstrap sampling (bootstrap=True).",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "random_forest.html#random-forest",
    "href": "random_forest.html#random-forest",
    "title": "7  Random Forests",
    "section": "7.2 Random Forest",
    "text": "7.2 Random Forest\nWhile diversity is the core strength of bagging, Random Forest further improves upon bagging by introducing even more diversity among the individual trees.\nThe goal of Random Forest is to further decorrelate the trees, which leads to improved generalization and predictive performance.\n\n7.2.1 Idea Behind Random Forest\nRandom Forest introduces an additional source of randomness:\n\nAt each split in a tree, instead of considering all predictors, Random Forest randomly selects a subset of predictors to evaluate.\n\nThis approach:\n\nIncreases diversity among the trees.\nDecreases correlation between trees.\nFurther reduces the variance of the aggregated model.\n\n\nResult: Random Forest generally achieves better performance than standard bagging, especially on high-dimensional datasets.\n\n\n\n7.2.2 Key Hyperparameter Comparison\n\n\n\n\n\n\n\n\nHyperparameter\nBagging\nRandom Forest\n\n\n\n\nbootstrap\n✅ Yes\n✅ Yes\n\n\nmax_features\n🧩 All features considered at each split\n🧩 Random subset of features at each split\n\n\noob_score\n✅ Often used for evaluation\n✅ Often used for evaluation\n\n\nn_estimators\n✅ Number of trees\n✅ Number of trees\n\n\n\n\n\n7.2.3 Let’s Build a Random Forest Model Using the Default Settings\nThe max_features hyperparameter controls the number of features considered when searching for the best split at each node.\nBy default, max_features=1.0, meaning all features are considered at every split, similar to standard bagging.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf_regressor = RandomForestRegressor(\n    n_estimators=50,\n    random_state=42\n)\n\n# create a pipeline with the preprocessor and the bagging regressor\nrf_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('rf', rf_regressor)\n])\n\n# fit the pipeline to the training data\nrf_pipeline.fit(X_train, y_train)\n# make predictions on the test data\ny_pred = rf_pipeline.predict(X_test)\n\n# calculate the RMSE and R^2 score\nrmse = root_mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'RMSE using random forest: {rmse:.2f}')\nprint(f'R^2 using random forest: {r2:.2f}')\n\n# calculate the training rmse and r^2 score\ny_train_pred = rf_pipeline.predict(X_train)\ntrain_rmse = root_mean_squared_error(y_train, y_train_pred)\ntrain_r2 = r2_score(y_train, y_train_pred)\nprint(f'Training RMSE using random forest: {train_rmse:.2f}')\nprint(f'Training R^2 using random forest: {train_r2:.2f}')\n\nRMSE: 3747.58\nR^2: 0.95\nTraining RMSE: 1395.03\nTraining R^2: 0.99\n\n\nThis result is close to that of the bagging model with bootstrap sampling (RMSE: 3756 vs. 3747).\nTo further decorrelate the trees, we can adjust the max_features parameter. Reducing max_features limits the number of features considered at each split, which increases diversity among the trees and helps further reduce variance.\n\n\n7.2.4 Let’s Build a Random Forest Model with sqrt max_features\nThere are two common options to reduce the number of features considered at each split: sqrt and log2.\nHere, we will set max_features='sqrt' and observe how it affects the model’s performance.\n\nrf_sqrt_regressor = RandomForestRegressor(\n    n_estimators=50,\n    max_features='sqrt',\n    random_state=42\n)\n\n# create a pipeline with the preprocessor and the bagging regressor\nrf_sqrt_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('bagging', rf_sqrt_regressor)\n])\n\n# fit the pipeline to the training data\nrf_sqrt_pipeline.fit(X_train, y_train)\n# make predictions on the test data\ny_pred = rf_sqrt_pipeline.predict(X_test)\n\n# calculate the RMSE and R^2 score\nrmse = root_mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'RMSE: {rmse:.2f}')\nprint(f'R^2: {r2:.2f}')\n\n# calculate the training rmse and r^2 score\ny_train_pred = rf_sqrt_pipeline.predict(X_train)\ntrain_rmse = root_mean_squared_error(y_train, y_train_pred)\ntrain_r2 = r2_score(y_train, y_train_pred)\nprint(f'Training RMSE: {train_rmse:.2f}')\nprint(f'Training R^2: {train_r2:.2f}')\n\n\nRMSE: 3424.28\nR^2: 0.96\nTraining RMSE: 1279.85\nTraining R^2: 0.99\n\n\nBy using sqrt for max_features, we further decorrelate the trees, resulting in a lower RMSE of 3424 compared to 3747 when using all features at each split.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "random_forest.html#lets-explore-how-max_features-affects-performance",
    "href": "random_forest.html#lets-explore-how-max_features-affects-performance",
    "title": "7  Random Forests",
    "section": "7.3 Let’s Explore How max_features Affects Performance",
    "text": "7.3 Let’s Explore How max_features Affects Performance\nThe max_features parameter controls the degree of feature decorrelation among trees.\nLet’s explore different values:\n\n# explore how the max_features parameter affects the model performance\nmax_features = ['sqrt', 'log2', 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nrmse_list = []\nr2_list = []\nfor max_feature in max_features:\n    rf_regressor = RandomForestRegressor(\n        n_estimators=50,\n        max_features=max_feature,\n        random_state=42\n    )\n\n    # create a pipeline with the preprocessor and the bagging regressor\n    rf_pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('bagging', rf_regressor)\n    ])\n\n    # fit the pipeline to the training data\n    rf_pipeline.fit(X_train, y_train)\n    # make predictions on the test data\n    y_pred = rf_pipeline.predict(X_test)\n\n    # calculate the RMSE and R^2 score\n    rmse = root_mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    rmse_list.append(rmse)\n    r2_list.append(r2)\n\n\n# plot the RMSE and R^2 score against the max_features parameter\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(max_features, rmse_list, marker='o')\nplt.xlabel('max_features')\nplt.ylabel('RMSE')\nplt.title('RMSE vs max_features')\nplt.grid(True)\nplt.subplot(1, 2, 2)\nplt.plot(max_features, r2_list, marker='o')\nplt.xlabel('max_features')\nplt.ylabel('R^2')\nplt.title('R^2 vs max_features')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs observed from the plots, Random Forest performs best when tree decorrelation is balanced.\nSetting max_features too low or too high hurts the model’s generalization ability.\n\nR² peaks when max_features is around 0.5 to 0.6, consistent with the lowest RMSE values.\nR² drops at both extremes:\n\nUsing too few features (sqrt, log2, or very small proportions) leads to underfitting.\nUsing all features (max_features=1.0) increases correlation between trees, leading to overfitting.\n\n\n\n🔑 Key takeaway: Carefully tuning max_features is critical for achieving the best balance between bias and variance in Random Forest models.\n\nLet’s get the minimum RMSE and the corresponding max_features value from the result\n\n\nmin_rmse = min(rmse_list)\nmin_rmse_index = rmse_list.index(min_rmse)\nbest_max_feature = max_features[min_rmse_index]\nprint(f'Minimum RMSE: {min_rmse:.2f}')\nprint(f'Best max_features: {best_max_feature}')\n# get the maximum R^2 and the corresponding max_features parameter\nmax_r2 = max(r2_list)\nmax_r2_index = r2_list.index(max_r2)\nbest_max_feature_r2 = max_features[max_r2_index]\nprint(f'Maximum R^2: {max_r2:.2f}')\nprint(f'Best max_features: {best_max_feature_r2}')\n\nMinimum RMSE: 3338.02\nBest max_features: 0.5\nMaximum R^2: 0.96\nBest max_features: 0.5\n\n\nAdjusting max_features from ‘sqrt’ to 0.5 led to a slight improvement in RMSE, reducing it from 3424 to 3338",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "random_forest.html#other-hyperparameters-in-random-forest",
    "href": "random_forest.html#other-hyperparameters-in-random-forest",
    "title": "7  Random Forests",
    "section": "7.4 Other Hyperparameters in Random Forest",
    "text": "7.4 Other Hyperparameters in Random Forest\n\n7.4.1 Why Bagging Uses Unpruned Trees\n\nBagging’s main strength lies in reducing variance, not bias.\nDeep, unpruned decision trees tend to overfit (high variance), but bagging effectively reduces this variance through aggregation.\nUsing pruned trees reduces variance but increases bias — and since bagging does not correct bias, this would weaken overall performance.\n\n\nTherefore, in bagging, it is common to let each tree grow fully to preserve low bias and rely on bagging to reduce variance.\n\n\n\n7.4.2 Hyperparameters That Control Tree Complexity in Random Forest\n\n\n\n\n\n\n\n\nSetting\nEffect\nApplies to\n\n\n\n\nmax_depth=None\nFull trees, low bias, high variance\nBagging, RF\n\n\nmax_depth=some int\nPruned trees, more bias, less variance\nEspecially helpful in RF\n\n\nmin_samples_split/leaves\nPrevents small, unreliable branches\nBoth\n\n\n\n\n\n7.4.3 Why Random Forest Often Limits Tree Depth\nIn Random Forest, only a subset of features is considered at each split.\nAs a result, fully growing trees without any depth constraint can cause them to overfit to noise within these smaller subsets.\nLimiting tree complexity in Random Forest:\n\nPrevents deep trees from chasing noise and overfitting.\nImproves generalization, especially in high-dimensional or noisy datasets.\nBalances the bias-variance tradeoff more effectively than using full trees.\n\n\nCareful tuning of tree depth and other complexity-controlling hyperparameters is critical to maximizing Random Forest performance.\n\n\n\n7.4.4 Let’s Tune Multiple Hyperparameters Simultaneously Using Cross-Validation\nGiven the number of hyperparameters involved, we will use BayesSearchCV to efficiently perform tuning.\nThis approach helps reduce computational cost while exploring a wide range of hyperparameter combinations.\n\n# hyperparameter tuning for the random forest regressor\n\nfrom skopt.space import Integer, Categorical, Real\nfrom skopt import BayesSearchCV\n\n# Rename the pipeline step for clarity (recommended)\nrandom_forest_regressor = RandomForestRegressor(\n    random_state=42\n)\n\nrandome_forest_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('rf', random_forest_regressor)  # Renamed from 'bagging' to 'rf'\n])\n\nparam_space = {\n    # Tree structure (control complexity)\n    \"rf__max_depth\": Integer(5, 35),  \n    \"rf__min_samples_split\": Integer(2, 20),\n    \"rf__min_samples_leaf\": Integer(1, 10),\n    \"rf__max_features\": Real(0.1, 1.0),\n    \n    # Ensemble settings\n    \"rf__n_estimators\": Integer(20, 60),\n    \n    # Advanced\n    \"rf__max_samples\": Real(0.1, 1.0),\n}\n\nopt = BayesSearchCV(\n    randome_forest_pipeline,\n    param_space,\n    n_iter=50,  # Adjust based on computational resources\n    cv=5,\n    n_jobs=-1,\n    random_state=42\n)\nopt.fit(X_train, y_train)  \n\n# make predictions on the test data\ny_pred = opt.predict(X_test)\n# calculate the RMSE and R^2 score\nrmse = root_mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'RMSE: {rmse:.2f}')\nprint(f'R^2: {r2:.2f}')\n# calculate the training rmse and r^2 score\ny_train_pred = opt.predict(X_train)\ntrain_rmse = root_mean_squared_error(y_train, y_train_pred)\ntrain_r2 = r2_score(y_train, y_train_pred)\nprint(f'Training RMSE: {train_rmse:.2f}')\nprint(f'Training R^2: {train_r2:.2f}')\n\nRMSE: 3278.42\nR^2: 0.96\nTraining RMSE: 1220.88\nTraining R^2: 0.99\n\n\nAs observed in the results, RMSE was further reduced after simultaneously tuning multiple hyperparameters compared to only tuning max_features (from 3338 to 3278).\nHowever, due to the small and simple nature of the dataset, the performance improvement is relatively marginal.\nOn larger and more complex datasets, the performance gains from comprehensive hyperparameter tuning would likely be much more substantial.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "random_forest.html#feature-importance-in-random-forest",
    "href": "random_forest.html#feature-importance-in-random-forest",
    "title": "7  Random Forests",
    "section": "7.5 Feature Importance in Random Forest",
    "text": "7.5 Feature Importance in Random Forest\nRandom Forest provides a natural way to estimate feature importance.\nEach time a feature is used to split a node, it contributes to reducing impurity (such as Gini impurity for classification or variance for regression).\nBy averaging these contributions over all trees in the forest, we can rank features by how important they are to the model’s predictive performance.\n\n7.5.1 How Feature Importance Is Calculated\n\nA feature’s importance is based on the total reduction of the criterion (e.g., variance for regression) it brings across all splits it is used in.\nFeatures that result in larger reductions in impurity are assigned higher importance scores.\nThe importance scores are normalized so that they sum to 1 across all features.\n\n\n\n7.5.2 Accessing Feature Importances\nAfter fitting a Random Forest model, feature importances can be accessed through the attribute:\nmodel.feature_importances_\n\n# get numerical_feature and categorical_feature from the pipeline\nnum_features = numerical_feature\ncat_transformer = opt.best_estimator_.named_steps['preprocessor'].named_transformers_['cat']\ncat_features = cat_transformer.get_feature_names_out(categorical_feature)\n\n# concatenate all feature names\nfeature_names = np.concatenate([num_features, cat_features])\n\n\n# output feature importances\n\nimportances = opt.best_estimator_.named_steps['rf'].feature_importances_\nfeature_importances = pd.DataFrame(importances, index=feature_names, columns=['importance']).sort_values('importance', ascending=False)\n\n# select top 10 features\ntop_10 = feature_importances.head(10)\n\n# plot the top 10 feature importances\nplt.figure(figsize=(12, 6))\nplt.barh(top_10.index[::-1], top_10['importance'][::-1])  # reverse for top-to-bottom order\nplt.xlabel('Importance')\nplt.title('Top 10 Feature Importances')\nplt.grid(axis='x')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "random_forest.html#in-summary",
    "href": "random_forest.html#in-summary",
    "title": "7  Random Forests",
    "section": "7.6 In Summary",
    "text": "7.6 In Summary\nRandom Forest is a special case of bagging.\nThe n_estimators and oob_score hyperparameters function similarly in both methods, helping to aggregate multiple decision trees into a strong ensemble.\nIn this notebook, we focused on the key differences between Random Forest and standard bagging.\nRandom Forest generally outperforms bagging by introducing an additional layer of randomness:\nat each split, only a random subset of features is considered.\nThis strategy decorrelates the individual trees, increases diversity within the ensemble, and further reduces variance, leading to stronger generalization performance.\n\n🎯 Key takeaway: Bagging reduces variance by aggregating independent models, while Random Forest improves further by strategically injecting feature-level randomness to strengthen ensemble diversity.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "random_forest.html#next-step",
    "href": "random_forest.html#next-step",
    "title": "7  Random Forests",
    "section": "7.7 Next Step",
    "text": "7.7 Next Step\nIn the next section, we will explore Boosting methods,\nwhere models are built sequentially, each one focusing on correcting the errors made by the previous models.\nBoosting shifts the focus from reducing variance to reducing bias, offering another powerful strategy for improving model performance.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "adaboost.html",
    "href": "adaboost.html",
    "title": "8  Adaptive Boosting",
    "section": "",
    "text": "8.1 What is AdaBoost?\nAfter learning how Bagging and Random Forest reduce variance by aggregating many trees,\nwe now turn to a different strategy: Boosting.\nBoosting builds trees sequentially, with each new tree focusing on correcting the mistakes made by the previous ones.\nRead section 8.2.3 of the book before using these notes.\nFor the exact algorithms underlying the AdaBoost algorithm, check out the papers AdaBoostRegressor() and AdaBoostClassifier().\nAdaBoost stands for Adaptive Boosting.\nIt was one of the first boosting algorithms developed.\nThe core idea behind AdaBoost:\nThe final prediction is a weighted combination of all the weak learners.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "adaboost.html#what-is-adaboost",
    "href": "adaboost.html#what-is-adaboost",
    "title": "8  Adaptive Boosting",
    "section": "",
    "text": "Train a weak learner (usually a shallow decision tree) on the original data.\nIncrease the weights of examples that the learner misclassified.\nTrain the next learner on this updated, reweighted data.\nRepeat this process, focusing more and more on hard-to-predict examples.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "adaboost.html#adaboost-intuition",
    "href": "adaboost.html#adaboost-intuition",
    "title": "8  Adaptive Boosting",
    "section": "8.2 AdaBoost Intuition",
    "text": "8.2 AdaBoost Intuition\n\nEasy-to-classify points are de-emphasized.\nHard-to-classify points are emphasized.\nEach learner adapts to the mistakes made by previous learners — hence “adaptive” boosting.\nBetter-performing learners are given higher weight in the final prediction.\n\n\nOver time, the model becomes better at handling difficult cases.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "adaboost.html#how-adaboost-works-high-level-steps",
    "href": "adaboost.html#how-adaboost-works-high-level-steps",
    "title": "8  Adaptive Boosting",
    "section": "8.3 How AdaBoost Works (High-Level Steps)",
    "text": "8.3 How AdaBoost Works (High-Level Steps)\n\nInitialize equal weights for all training examples.\nTrain a weak learner (e.g., decision stump).\nEvaluate its performance:\n\nIncrease weights for misclassified points.\nDecrease weights for correctly classified points.\n\nTrain the next learner using the updated weights.\nRepeat for a set number of learners (n_estimators).\nCombine all learners into a final weighted model.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "adaboost.html#key-hyperparameters-in-adaboost",
    "href": "adaboost.html#key-hyperparameters-in-adaboost",
    "title": "8  Adaptive Boosting",
    "section": "8.4 Key Hyperparameters in AdaBoost",
    "text": "8.4 Key Hyperparameters in AdaBoost\n\n\n\n\n\n\n\n\nHyperparameter\nMeaning\nTypical Values\n\n\n\n\nn_estimators\nNumber of weak learners\n50–500\n\n\nlearning_rate\nShrinks each learner’s contribution\n0.01–1.0\n\n\nestimator\nType of weak learner (default: decision stump)\nShallow trees\n\n\n\n\nLowering learning_rate typically requires more estimators but improves generalization.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "adaboost.html#adaboost-for-regression",
    "href": "adaboost.html#adaboost-for-regression",
    "title": "8  Adaptive Boosting",
    "section": "8.5 AdaBoost for Regression",
    "text": "8.5 AdaBoost for Regression\nWe will revisit the car dataset we used earlier and evaluate how AdaBoost performs compared to a single decision tree and a bagging ensemble\n\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\ncar.head()\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\nvw\nBeetle\n2014\nManual\n55457\nDiesel\n30\n65.3266\n1.6\n7490\n\n\n1\nvauxhall\nGTC\n2017\nManual\n15630\nPetrol\n145\n47.2049\n1.4\n10998\n\n\n2\nmerc\nG Class\n2012\nAutomatic\n43000\nDiesel\n570\n25.1172\n3.0\n44990\n\n\n3\naudi\nRS5\n2019\nAutomatic\n10\nPetrol\n145\n30.5593\n2.9\n51990\n\n\n4\nmerc\nX-CLASS\n2018\nAutomatic\n14000\nDiesel\n240\n35.7168\n2.3\n28990\n\n\n\n\n\n\n\n\nprint(car.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7632 entries, 0 to 7631\nData columns (total 10 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   brand         7632 non-null   object \n 1   model         7632 non-null   object \n 2   year          7632 non-null   int64  \n 3   transmission  7632 non-null   object \n 4   mileage       7632 non-null   int64  \n 5   fuelType      7632 non-null   object \n 6   tax           7632 non-null   int64  \n 7   mpg           7632 non-null   float64\n 8   engineSize    7632 non-null   float64\n 9   price         7632 non-null   int64  \ndtypes: float64(2), int64(4), object(4)\nmemory usage: 596.4+ KB\nNone\n\n\n\nX = car.drop(columns=['price'])\ny = car['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# extract the categorical columns and put them in a list\ncategorical_feature = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumerical_feature = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\nX_train_encoded = encoder.fit_transform(X_train[categorical_feature])\nX_test_encoded = encoder.transform(X_test[categorical_feature])\n\n# Convert the encoded features back to DataFrame\nX_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_feature))\nX_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_feature))\n\n# Concatenate the encoded features with the original numerical features\nX_train_final = pd.concat([X_train_encoded_df, X_train[numerical_feature].reset_index(drop=True)], axis=1)\nX_test_final = pd.concat([X_test_encoded_df, X_test[numerical_feature].reset_index(drop=True)], axis=1)\n\n\n8.5.1 Let’s build a adaboost regressor model with default settings\n\n\n# build a adaboost regressor model with default parameters\nadaboost_regressor = AdaBoostRegressor(random_state=0)\n\n# fit the model\nadaboost_regressor.fit(X_train_final, y_train)\n\n# predict the test set\ny_pred = adaboost_regressor.predict(X_test_final)\n\n# calculate the mean squared error\nrmse = root_mean_squared_error(y_test, y_pred)\nprint(f'RMSE: {rmse:.2f}')\nprint(f'R2 Score: {r2_score(y_test, y_pred)}')\n\n\n# calculate the RMSE and R^2 score for the training data\ny_pred_train = adaboost_regressor.predict(X_train_final)\nrmse_train = root_mean_squared_error(y_train, y_pred_train)\nr2_train = r2_score(y_train, y_pred_train)\nprint(f'Adaboost Train RMSE: {rmse_train:.2f}')\nprint(f'Adaboost Train R^2: {r2_train:.2f}')\n\n# calculate the test score\n\nRMSE: 10239.74\nR2 Score: 0.6426025126917443\nAdaboost Train RMSE: 10081.26\nAdaboost Train R^2: 0.62\n\n\n\n8.5.1.1 ❓ Why AdaBoost Perform Worse Here\n\nDefault base estimator is very weak:\nBy default, AdaBoost uses Decision Stumps (DecisionTreeRegressor(max_depth=1)), which are extremely shallow and tend to underfit the data badly.\nLearning rate (learning_rate=1.0) is too aggressive:\nWhen using very weak learners, a high learning rate can cause the boosting process to fail to properly build up model strength, leading to poor performance.\nDataset characteristics:\nThis dataset is small and not very noisy, using very shallow trees combined with a high learning rate can cause severe underfitting.\n\nWhat should we do next to reduce bias\n\nUse deeper Trees as Base Learners\nTune Learning Rate\nIncrease the Number of Estimators\n\n\n\n\n8.5.2 Impact of Tree Depth\nBy default, AdaBoost uses shallow decision stumps (max_depth=1) as weak learners.\nHowever, slightly increasing the tree depth can make each learner more expressive,\nhelping the ensemble capture more complex patterns in the data.\nThis often leads to a reduction in cross-validation RMSE and improved model performance,\nespecially when the underlying relationships in the data are non-linear.\nFrom our previous exploration, we found that the fully grown decision tree has a depth of 34 on this dataset.\nIn this section, we’ll experiment with limiting the tree depth and observe how it affects the model’s RMSE.\nThe goal is to find a depth that balances bias and variance, leading to better generalization.\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,34):\n        # define base model\n        base = DecisionTreeRegressor(max_depth=i)\n        # define ensemble model\n        models[str(i)] = AdaBoostRegressor(estimator=base,n_estimators=100, learning_rate=0.1, random_state=0)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X_train_final, y_train)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15);\n\n&gt;1 13770.872 (518.361)\n&gt;2 9673.586 (398.116)\n&gt;3 7783.875 (393.200)\n&gt;4 6686.293 (253.234)\n&gt;5 5575.918 (176.859)\n&gt;6 5106.235 (342.995)\n&gt;7 4695.491 (353.584)\n&gt;8 4395.372 (366.340)\n&gt;9 4143.296 (422.938)\n&gt;10 4064.871 (414.840)\n&gt;11 3880.994 (433.105)\n&gt;12 3831.714 (396.971)\n&gt;13 3773.891 (437.190)\n&gt;14 3771.442 (425.043)\n&gt;15 3769.082 (388.875)\n&gt;16 3741.356 (394.563)\n&gt;17 3740.153 (415.062)\n&gt;18 3721.954 (444.760)\n&gt;19 3765.976 (425.594)\n&gt;20 3777.496 (425.025)\n&gt;21 3827.491 (434.580)\n&gt;22 3761.119 (409.760)\n&gt;23 3773.776 (429.259)\n&gt;24 3763.150 (408.907)\n&gt;25 3763.396 (417.512)\n&gt;26 3782.507 (405.136)\n&gt;27 3791.885 (446.067)\n&gt;28 3812.705 (406.567)\n&gt;29 3792.121 (445.264)\n&gt;30 3780.123 (429.651)\n&gt;31 3739.596 (472.987)\n&gt;32 3797.480 (426.929)\n&gt;33 3754.727 (417.368)\n\n\n\n\n\n\n\n\n\nAs shown in the plot, very shallow trees (e.g., max_depth=1 to 3) result in high cross-validation error due to underfitting.\nAs tree depth increases, the model becomes more expressive, and the error drops sharply up to around depth 10.\nBeyond this point, deeper trees offer diminishing returns, and performance stabilizes.\n&gt; 🔍 This suggests that slightly deeper trees (e.g., depth 5–10) strike a good balance between model complexity and generalization in AdaBoost.\n\n\n8.5.3 Impact of Learning Rate\nIn boosting algorithms such as AdaBoost or Gradient Boosting, the learning_rate controls how much each new tree contributes to the overall model.\nEach new tree makes a correction to the current prediction, and the learning rate scales how aggressively that correction is applied.\n\n🔺 A high learning rate takes large correction steps — fast learning, but higher risk of overshooting or overfitting.\n🔹 A low learning rate takes small correction steps — more stable, but may underfit unless paired with enough trees.\n\n\n8.5.3.1 Effect on Performance\n\n\n\n\n\n\n\n\nLearning Rate\nBehavior\nRisk\n\n\n\n\nVery Small (e.g., 0.01)\nLearns slowly, needs many trees\nUnderfitting if not enough trees\n\n\nModerate (e.g., 0.1–0.2)\nBalanced correction, stable learning\nOften optimal\n\n\nLarge (e.g., 0.5–1.0)\nLearns quickly, may overshoot\nOverfitting or unstable learning\n\n\n\n\nKey takeaway: Small learning rates usually generalize better — especially when combined with more estimators.\n\n\ndef get_models():\n    models = dict()\n    learning_rates = [0.01, 0.02, 0.04, 0.08, 0.1, 0.15, 0.2, 0.3, 0.6, 1.0]\n    for i in range(len(learning_rates)):\n        key = learning_rates[i]\n        models[key] = AdaBoostRegressor(learning_rate=learning_rates[i])\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X_train_final, y_train)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15);\n\n&gt;0.01 8877.6 (725.1)\n&gt;0.02 8797.2 (656.7)\n&gt;0.04 8554.4 (540.3)\n&gt;0.08 7988.1 (479.9)\n&gt;0.1 7763.7 (345.8)\n&gt;0.15 7754.0 (380.3)\n&gt;0.2 7862.9 (368.8)\n&gt;0.3 8024.6 (345.3)\n&gt;0.6 9078.9 (205.6)\n&gt;1.0 10508.4 (507.6)\n\n\n\n\n\n\n\n\n\nThe plot shows that moderate learning rates (0.1–0.2) yield the best and most stable model performance, while very small or very large values hurt generalization — likely due to underfitting or overfitting.\n\n\n\n8.5.4 Impact of Number of Trees in Boosting\nAs the number of trees increases in a boosting model, the prediction bias tends to decrease, while the variance may increase.\nThis creates a trade-off:\n\nToo few trees → underfitting (high bias)\n\nToo many trees → potential overfitting (high variance)\n\n\nThere is typically an optimal number of trees that minimizes the overall prediction error, which can be identified using cross-validation.\n\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [10, 20, 30, 40,  50, 60, 70,  80, 90, 100,  200, 300, 500]\n    for n in n_trees:\n        models[str(n)] = AdaBoostRegressor(n_estimators=n,random_state=1, learning_rate=0.1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X_train_final, y_train)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15);\n\n&gt;10 8901.126 (529.620)\n&gt;20 8640.382 (495.164)\n&gt;30 8328.349 (539.563)\n&gt;40 7972.809 (387.803)\n&gt;50 7907.280 (359.779)\n&gt;60 7927.212 (305.995)\n&gt;70 7904.131 (281.108)\n&gt;80 7914.196 (295.777)\n&gt;90 7917.841 (274.357)\n&gt;100 7927.393 (260.542)\n&gt;200 8286.386 (180.913)\n&gt;300 8884.444 (230.006)\n&gt;500 10024.047 (421.340)\n\n\n\n\n\n\n\n\n\nWith a learning rate of 0.1, the validation error initially decreases, then levels off, and eventually starts to increase — indicating that overfitting is beginning to occur\n\n\n8.5.5 Tuning Hyperparameters Simultaneously\nIn the following section, we will use BayesSearchCV instead of GridSearchCV to efficiently tune multiple hyperparameters at once.\nUnlike grid search, which exhaustively evaluates all combinations, Bayesian optimization intelligently explores the hyperparameter space by learning from previous evaluations.\nThis allows us to find a high-performing model using fewer iterations and less computation.\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer\n\n# Define the base estimator search space (DecisionTreeRegressor)\nbase_estimator = DecisionTreeRegressor()\n\n# AdaBoost model (wrapped for BayesSearchCV)\nadaboost = AdaBoostRegressor(estimator=base_estimator, random_state=42)\n\n# Search space for tuning\nsearch_space = {\n    'estimator__max_depth': Integer(5, 25),\n    'n_estimators': Integer(100, 500),\n    'learning_rate': Real(0.01, 2.0, prior='log-uniform')\n}\n\n# Set up the BayesSearchCV\nopt = BayesSearchCV(\n    estimator=adaboost,\n    search_spaces=search_space,\n    n_iter=50,\n    scoring='neg_root_mean_squared_error',  # or use 'r2'\n    cv=10,\n    random_state=42,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit on training data\nopt.fit(X_train_final, y_train)\n\n# Best parameters\nprint(\"Best parameters found:\")\nprint(opt.best_params_)\n\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nFitting 10 folds for each of 1 candidates, totalling 10 fits\nBest parameters found:\nOrderedDict({'estimator__max_depth': 16, 'learning_rate': 1.3460276374020355, 'n_estimators': 398})\n\n\n\n# Best score\nprint(\"Best score (RMSE):\")\nprint(-opt.best_score_)\n\nBest score (RMSE):\n3280.273309604182\n\n\n\n# evaluate the best model on the test set\nbest_model = opt.best_estimator_\ny_pred_test = best_model.predict(X_test_final)\nrmse_test = root_mean_squared_error(y_test, y_pred_test)\nprint(f'Test RMSE: {rmse_test:.2f}')\nprint(f'Test R^2: {r2_score(y_test, y_pred_test):.2f}')\n\nTest RMSE: 3989.52\nTest R^2: 0.95\n\n\nBelow is the plot showing the minimum cross-validated score computed obtained until ‘n’ hyperparameter values are considered for cross-validation.\n\n# import plot_convergence from skopt\nfrom skopt.plots import plot_convergence\n\nplot_convergence(opt.optimizer_results_)\nplt.show()\n\n\n\n\n\n\n\n\n\n# access the full results\nresults_df = pd.DataFrame(opt.cv_results_)\nresults_df['mean_test_score'] = -results_df['mean_test_score'] \nresults_df.head()\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_estimator__max_depth\nparam_learning_rate\nparam_n_estimators\nparams\nsplit0_test_score\nsplit1_test_score\n...\nsplit3_test_score\nsplit4_test_score\nsplit5_test_score\nsplit6_test_score\nsplit7_test_score\nsplit8_test_score\nsplit9_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n27.288841\n0.743828\n0.243362\n0.085644\n13\n0.472627\n473\n{'estimator__max_depth': 13, 'learning_rate': ...\n-3294.069041\n-3813.012426\n...\n-2756.999972\n-2932.996457\n-3575.875394\n-3939.806709\n-3177.591857\n-3766.324137\n-3573.518290\n3424.096527\n366.570915\n19\n\n\n1\n17.667935\n0.272929\n0.097709\n0.037637\n22\n1.077792\n221\n{'estimator__max_depth': 22, 'learning_rate': ...\n-3425.092032\n-3911.963174\n...\n-2779.928592\n-2983.762090\n-2930.152797\n-3924.408565\n-3440.000215\n-3206.956114\n-3554.256617\n3353.027977\n368.446075\n14\n\n\n2\n8.346380\n0.144302\n0.054507\n0.011088\n14\n1.300194\n142\n{'estimator__max_depth': 14, 'learning_rate': ...\n-3321.981911\n-4164.975787\n...\n-2725.368888\n-2880.655623\n-3542.504943\n-3951.995509\n-3251.140610\n-3233.160256\n-3651.305989\n3403.423338\n419.768357\n17\n\n\n3\n30.282255\n0.495589\n0.178685\n0.054628\n21\n0.024859\n339\n{'estimator__max_depth': 21, 'learning_rate': ...\n-3409.024380\n-5096.657461\n...\n-2763.126306\n-3056.556831\n-3554.147448\n-4526.278591\n-3471.156642\n-3489.118289\n-3832.937027\n3671.805682\n645.785144\n29\n\n\n4\n27.381998\n0.592005\n0.195445\n0.069875\n21\n0.101840\n311\n{'estimator__max_depth': 21, 'learning_rate': ...\n-3554.834507\n-5030.730461\n...\n-2786.992358\n-3032.669232\n-3554.205187\n-4512.125597\n-3486.494604\n-3709.185433\n-3678.304514\n3702.954168\n616.274481\n35\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n8.5.5.1 Analyzing BayesSearchCV Results\n\n# Create 1x3 subplots\nfig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n\n# List of hyperparameters and axis labels\nparams = ['param_learning_rate', 'param_estimator__max_depth', 'param_n_estimators']\nlabels = ['Learning Rate', 'Max Depth', 'Number of Estimators']\n\n# Plot each subplot\nfor ax, param, label in zip(axes, params, labels):\n    sc = ax.scatter(\n        results_df[param],\n        results_df['mean_test_score'],\n        c=results_df['rank_test_score'],\n        cmap='viridis',\n        s=100,\n        alpha=0.7\n    )\n    ax.set_xlabel(label, fontsize=13)\n    ax.grid(True)\n\n# Set shared y-axis label and title\naxes[0].set_ylabel('Mean Test Score (CV)', fontsize=13)\nfig.suptitle('Hyperparameter Tuning Results', fontsize=16)\n\n# Add shared colorbar\ncbar = fig.colorbar(sc, ax=axes.ravel().tolist(), label='Rank Test Score');\n\n\n\n\n\n\n\n\n3D scatterplot\n\nEach point is a combination of the 3 hyperparameters.\nColor indicates performance (darker = better).\nYou can rotate the 3D plot in Jupyter interactively!\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\n\np = ax.scatter(\n    results_df['param_learning_rate'],\n    results_df['param_estimator__max_depth'],\n    results_df['param_n_estimators'],\n    c=results_df['mean_test_score'],\n    cmap='viridis',\n    s=60,\n    alpha=0.8\n)\n\nax.set_xlabel('Learning Rate')\nax.set_ylabel('Max Depth')\nax.set_zlabel('N Estimators')\nfig.colorbar(p, label='Mean Test Score')\nplt.title('3D Interaction of Hyperparameters')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsorted_results = results_df.copy()\nsorted_results = sorted_results[['param_learning_rate', 'param_estimator__max_depth', 'param_n_estimators', 'mean_test_score', 'std_test_score', 'rank_test_score']]  # Convert to RMSE\nsorted_results = sorted_results.sort_values(by='rank_test_score')\nsorted_results.reset_index(drop=True, inplace=True)\nsorted_results[:10] # Display the top 10 results\n\n\n\n\n\n\n\n\nparam_learning_rate\nparam_estimator__max_depth\nparam_n_estimators\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n1.346028\n16\n398\n3280.273310\n368.679254\n1\n\n\n1\n1.339271\n17\n436\n3285.379694\n347.295833\n2\n\n\n2\n1.318424\n17\n399\n3286.757311\n349.655468\n3\n\n\n3\n1.386113\n18\n478\n3289.156528\n363.813811\n4\n\n\n4\n1.342019\n17\n434\n3291.786531\n354.169181\n5\n\n\n5\n1.340962\n17\n100\n3300.807523\n343.109325\n6\n\n\n6\n1.313334\n16\n297\n3304.849191\n342.465107\n7\n\n\n7\n1.401063\n20\n100\n3309.832144\n341.978057\n8\n\n\n8\n1.309088\n16\n299\n3312.005185\n332.872216\n9\n\n\n9\n1.396356\n18\n500\n3317.350680\n334.036525\n10\n\n\n\n\n\n\n\nLet’s analyze radeoffs/interactions\n\nsns.pairplot(\n    results_df,\n    vars=[\n        'param_learning_rate',\n        'param_estimator__max_depth',\n        'param_n_estimators'\n    ],\n    hue='rank_test_score',\n    palette='viridis'\n);\n\n\n\n\n\n\n\n\n\nfrom skopt.plots import plot_objective\n\nplot_objective(opt.optimizer_results_[0], dimensions=None, size = 3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n8.5.6 Using Optuna for Hyperparameter Tuning\n\npip install optuna\n\nCollecting optuna\n  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\nCollecting alembic&gt;=1.5.0 (from optuna)\n  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\nCollecting colorlog (from optuna)\n  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging&gt;=20.0 in c:\\users\\lsi8012\\appdata\\roaming\\python\\python312\\site-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy&gt;=1.4.2 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from optuna) (2.0.30)\nRequirement already satisfied: tqdm in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from optuna) (4.66.4)\nRequirement already satisfied: PyYAML in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\nCollecting Mako (from alembic&gt;=1.5.0-&gt;optuna)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nCollecting typing-extensions&gt;=4.12 (from alembic&gt;=1.5.0-&gt;optuna)\n  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: greenlet!=0.4.17 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from sqlalchemy&gt;=1.4.2-&gt;optuna) (3.0.1)\nRequirement already satisfied: colorama in c:\\users\\lsi8012\\appdata\\roaming\\python\\python312\\site-packages (from colorlog-&gt;optuna) (0.4.6)\nRequirement already satisfied: MarkupSafe&gt;=0.9.2 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from Mako-&gt;alembic&gt;=1.5.0-&gt;optuna) (2.1.3)\nDownloading optuna-4.3.0-py3-none-any.whl (386 kB)\n   ---------------------------------------- 0.0/386.6 kB ? eta -:--:--\n   ------------------ --------------------- 174.1/386.6 kB 3.5 MB/s eta 0:00:01\n   ---------------------------------------- 386.6/386.6 kB 4.8 MB/s eta 0:00:00\nDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n   ---------------------------------------- 0.0/231.9 kB ? eta -:--:--\n   ---------------------------------------- 231.9/231.9 kB 7.2 MB/s eta 0:00:00\nDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\nDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n   ---------------------------------------- 0.0/45.8 kB ? eta -:--:--\n   ---------------------------------------- 45.8/45.8 kB 2.4 MB/s eta 0:00:00\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\n   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n   ---------------------------------------- 78.5/78.5 kB 4.6 MB/s eta 0:00:00\nInstalling collected packages: typing-extensions, Mako, colorlog, alembic, optuna\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.11.0\n    Uninstalling typing_extensions-4.11.0:\n      Successfully uninstalled typing_extensions-4.11.0\nSuccessfully installed Mako-1.3.10 alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0 typing-extensions-4.13.2\nNote: you may need to restart the kernel to use updated packages.\n\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nstreamlit 1.32.0 requires packaging&lt;24,&gt;=16.8, but you have packaging 24.2 which is incompatible.\n\n\nStep 1: Import\n\n# import optuna\nimport optuna\n\nStep 2: Define the Objective Function\n\ndef objective(trial):\n    # Suggest hyperparameters\n    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 2.0)\n    max_depth = trial.suggest_int(\"max_depth\", 5, 25)\n    n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n\n    # Define model with trial parameters\n    base_estimator = DecisionTreeRegressor(max_depth=max_depth)\n    model = AdaBoostRegressor(\n        estimator=base_estimator,\n        learning_rate=learning_rate,\n        n_estimators=n_estimators,\n        random_state=42\n    )\n\n    # Cross-validation score (negative RMSE)\n    score = cross_val_score(model, X_train_final, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n    return -np.mean(score)\n\nStep 3: Run the study\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=20, timeout=600)  # 50 trials or 10 min\n\n[I 2025-04-29 18:13:37,366] A new study created in memory with name: no-name-8c1fd49f-a877-442b-99b1-047e129cf2d6\n[I 2025-04-29 18:13:55,695] Trial 0 finished with value: 3604.108244895723 and parameters: {'learning_rate': 0.2657114590825371, 'max_depth': 16, 'n_estimators': 118}. Best is trial 0 with value: 3604.108244895723.\n[I 2025-04-29 18:14:39,369] Trial 1 finished with value: 3418.989299711666 and parameters: {'learning_rate': 1.3034198280227978, 'max_depth': 19, 'n_estimators': 299}. Best is trial 1 with value: 3418.989299711666.\n[I 2025-04-29 18:15:01,876] Trial 2 finished with value: 6675.343723783454 and parameters: {'learning_rate': 1.8424651045294824, 'max_depth': 19, 'n_estimators': 422}. Best is trial 1 with value: 3418.989299711666.\n[I 2025-04-29 18:15:51,382] Trial 3 finished with value: 3534.3761450527854 and parameters: {'learning_rate': 0.5334103211344221, 'max_depth': 12, 'n_estimators': 454}. Best is trial 1 with value: 3418.989299711666.\n[I 2025-04-29 18:16:17,255] Trial 4 finished with value: 4146.601971284103 and parameters: {'learning_rate': 0.3024670447553495, 'max_depth': 8, 'n_estimators': 291}. Best is trial 1 with value: 3418.989299711666.\n[I 2025-04-29 18:16:51,329] Trial 5 finished with value: 3373.6473210617305 and parameters: {'learning_rate': 1.5257826326174242, 'max_depth': 16, 'n_estimators': 256}. Best is trial 5 with value: 3373.6473210617305.\n[I 2025-04-29 18:17:22,661] Trial 6 finished with value: 4645.349739417665 and parameters: {'learning_rate': 0.5297828519634237, 'max_depth': 7, 'n_estimators': 367}. Best is trial 5 with value: 3373.6473210617305.\n[I 2025-04-29 18:17:48,173] Trial 7 finished with value: 3783.098791065398 and parameters: {'learning_rate': 1.4149558081471842, 'max_depth': 10, 'n_estimators': 263}. Best is trial 5 with value: 3373.6473210617305.\n[I 2025-04-29 18:18:07,203] Trial 8 finished with value: 3642.6233815254373 and parameters: {'learning_rate': 1.9601482801398697, 'max_depth': 11, 'n_estimators': 188}. Best is trial 5 with value: 3373.6473210617305.\n[I 2025-04-29 18:19:28,631] Trial 9 finished with value: 3815.355472103091 and parameters: {'learning_rate': 0.259029718590615, 'max_depth': 25, 'n_estimators': 452}. Best is trial 5 with value: 3373.6473210617305.\n[I 2025-04-29 18:19:52,259] Trial 10 finished with value: 3460.264767659689 and parameters: {'learning_rate': 1.0867872885809007, 'max_depth': 23, 'n_estimators': 145}. Best is trial 5 with value: 3373.6473210617305.\n[I 2025-04-29 18:20:24,557] Trial 11 finished with value: 3354.0174995460206 and parameters: {'learning_rate': 1.4283231452601248, 'max_depth': 17, 'n_estimators': 232}. Best is trial 11 with value: 3354.0174995460206.\n[I 2025-04-29 18:20:51,602] Trial 12 finished with value: 3424.5702876072514 and parameters: {'learning_rate': 1.5895892175714832, 'max_depth': 15, 'n_estimators': 217}. Best is trial 11 with value: 3354.0174995460206.\n[I 2025-04-29 18:21:42,335] Trial 13 finished with value: 3428.9633976083956 and parameters: {'learning_rate': 0.9643138493113337, 'max_depth': 17, 'n_estimators': 364}. Best is trial 11 with value: 3354.0174995460206.\n[I 2025-04-29 18:22:07,861] Trial 14 finished with value: 3363.7031695680153 and parameters: {'learning_rate': 1.62982211920311, 'max_depth': 20, 'n_estimators': 234}. Best is trial 11 with value: 3354.0174995460206.\n[I 2025-04-29 18:22:38,820] Trial 15 finished with value: 3467.0742139036493 and parameters: {'learning_rate': 1.0494403165070592, 'max_depth': 21, 'n_estimators': 195}. Best is trial 11 with value: 3354.0174995460206.\n[I 2025-04-29 18:23:03,761] Trial 16 finished with value: 4339.523251732014 and parameters: {'learning_rate': 1.7528718262914982, 'max_depth': 21, 'n_estimators': 347}. Best is trial 11 with value: 3354.0174995460206.\n[I 2025-04-29 18:23:32,216] Trial 17 finished with value: 3429.3383627414196 and parameters: {'learning_rate': 1.2930834352035752, 'max_depth': 14, 'n_estimators': 235}. Best is trial 11 with value: 3354.0174995460206.\n[I 2025-04-29 18:23:46,880] Trial 18 finished with value: 3432.187531277779 and parameters: {'learning_rate': 1.6679028063369739, 'max_depth': 19, 'n_estimators': 145}. Best is trial 11 with value: 3354.0174995460206.\n\n\nStep 4: Review Best Result\n\nprint(\"Best RMSE:\", study.best_value)\nprint(\"Best hyperparameters:\", study.best_params)\n\nBest RMSE: 3354.0174995460206\nBest hyperparameters: {'learning_rate': 1.4283231452601248, 'max_depth': 17, 'n_estimators': 232}\n\n\n\n# make a prediction using the best hyperparameters\nbest_params = study.best_params\nbase_estimator = DecisionTreeRegressor(max_depth=best_params['max_depth'])\nmodel = AdaBoostRegressor(\n    estimator=base_estimator,\n    learning_rate=best_params['learning_rate'],\n    n_estimators=best_params['n_estimators'],\n    random_state=42\n)\n# fit the model\nmodel.fit(X_train_final, y_train)\n# predict the test set\ny_pred = model.predict(X_test_final)\n# calculate the mean squared error\nrmse = root_mean_squared_error(y_test, y_pred)\nprint(f'RMSE: {rmse:.2f}')\nprint(f'R2 Score: {r2_score(y_test, y_pred)}')\n\nRMSE: 3524.38\nR2 Score: 0.9576611696103872\n\n\nStep 5: Visualize the Search\n\noptuna.visualization.plot_optimization_history(study).show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\noptuna.visualization.plot_param_importances(study).show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\noptuna.visualization.plot_parallel_coordinate(study).show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nInsights:\n\nBest Hyperparameter Region:\n\nlearning_rate: ~0.2–0.5\nmax_depth: ~7–10\nn_estimators: ~300–450\n\nTrade-offs:\n\nIncreasing n_estimators improves performance but increases computation time.\nLower learning_rate values require more estimators to achieve good performance.\n\nNext Steps:\n\nFocus on fine-tuning within the identified ranges for learning_rate, max_depth, and n_estimators.\nUse these insights to narrow the search space for further optimization.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Gradient_Boosting.html",
    "href": "Gradient_Boosting.html",
    "title": "9  Gradient Boosting",
    "section": "",
    "text": "9.1 What is Gradient Boosting?\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nGradient Boosting is a boosting technique that builds an additive model in a forward stage-wise manner. Unlike AdaBoost, which adjusts weights on training instances, Gradient Boosting fits new models to the residual errors made by prior models using the gradient of a specified loss function.\nAt each stage, a new weak learner is trained to minimize the loss function by correcting the errors of the current ensemble.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Gradient_Boosting.html#gradient-boosting-intuition",
    "href": "Gradient_Boosting.html#gradient-boosting-intuition",
    "title": "9  Gradient Boosting",
    "section": "9.2 Gradient Boosting Intuition",
    "text": "9.2 Gradient Boosting Intuition\nGradient Boosting can be understood as functional gradient descent:\n\nWe start with an initial prediction (e.g., the mean of the targets).\nAt each iteration, we fit a new model to the negative gradient of the loss function with respect to the current predictions.\nThis negative gradient plays a similar role to residuals in squared loss regression—it points in the direction that most reduces the loss.\nThe new model’s predictions are then added to the current model, scaled by a learning rate.\n\nBy sequentially adding models that reduce the remaining error, the ensemble gradually improves.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Gradient_Boosting.html#how-gradient-boosting-works-regression-example",
    "href": "Gradient_Boosting.html#how-gradient-boosting-works-regression-example",
    "title": "9  Gradient Boosting",
    "section": "9.3 How Gradient Boosting Works (Regression Example)",
    "text": "9.3 How Gradient Boosting Works (Regression Example)\n\nInitialize the model with a constant prediction:\n\\[\n\\hat{f}^{(0)}(x) = \\arg\\min_c \\sum_{i=1}^n L(y_i, c)\n\\]\nFor \\(m = 1\\) to \\(M\\) (number of boosting rounds):\n\nCompute the negative gradient (pseudo-residuals):\n\\[\nr_i^{(m)} = - \\left[ \\frac{\\partial L(y_i, \\hat{f}(x_i))}{\\partial \\hat{f}(x_i)} \\right]_{\\hat{f}(x) = \\hat{f}^{(m-1)}(x)}\n\\]\nFit a base learner \\(h^{(m)}(x)\\) to the residuals \\(r_i^{(m)}\\).\nDetermine the optimal step size (line search):\n\\[\n\\gamma^{(m)} = \\arg\\min_\\gamma \\sum_{i=1}^n L\\left(y_i, \\hat{f}^{(m-1)}(x_i) + \\gamma \\cdot h^{(m)}(x_i)\\right)\n\\]\nUpdate the model:\n\\[\n\\hat{f}^{(m)}(x) = \\hat{f}^{(m-1)}(x) + \\eta \\cdot \\gamma^{(m)} h^{(m)}(x)\n\\] where \\(\\eta\\) is the learning rate.\n\nFinal prediction:\n\\[\n\\hat{f}^{(M)}(x)\n\\]\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\nfrom sklearn.metrics import root_mean_squared_error, mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\nrecall_score, precision_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nimport itertools as it\nimport time as time\n\nimport optuna\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_convergence\nimport warnings\nfrom IPython import display",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Gradient_Boosting.html#gradient-boosting-in-scikit-learn",
    "href": "Gradient_Boosting.html#gradient-boosting-in-scikit-learn",
    "title": "9  Gradient Boosting",
    "section": "9.4 Gradient Boosting in Scikit-Learn",
    "text": "9.4 Gradient Boosting in Scikit-Learn\nScikit-learn offers a standard implementation of Gradient Boosting through two primary estimators:\n\nGradientBoostingClassifier for classification tasks\nGradientBoostingRegressor for regression tasks\n\nThese estimators build an additive model in a forward stage-wise fashion, allowing for the optimization of arbitrary differentiable loss functions. They are suitable for small to medium-sized datasets and provide flexibility in model tuning.\nFor larger datasets (typically with n_samples &gt;= 10,000), consider using the histogram-based variants:\n\nHistGradientBoostingClassifier\nHistGradientBoostingRegressor",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Gradient_Boosting.html#core-hyperparameters-categories",
    "href": "Gradient_Boosting.html#core-hyperparameters-categories",
    "title": "9  Gradient Boosting",
    "section": "9.5 Core Hyperparameters Categories",
    "text": "9.5 Core Hyperparameters Categories\nThe primary hyperparameters for GradientBoostingClassifier and GradientBoostingRegressor can be grouped into the following categories:\n\nNumber of Trees (n_estimators)\n\nUse early stopping (via n_iter_no_change and validation_fraction in scikit-learn) to avoid overfitting.\n\nStart with a large value (e.g., 500–1000) and let early stopping prune unnecessary trees.\n\nEarly Stopping\n\nPrevents overfitting by halting training once the validation performance stops improving.\n\nControlled using:\n\nn_iter_no_change: Number of rounds with no improvement before stopping (e.g., 10).\nvalidation_fraction: Fraction of training data reserved as internal validation set (e.g., 0.1).\ntol: Minimum improvement to be considered significant (e.g., 1e-4).\n\nSet a large n_estimators, and let early stopping determine the optimal number of boosting iterations.\n\nLearning Rate (learning_rate)\n\nShrinks the contribution of each tree to improve generalization.\n\nTypical range: 0.01–0.2 (lower values require more trees).\n\nTree Complexity\n\nmax_depth: Depth of individual trees. Start shallow (3–6) to limit overfitting.\n\nmin_samples_split: Minimum samples required to split a node (e.g., 10–50).\n\nmin_samples_leaf: Minimum samples required in a leaf node (e.g., 5–20).\n\nStochastic Gradient Boosting\n\nsubsample: Fraction of training data sampled per tree (e.g., 0.5–1.0).\n\nmax_features: Fraction/absolute number of features used per split (e.g., sqrt(n_features) or 0.8).\n\nLoss Function (loss)\n\nMatches the problem type:\n\nRegression: squared_error, absolute_error\nClassification: log_loss (binary/multinomial deviance)\n\n\n\nFor a comprehensive list and detailed explanations of all hyperparameters, refer to the official Scikit-learn documentation:\n\nGradientBoostingClassifier Documentation\nGradientBoostingRegressor Documentation",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Gradient_Boosting.html#hyperparameter-tuning",
    "href": "Gradient_Boosting.html#hyperparameter-tuning",
    "title": "9  Gradient Boosting",
    "section": "9.6 Hyperparameter Tuning",
    "text": "9.6 Hyperparameter Tuning\nLet’s reuse the car dataset to evaluate how different hyperparameter settings affect the performance of gradient boosting\n\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\ncar.head()\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\nvw\nBeetle\n2014\nManual\n55457\nDiesel\n30\n65.3266\n1.6\n7490\n\n\n1\nvauxhall\nGTC\n2017\nManual\n15630\nPetrol\n145\n47.2049\n1.4\n10998\n\n\n2\nmerc\nG Class\n2012\nAutomatic\n43000\nDiesel\n570\n25.1172\n3.0\n44990\n\n\n3\naudi\nRS5\n2019\nAutomatic\n10\nPetrol\n145\n30.5593\n2.9\n51990\n\n\n4\nmerc\nX-CLASS\n2018\nAutomatic\n14000\nDiesel\n240\n35.7168\n2.3\n28990\n\n\n\n\n\n\n\n\nX = car.drop(columns=['price'])\ny = car['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# extract the categorical columns and put them in a list\ncategorical_feature = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumerical_feature = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\nX_train_encoded = encoder.fit_transform(X_train[categorical_feature])\nX_test_encoded = encoder.transform(X_test[categorical_feature])\n\n# Convert the encoded features back to DataFrame\nX_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_feature))\nX_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_feature))\n\n# Concatenate the encoded features with the original numerical features\nX_train_final = pd.concat([X_train_encoded_df, X_train[numerical_feature].reset_index(drop=True)], axis=1)\nX_test_final = pd.concat([X_test_encoded_df, X_test[numerical_feature].reset_index(drop=True)], axis=1)\n\n\n9.6.1 Individual Hyperparameter Impact Analysis\n\n9.6.1.1 Effect of Number of Trees on Cross-Validation Error\nEffect of Number of Trees on Cross-Validation Error In Gradient Boosting, the number of trees (n_estimators) controls how many boosting rounds the model performs. Adding more trees can reduce bias and improve training accuracy, but it also increases the risk of overfitting, especially with a high learning rate.\nThe optimal number of trees is often found by balancing model complexity and generalization performance using cross-validation.\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [50, 100, 500, 800, 1000, 1500, 2000]\n    for n in n_trees:\n        models[str(n)] = GradientBoostingRegressor(n_estimators=n,random_state=1,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X_train_final, y_train)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15);\n\n# get the optimal number of trees\nbest_index = np.argmin([np.mean(r) for r in results])\nbest_n_trees = names[best_index]\nbest_score = np.mean(results[best_index])\n\n\n# Highlight the best model on the plot\nplt.axvline(x=best_index+1, color='red', linestyle='--', alpha=0.7)\nplt.text(best_index + 1 - 0.4, best_score+700, \n         f'Best: {best_n_trees} (RMSE: {best_score:.1f})', \n         color='red', fontweight='bold')\n\nprint(f\"Best number of trees: {best_n_trees} with RMSE: {best_score:.3f}\")\n\n&gt;50 6549.576 (722.462)\n&gt;100 5232.949 (656.216)\n&gt;500 3419.467 (262.753)\n&gt;800 3202.489 (194.072)\n&gt;1000 3106.002 (184.799)\n&gt;1500 3039.520 (210.989)\n&gt;2000 3194.874 (293.134)\nBest number of trees: 1500 with RMSE: 3039.520\n\n\n\n\n\n\n\n\n\n\n\n9.6.1.2 Early stopping in Gradient Boosting\nWhy Early Stopping Matters\nSpecifying a fixed number of trees means deciding in advance how many boosting rounds (i.e., trees) the model will train.\nThis approach can be inefficient or risky:\n\nIf too few trees are used, the model may underfit.\nIf too many, the model may overfit or waste computation.\n\nThat’s why early stopping is useful — it allows the model to stop training once performance on a validation set no longer improves, effectively selecting the optimal number of trees automatically.\nHow Early Stopping Works\nInstead of specifying a fixed number of trees (n_estimators), the algorithm monitors performance on a validation set and stops adding new trees once the model’s improvement has plateaued.\nIn scikit-learn, early stopping can be enabled using:\n\nearly_stopping=True\nvalidation_fraction: The fraction of training data used as a validation set\nn_iter_no_change: Number of iterations to wait without improvement before stopping\n\nThis approach not only improves generalization but also reduces training time by avoiding unnecessary trees.\n\nparams = dict(n_estimators=2000, max_depth=5, learning_rate=0.1, random_state=42)\n\ngbm_full = GradientBoostingRegressor(**params)\ngbm_early_stopping = GradientBoostingRegressor(\n    **params,\n    validation_fraction=0.1,\n    n_iter_no_change=10,\n)\n\nstart_time = time.time()\ngbm_full.fit(X_train_final, y_train)\ntraining_time_full = time.time() - start_time\nn_estimators_full = gbm_full.n_estimators_\n\nstart_time = time.time()\ngbm_early_stopping.fit(X_train_final, y_train)\ntraining_time_early_stopping = time.time() - start_time\nestimators_early_stopping = gbm_early_stopping.n_estimators_\n\nLet’s calculate the RMSE on both the training and test datasets for each model, which will be used for later visualization.\n\n# import root mean squared error function\nfrom sklearn.metrics import root_mean_squared_error\n\ntrain_errors_without = []\ntest_errors_without = []\n\ntrain_errors_with = []\ntest_errors_with = []\n\nfor i, (train_pred, test_pred) in enumerate(\n    zip(\n        gbm_full.staged_predict(X_train_final),\n        gbm_full.staged_predict(X_test_final),\n    )\n):\n    train_errors_without.append(root_mean_squared_error(y_train, train_pred))\n    test_errors_without.append(root_mean_squared_error(y_test, test_pred))\n\nfor i, (train_pred, test_pred) in enumerate(\n    zip(\n        gbm_early_stopping.staged_predict(X_train_final),\n        gbm_early_stopping.staged_predict(X_test_final),\n    )\n):\n    train_errors_with.append(root_mean_squared_error(y_train, train_pred))\n    test_errors_with.append(root_mean_squared_error(y_test, test_pred))\n\nLet’s visulize Comparison. It includes three subplots:\n\nPlotting training errors of both models over boosting iterations.\nPlotting test errors of both models over boosting iterations.\nCreating a bar chart to compare the training times and the number of estimators used by the models with and without early stopping.\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(12, 4))\n\naxes[0].plot(train_errors_without, label=\"gbm_full\")\naxes[0].plot(train_errors_with, label=\"gbm_early_stopping\")\naxes[0].set_xlabel(\"Boosting Iterations\")\naxes[0].set_ylabel(\"RMSE (Training)\")\naxes[0].set_yscale(\"log\")\naxes[0].legend()\naxes[0].set_title(\"Training Error\")\n\naxes[1].plot(test_errors_without, label=\"gbm_full\")\naxes[1].plot(test_errors_with, label=\"gbm_early_stopping\")\naxes[1].set_xlabel(\"Boosting Iterations\")\naxes[1].set_ylabel(\"RMSE (Test)\")\naxes[1].set_yscale(\"log\")\naxes[1].legend()\naxes[1].set_title(\"Test Error\")\n\ntraining_times = [training_time_full, training_time_early_stopping]\nlabels = [\"gbm_full\", \"gbm_early_stopping\"]\nbars = axes[2].bar(labels, training_times)\naxes[2].set_ylabel(\"Training Time (s)\")\n\nfor bar, n_estimators in zip(bars, [n_estimators_full, estimators_early_stopping]):\n    height = bar.get_height()\n    axes[2].text(\n        bar.get_x() + bar.get_width() / 2,\n        height + 0.001,\n        f\"Estimators: {n_estimators}\",\n        ha=\"center\",\n        va=\"bottom\",\n    )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe difference in training error between the gbm_full and the gbm_early_stopping stems from the fact that\ngbm_early_stopping sets aside validation_fraction of the training data as an internal validation set.\nEarly stopping is decided based on this internal validation score.\nBenefits of Using Early Stopping in Boosting:\n\nPreventing Overfitting\nEarly stopping helps avoid overfitting by monitoring the test error.\nWhen the error stabilizes or starts increasing, training stops — resulting in better generalization to unseen data.\nImproving Training Efficiency\nModels with early stopping often require fewer estimators while achieving similar accuracy.\nThis reduces training time significantly compared to training without early stopping.\n\n\n\n9.6.1.3 Effect of Learning Rate on Cross-Validation Error\nThe learning rate (learning_rate) determines how much each new tree contributes to the overall model. A smaller learning rate results in slower learning and often requires more trees to achieve good performance. A larger learning rate speeds up learning but increases the risk of overfitting.\nFinding the optimal learning rate involves balancing: - High learning rate → faster convergence, but higher risk of overfitting\n- Low learning rate → better generalization, but requires more trees and longer training time\nCross-validation helps identify the learning rate that minimizes prediction error while ensuring model stability.\n\ndef get_models():\n    models = dict()\n    # create 9 evenly spaced values between 0.2 and 1.0\n    learning_rates = np.linspace(0.2, 1.0, 9)\n    for learning_rate in learning_rates:\n        # Round to 2 decimal places for clean keys\n        lr_rounded = round(learning_rate, 2)\n        key = f\"{lr_rounded:.2f}\"\n        models[key] = GradientBoostingRegressor(learning_rate=lr_rounded, random_state=1, loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n\n# evaluate the models and store results\nresults, names = list(), list()\nmean_scores = []  # Track mean scores separately\n\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X_train_final, y_train)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # Calculate and store mean score\n    mean_score = np.mean(scores)\n    mean_scores.append(mean_score)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, mean_score, np.std(scores)))\n\n# plot model performance for comparison\nplt.figure(figsize=(10, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error', fontsize=15)\nplt.xlabel('Learning rate', fontsize=15)\nplt.title('Model Performance by Learning Rate', fontsize=16)\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Find the best model using the saved mean scores\nbest_index = np.argmin(mean_scores)\nbest_lr = names[best_index]\nbest_score = mean_scores[best_index]\n\n# Highlight the best model on the plot\nplt.axvline(x=best_index+1, color='red', linestyle='--', alpha=0.7)\nplt.text(best_index+1.2, min(mean_scores)*0.95, \n         f'Best: {best_lr} (RMSE: {best_score:.1f})', \n         color='red', fontweight='bold')\n\nplt.show()\n\n# Print the best model information\nprint(f\"\\nBest model: {best_lr} with RMSE: {best_score:.3f}\")\n\n&gt;0.20 4193.7 (301.2)\n&gt;0.30 3740.3 (306.3)\n&gt;0.40 3630.0 (212.2)\n&gt;0.50 3529.6 (181.5)\n&gt;0.60 3650.2 (169.0)\n&gt;0.70 3644.7 (142.5)\n&gt;0.80 3908.9 (260.6)\n&gt;0.90 3968.7 (201.1)\n&gt;1.00 4208.3 (368.5)\n\n\n\n\n\n\n\n\n\n\nBest model: 0.50 with RMSE: 3529.563\n\n\n\n\n9.6.1.4 Learning Rate and Number of Trees Are Closely Linked\nThe learning rate and number of trees (n_estimators) are tightly coupled hyperparameters in gradient boosting. Their balance plays a key role in model performance and overfitting control.\n\nA lower learning rate slows the learning process, requiring more trees to achieve strong performance.\nA higher learning rate speeds up training but may cause the model to overfit if not regularized properly.\n\n⚠️ A high learning rate with too few trees can lead to poor generalization, while a very low learning rate with too many trees may improve accuracy but increase training time significantly.\nBest practice: Use a low to moderate learning rate (e.g., 0.01–0.1) combined with early stopping to find the optimal number of trees.\n\n\n9.6.1.5 Effect of Stochastic Gradient Boosting on Cross-Validation Error\nStochastic Gradient Boosting enhances generalization by introducing randomness into the model-building process. Two key hyperparameters that control this are subsample and max_features, and they operate on different dimensions of the data:\n\n\n\n\n\n\n\n\nParameter\nApplies To\nPurpose\n\n\n\n\nsubsample\nRows (data points)\nRandomly samples a fraction of the training data for each tree\n\n\nmax_features\nColumns (features)\nRandomly samples a fraction of the features for each tree or split\n\n\n\nBy tuning these parameters, we can reduce overfitting and increase model robustness. However, setting them too low may lead to underfitting due to insufficient information per tree.\n\nfrom sklearn.metrics import make_scorer, mean_squared_error\n\n# Define model\nmodel = GradientBoostingRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=1)\n\n# Define param grid\nparam_grid = {\n    'subsample': np.linspace(0.2, 1.0, 9),\n    'max_features': np.linspace(0.2, 1.0, 9)\n}\n\n# RMSE scoring\nscorer = make_scorer(mean_squared_error, greater_is_better=False)\n\n# Grid search\ngrid = GridSearchCV(estimator=model, param_grid=param_grid,\n                    scoring=scorer, cv=5, n_jobs=-1, verbose=1)\ngrid.fit(X_train_final, y_train)\n\n# Create DataFrame from results\nresults_df = pd.DataFrame(grid.cv_results_)\nresults_df['mean_rmse'] = np.sqrt(-results_df['mean_test_score'])\n\nFitting 5 folds for each of 81 candidates, totalling 405 fits\n\n\n\n# Round subsample and max_features to 2 decimal places for display\nresults_df['subsample'] = results_df['param_subsample'].astype(float).round(2)\nresults_df['max_features'] = results_df['param_max_features'].astype(float).round(2)\n\n# Then pivot using the rounded values\nheatmap_data = results_df.pivot(index='subsample', columns='max_features', values='mean_rmse')\n\n# Plot heatmap\nplt.figure(figsize=(12, 9))\nsns.heatmap(heatmap_data, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", cbar_kws={'label': 'CV RMSE'})\nplt.title('Grid Search: CV RMSE by Subsample and Max Features')\nplt.ylabel('Subsample')\nplt.xlabel('Max Features')\nplt.tight_layout()\nplt.show()\n\n# Find the location (subsample, max_features) of the minimum RMSE\nmin_rmse = heatmap_data.min().min()\nbest_location = heatmap_data.stack().idxmin()  # returns a tuple: (subsample, max_features)\n\nprint(f\"Best RMSE: {min_rmse:.3f} at subsample = {best_location[0]}, max_features = {best_location[1]}\")\n\n\n\n\n\n\n\n\nBest RMSE: 3748.534 at subsample = 0.5, max_features = 0.7\n\n\n\n\n9.6.1.6 Effect of Tree Complexity on Cross-Validation Error (Not Tuned Here)\nTree complexity controls how expressive and flexible each individual tree in the gradient boosting ensemble can be. While deeper and more complex trees can capture intricate patterns in the data, they are also more prone to overfitting, especially when combined with many trees.\nKey parameters include:\n\nmax_depth: Limits the depth of each tree. Shallower trees (e.g., depth 3–6) are preferred for reducing overfitting.\nmin_samples_split: Specifies the minimum number of samples required to split an internal node. Higher values make the tree more conservative.\nmin_samples_leaf: Sets the minimum number of samples required to be at a leaf node. This also helps smooth the model and avoid capturing noise.\n\nThese parameters influence the bias-variance trade-off by adjusting how expressive each tree can be.\nSince we have already discussed and tuned these parameters in earlier lessons (decision trees and random forests), we will not tune them again here.\n\n\n9.6.1.7 Loss Function (loss)\nIn gradient boosting, the loss function determines how the model measures prediction errors and guides the optimization process during training. Here’s a breakdown of common loss functions for regression and classification tasks:\n\nRegression:\n\nsquared_error: Penalizes larger errors more heavily; sensitive to outliers. (Default for regression)\nabsolute_error: Penalizes all errors equally; more robust to outliers.\nhuber: Combines squared and absolute error; less sensitive to outliers than squared_error and smoother than absolute_error.\n\nClassification:\n\nlog_loss: Also known as logistic loss or deviance; commonly used for binary and multiclass classification.\nexponential: Used by AdaBoost; heavily penalizes misclassified points, making it more sensitive to outliers.\n\n\nChoosing an appropriate loss function ensures the model is optimized for the specific structure and goals of the problem.\n\n\n\n9.6.2 Joint Hyperparameter Optimization\nSince the optimal values of hyperparameters are often interdependent, they should be tuned together rather than in isolation to achieve the best performance.Next we will simultaneously tune multiple core hyperparameters to find the best combination for overall model performance.\n\n9.6.2.1 Using BayesSearchCV for Hyperparameter Tuning\nWe can use BayesSearchCV with early stopping to simultaneously tune multiple hyperparameters in a more efficient and automated way.\n\n# time the search\nstart = time.time()\n# Define the search space\nsearch_space = {\n    'learning_rate': Real(0.01, 0.8, prior='log-uniform'),  # Prefer lower rates\n    'max_depth': Integer(4, 32),          # Shallow trees to prevent overfitting\n    'min_samples_split': Integer(2, 100), # Regularize splits\n    'min_samples_leaf': Integer(1, 30),  # Regularize leaves\n    'subsample': Real(0.1, 1.0),         # Stochastic sampling\n    'max_features': Categorical([\n        'sqrt', 'log2', None,  # String options\n        0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9  # Fractional options (discrete)\n    ])  # Feature sampling\n}\n\n# Define the model\nmodel_with_early_stopping = GradientBoostingRegressor(\n    n_estimators=10000,  # Start with a large number of trees\n    validation_fraction=0.1,  # Reserve 10% of training data for validation\n    n_iter_no_change=10,      # Stop after 20 rounds of no improvement\n    tol=0.001,           # Tolerance for early stopping\n    random_state=42\n)\n# Define the search\nbayes_cv  = BayesSearchCV(\n    model_with_early_stopping,\n    search_space,\n    n_iter=50,  # Number of iterations\n    scoring='neg_mean_squared_error',\n    cv=5,  # Cross-validation folds\n    n_jobs=-1,  # Use all available cores\n    verbose=1,  # Verbosity level\n    random_state=42  # For reproducibility\n)\n# Fit the model\nbayes_cv.fit(X_train_final, y_train)\n# Stop the timer\nend = time.time()\n# Calculate elapsed time\nelapsed_time = (end - start)/60  # Convert to minutes\n# Print elapsed time\nprint(f\"Elapsed time for Bayesian optimization with early stopping: {elapsed_time:.2f} minutes\")\n\n# Extract the best parameters and score\nbest_params = bayes_cv.best_params_\nbest_score = np.sqrt(-bayes_cv.best_score_)\n\nprint(f\"Best Parameters: {best_params}\")\nprint(f\"Best CV RMSE: {best_score:.3f}\")\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nElapsed time for Bayesian optimization with early stopping: 10.00 minutes\nBest Parameters: OrderedDict({'learning_rate': 0.01, 'max_depth': 32, 'max_features': 0.6, 'min_samples_leaf': 1, 'min_samples_split': 2, 'subsample': 0.302790110221997})\nBest CV RMSE: 3142.483\n\n\n\n# Plot the optimization results\nplot_convergence(bayes_cv.optimizer_results_);\n\n\n\n\n\n\n\n\n\n# Plot the objective function\nplot_objective(bayes_cv.optimizer_results_[0])\nplt.title('Bayesian Optimization: Objective Function')\nplt.xlabel('Parameter Value')\nplt.ylabel('Objective Value (RMSE)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.6.2.2 Hyperparameter Optimization with Optuna\n\ndef objective(trial):\n    # Define hyperparameters to optimize\n    params = {\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.8, log=True),\n        'max_depth': trial.suggest_int('max_depth', 4, 32),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 100),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 30),\n        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n        'max_features': trial.suggest_categorical(\n            'max_features', \n            ['sqrt', 'log2', None, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n        ),\n        'n_iter_no_change': 10,  # Stop if no improvement in 50 rounds\n        'validation_fraction': 0.1,  # 10% of training data for validation\n        'tol': 0.001,  # Tolerance for early stopping\n        'n_estimators': 10000,  # Start with a large number of trees\n        'random_state': 42\n    }\n\n    # Initialize the model with the parameters, adding early stopping\n    model = GradientBoostingRegressor(\n        **params\n    )\n    model = GradientBoostingRegressor(**params)\n    # Define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # Perform cross-validation\n    scores = cross_val_score(model, X_train_final, y_train, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)\n    return np.mean(np.sqrt(-scores)) \n\n\n\nstart = time.time()\n# Create a study object\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=70, timeout=600)  # 50 trials or 10 min\n# Stop the timer\nend = time.time()\n# Calculate elapsed time\nelapsed_time = (end - start)/60  # Convert to minutes\nprint(f\"Elapsed time for Optuna optimization: {elapsed_time:.2f} minutes\")\n# Extract the best parameters and score\nbest_params_optuna = study.best_params\nbest_score_optuna = study.best_value\nprint(f\"Best Parameters: {best_params_optuna}\")\nprint(f\"Best CV RMSE: {best_score_optuna:.3f}\")\n\n[I 2025-05-08 11:07:29,094] A new study created in memory with name: no-name-84a8f2a9-1577-4460-b0fa-ddf16779a99d\n[I 2025-05-08 11:07:30,013] Trial 0 finished with value: 6402.677876019744 and parameters: {'learning_rate': 0.035073388881695464, 'max_depth': 30, 'min_samples_split': 46, 'min_samples_leaf': 26, 'subsample': 0.1177589817042714, 'max_features': 0.8}. Best is trial 0 with value: 6402.677876019744.\n[I 2025-05-08 11:07:31,621] Trial 1 finished with value: 5817.716936023959 and parameters: {'learning_rate': 0.01478220034105809, 'max_depth': 10, 'min_samples_split': 36, 'min_samples_leaf': 21, 'subsample': 0.15640172328228477, 'max_features': 0.2}. Best is trial 1 with value: 5817.716936023959.\n[I 2025-05-08 11:07:33,048] Trial 2 finished with value: 4639.695965157119 and parameters: {'learning_rate': 0.03787502889529579, 'max_depth': 7, 'min_samples_split': 42, 'min_samples_leaf': 16, 'subsample': 0.4581205314481861, 'max_features': 'log2'}. Best is trial 2 with value: 4639.695965157119.\n[I 2025-05-08 11:07:33,778] Trial 3 finished with value: 3888.640108615851 and parameters: {'learning_rate': 0.18174509299300778, 'max_depth': 17, 'min_samples_split': 78, 'min_samples_leaf': 14, 'subsample': 0.5795668347371169, 'max_features': 0.1}. Best is trial 3 with value: 3888.640108615851.\n[I 2025-05-08 11:07:34,527] Trial 4 finished with value: 5637.440291328167 and parameters: {'learning_rate': 0.1380752210199847, 'max_depth': 19, 'min_samples_split': 90, 'min_samples_leaf': 28, 'subsample': 0.23325390500017087, 'max_features': 0.7}. Best is trial 3 with value: 3888.640108615851.\n[I 2025-05-08 11:08:15,193] Trial 5 finished with value: 4349.349334283657 and parameters: {'learning_rate': 0.012573622109971098, 'max_depth': 11, 'min_samples_split': 59, 'min_samples_leaf': 29, 'subsample': 0.8662915238008442, 'max_features': None}. Best is trial 3 with value: 3888.640108615851.\n[I 2025-05-08 11:08:16,690] Trial 6 finished with value: 3538.201600150815 and parameters: {'learning_rate': 0.20506627729566493, 'max_depth': 22, 'min_samples_split': 29, 'min_samples_leaf': 1, 'subsample': 0.7644887945675666, 'max_features': 0.4}. Best is trial 6 with value: 3538.201600150815.\n[I 2025-05-08 11:08:40,300] Trial 7 finished with value: 4222.0969339973235 and parameters: {'learning_rate': 0.013692887049401567, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 29, 'subsample': 0.7647042705751687, 'max_features': 0.7}. Best is trial 6 with value: 3538.201600150815.\n[I 2025-05-08 11:08:40,844] Trial 8 finished with value: 5987.876440565123 and parameters: {'learning_rate': 0.07695507379792597, 'max_depth': 4, 'min_samples_split': 47, 'min_samples_leaf': 30, 'subsample': 0.2875628082638476, 'max_features': 0.1}. Best is trial 6 with value: 3538.201600150815.\n[I 2025-05-08 11:08:41,963] Trial 9 finished with value: 5930.236597113848 and parameters: {'learning_rate': 0.03751759109605761, 'max_depth': 23, 'min_samples_split': 89, 'min_samples_leaf': 14, 'subsample': 0.12412065494979885, 'max_features': 0.4}. Best is trial 6 with value: 3538.201600150815.\n[I 2025-05-08 11:08:43,184] Trial 10 finished with value: 4249.647691193036 and parameters: {'learning_rate': 0.7095372554176897, 'max_depth': 29, 'min_samples_split': 11, 'min_samples_leaf': 2, 'subsample': 0.9605533508261285, 'max_features': 0.4}. Best is trial 6 with value: 3538.201600150815.\n[I 2025-05-08 11:08:43,792] Trial 11 finished with value: 3721.412018003831 and parameters: {'learning_rate': 0.30049152507822724, 'max_depth': 18, 'min_samples_split': 70, 'min_samples_leaf': 4, 'subsample': 0.6346922402417411, 'max_features': 0.1}. Best is trial 6 with value: 3538.201600150815.\n[I 2025-05-08 11:08:44,200] Trial 12 finished with value: 3992.3681134868325 and parameters: {'learning_rate': 0.3980500012172675, 'max_depth': 25, 'min_samples_split': 24, 'min_samples_leaf': 1, 'subsample': 0.673568941345692, 'max_features': 'sqrt'}. Best is trial 6 with value: 3538.201600150815.\n[I 2025-05-08 11:08:45,596] Trial 13 finished with value: 3745.0231375714984 and parameters: {'learning_rate': 0.2685874728814681, 'max_depth': 18, 'min_samples_split': 64, 'min_samples_leaf': 7, 'subsample': 0.46246048029745535, 'max_features': 0.6}. Best is trial 6 with value: 3538.201600150815.\n[I 2025-05-08 11:08:46,415] Trial 14 finished with value: 4496.274561417728 and parameters: {'learning_rate': 0.5835585949486959, 'max_depth': 23, 'min_samples_split': 73, 'min_samples_leaf': 7, 'subsample': 0.6950785249386102, 'max_features': 0.5}. Best is trial 6 with value: 3538.201600150815.\n[I 2025-05-08 11:08:50,798] Trial 15 finished with value: 3532.351997763184 and parameters: {'learning_rate': 0.09745878730555008, 'max_depth': 15, 'min_samples_split': 25, 'min_samples_leaf': 6, 'subsample': 0.8242687895348413, 'max_features': 0.9}. Best is trial 15 with value: 3532.351997763184.\n[I 2025-05-08 11:08:57,644] Trial 16 finished with value: 3712.683820416855 and parameters: {'learning_rate': 0.08381685480205123, 'max_depth': 14, 'min_samples_split': 24, 'min_samples_leaf': 9, 'subsample': 0.8376225281110229, 'max_features': 0.9}. Best is trial 15 with value: 3532.351997763184.\n[I 2025-05-08 11:09:08,351] Trial 17 finished with value: 3878.4225603020363 and parameters: {'learning_rate': 0.13671338236610117, 'max_depth': 26, 'min_samples_split': 25, 'min_samples_leaf': 10, 'subsample': 0.9981438257438544, 'max_features': 0.9}. Best is trial 15 with value: 3532.351997763184.\n[I 2025-05-08 11:09:12,426] Trial 18 finished with value: 3258.6536376666345 and parameters: {'learning_rate': 0.054298295833061686, 'max_depth': 15, 'min_samples_split': 32, 'min_samples_leaf': 5, 'subsample': 0.8705317335073132, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.\n[I 2025-05-08 11:09:14,871] Trial 19 finished with value: 3299.8057419420575 and parameters: {'learning_rate': 0.05307237898099509, 'max_depth': 15, 'min_samples_split': 15, 'min_samples_leaf': 5, 'subsample': 0.8923755963839227, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.\n[I 2025-05-08 11:09:22,909] Trial 20 finished with value: 3558.1548802314146 and parameters: {'learning_rate': 0.024374699406738868, 'max_depth': 8, 'min_samples_split': 13, 'min_samples_leaf': 11, 'subsample': 0.9312911241232275, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.\n[I 2025-05-08 11:09:24,627] Trial 21 finished with value: 3344.1360986274194 and parameters: {'learning_rate': 0.05641174821037, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 5, 'subsample': 0.8413643277096872, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.\n[I 2025-05-08 11:09:26,856] Trial 22 finished with value: 3346.290872146259 and parameters: {'learning_rate': 0.05576222901451701, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 4, 'subsample': 0.8899852988884344, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.\n[I 2025-05-08 11:09:30,886] Trial 23 finished with value: 3682.572188125887 and parameters: {'learning_rate': 0.053636711663155555, 'max_depth': 20, 'min_samples_split': 12, 'min_samples_leaf': 18, 'subsample': 0.7533492065291382, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.\n[I 2025-05-08 11:09:36,517] Trial 24 finished with value: 3324.1356706329725 and parameters: {'learning_rate': 0.023446567191311374, 'max_depth': 16, 'min_samples_split': 17, 'min_samples_leaf': 5, 'subsample': 0.8976166400827316, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.\n[I 2025-05-08 11:09:48,021] Trial 25 finished with value: 3477.877832840085 and parameters: {'learning_rate': 0.02114726465183569, 'max_depth': 12, 'min_samples_split': 33, 'min_samples_leaf': 11, 'subsample': 0.925317351270135, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.\n[I 2025-05-08 11:09:51,103] Trial 26 finished with value: 3543.4521345710564 and parameters: {'learning_rate': 0.022722851512216705, 'max_depth': 16, 'min_samples_split': 16, 'min_samples_leaf': 8, 'subsample': 0.4238807472210913, 'max_features': 0.3}. Best is trial 18 with value: 3258.6536376666345.\n[I 2025-05-08 11:09:54,116] Trial 27 finished with value: 3234.0013427023514 and parameters: {'learning_rate': 0.0283832867263049, 'max_depth': 21, 'min_samples_split': 18, 'min_samples_leaf': 3, 'subsample': 0.5528086908100067, 'max_features': 0.3}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:10:00,202] Trial 28 finished with value: 3508.54835466015 and parameters: {'learning_rate': 0.046930229833580556, 'max_depth': 27, 'min_samples_split': 38, 'min_samples_leaf': 3, 'subsample': 0.5371065252040612, 'max_features': None}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:10:02,298] Trial 29 finished with value: 5004.42107047821 and parameters: {'learning_rate': 0.031927515240275074, 'max_depth': 20, 'min_samples_split': 52, 'min_samples_leaf': 21, 'subsample': 0.39582140307352154, 'max_features': 'log2'}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:10:03,666] Trial 30 finished with value: 4324.686970916422 and parameters: {'learning_rate': 0.07372182576303132, 'max_depth': 21, 'min_samples_split': 54, 'min_samples_leaf': 12, 'subsample': 0.3360673538685467, 'max_features': 0.5}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:10:11,068] Trial 31 finished with value: 3484.424151386007 and parameters: {'learning_rate': 0.028420783094759556, 'max_depth': 13, 'min_samples_split': 18, 'min_samples_leaf': 5, 'subsample': 0.7793810894071969, 'max_features': 0.8}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:10:18,699] Trial 32 finished with value: 3291.106187349627 and parameters: {'learning_rate': 0.015581688189605165, 'max_depth': 17, 'min_samples_split': 19, 'min_samples_leaf': 3, 'subsample': 0.7003002093460347, 'max_features': 0.3}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:10:22,168] Trial 33 finished with value: 3325.6927625938033 and parameters: {'learning_rate': 0.01749834216320663, 'max_depth': 9, 'min_samples_split': 34, 'min_samples_leaf': 2, 'subsample': 0.5707418196808265, 'max_features': 0.2}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:10:27,550] Trial 34 finished with value: 3243.0601177315375 and parameters: {'learning_rate': 0.01860910283944705, 'max_depth': 32, 'min_samples_split': 40, 'min_samples_leaf': 3, 'subsample': 0.519199766539664, 'max_features': 0.3}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:10:36,064] Trial 35 finished with value: 3258.8499173154833 and parameters: {'learning_rate': 0.010243612859981979, 'max_depth': 30, 'min_samples_split': 43, 'min_samples_leaf': 1, 'subsample': 0.623847104984506, 'max_features': 0.3}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:10:40,944] Trial 36 finished with value: 4733.54257152398 and parameters: {'learning_rate': 0.010552375239783827, 'max_depth': 31, 'min_samples_split': 44, 'min_samples_leaf': 24, 'subsample': 0.4832981433471137, 'max_features': 'sqrt'}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:10:55,316] Trial 37 finished with value: 3330.1658474531714 and parameters: {'learning_rate': 0.011303182659432521, 'max_depth': 32, 'min_samples_split': 40, 'min_samples_leaf': 1, 'subsample': 0.6192904496322136, 'max_features': 0.6}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:11:08,984] Trial 38 finished with value: 3657.4081420035955 and parameters: {'learning_rate': 0.01817928172328691, 'max_depth': 29, 'min_samples_split': 47, 'min_samples_leaf': 8, 'subsample': 0.5215847207874392, 'max_features': 0.8}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:11:11,528] Trial 39 finished with value: 3246.3324692045935 and parameters: {'learning_rate': 0.03864849232157439, 'max_depth': 28, 'min_samples_split': 58, 'min_samples_leaf': 2, 'subsample': 0.600994007175315, 'max_features': 0.2}. Best is trial 27 with value: 3234.0013427023514.\n[I 2025-05-08 11:11:14,072] Trial 40 finished with value: 3223.908916424979 and parameters: {'learning_rate': 0.04161574178039716, 'max_depth': 27, 'min_samples_split': 61, 'min_samples_leaf': 3, 'subsample': 0.38840542153155744, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.\n[I 2025-05-08 11:11:16,764] Trial 41 finished with value: 3262.2664150628366 and parameters: {'learning_rate': 0.040900390028730825, 'max_depth': 28, 'min_samples_split': 57, 'min_samples_leaf': 3, 'subsample': 0.39000153769914736, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.\n[I 2025-05-08 11:11:18,027] Trial 42 finished with value: 3536.1048831110393 and parameters: {'learning_rate': 0.0373869523765574, 'max_depth': 25, 'min_samples_split': 63, 'min_samples_leaf': 3, 'subsample': 0.22414441067489127, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.\n[I 2025-05-08 11:11:20,006] Trial 43 finished with value: 3725.757422297686 and parameters: {'learning_rate': 0.028313793871723975, 'max_depth': 31, 'min_samples_split': 79, 'min_samples_leaf': 6, 'subsample': 0.33977679725241394, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.\n[I 2025-05-08 11:11:21,612] Trial 44 finished with value: 3401.5545066433783 and parameters: {'learning_rate': 0.06669527321370879, 'max_depth': 24, 'min_samples_split': 48, 'min_samples_leaf': 4, 'subsample': 0.48718984715872016, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.\n[I 2025-05-08 11:11:25,264] Trial 45 finished with value: 3635.4930797564184 and parameters: {'learning_rate': 0.0961058290284304, 'max_depth': 27, 'min_samples_split': 30, 'min_samples_leaf': 7, 'subsample': 0.5935867492892174, 'max_features': 0.7}. Best is trial 40 with value: 3223.908916424979.\n[I 2025-05-08 11:11:26,855] Trial 46 finished with value: 3297.8485341280875 and parameters: {'learning_rate': 0.04204785692165129, 'max_depth': 32, 'min_samples_split': 61, 'min_samples_leaf': 2, 'subsample': 0.5167714519960092, 'max_features': 'log2'}. Best is trial 40 with value: 3223.908916424979.\n[I 2025-05-08 11:11:28,066] Trial 47 finished with value: 3484.8710748592084 and parameters: {'learning_rate': 0.031490405131204394, 'max_depth': 29, 'min_samples_split': 68, 'min_samples_leaf': 1, 'subsample': 0.20255680688589325, 'max_features': 0.2}. Best is trial 40 with value: 3223.908916424979.\n[I 2025-05-08 11:11:29,916] Trial 48 finished with value: 5067.6932650619365 and parameters: {'learning_rate': 0.13316769809796983, 'max_depth': 22, 'min_samples_split': 81, 'min_samples_leaf': 18, 'subsample': 0.31148972269250275, 'max_features': None}. Best is trial 40 with value: 3223.908916424979.\n[I 2025-05-08 11:11:33,570] Trial 49 finished with value: 3519.5312534022605 and parameters: {'learning_rate': 0.020121256369718885, 'max_depth': 27, 'min_samples_split': 37, 'min_samples_leaf': 6, 'subsample': 0.43073583851342867, 'max_features': 0.1}. Best is trial 40 with value: 3223.908916424979.\n[I 2025-05-08 11:11:38,292] Trial 50 finished with value: 4947.269431523515 and parameters: {'learning_rate': 0.01401704983937264, 'max_depth': 30, 'min_samples_split': 56, 'min_samples_leaf': 13, 'subsample': 0.2680374684875922, 'max_features': 0.4}. Best is trial 40 with value: 3223.908916424979.\n[I 2025-05-08 11:11:42,794] Trial 51 finished with value: 3223.184387572671 and parameters: {'learning_rate': 0.026412005607226167, 'max_depth': 30, 'min_samples_split': 42, 'min_samples_leaf': 2, 'subsample': 0.65936728502067, 'max_features': 0.3}. Best is trial 51 with value: 3223.184387572671.\n[I 2025-05-08 11:11:47,351] Trial 52 finished with value: 3215.28175313329 and parameters: {'learning_rate': 0.026909377347601734, 'max_depth': 31, 'min_samples_split': 30, 'min_samples_leaf': 2, 'subsample': 0.6638539074973421, 'max_features': 0.3}. Best is trial 52 with value: 3215.28175313329.\n[I 2025-05-08 11:11:51,995] Trial 53 finished with value: 3251.8248895524334 and parameters: {'learning_rate': 0.029697182156018534, 'max_depth': 31, 'min_samples_split': 51, 'min_samples_leaf': 2, 'subsample': 0.6712307703997793, 'max_features': 0.2}. Best is trial 52 with value: 3215.28175313329.\n[I 2025-05-08 11:12:03,513] Trial 54 finished with value: 3397.6799413335893 and parameters: {'learning_rate': 0.01643638758357564, 'max_depth': 28, 'min_samples_split': 67, 'min_samples_leaf': 3, 'subsample': 0.5673934972094804, 'max_features': 0.7}. Best is trial 52 with value: 3215.28175313329.\n[I 2025-05-08 11:12:10,304] Trial 55 finished with value: 3260.8279178294406 and parameters: {'learning_rate': 0.026362863130942972, 'max_depth': 30, 'min_samples_split': 27, 'min_samples_leaf': 4, 'subsample': 0.7153814770338778, 'max_features': 0.3}. Best is trial 52 with value: 3215.28175313329.\n[I 2025-05-08 11:12:12,294] Trial 56 finished with value: 3192.9791005502093 and parameters: {'learning_rate': 0.03446122898299111, 'max_depth': 25, 'min_samples_split': 22, 'min_samples_leaf': 1, 'subsample': 0.6677588032576087, 'max_features': 'sqrt'}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:12:13,927] Trial 57 finished with value: 3269.998258903558 and parameters: {'learning_rate': 0.033447603873263364, 'max_depth': 26, 'min_samples_split': 21, 'min_samples_leaf': 1, 'subsample': 0.669383626299888, 'max_features': 'sqrt'}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:12:17,716] Trial 58 finished with value: 3457.122430183835 and parameters: {'learning_rate': 0.019680446694802907, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 8, 'subsample': 0.6399247722507589, 'max_features': 'sqrt'}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:12:20,307] Trial 59 finished with value: 3247.949537776105 and parameters: {'learning_rate': 0.04792784341369982, 'max_depth': 32, 'min_samples_split': 29, 'min_samples_leaf': 4, 'subsample': 0.7314524099096733, 'max_features': 'sqrt'}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:12:51,502] Trial 60 finished with value: 4050.280737550346 and parameters: {'learning_rate': 0.013362536181651579, 'max_depth': 25, 'min_samples_split': 21, 'min_samples_leaf': 26, 'subsample': 0.7856311694762674, 'max_features': 0.6}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:12:56,091] Trial 61 finished with value: 3249.9684847878243 and parameters: {'learning_rate': 0.025286437646103447, 'max_depth': 28, 'min_samples_split': 35, 'min_samples_leaf': 2, 'subsample': 0.5916004227404528, 'max_features': 0.3}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:13:01,774] Trial 62 finished with value: 3328.4822044271473 and parameters: {'learning_rate': 0.037252636958638265, 'max_depth': 29, 'min_samples_split': 60, 'min_samples_leaf': 2, 'subsample': 0.5509938512062063, 'max_features': 0.5}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:13:12,800] Trial 63 finished with value: 3490.4301182885783 and parameters: {'learning_rate': 0.04480753984980458, 'max_depth': 26, 'min_samples_split': 40, 'min_samples_leaf': 6, 'subsample': 0.6501238052668078, 'max_features': 0.9}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:13:15,262] Trial 64 finished with value: 3540.8369062274187 and parameters: {'learning_rate': 0.0651918522339886, 'max_depth': 5, 'min_samples_split': 50, 'min_samples_leaf': 1, 'subsample': 0.6047519410402453, 'max_features': 0.3}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:13:20,471] Trial 65 finished with value: 3294.353628295925 and parameters: {'learning_rate': 0.02196859905859851, 'max_depth': 23, 'min_samples_split': 98, 'min_samples_leaf': 3, 'subsample': 0.5103851076291236, 'max_features': 0.3}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:13:29,893] Trial 66 finished with value: 3326.4094166662317 and parameters: {'learning_rate': 0.03527083345645445, 'max_depth': 31, 'min_samples_split': 23, 'min_samples_leaf': 4, 'subsample': 0.687510121444283, 'max_features': 0.4}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:13:44,533] Trial 67 finished with value: 3548.591414006273 and parameters: {'learning_rate': 0.026050250772540858, 'max_depth': 29, 'min_samples_split': 74, 'min_samples_leaf': 16, 'subsample': 0.8079428723552805, 'max_features': 0.3}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:13:51,439] Trial 68 finished with value: 3228.1005724210245 and parameters: {'learning_rate': 0.018440524791042332, 'max_depth': 28, 'min_samples_split': 45, 'min_samples_leaf': 5, 'subsample': 0.7401907036295743, 'max_features': 0.1}. Best is trial 56 with value: 3192.9791005502093.\n[I 2025-05-08 11:13:58,247] Trial 69 finished with value: 3264.495066558836 and parameters: {'learning_rate': 0.015627930519455754, 'max_depth': 19, 'min_samples_split': 31, 'min_samples_leaf': 5, 'subsample': 0.6551335248065494, 'max_features': 0.1}. Best is trial 56 with value: 3192.9791005502093.\n\n\nElapsed time for Optuna optimization: 6.49 minutes\nBest Parameters: {'learning_rate': 0.03446122898299111, 'max_depth': 25, 'min_samples_split': 22, 'min_samples_leaf': 1, 'subsample': 0.6677588032576087, 'max_features': 'sqrt'}\nBest CV RMSE: 3192.979\n\n\n\noptuna.visualization.plot_optimization_history(study).show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\noptuna.visualization.plot_param_importances(study).show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\noptuna.visualization.plot_parallel_coordinate(study).show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Gradient_Boosting.html#independent-study",
    "href": "Gradient_Boosting.html#independent-study",
    "title": "9  Gradient Boosting",
    "section": "9.7 Independent Study",
    "text": "9.7 Independent Study\nIn this notebook, we used the car dataset for a guided regression task to illustrate the core hyperparameters in gradient boosting and how to tune them to balance bias and variance.\nFor your practice, please work with the diabetes dataset and complete the following:\n\nFit a baseline gradient boosting classifier.\nTune key hyperparameters: learning_rate, n_estimators, max_depth, and subsample.\nUse early stopping to determine the optimal number of trees.\nCompare training and test roc_auc before and after tuning.\nVisualize the learning curve (training vs test error across iterations).\nSummarize what combination of hyperparameters yielded the best performance and how they impacted bias and variance.\n\nFeel free to use GridSearchCV, BayesSearchCV, or other tuning as you prefer.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nprint(train.shape, test.shape)\ntrain.head()\n\n(614, 9) (154, 9)\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n2\n88\n74\n19\n53\n29.0\n0.229\n22\n0\n\n\n1\n2\n129\n84\n0\n0\n28.0\n0.284\n27\n0\n\n\n2\n0\n102\n78\n40\n90\n34.5\n0.238\n24\n0\n\n\n3\n0\n123\n72\n0\n0\n36.3\n0.258\n52\n1\n\n\n4\n1\n144\n82\n46\n180\n46.1\n0.335\n46\n1\n\n\n\n\n\n\n\n\n# check the distribution of the target variable\ntrain['Outcome'].value_counts(normalize=True)\n\nOutcome\n0    0.662866\n1    0.337134\nName: proportion, dtype: float64\n\n\n\n# define the features and target variable\nX_train = train.drop(columns=['Outcome'])\ny_train = train['Outcome']\nX_test = test.drop(columns=['Outcome'])\ny_test = test['Outcome']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Gradient_Boosting.html#foundational-paper",
    "href": "Gradient_Boosting.html#foundational-paper",
    "title": "9  Gradient Boosting",
    "section": "9.8 Foundational Paper",
    "text": "9.8 Foundational Paper\nThe foundational paper introducing “vanilla” Gradient Boosting is:\nGreedy Function Approximation: A Gradient Boosting Machine\nAuthor: Jerome H. Friedman\nPublished in: The Annals of Statistics, 2001, Vol. 29, No. 5, pp. 1189–1232\nDOI: 10.1214/aos/1013203451",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "XGBoost.html",
    "href": "XGBoost.html",
    "title": "10  XGBoost",
    "section": "",
    "text": "10.1 What is XGBoost?\nXGBoost (Extreme Gradient Boosting) is a scalable and efficient implementation of gradient boosting developed by Tianqi Chen and Carlos Guestrin in 2016. It has become one of the most popular machine learning algorithms for structured/tabular data, widely used in Kaggle competitions and production environments.\nCompared to vanilla Gradient Boosting, XGBoost includes additional system-level and algorithmic optimizations such as:",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#what-is-xgboost",
    "href": "XGBoost.html#what-is-xgboost",
    "title": "10  XGBoost",
    "section": "",
    "text": "Regularization (to reduce overfitting)\nTree pruning\nParallelized tree construction\nMissing value handling\nOut-of-core computation for large datasets",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#xgboost-intuition",
    "href": "XGBoost.html#xgboost-intuition",
    "title": "10  XGBoost",
    "section": "10.2 XGBoost Intuition",
    "text": "10.2 XGBoost Intuition\nXGBoost extends vanilla gradient boosting with:\n\nRegularization: Penalizes complex models via L1/L2 terms in the loss function.\nSecond-order optimization: Uses both gradients and hessians for faster convergence and more accurate splits.\nSplit constraints: Prevents splits with insufficient gain (via gamma) during tree growth, avoiding the need for post-pruning.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#how-xgboost-works-regression-example",
    "href": "XGBoost.html#how-xgboost-works-regression-example",
    "title": "10  XGBoost",
    "section": "10.3 How XGBoost Works (Regression Example)",
    "text": "10.3 How XGBoost Works (Regression Example)\nXGBoost minimizes the following regularized objective at each boosting round \\(t\\):\n\\[\n\\mathcal{L}^{(t)} = \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\Omega(f_t)\n\\]\nWhere: - \\(l\\) is a differentiable convex loss function (e.g., squared error) - \\(f_t\\) is the prediction function at iteration \\(t\\) (a tree) - \\(\\Omega(f_t) = \\gamma T + \\frac{1}{2} \\lambda \\sum w_j^2\\) is the regularization term (penalizes the number of leaves \\(T\\) and leaf weights \\(w_j\\))\nTo simplify optimization, XGBoost applies a second-order Taylor approximation of the loss function:\n\\[\n\\mathcal{L}^{(t)} \\approx \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n\\]\nWhere: - \\(g_i = \\frac{\\partial l(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}\\) is the first-order derivative (gradient) - \\(h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i^2}\\) is the second-order derivative (hessian)\nXGBoost then chooses the tree structure and leaf values that minimize this approximate objective.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#using-xgboost",
    "href": "XGBoost.html#using-xgboost",
    "title": "10  XGBoost",
    "section": "10.4 Using XGBoost",
    "text": "10.4 Using XGBoost\nAlthough XGBoost is not part of Scikit-learn, it provides a Scikit-learn-compatible API through the xgboost.sklearn module. This allows you to use XGBoost models seamlessly with Scikit-learn tools such as Pipeline, GridSearchCV, and cross_val_score.\nThe main classes are:\n\nXGBRegressor: for regression tasks\n\nXGBClassifier: for classification tasks\n\nTo install the package:\npip install xgboost\n\nNote: XGBoost is a separate library, not part of Scikit-learn, but it provides a Scikit-learn-compatible API via XGBClassifier and XGBRegressor.\nThis makes it easy to integrate XGBoost models into Scikit-learn workflows such as Pipeline, GridSearchCV, and cross_val_score.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#core-hyperparameter-categories",
    "href": "XGBoost.html#core-hyperparameter-categories",
    "title": "10  XGBoost",
    "section": "10.5 Core Hyperparameter Categories",
    "text": "10.5 Core Hyperparameter Categories\n\n10.5.1 Model Complexity\n\nn_estimators: Number of boosting rounds\n\nmax_depth: Maximum depth of a tree\n\nmin_child_weight: Minimum sum of instance weight needed in a child\n\n\n\n10.5.2 Learning and Regularization\n\nlearning_rate (eta): Shrinkage rate to scale each tree’s contribution\n\nsubsample: Fraction of rows used per boosting round\n\ncolsample_bytree: Fraction of features used per tree\n\ncolsample_bylevel, colsample_bynode: Further control over feature subsampling\n\n\n\n10.5.3 Regularization\n\ngamma: Minimum loss reduction required to make a further partition\n\nreg_alpha: L1 regularization term on weights (Lasso)\n\nreg_lambda: L2 regularization term on weights (Ridge)\n\n\n\n10.5.4 Optimization Control\n\nobjective: Loss function (e.g., 'reg:squarederror', 'binary:logistic')\n\ntree_method: Tree construction algorithm ('auto', 'hist', 'gpu_hist')\n\nearly_stopping_rounds: Stop if validation score doesn’t improve after N rounds\n\nHowever, there are other hyperparameters that can be tuned as well. Check out the list of all hyperparameters in the XGBoost documentation.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import root_mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\nfrom xgboost import XGBRegressor, XGBClassifier\nimport seaborn as sns\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_convergence\nimport warnings\nfrom IPython import display\n\n\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\ncar.head()\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\nvw\nBeetle\n2014\nManual\n55457\nDiesel\n30\n65.3266\n1.6\n7490\n\n\n1\nvauxhall\nGTC\n2017\nManual\n15630\nPetrol\n145\n47.2049\n1.4\n10998\n\n\n2\nmerc\nG Class\n2012\nAutomatic\n43000\nDiesel\n570\n25.1172\n3.0\n44990\n\n\n3\naudi\nRS5\n2019\nAutomatic\n10\nPetrol\n145\n30.5593\n2.9\n51990\n\n\n4\nmerc\nX-CLASS\n2018\nAutomatic\n14000\nDiesel\n240\n35.7168\n2.3\n28990\n\n\n\n\n\n\n\n\nX = car.drop(columns=['price'])\ny = car['price']\n\n\n# Identify categorical and numerical columns\ncategorical_cols = X.select_dtypes(include=['object']).columns.tolist()\nnumerical_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nLet’s define some helper functions before building any models.\n\n# Create preprocessing for numerical and categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', numerical_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n    ]\n)\n\n# Function to evaluate model\ndef evaluate_model(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    rmse = root_mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f\"Root Mean Squared Error: {rmse:.2f}\")\n    print(f\"R² Score: {r2:.4f}\")\n    \n    return rmse,  r2\n\n# Function to plot feature importance\ndef plot_feature_importance(model, preprocessor, X):\n    if hasattr(model, 'feature_importances_'):\n        # Get feature names after one-hot encoding\n        cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n        all_features = np.append(numerical_cols, cat_features)\n        \n        # Get feature importances\n        importances = model.feature_importances_\n        \n        # Sort feature importances in descending order\n        indices = np.argsort(importances)[::-1]\n        \n        # Create a DataFrame for easier visualization\n        importance_df = pd.DataFrame({\n            'Feature': all_features[indices][:20],  # Top 20 features\n            'Importance': importances[indices][:20]\n        })\n        importance_df = importance_df.sort_values(by='Importance', ascending=False)\n        return importance_df\n\n\n\n10.5.5 Baseline Model\n\n# ===== 1. Baseline Model =====\nprint(\"\\n===== Baseline XGBoost Model =====\")\nbaseline_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', XGBRegressor(random_state=42))\n])\n\nbaseline_pipeline.fit(X_train, y_train)\nprint(\"\\nBaseline Model Evaluation:\")\nbaseline_metrics = evaluate_model(baseline_pipeline, X_test, y_test)\n\n# Plot feature importance for baseline model\nbaseline_importance = plot_feature_importance(baseline_pipeline.named_steps['regressor'], \n                                             baseline_pipeline.named_steps['preprocessor'], \n                                             X)\n\n\n===== Baseline XGBoost Model =====\n\nBaseline Model Evaluation:\nRoot Mean Squared Error: 3354.16\nR² Score: 0.9617\n\n\n\n\n10.5.6 Early Stopping in XGBoost\nEarly stopping is a technique that stops training when the model’s performance on a validation set stops improving, helping to prevent overfitting and reduce training time.\n\n10.5.6.1 How It Works\nAt each boosting round, XGBoost tracks a performance metric (e.g., RMSE or log loss) on a validation set. If the metric doesn’t improve for a specified number of rounds (early_stopping_rounds), training is halted.\n\nSaves computation by avoiding unnecessary boosting rounds.\nReturns the model from the best iteration (with the lowest validation error).\n\n\n\n10.5.6.2 Requirements\n\nYou must provide an eval_set containing a validation set.\nThe evaluation metric must be one that XGBoost can track (eval_metric is optional but recommended).\n\n\n# ===== 2. Early Stopping =====\nprint(\"\\n===== XGBoost with Early Stopping =====\")\n# Create validation set for early stopping\nX_train_es, X_val, y_train_es, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n\n# Preprocess the validation set\npreprocessor_fit = preprocessor.fit(X_train_es)\nX_train_es_transformed = preprocessor_fit.transform(X_train_es)\nX_val_transformed = preprocessor_fit.transform(X_val)\n\n\n# Train with early stopping\nearly_stop_model = XGBRegressor(\n    random_state=42,\n    n_estimators=1000,\n    early_stopping_rounds=20\n)\nearly_stop_model.fit(\n    X_train_es_transformed, y_train_es,\n    eval_set=[(X_val_transformed, y_val)],\n    verbose=False\n)\n\n\n===== XGBoost with Early Stopping =====\n\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=20,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n             n_jobs=None, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressor?Documentation for XGBRegressoriFittedXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=20,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n             n_jobs=None, num_parallel_tree=None, ...) \n\n\n\n# Update the pipeline with the best model\nearly_stop_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', XGBRegressor(\n        random_state=42,\n        n_estimators=early_stop_model.best_iteration,  # Use the best number of iterations\n    ))\n])\n\nearly_stop_pipeline.fit(X_train, y_train)\nprint(\"\\nEarly Stopping Model Evaluation:\")\nearly_stop_metrics = evaluate_model(early_stop_pipeline, X_test, y_test)\nprint(f\"Best number of iterations: {early_stop_model.best_iteration}\")\n\n\nEarly Stopping Model Evaluation:\nRoot Mean Squared Error: 3332.03\nR² Score: 0.9622\nBest number of iterations: 193\n\n\n\n\n\n10.5.7 gamma in XGBoost\nDefinition:\ngamma (also called min_split_loss) specifies the minimum loss reduction required to make a further partition (split) on a leaf node of the tree.\nHow it works:\n\nDuring tree construction, XGBoost evaluates whether splitting a node reduces the overall training loss.\nIf the reduction in loss is less than gamma, the split is discarded, and the node becomes a leaf.\nHigher values of gamma make the algorithm more conservative, leading to simpler trees.\n\nFormula:\nAt each split, XGBoost calculates the gain (reduction in regularized loss):\n\\[\n\\text{Gain} = \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right) - \\gamma\n\\]\nWhere: - \\(G_L\\), \\(H_L\\): gradient and hessian sums for the left child - \\(G_R\\), \\(H_R\\): gradient and hessian sums for the right child - \\(\\lambda\\): L2 regularization term - \\(\\gamma\\): minimum loss reduction required to make a split\nEffect of gamma:\n\n\n\n\n\n\n\n\ngamma Value\nBehavior\nRisk\n\n\n\n\n0 (default)\nMost splits are allowed\nOverfitting possible\n\n\nModerate\nSmall-gain splits are blocked\nMore robust trees\n\n\nHigh\nVery few splits allowed\nUnderfitting possible\n\n\n\nUse case: - Tune gamma to prune noisy or unnecessary splits. - Helpful when the model is overfitting, especially on small datasets.\nExample:\nXGBRegressor(gamma=1.0)\n\n# ===== 3. Regularization Experiments: varying gamma =====\n\nprint(\"\\n===== XGBoost with Regularization: Varying Gamma =====\")\n# Define the parameter grid for gamma\nparam_grid = {\n    'regressor__gamma': [0, 0.1, 0.5, 1, 5, 10, 100],\n}\n# Create a new pipeline for the regularization experiment\nregularization_gamma_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', XGBRegressor(random_state=42, n_estimators=early_stop_model.best_iteration))\n])\n\n# Perform grid search with cross-validation\ngrid_search_gamma = GridSearchCV(\n    regularization_gamma_pipeline,\n    param_grid,\n    scoring='neg_mean_squared_error',\n    n_jobs=-1\n)\n\ngrid_search_gamma.fit(X_train, y_train)\nprint(\"\\nBest Parameters from Regularization Tuning (γ - gamma)::\")\nprint(grid_search_gamma.best_params_)\nprint(\"Best Cross-Validation RMSE: {:.2f}\".format(np.sqrt(-grid_search_gamma.best_score_)))\n\n# Evaluate the best model from grid search\nbest_gamma_model = grid_search_gamma.best_estimator_\nprint(\"\\nTest Set Evaluation for Best Model from Gamma Regularization Tuning:\")\nregularization_metrics = evaluate_model(best_gamma_model, X_test, y_test)\n\n\n===== XGBoost with Regularization: Varying Gamma =====\n\nBest Parameters from Regularization Tuning (γ - gamma)::\n{'regressor__gamma': 100}\nBest Cross-Validation RMSE: 3428.44\n\nTest Set Evaluation for Best Model from Gamma Regularization Tuning:\nRoot Mean Squared Error: 3332.02\nR² Score: 0.9622\n\n\n\n# Extract gamma values and corresponding mean CV RMSE\ngamma_values = grid_search_gamma.cv_results_['param_regressor__gamma'].data\n\n# Convert mean test scores to RMSE, rounding to 2 decimal places\nmean_rmse_scores = np.sqrt(-grid_search_gamma.cv_results_['mean_test_score'])\n# Round the RMSE scores to 2 decimal places\nmean_rmse_scores = np.round(mean_rmse_scores, 4)\n\n# Plot gamma vs RMSE with plain y-axis tick labels\nplt.figure(figsize=(8, 5))\nplt.plot(gamma_values, mean_rmse_scores, marker='o')\nplt.xlabel('Gamma (min_split_loss)', fontsize=12)\nplt.ylabel('CV RMSE', fontsize=12)\nplt.title('Effect of gamma on XGBoost Performance', fontsize=14)\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nEffect of gamma on XGBoost Performance\nAs gamma increases, XGBoost becomes more selective about making splits.\n\nLow gamma (0–5): Trees grow freely → higher RMSE due to possible overfitting.\nModerate to high gamma (10–100): Blocks weak splits → simpler trees with better validation performance.\n\nIn this dataset, higher gamma values improved generalization by preventing unnecessary splits.\n\n\n10.5.8 reg_lambda and reg_alpha in XGBoost\nXGBoost includes regularization to help prevent overfitting by penalizing complex trees.\n\nreg_lambda (L2 regularization):\n\nPenalizes large leaf weights using a squared penalty.\nEncourages smaller, smoother weight values (like Ridge regression).\nHelps when many features contribute weakly.\n\nreg_alpha (L1 regularization):\n\nPenalizes absolute values of leaf weights.\nCan shrink some weights to zero, effectively performing feature selection (like Lasso).\nUseful when you expect only a few strong features.\n\n\nObjective function with regularization: \\[\n\\mathcal{L} = \\text{Loss} + \\gamma T + \\frac{1}{2} \\lambda \\sum_j w_j^2 + \\alpha \\sum_j |w_j|\n\\]\nWhere: - \\(\\text{Loss}\\): training loss (e.g., squared error or log loss) - \\(T\\): number of leaves in the tree - \\(w_j\\): weight of the \\(j\\)-th leaf - \\(\\lambda\\): L2 regularization (Ridge penalty) - \\(\\alpha\\): L1 regularization (Lasso penalty) - \\(\\gamma\\): cost for adding a new leaf (controls tree growth)\nUnderstanding them via Ridge, Lasso, and ElasticNet you learned in STAT303-2\nJust like Ridge/Lasso regularization helps linear models generalize better, reg_lambda and reg_alpha help XGBoost prevent overfitting by controlling how complex the trees become through leaf weight penalties.\n\nreg_alpha = 0 → No L1 penalty, behaves like Ridge (only L2 used)\nreg_lambda = 0 → No L2 penalty, behaves like Lasso (only L1 used)\nreg_alpha &gt; 0 and reg_lambda &gt; 0 → behaves like ElasticNet\n\nThis analogy helps understand how XGBoost controls model complexity:\n\n\n\n\n\n\n\nSetting\nBehavior\n\n\n\n\nreg_alpha=0, reg_lambda&gt;0\nLike Ridge → smooth leaf weights, all included\n\n\nreg_alpha&gt;0, reg_lambda=0\nLike Lasso → some leaf weights may shrink to zero\n\n\nBoth &gt; 0\nLike ElasticNet → balance shrinkage and sparsity\n\n\nBoth = 0 (default)\nNo regularization → may overfit on small/noisy data\n\n\n\n\n# ===== 3. Regularization Experiments: tuning reg_lambda and reg_alpha =====\nprint(\"\\n===== Exploring Regularization Parameters: reg_lambda and reg_alpha =====\")\n\n# Define the parameter grid for reg_lambda and reg_alpha\nparam_grid_reg = {\n    'regressor__reg_lambda': [0, 0.1, 0.5, 1, 5, 10, 100],\n    'regressor__reg_alpha': [0, 0.1, 0.5, 1, 5, 10, 100],\n}\n\n# Create a new pipeline for the regularization experiment\nregularization_lambda_alpha_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', XGBRegressor(random_state=42, n_estimators=early_stop_model.best_iteration))\n])\n\n# Perform grid search with cross-validation\ngrid_search_lambda_alpha_reg = GridSearchCV(\n    regularization_lambda_alpha_pipeline,\n    param_grid_reg,\n    scoring='neg_mean_squared_error',\n    n_jobs=-1\n)\ngrid_search_lambda_alpha_reg.fit(X_train, y_train)\nprint(\"\\nBest Parameters from Lambda and Alpha Regularization Tuning:\")\nprint(grid_search_lambda_alpha_reg.best_params_)\nprint(\"Best Cross-Validation RMSE: {:.2f}\".format(np.sqrt(-grid_search_lambda_alpha_reg.best_score_)))\n\n# Evaluate the best model from grid search\nbest_lambda_alpha_model = grid_search_lambda_alpha_reg.best_estimator_\nprint(\"\\nTest Set Evaluation for Best Regularization Model (λ and α):\")\nregularization_metrics_reg = evaluate_model(best_lambda_alpha_model, X_test, y_test)\n\n\n===== Exploring Regularization Parameters: reg_lambda and reg_alpha =====\n\nBest Parameters from Lambda and Alpha Regularization Tuning:\n{'regressor__reg_alpha': 1, 'regressor__reg_lambda': 0}\nBest Cross-Validation RMSE: 3361.75\n\nTest Set Evaluation for Best Regularization Model (λ and α):\nRoot Mean Squared Error: 3551.09\nR² Score: 0.9570\n\n\n\n# Extract reg_lambda and reg_alpha values and corresponding mean CV RMSE\nreg_lambda_values = grid_search_lambda_alpha_reg.cv_results_['param_regressor__reg_lambda'].data\nreg_alpha_values = grid_search_lambda_alpha_reg.cv_results_['param_regressor__reg_alpha'].data\n\n# Convert mean test scores to RMSE, rounding to 2 decimal places\nmean_rmse_scores_reg = np.sqrt(-grid_search_lambda_alpha_reg.cv_results_['mean_test_score'])\n# Round the RMSE scores to 2 decimal places\nmean_rmse_scores_reg = np.round(mean_rmse_scores_reg, 4)\n\n# Create a DataFrame for easier plotting\nreg_lambda_alpha_results_df = pd.DataFrame({\n    'reg_lambda': reg_lambda_values,\n    'reg_alpha': reg_alpha_values,\n    'mean_rmse': mean_rmse_scores_reg\n})\n\n# Pivot the DataFrame for heatmap\nregreg_lambda_alpha_results_df_pivot_df = reg_lambda_alpha_results_df.pivot(index='reg_lambda', columns='reg_alpha', values='mean_rmse')\n\n# Plotting the heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(regreg_lambda_alpha_results_df_pivot_df, annot=True, fmt=\".2f\", cmap='viridis', cbar_kws={'label': 'CV RMSE'})\nplt.title('Effect of reg_lambda and reg_alpha on XGBoost Performance', fontsize=14)\nplt.xlabel('reg_alpha', fontsize=12)\nplt.ylabel('reg_lambda', fontsize=12)\nplt.tight_layout();\n\n\n\n\n\n\n\n\n\n\n10.5.9 Exploring Regularization Hyperparameters Simultaneously\nIn addition to gamma, reg_lambda, and reg_alpha, the parameters max_depth and min_child_weight also control the complexity of XGBoost models. These parameters behave similarly to how they work in other tree-based models.\nRather than tuning them in isolation, it’s important to recognize that these parameters interact with one another. In the next step, we will tune them simultaneously to better capture their combined effect on model performance.\n\n\n# ===== 3. Regularization Experiments: Simultaneous Exploration  =====\nprint(\"\\n===== Exploring Regularization Parameters Simultaneously =====\")\n# Define regularization parameters to test\nreg_params = [\n    {'regressor__max_depth': 3, 'regressor__min_child_weight': 1, 'regressor__gamma': 0, \n     'regressor__reg_alpha': 0, 'regressor__reg_lambda': 1},\n    {'regressor__max_depth': 3, 'regressor__min_child_weight': 1, 'regressor__gamma': 0, \n     'regressor__reg_alpha': 1, 'regressor__reg_lambda': 1},\n    {'regressor__max_depth': 5, 'regressor__min_child_weight': 3, 'regressor__gamma': 0.1, \n     'regressor__reg_alpha': 0, 'regressor__reg_lambda': 1},\n    {'regressor__max_depth': 5, 'regressor__min_child_weight': 3, 'regressor__gamma': 0.1, \n     'regressor__reg_alpha': 1, 'regressor__reg_lambda': 5},\n    {'regressor__max_depth': 7, 'regressor__min_child_weight': 1, 'regressor__gamma': 0.2, \n     'regressor__reg_alpha': 5, 'regressor__reg_lambda': 10}\n]\n\n# Store results for comparison\nreg_results = []\n\nfor i, params in enumerate(reg_params):\n    print(f\"\\nRegularization Test {i+1}:\")\n    print(params)\n    \n    # Create pipeline with these parameters\n    reg_pipeline = Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', XGBRegressor(\n            random_state=42,\n            n_estimators=early_stop_model.best_iteration,\n            **{k.replace('regressor__', ''): v for k, v in params.items()}\n        ))\n    ])\n    \n    # Train and evaluate\n    reg_pipeline.fit(X_train, y_train)\n    print(\"\\nModel Evaluation:\")\n    metrics = evaluate_model(reg_pipeline, X_test, y_test)\n    rmse, r2 = metrics\n\n    # Store results\n    reg_results.append({\n        'test': i+1,\n        'params': params,\n        'rmse': rmse,\n        'r2': r2\n    })\n\n# Find best regularization parameters\nreg_df = pd.DataFrame(reg_results)\nbest_reg_idx = reg_df['rmse'].idxmin()\nbest_reg_params = reg_df.loc[best_reg_idx, 'params']\nprint(\"\\nBest Regularization Parameters:\")\nprint(best_reg_params)\nprint(\"Best RMSE: {:.2f}\".format(reg_df['rmse'].min()))\n\n\n===== Exploring Regularization Parameters Simultaneously =====\n\nRegularization Test 1:\n{'regressor__max_depth': 3, 'regressor__min_child_weight': 1, 'regressor__gamma': 0, 'regressor__reg_alpha': 0, 'regressor__reg_lambda': 1}\n\nModel Evaluation:\nRoot Mean Squared Error: 3774.48\nR² Score: 0.9514\n\nRegularization Test 2:\n{'regressor__max_depth': 3, 'regressor__min_child_weight': 1, 'regressor__gamma': 0, 'regressor__reg_alpha': 1, 'regressor__reg_lambda': 1}\n\nModel Evaluation:\nRoot Mean Squared Error: 3774.48\nR² Score: 0.9514\n\nRegularization Test 3:\n{'regressor__max_depth': 5, 'regressor__min_child_weight': 3, 'regressor__gamma': 0.1, 'regressor__reg_alpha': 0, 'regressor__reg_lambda': 1}\n\nModel Evaluation:\nRoot Mean Squared Error: 3334.18\nR² Score: 0.9621\n\nRegularization Test 4:\n{'regressor__max_depth': 5, 'regressor__min_child_weight': 3, 'regressor__gamma': 0.1, 'regressor__reg_alpha': 1, 'regressor__reg_lambda': 5}\n\nModel Evaluation:\nRoot Mean Squared Error: 3298.88\nR² Score: 0.9629\n\nRegularization Test 5:\n{'regressor__max_depth': 7, 'regressor__min_child_weight': 1, 'regressor__gamma': 0.2, 'regressor__reg_alpha': 5, 'regressor__reg_lambda': 10}\n\nModel Evaluation:\nRoot Mean Squared Error: 3222.51\nR² Score: 0.9646\n\nBest Regularization Parameters:\n{'regressor__max_depth': 7, 'regressor__min_child_weight': 1, 'regressor__gamma': 0.2, 'regressor__reg_alpha': 5, 'regressor__reg_lambda': 10}\nBest RMSE: 3222.51\n\n\n\n\n10.5.10 Comprehensive Hyperparameter Tuning\nIn this step, we expand our search to include a broader set of influential hyperparameters that govern both model complexity and regularization strength in XGBoost. These include:\n\nlearning_rate: Controls the contribution of each tree in the ensemble.\nmax_depth and min_child_weight: Control tree complexity and can help prevent overfitting.\ngamma: Adds regularization by requiring a minimum loss reduction for a split.\nsubsample and colsample_bytree: Introduce stochasticity to reduce overfitting by sampling rows and features, respectively.\nreg_alpha (L1 regularization) and reg_lambda (L2 regularization): Add penalties to leaf weights to shrink overly complex trees.\n\nRather than optimizing these parameters independently, we will tune them together using a grid search to capture the complex interactions between them. This comprehensive search aims to identify a well-balanced model that generalizes well to unseen data.\nThis comprehensive tuning process helps us identify the most effective combination of hyperparameters for maximizing predictive performance while minimizing overfitting.\n\n10.5.10.1 Why GridSearchCV Is Not a Practical Option\nWhile GridSearchCV is a straightforward and exhaustive approach, it can be extremely time-consuming, especially when tuning many hyperparameters over multiple values. In our case, the parameter grid includes:\n\n3 values for learning_rate\n3 values for max_depth\n3 values for min_child_weight\n3 values for gamma\n3 values each for subsample and colsample_bytree\n3 values for reg_alpha\n3 values for reg_lambda\n\nThis results in a total of 3⁸ = 6,561 combinations. With 3-fold cross-validation, this would involve training and evaluating over 19,000 models, making it computationally expensive and inefficient.\nThe code is included below if you’re curious to try it out — just uncomment the .fit() line to experience how long it takes.\n\n# ===== 4. Comprehensive Hyperparameter Tuning =====\nprint(\"\\n===== Comprehensive Hyperparameter Tuning Using GridSearchCV=====\")\n# Define hyperparameter grid\nparam_grid = {\n    'regressor__learning_rate': [0.01, 0.05, 0.1],\n    'regressor__max_depth': [3, 5, 7],\n    'regressor__min_child_weight': [1, 3, 5],\n    'regressor__gamma': [0, 0.1, 0.2],\n    'regressor__subsample': [0.8, 0.9, 1.0],\n    'regressor__colsample_bytree': [0.8, 0.9, 1.0],\n    'regressor__reg_alpha': [0, 1, 5],\n    'regressor__reg_lambda': [1, 5, 10]\n}\n\n# Create a pipeline for grid search\ntune_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', XGBRegressor(\n        random_state=42,\n        n_estimators=early_stop_model.best_iteration\n    ))\n])\n\n# Set up grid search with cross-validation\ngrid_search = GridSearchCV(\n    tune_pipeline,\n    param_grid,\n    cv=3,\n    scoring='neg_root_mean_squared_error',\n    n_jobs=-1,\n    verbose=1\n)\n\n# uncomment the line below to run the grid search (it may take a long time)\n#grid_search.fit(X_train, y_train)\n\n\n===== Comprehensive Hyperparameter Tuning =====\n\n\n\n\n10.5.10.2 Smarter Tuning with Optuna or BayesSearchCV\nInstead of exhaustively evaluating every combination like GridSearchCV, we can use smarter search strategies like:\n\nOptuna: A powerful hyperparameter optimization framework that uses Tree-structured Parzen Estimators (TPE) to efficiently explore the search space. It dynamically chooses the next set of hyperparameters to try based on past performance.\nBayesSearchCV (from scikit-optimize): Implements Bayesian optimization, which builds a probabilistic model of the objective function and selects the most promising hyperparameters to try next.\n\nThese methods are:\n\nMore efficient: They converge to good solutions with far fewer iterations.\nFlexible: They support conditional hyperparameter tuning.\nScalable: Much better suited for high-dimensional or expensive-to-evaluate models.\n\nIn summary, we commented out the grid search due to its high cost and instead favor more intelligent, efficient hyperparameter search methods like Optuna or BayesSearchCV for practical use.\n\n10.5.10.2.1 BayesSearchCV (from skopt)\nYou define the search space using a dictionary where:\n\nFor pipelines and scikit-learn integration, BayesSearchCV is simpler\nKeys are hyperparameter names (matching pipeline step names like 'regressor__max_depth')\nValues are distributions or discrete ranges from skopt.space\n\n\n🔹 Key Tip: Use Real(..., prior='log-uniform') for parameters like learning_rate, which benefit from exploring small values on a logarithmic scale.\nThis helps the search algorithm better identify optimal values in ranges where performance is sensitive to small changes (e.g., between 0.01 and 0.1).\n\n\n# define the search space for Bayesian optimization\n\nsearch_space = {\n    'regressor__learning_rate': Real(0.01, 0.5, prior='uniform'),\n    'regressor__max_depth': Integer(3, 7),\n    'regressor__min_child_weight': Integer(1, 5),\n    'regressor__gamma': Real(0, 0.2),\n    'regressor__subsample': Real(0.5, 1.0),\n    'regressor__colsample_bytree': Real(0.5, 1.0),\n    'regressor__reg_alpha': Real(0, 5),\n    'regressor__reg_lambda': Real(1, 10)\n}\n\n# Create a pipeline for Bayesian optimization\nbayes_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', XGBRegressor(\n        random_state=42,\n        n_estimators=early_stop_model.best_iteration\n    ))\n])\n# Set up Bayesian optimization with cross-validation\nbayes_search = BayesSearchCV(\n    bayes_pipeline,\n    search_space,\n    cv=3,\n    n_iter=50,  # Number of iterations for Bayesian optimization\n    scoring='neg_root_mean_squared_error',\n    n_jobs=-1,\n    verbose=0\n)\n# Perform Bayesian optimization\nbayes_search.fit(X_train, y_train)\n\n# Print the best parameters and score\nprint(\"\\nBest Parameters from Bayesian Optimization:\")\nprint(bayes_search.best_params_)\nprint(\"Best Cross-Validation RMSE: {:.2f}\".format(-bayes_search.best_score_))\n# Evaluate the best model from Bayesian optimization\nbest_bayes_model = bayes_search.best_estimator_\nprint(\"\\nBayesian Optimization Model Evaluation:\")\nbayes_metrics = evaluate_model(best_bayes_model, X_test, y_test)\n\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\nFitting 3 folds for each of 1 candidates, totalling 3 fits\n\nBest Parameters from Bayesian Optimization:\nOrderedDict({'regressor__colsample_bytree': 0.6350099974437949, 'regressor__gamma': 0.0, 'regressor__learning_rate': 0.15012576802968475, 'regressor__max_depth': 7, 'regressor__min_child_weight': 1, 'regressor__reg_alpha': 5.0, 'regressor__reg_lambda': 9.330007518126198, 'regressor__subsample': 0.77576084288087})\nBest Cross-Validation RMSE: 3212.04\n\nBayesian Optimization Model Evaluation:\nRoot Mean Squared Error: 3083.10\nR² Score: 0.9676\n\n\nLet’s visualize the search results\n\n# plot convergence\nplot_convergence(bayes_search.optimizer_results_);\n\n\n\n\n\n\n\n\n\n# Plot the objective function\nplot_objective(bayes_search.optimizer_results_[0])\nplt.title('Bayesian Optimization: Objective Function')\nplt.xlabel('Parameter Value')\nplt.ylabel('Objective Value (RMSE)')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create the final model with the best hyperparameters\nprint(\"\\n===== Final Model with Best Hyperparameters =====\")\nfinal_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', XGBRegressor(\n        random_state=42,\n        n_estimators=early_stop_model.best_iteration,\n        **{k.replace('regressor__', ''): v for k, v in bayes_search.best_params_.items()}\n    ))\n])\n\n# Train the final model\nfinal_pipeline.fit(X_train, y_train)\n\n# Evaluate final model\nprint(\"\\nFinal Model Evaluation:\")\nfinal_metrics = evaluate_model(final_pipeline, X_test, y_test)\n\n\n===== Final Model with Best Hyperparameters =====\n\nFinal Model Evaluation:\nRoot Mean Squared Error: 3083.10\nR² Score: 0.9676\n\n\n\n# Display actual vs predicted values for the final model\ny_pred = final_pipeline.predict(X_test)\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Actual vs Predicted Values (Final Model)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# output the feature importance for the final model\nfinal_importance = plot_feature_importance(final_pipeline.named_steps['regressor'], \n                                             final_pipeline.named_steps['preprocessor'], \n                                             X)\nfinal_importance\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n0\nmodel_ I800\n0.109385\n\n\n1\nengineSize\n0.085000\n\n\n2\ntransmission_Manual\n0.080872\n\n\n3\nbrand_hyundi\n0.045237\n\n\n4\nbrand_vw\n0.041919\n\n\n5\nbrand_ford\n0.038523\n\n\n6\nmodel_ Mustang\n0.031646\n\n\n7\nmodel_ i8\n0.030787\n\n\n8\nbrand_bmw\n0.029788\n\n\n9\nyear\n0.025983\n\n\n10\nbrand_merc\n0.024978\n\n\n11\nmodel_ R8\n0.024132\n\n\n12\nmodel_ S Class\n0.024024\n\n\n13\nbrand_audi\n0.019605\n\n\n14\nmodel_ X7\n0.019070\n\n\n15\nmodel_ V Class\n0.014943\n\n\n16\nmodel_ X-CLASS\n0.013910\n\n\n17\nbrand_toyota\n0.013511\n\n\n18\nmpg\n0.012854\n\n\n19\nmodel_ X4\n0.011403\n\n\n\n\n\n\n\n\n\n10.5.10.2.2 Tuning with Optuna\nWith Optuna, you define the search space inside an objective function using trial suggestions:\n\nimport optuna\nfrom sklearn.model_selection import cross_val_score\n# Define the objective function for Optuna\ndef objective(trial):\n    # Suggest hyperparameters\n    params = {\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n        'max_depth': trial.suggest_int('max_depth', 3, 7),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),\n        'gamma': trial.suggest_float('gamma', 0, 0.2),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1, 10),\n        'random_state': 42,\n        'n_estimators': 1000\n    }\n\n    # Define the model\n    model = XGBRegressor(**params)\n\n    # Optionally: wrap in pipeline if preprocessing is needed\n    pipeline = Pipeline([\n        ('preprocessor', preprocessor),  # assumed to be defined earlier\n        ('regressor', model)\n    ])\n\n    # Evaluate with cross-validation\n    score = cross_val_score(pipeline, X_train, y_train, cv=3, scoring='neg_root_mean_squared_error').mean()\n    return score  # Maximize negative RMSE (i.e., minimize RMSE)\n\n# Run the Optuna study\nstudy = optuna.create_study(direction='maximize')  # maximizing negative RMSE\nstudy.optimize(objective, n_trials=50, timeout=600)\n\n# Display the best result\nprint(\"Best trial:\")\nprint(f\"  RMSE (CV): {-study.best_value:.4f}\")\nprint(\"  Best hyperparameters:\")\nfor key, value in study.best_params.items():\n    print(f\"    {key}: {value}\")\n\n[I 2025-05-14 04:47:06,217] A new study created in memory with name: no-name-59242cf6-161a-4605-a35c-8961a98f403e\n[I 2025-05-14 04:47:07,258] Trial 0 finished with value: -3162.7913411458335 and parameters: {'learning_rate': 0.1641733170152142, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.15950428057497823, 'subsample': 0.8687525001570833, 'colsample_bytree': 0.5001903459871172, 'reg_alpha': 2.767789824640966, 'reg_lambda': 8.129162242032194}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:09,975] Trial 1 finished with value: -3368.8380533854165 and parameters: {'learning_rate': 0.15178125229556488, 'max_depth': 7, 'min_child_weight': 1, 'gamma': 0.050258482002987326, 'subsample': 0.5898378683092566, 'colsample_bytree': 0.9917924243351735, 'reg_alpha': 0.23111115691692774, 'reg_lambda': 2.0125181234384675}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:10,925] Trial 2 finished with value: -3432.02392578125 and parameters: {'learning_rate': 0.42391219724854534, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.13708107402474898, 'subsample': 0.6148247399767317, 'colsample_bytree': 0.7362060869492091, 'reg_alpha': 2.489679115528034, 'reg_lambda': 2.1233179056863065}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:12,191] Trial 3 finished with value: -3344.6248372395835 and parameters: {'learning_rate': 0.265690187570691, 'max_depth': 4, 'min_child_weight': 2, 'gamma': 0.16133880047200677, 'subsample': 0.6611070905675426, 'colsample_bytree': 0.6704715035156631, 'reg_alpha': 4.144973354445489, 'reg_lambda': 3.9063999949186705}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:13,975] Trial 4 finished with value: -3270.3804524739585 and parameters: {'learning_rate': 0.028752592609356385, 'max_depth': 5, 'min_child_weight': 2, 'gamma': 0.03302184395133658, 'subsample': 0.721688869095855, 'colsample_bytree': 0.632919922307615, 'reg_alpha': 3.0139736364453245, 'reg_lambda': 8.336475479937352}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:16,075] Trial 5 finished with value: -3520.0126953125 and parameters: {'learning_rate': 0.3795040467605126, 'max_depth': 5, 'min_child_weight': 4, 'gamma': 0.0923172410962489, 'subsample': 0.6576538128571611, 'colsample_bytree': 0.5874178737107301, 'reg_alpha': 4.327790643794075, 'reg_lambda': 5.539914427905474}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:18,141] Trial 6 finished with value: -3230.2576497395835 and parameters: {'learning_rate': 0.01887765935706954, 'max_depth': 7, 'min_child_weight': 4, 'gamma': 0.08395864006045435, 'subsample': 0.9650583569644329, 'colsample_bytree': 0.5068836778426125, 'reg_alpha': 4.319194027803546, 'reg_lambda': 2.4577201842849408}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:20,158] Trial 7 finished with value: -3265.4407552083335 and parameters: {'learning_rate': 0.10762455895584233, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.18945099975997307, 'subsample': 0.6082738049159904, 'colsample_bytree': 0.8839951047303432, 'reg_alpha': 1.7804602782057595, 'reg_lambda': 4.01856427883501}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:22,225] Trial 8 finished with value: -3521.05517578125 and parameters: {'learning_rate': 0.41352773552997424, 'max_depth': 5, 'min_child_weight': 2, 'gamma': 0.12690667210056658, 'subsample': 0.5821082070503734, 'colsample_bytree': 0.5074336963373632, 'reg_alpha': 3.7306140414159943, 'reg_lambda': 3.3055935850297096}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:23,242] Trial 9 finished with value: -3271.8055013020835 and parameters: {'learning_rate': 0.08043112986012362, 'max_depth': 3, 'min_child_weight': 4, 'gamma': 0.18113380178376634, 'subsample': 0.972342410237745, 'colsample_bytree': 0.9814376966070582, 'reg_alpha': 4.5010583121903815, 'reg_lambda': 8.046775880725079}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:25,226] Trial 10 finished with value: -3261.6067708333335 and parameters: {'learning_rate': 0.22708806215964497, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.0009135391068176013, 'subsample': 0.8494117211823278, 'colsample_bytree': 0.8257696123522402, 'reg_alpha': 1.0416025541499645, 'reg_lambda': 6.671020134916106}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:27,859] Trial 11 finished with value: -3377.5233561197915 and parameters: {'learning_rate': 0.20808419263829792, 'max_depth': 7, 'min_child_weight': 4, 'gamma': 0.09467108637827251, 'subsample': 0.9298835740565166, 'colsample_bytree': 0.5010865276579407, 'reg_alpha': 4.982274512288781, 'reg_lambda': 6.398529741869817}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:30,092] Trial 12 finished with value: -3359.8211263020835 and parameters: {'learning_rate': 0.01826323043937967, 'max_depth': 6, 'min_child_weight': 3, 'gamma': 0.07224880626353157, 'subsample': 0.8525172381777258, 'colsample_bytree': 0.566498265995001, 'reg_alpha': 3.0087037386707145, 'reg_lambda': 9.999053425477364}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:32,225] Trial 13 finished with value: -3389.99609375 and parameters: {'learning_rate': 0.13857618173924105, 'max_depth': 6, 'min_child_weight': 5, 'gamma': 0.1293470109573417, 'subsample': 0.8631896630954577, 'colsample_bytree': 0.7029461161450377, 'reg_alpha': 3.4076274113715366, 'reg_lambda': 1.1597707357570677}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:33,783] Trial 14 finished with value: -3303.03955078125 and parameters: {'learning_rate': 0.3047205728875082, 'max_depth': 4, 'min_child_weight': 4, 'gamma': 0.14361295278041297, 'subsample': 0.7937194720693732, 'colsample_bytree': 0.5727779194914605, 'reg_alpha': 2.129378952206828, 'reg_lambda': 9.80217028801741}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:36,609] Trial 15 finished with value: -3350.712158203125 and parameters: {'learning_rate': 0.17656556325674955, 'max_depth': 6, 'min_child_weight': 3, 'gamma': 0.10914046017834621, 'subsample': 0.9996361890269867, 'colsample_bytree': 0.821830315105234, 'reg_alpha': 1.4844593608366676, 'reg_lambda': 8.251897448342861}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:39,729] Trial 16 finished with value: -3751.3375651041665 and parameters: {'learning_rate': 0.49732527342995736, 'max_depth': 7, 'min_child_weight': 5, 'gamma': 0.06646629587180926, 'subsample': 0.9191757507198699, 'colsample_bytree': 0.6417276413491501, 'reg_alpha': 3.7131164539205996, 'reg_lambda': 4.691801429872557}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:41,442] Trial 17 finished with value: -3220.991943359375 and parameters: {'learning_rate': 0.06349399117329421, 'max_depth': 4, 'min_child_weight': 4, 'gamma': 0.16152083780312498, 'subsample': 0.5193007623949828, 'colsample_bytree': 0.5378275423151795, 'reg_alpha': 4.957968250711724, 'reg_lambda': 6.887432633162225}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:42,826] Trial 18 finished with value: -3196.406494140625 and parameters: {'learning_rate': 0.09062768738247484, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.16932220309933962, 'subsample': 0.5102446555382051, 'colsample_bytree': 0.567680668921065, 'reg_alpha': 0.05899287011786036, 'reg_lambda': 7.095135377059309}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:43,876] Trial 19 finished with value: -3185.7190755208335 and parameters: {'learning_rate': 0.26626727039895703, 'max_depth': 3, 'min_child_weight': 2, 'gamma': 0.19772641560218734, 'subsample': 0.770138637973236, 'colsample_bytree': 0.6097234828156883, 'reg_alpha': 0.4836011903906194, 'reg_lambda': 9.025056369210269}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:45,326] Trial 20 finished with value: -3203.70068359375 and parameters: {'learning_rate': 0.26090742668861305, 'max_depth': 3, 'min_child_weight': 2, 'gamma': 0.19121127716686365, 'subsample': 0.7677836477495147, 'colsample_bytree': 0.6255725064175632, 'reg_alpha': 0.9445649986769805, 'reg_lambda': 9.184359429957107}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:46,993] Trial 21 finished with value: -3228.9916178385415 and parameters: {'learning_rate': 0.31856834300340886, 'max_depth': 3, 'min_child_weight': 3, 'gamma': 0.167803945880308, 'subsample': 0.7143519606311484, 'colsample_bytree': 0.5838931745172615, 'reg_alpha': 0.12405952462738234, 'reg_lambda': 7.282731249021393}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:48,459] Trial 22 finished with value: -3219.378662109375 and parameters: {'learning_rate': 0.19033406785822546, 'max_depth': 4, 'min_child_weight': 2, 'gamma': 0.1736546025660118, 'subsample': 0.7757362268856869, 'colsample_bytree': 0.5444873799616068, 'reg_alpha': 0.522783087786272, 'reg_lambda': 8.840854511279815}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:49,809] Trial 23 finished with value: -3211.786865234375 and parameters: {'learning_rate': 0.12106773616337452, 'max_depth': 3, 'min_child_weight': 3, 'gamma': 0.15243869650940434, 'subsample': 0.5134972988786516, 'colsample_bytree': 0.622819927934837, 'reg_alpha': 0.9827289505727954, 'reg_lambda': 7.603828126201174}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:51,676] Trial 24 finished with value: -3334.4794921875 and parameters: {'learning_rate': 0.3147366533286231, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.1991269835131424, 'subsample': 0.8079082775226714, 'colsample_bytree': 0.6818057638014908, 'reg_alpha': 1.552038632861544, 'reg_lambda': 6.041540266193317}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:53,576] Trial 25 finished with value: -3267.5618489583335 and parameters: {'learning_rate': 0.23472812275507773, 'max_depth': 4, 'min_child_weight': 1, 'gamma': 0.11335743801729878, 'subsample': 0.8916931874251384, 'colsample_bytree': 0.7531828256664181, 'reg_alpha': 0.5869522985399849, 'reg_lambda': 9.145754803235887}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:54,926] Trial 26 finished with value: -3202.6060384114585 and parameters: {'learning_rate': 0.08193443679035439, 'max_depth': 3, 'min_child_weight': 2, 'gamma': 0.1774109969796935, 'subsample': 0.8212131624598183, 'colsample_bytree': 0.54479642033082, 'reg_alpha': 2.4562856136420543, 'reg_lambda': 7.804051665963241}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:56,627] Trial 27 finished with value: -3199.9505208333335 and parameters: {'learning_rate': 0.1648392550100305, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.1995055644785968, 'subsample': 0.7369161561012435, 'colsample_bytree': 0.603109126467272, 'reg_alpha': 1.9696422807593317, 'reg_lambda': 8.794238617217513}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:58,059] Trial 28 finished with value: -3233.1486002604165 and parameters: {'learning_rate': 0.2816142969893795, 'max_depth': 3, 'min_child_weight': 2, 'gamma': 0.14825724873193188, 'subsample': 0.6758392394896388, 'colsample_bytree': 0.5464275473497652, 'reg_alpha': 0.5537122249305592, 'reg_lambda': 7.1212918481316265}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:47:59,893] Trial 29 finished with value: -3205.3538411458335 and parameters: {'learning_rate': 0.1469906386117053, 'max_depth': 4, 'min_child_weight': 1, 'gamma': 0.11925438218317995, 'subsample': 0.8909004703392734, 'colsample_bytree': 0.7322050029057333, 'reg_alpha': 0.037746661579750096, 'reg_lambda': 5.289887258623613}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:48:01,494] Trial 30 finished with value: -3383.5137532552085 and parameters: {'learning_rate': 0.37120330910823546, 'max_depth': 3, 'min_child_weight': 3, 'gamma': 0.1833492386708398, 'subsample': 0.5602887881874641, 'colsample_bytree': 0.6518986122343293, 'reg_alpha': 3.0231284972065575, 'reg_lambda': 9.61973061024477}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:48:03,243] Trial 31 finished with value: -3167.3258463541665 and parameters: {'learning_rate': 0.1863689590113835, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.19908264163989292, 'subsample': 0.7278685694121224, 'colsample_bytree': 0.5861480239301309, 'reg_alpha': 2.026116015039801, 'reg_lambda': 8.699297071780851}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:48:05,445] Trial 32 finished with value: -3281.510986328125 and parameters: {'learning_rate': 0.20151346925924155, 'max_depth': 5, 'min_child_weight': 3, 'gamma': 0.16458591948116905, 'subsample': 0.6958914280325602, 'colsample_bytree': 0.6042564555277344, 'reg_alpha': 1.3700762573158425, 'reg_lambda': 8.518791773797872}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:48:07,111] Trial 33 finished with value: -3204.1426595052085 and parameters: {'learning_rate': 0.12026108279051516, 'max_depth': 4, 'min_child_weight': 2, 'gamma': 0.15438462102064923, 'subsample': 0.6242819620230452, 'colsample_bytree': 0.5342892669503979, 'reg_alpha': 0.4963669112142102, 'reg_lambda': 7.666852561159716}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:48:08,476] Trial 34 finished with value: -3220.5049641927085 and parameters: {'learning_rate': 0.2337807893313772, 'max_depth': 3, 'min_child_weight': 3, 'gamma': 0.188224524248803, 'subsample': 0.7519391438650187, 'colsample_bytree': 0.6734690653266131, 'reg_alpha': 2.366857352866043, 'reg_lambda': 9.337266831653581}. Best is trial 0 with value: -3162.7913411458335.\n[I 2025-05-14 04:48:10,219] Trial 35 finished with value: -3141.7112630208335 and parameters: {'learning_rate': 0.05427916910483564, 'max_depth': 5, 'min_child_weight': 2, 'gamma': 0.17217342372478064, 'subsample': 0.6486895737288911, 'colsample_bytree': 0.5748420678684818, 'reg_alpha': 2.823104446765354, 'reg_lambda': 6.123835171658451}. Best is trial 35 with value: -3141.7112630208335.\n[I 2025-05-14 04:48:12,727] Trial 36 finished with value: -3424.205810546875 and parameters: {'learning_rate': 0.3434536736887878, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.19798037524773468, 'subsample': 0.6462109533575047, 'colsample_bytree': 0.6013661462288122, 'reg_alpha': 2.8058983646067652, 'reg_lambda': 5.836376792997449}. Best is trial 35 with value: -3141.7112630208335.\n[I 2025-05-14 04:48:14,696] Trial 37 finished with value: -3118.7006022135415 and parameters: {'learning_rate': 0.050933621089207015, 'max_depth': 5, 'min_child_weight': 2, 'gamma': 0.17859904779145075, 'subsample': 0.687046200984366, 'colsample_bytree': 0.5234986611495844, 'reg_alpha': 2.814493606788762, 'reg_lambda': 4.8395816535714555}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:16,481] Trial 38 finished with value: -3122.8831380208335 and parameters: {'learning_rate': 0.06168478840447832, 'max_depth': 5, 'min_child_weight': 2, 'gamma': 0.13797722131867746, 'subsample': 0.6960007534848832, 'colsample_bytree': 0.5182244063678463, 'reg_alpha': 2.731920033783212, 'reg_lambda': 4.550908916595087}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:18,515] Trial 39 finished with value: -3139.6028645833335 and parameters: {'learning_rate': 0.05316829440470475, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.13489818880210416, 'subsample': 0.6912991523076674, 'colsample_bytree': 0.517984422252086, 'reg_alpha': 3.408188018857992, 'reg_lambda': 4.62131346940151}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:21,326] Trial 40 finished with value: -3165.2281901041665 and parameters: {'learning_rate': 0.040909566934035954, 'max_depth': 6, 'min_child_weight': 1, 'gamma': 0.13647271109557557, 'subsample': 0.6868429049917079, 'colsample_bytree': 0.5098868980895536, 'reg_alpha': 3.3309798162772895, 'reg_lambda': 4.842339064732724}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:22,927] Trial 41 finished with value: -3153.988525390625 and parameters: {'learning_rate': 0.04764134215278892, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.13734423615317123, 'subsample': 0.6396914085172828, 'colsample_bytree': 0.5222333533007302, 'reg_alpha': 2.7683392736744907, 'reg_lambda': 3.4848817151425147}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:25,194] Trial 42 finished with value: -3138.40869140625 and parameters: {'learning_rate': 0.04665794491311853, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.14050710663623797, 'subsample': 0.6466622773319057, 'colsample_bytree': 0.5243207754378303, 'reg_alpha': 2.742598397314503, 'reg_lambda': 3.4532539995737297}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:27,181] Trial 43 finished with value: -3634.5953776041665 and parameters: {'learning_rate': 0.010559089516967338, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.10464776864762987, 'subsample': 0.590380693415102, 'colsample_bytree': 0.5527271103337551, 'reg_alpha': 3.3406191855405254, 'reg_lambda': 4.270515611372069}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:28,967] Trial 44 finished with value: -3160.775146484375 and parameters: {'learning_rate': 0.060736025914241036, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.12354110870609715, 'subsample': 0.7089501071182204, 'colsample_bytree': 0.5237902329649554, 'reg_alpha': 3.7441364304748164, 'reg_lambda': 2.9387955530200216}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:31,730] Trial 45 finished with value: -3168.1395670572915 and parameters: {'learning_rate': 0.037531801199779405, 'max_depth': 6, 'min_child_weight': 2, 'gamma': 0.14149686953692464, 'subsample': 0.6665114574999642, 'colsample_bytree': 0.5033999882927995, 'reg_alpha': 3.0970914358769033, 'reg_lambda': 4.899606528403381}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:34,529] Trial 46 finished with value: -3193.5437825520835 and parameters: {'learning_rate': 0.09959112301640272, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.15729275651842772, 'subsample': 0.622910390210815, 'colsample_bytree': 0.5659218113820054, 'reg_alpha': 2.5987048583369248, 'reg_lambda': 4.437051597406045}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:37,245] Trial 47 finished with value: -3228.6897786458335 and parameters: {'learning_rate': 0.07037240171555721, 'max_depth': 5, 'min_child_weight': 2, 'gamma': 0.1344051098663805, 'subsample': 0.565145406031194, 'colsample_bytree': 0.9281697283786465, 'reg_alpha': 2.2287279424305964, 'reg_lambda': 3.764118180558043}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:40,511] Trial 48 finished with value: -3211.089111328125 and parameters: {'learning_rate': 0.10995519780575633, 'max_depth': 6, 'min_child_weight': 2, 'gamma': 0.09688240640580331, 'subsample': 0.6949408506367961, 'colsample_bytree': 0.5247184310894689, 'reg_alpha': 3.9418544972703486, 'reg_lambda': 5.287998554183364}. Best is trial 37 with value: -3118.7006022135415.\n[I 2025-05-14 04:48:42,711] Trial 49 finished with value: -3200.5746256510415 and parameters: {'learning_rate': 0.028839725004554832, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.024679179556514927, 'subsample': 0.6593982063331032, 'colsample_bytree': 0.5591427707331742, 'reg_alpha': 2.638934979392886, 'reg_lambda': 2.9603331872221825}. Best is trial 37 with value: -3118.7006022135415.\n\n\nBest trial:\n  RMSE (CV): 3118.7006\n  Best hyperparameters:\n    learning_rate: 0.050933621089207015\n    max_depth: 5\n    min_child_weight: 2\n    gamma: 0.17859904779145075\n    subsample: 0.687046200984366\n    colsample_bytree: 0.5234986611495844\n    reg_alpha: 2.814493606788762\n    reg_lambda: 4.8395816535714555\n\n\nLet’s visualize the result\n\nimport optuna.visualization as vis\n\nfig1 = vis.plot_optimization_history(study)\nfig1.show()\n    \nfig2 = vis.plot_param_importances(study)\nfig2.show()\n\nfig3 = vis.plot_slice(study)\nfig3.show()\n    \n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\n10.5.10.3 After Training: Analyze and Refine\nOnce the tuning is complete, don’t forget to visualize the search results to understand how different hyperparameters affected performance. This helps you:\n\nIdentify which parameters had the most impact.\nSpot trends (e.g., performance plateaus or sharp drop-offs).\nDetect boundary effects (e.g., best values lie at the edge of the current search space).\n\n\n🔹 Key Tip: If the best values are near the boundary of your current search space, consider fine-tuning the search space and re-running the optimization.\n\nYou can visualize results using:\n\nOptuna’s built-in plots like plot_optimization_history() and plot_param_importances().\nBayesSearchCV’s cv_results_ attribute to create custom plots using pandas or seaborn.\n\nEffective tuning is often iterative — let the data guide you!",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#xgboost-for-imbalanced-classification",
    "href": "XGBoost.html#xgboost-for-imbalanced-classification",
    "title": "10  XGBoost",
    "section": "10.6 XGBoost for Imbalanced Classification",
    "text": "10.6 XGBoost for Imbalanced Classification\n\n10.6.1 Common Strategies Across Libraries\nImbalanced classification arises when one class is significantly underrepresented—common in applications like fraud detection, rare disease diagnosis, and anomaly detection.\n\nUse Better Evaluation Metrics\nAvoid relying on accuracy. Instead, use metrics that reflect class imbalance, such as:\n\nF1-score\nAUC-PR (Area Under the Precision-Recall Curve)\nMatthews Correlation Coefficient (MCC)\n\nThreshold Tuning\nAdjust the decision threshold to balance between precision and recall based on your use case.\nStratified Sampling\nWhen splitting the dataset (for training/validation or cross-validation), use stratified sampling to maintain the class distribution in each fold.\n\n\n\n10.6.2 Handling Class Imbalance with scale_pos_weight in XGBoost\nWhile XGBoost (and other gradient boosting libraries) can perform well on imbalanced datasets, models can become biased toward the majority class if no corrective strategies are used. One of the most effective built-in solutions in XGBoost is the scale_pos_weight parameter.\n\n10.6.2.1 What Does scale_pos_weight Do?\nThe scale_pos_weight parameter adjusts the relative importance of positive class examples (label = 1) by scaling their gradients and Hessians during training.\n\nA higher value places more penalty on misclassifying positive samples\nThis encourages the model to focus more on the minority class, helping improve recall and F1-score\n\n\n\n10.6.2.2 When to Use It\nUse scale_pos_weight &gt; 1 when:\n\nThe dataset is heavily imbalanced\nYou care more about the positive class performance (e.g., improving recall, precision, or F1-score)\n\n\n\n\n10.6.3 How to Set It\nA commonly used heuristic:\n\\[\n\\text{scale\\_pos\\_weight} = \\frac{\\text{Number of negative samples}}{\\text{Number of positive samples}}\n\\]\nThis provides a balanced gradient contribution during training and serves as a good starting point. You can further fine-tune this value via cross-validation for optimal performance.\nNote: While scale_pos_weight adjusts learning behavior internally, you should still monitor metrics like AUC-PR, F1-score, or recall to ensure it’s improving your model’s performance on the minority class.\n\ndiabetes_train = pd.read_csv('./Datasets/diabetes_train.csv')\ndiabetes_test = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nprint(diabetes_train.shape, diabetes_test.shape)\ndiabetes_train.head()\n\n(614, 9) (154, 9)\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n2\n88\n74\n19\n53\n29.0\n0.229\n22\n0\n\n\n1\n2\n129\n84\n0\n0\n28.0\n0.284\n27\n0\n\n\n2\n0\n102\n78\n40\n90\n34.5\n0.238\n24\n0\n\n\n3\n0\n123\n72\n0\n0\n36.3\n0.258\n52\n1\n\n\n4\n1\n144\n82\n46\n180\n46.1\n0.335\n46\n1\n\n\n\n\n\n\n\n\n# check the outcome of the distribution in the training set and test set\nprint(diabetes_train['Outcome'].value_counts(normalize=True))\nprint(diabetes_test['Outcome'].value_counts(normalize=True))\n\nOutcome\n0    0.662866\n1    0.337134\nName: proportion, dtype: float64\nOutcome\n0    0.603896\n1    0.396104\nName: proportion, dtype: float64\n\n\n\n# Data Preprocessing\nX_diabetes = diabetes_train.drop(columns=['Outcome'])\ny_diabetes = diabetes_train['Outcome']\nX_diabetes_test = diabetes_test.drop(columns=['Outcome'])\ny_diabetes_test = diabetes_test['Outcome']\n\n# define categorical and numerical columns\ncategorical_cols_diabetes = X_diabetes.select_dtypes(include=['object']).columns.tolist()\nnumerical_cols_diabetes = X_diabetes.select_dtypes(exclude=['object']).columns.tolist()\n\n# define the preprocessor, passing the numerical and categorical columns\npreprocessor_diabetes = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', numerical_cols_diabetes),  # no scaling\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_diabetes)\n    ]\n)\n\n# Create a pipeline for the diabetes dataset\ndiabetes_pipeline = Pipeline([\n    ('preprocessor', preprocessor_diabetes),\n    ('regressor', XGBClassifier(random_state=42))\n])\n\n# Train the pipeline on the diabetes dataset\ndiabetes_pipeline.fit(X_diabetes, y_diabetes)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', 'passthrough',\n                                                  ['Pregnancies', 'Glucose',\n                                                   'BloodPressure',\n                                                   'SkinThickness', 'Insulin',\n                                                   'BMI',\n                                                   'DiabetesPedigreeFunction',\n                                                   'Age']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  [])])),\n                ('regressor',\n                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n                               colsample_bylevel=None, colsample_...\n                               feature_types=None, feature_weights=None,\n                               gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', 'passthrough',\n                                                  ['Pregnancies', 'Glucose',\n                                                   'BloodPressure',\n                                                   'SkinThickness', 'Insulin',\n                                                   'BMI',\n                                                   'DiabetesPedigreeFunction',\n                                                   'Age']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  [])])),\n                ('regressor',\n                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n                               colsample_bylevel=None, colsample_...\n                               feature_types=None, feature_weights=None,\n                               gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, ...))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', 'passthrough',\n                                 ['Pregnancies', 'Glucose', 'BloodPressure',\n                                  'SkinThickness', 'Insulin', 'BMI',\n                                  'DiabetesPedigreeFunction', 'Age']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 [])]) num['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'] passthroughpassthrough cat[] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') XGBClassifier?Documentation for XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              feature_weights=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=None, num_parallel_tree=None, ...) \n\n\n\n# make predictions on the test set\n\nprint(\"\\n===== Test Performance with Default Setting =====\")\ny_diabetes_pred = diabetes_pipeline.predict(X_diabetes_test)\n# evaluate the model in terms of accuracy, precision, recall, and f1-score\naccuracy = np.mean(y_diabetes_pred == y_diabetes_test)\nprint(f\"Accuracy on Diabetes Test Set: {accuracy:.2f}\")\nprecision = precision_score(y_diabetes_test, y_diabetes_pred)\nprint(f\"Precision on Diabetes Test Set: {precision:.2f}\")\nrecall = recall_score(y_diabetes_test, y_diabetes_pred)\nprint(f\"Recall on Diabetes Test Set: {recall:.2f}\")\nf1 = f1_score(y_diabetes_test, y_diabetes_pred)\nprint(f\"F1 Score on Diabetes Test Set: {f1:.2f}\")\n\n\n===== Test Performance with Default Setting =====\nAccuracy on Diabetes Test Set: 0.73\nPrecision on Diabetes Test Set: 0.68\nRecall on Diabetes Test Set: 0.59\nF1 Score on Diabetes Test Set: 0.63\n\n\n\n\n10.6.4 Using scale_pos_weight\n\nneg = (diabetes_train['Outcome'] == 0).sum()\npos = (diabetes_train['Outcome'] == 1).sum()\nbaseline_ratio = neg / pos\n\n# set a small grid for scale_pos_weight\nscale_pos_weight = [1, 2, 3, 5, 10]\n\n# Create a new pipeline for the diabetes dataset\ndiabetes_pipeline_scale = Pipeline([\n    ('preprocessor', preprocessor_diabetes),\n    ('regressor', XGBClassifier(random_state=42))\n])\n\n# Store results for comparison\ndiabetes_results = []\nfor i, weight in enumerate(scale_pos_weight):\n    print(f\"\\nScale Pos Weight Test {i+1}:\")\n    print(f\"scale_pos_weight = {weight}\")\n    \n    # Update the pipeline with the scale_pos_weight parameter\n    diabetes_pipeline_scale.set_params(regressor__scale_pos_weight=weight)\n    \n    # Train and evaluate\n    diabetes_pipeline_scale.fit(X_diabetes, y_diabetes)\n    y_diabetes_pred = diabetes_pipeline_scale.predict(X_diabetes_test)\n    \n    # Evaluate the model\n    accuracy = np.mean(y_diabetes_pred == y_diabetes_test)\n    precision = precision_score(y_diabetes_test, y_diabetes_pred)\n    recall = recall_score(y_diabetes_test, y_diabetes_pred)\n    f1 = f1_score(y_diabetes_test, y_diabetes_pred)\n    \n    # Store results\n    diabetes_results.append({\n        'test': i+1,\n        'scale_pos_weight': weight,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    })\n# Create a DataFrame for the results\ndiabetes_results_df = pd.DataFrame(diabetes_results)   \n\n# Find the best scale_pos_weight based on F1 score\nbest_f1_idx = diabetes_results_df['f1'].idxmax()\nbest_scale_pos_weight = diabetes_results_df.loc[best_f1_idx, 'scale_pos_weight']\nprint(\"\\n===== Test Performance with Best Scale Pos Weight: =====\")\nprint(best_scale_pos_weight)\nprint(\"Best F1 Score: {:.2f}\".format(diabetes_results_df['f1'].max()))\n# find the accuracy, precision, and recall for the best scale_pos_weight\nbest_accuracy = diabetes_results_df.loc[best_f1_idx, 'accuracy']\nbest_precision = diabetes_results_df.loc[best_f1_idx, 'precision']\nbest_recall = diabetes_results_df.loc[best_f1_idx, 'recall']\nprint(f\"Best Accuracy: {best_accuracy:.2f}\")\nprint(f\"Best Precision: {best_precision:.2f}\")\nprint(f\"Best Recall: {best_recall:.2f}\")\n\n\nScale Pos Weight Test 1:\nscale_pos_weight = 1\n\nScale Pos Weight Test 2:\nscale_pos_weight = 2\n\nScale Pos Weight Test 3:\nscale_pos_weight = 3\n\nScale Pos Weight Test 4:\nscale_pos_weight = 5\n\nScale Pos Weight Test 5:\nscale_pos_weight = 10\n\nBest Scale Pos Weight:\n3\nBest F1 Score: 0.69\nBest Accuracy: 0.76\nBest Precision: 0.71\nBest Recall: 0.67\n\n\n\n# Plot the results for accuracy, precision, and recall with different colors\nplt.figure(figsize=(10, 6))\nplt.plot(diabetes_results_df['scale_pos_weight'], diabetes_results_df['accuracy'], marker='o', color='blue', label='Accuracy')\nplt.plot(diabetes_results_df['scale_pos_weight'], diabetes_results_df['precision'], marker='o', color='orange', label='Precision')\nplt.plot(diabetes_results_df['scale_pos_weight'], diabetes_results_df['recall'], marker='o', color='green', label='Recall')\nplt.xlabel('Scale Pos Weight', fontsize=12)\nplt.ylabel('Score', fontsize=12)\nplt.title('Effect of Scale Pos Weight on Accuracy, Precision, and Recall', fontsize=14)\nplt.grid(True)\nplt.xticks(scale_pos_weight)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTuning scale_pos_weight changes the model’s internal learning dynamics, not just the decision threshold. This allows the model to adjust how it learns from imbalanced data.\nIn this case, as scale_pos_weight increases from 1 to 3:\n\nXGBoost starts giving more importance to the minority (positive) class.\nThe model becomes better at identifying true positives → ✅ Recall increases\nSimultaneously, it avoids more false positives → ✅ Precision increases\n\n✅ This indicates the model was previously underperforming on the positive class, and that moderate rebalancing (e.g., scale_pos_weight = 3) helped improve both recall and precision — something that can happen when the model is initially biased toward the majority class.\n\n\n10.6.5 Threshold adjustment\n\n# get prediction probabilities\ny_diabetes_pred_proba = diabetes_pipeline.predict_proba(X_diabetes_test)[:, 1]\n\n# plot the precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_diabetes_test, y_diabetes_pred_proba)\n\n# final threshold\nf1_scores = 2 * (precision * recall) / (precision + recall + 1e-9) # avoid division by zero\nbest_f1_threshold = thresholds[np.argmax(f1_scores)]\nbest_threshold = np.round(best_f1_threshold, 2)\nprint(f\"Best Threshold for F1 Score: {best_threshold:.2f}\")\n\n# adjust the threshold for the predictions\ny_diabetes_pred_adjusted = (y_diabetes_pred_proba &gt;= best_f1_threshold).astype(int)\n# evaluate the model with the adjusted threshold\naccuracy_adjusted = np.mean(y_diabetes_pred_adjusted == y_diabetes_test)\nprint(f\"Accuracy with Adjusted Threshold: {accuracy_adjusted:.2f}\")\nprecision_adjusted = precision_score(y_diabetes_test, y_diabetes_pred_adjusted)\nprint(f\"Precision with Adjusted Threshold: {precision_adjusted:.2f}\")\nrecall_adjusted = recall_score(y_diabetes_test, y_diabetes_pred_adjusted)\nprint(f\"Recall with Adjusted Threshold: {recall_adjusted:.2f}\")\n# plot the precision-recall curve\nplt.figure(figsize=(10, 6))\nplt.plot(recall, precision, marker='o')\nplt.xlabel('Recall', fontsize=12)\nplt.ylabel('Precision', fontsize=12)\nplt.title('Precision-Recall Curve', fontsize=14)\nplt.grid(True)\nplt.axvline(x=recall[np.argmax(f1_scores)], color='red', linestyle='--', label='Best Threshold')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nBest Threshold for F1 Score: 0.19\nAccuracy with Adjusted Threshold: 0.78\nPrecision with Adjusted Threshold: 0.68\nRecall with Adjusted Threshold: 0.82\n\n\n\n\n\n\n\n\n\n\n\n10.6.6 Alternative Method: Custom Instance Weights (sample_weight)\nYou can assign custom weights to individual training samples using the sample_weight parameter in fit() (scikit-learn API) or weight in DMatrix (native API). This gives you fine-grained control over how much each sample contributes to the loss and gradient during training.\nExample (scikit-learn API):\nfrom xgboost import XGBClassifier\nimport numpy as np\n\n# Assign higher weights to positive class\nweights = np.where(y_train == 1, 5, 1)\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train, sample_weight=weights)\nWhen to Use This:\n\nWhen you want more flexibility than scale_pos_weight allows\nWhen the imbalance is complex (e.g., multi-class or cost-sensitive)\nWhen you want to incorporate domain knowledge into weight assignments\n\n\n\n10.6.7 scale_pos_weight vs. sample_weight\n\n\n\n\n\n\n\n\nFeature\nscale_pos_weight\nsample_weight\n\n\n\n\nApplies to\nEntire positive class (label = 1)\nIndividual samples\n\n\nHow it works\nMultiplies gradients and Hessians of the positive class during training\nDirectly scales the loss function per instance\n\n\nUse case\nBinary classification with class imbalance\nAny situation needing custom weighting (e.g., multi-class, domain-driven)\n\n\nFlexibility\nOne global weight value\nFull per-sample control\n\n\nWhere to set\nscale_pos_weight parameter in XGBClassifier or DMatrix\nsample_weight in .fit() or weight= in DMatrix",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "XGBoost.html#resources-for-learning-xgboost",
    "href": "XGBoost.html#resources-for-learning-xgboost",
    "title": "10  XGBoost",
    "section": "10.7 Resources for Learning XGBoost",
    "text": "10.7 Resources for Learning XGBoost\nThe foundational paper is:\nXGBoost: A Scalable Tree Boosting System\nAuthors: Tianqi Chen and Carlos Guestrin\nConference: KDD 2016\nLink to paper (PDF)\nDOI: 10.1145/2939672.2939785\nXGBoost is a relatively recent algorithm (2016), and thus not yet included in many standard textbooks. Below are helpful learning resources:\n\nDocumentation\nSlides by Tianqi Chen\nReference Paper\nVideo by Tianqi Chen (author)\nStatQuest Video Explanation",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html",
    "href": "LightGBM_CatBoost.html",
    "title": "11  LightGBM and CatBoost",
    "section": "",
    "text": "11.1 What They Share with XGBoost\nGradient boosting is one of the most powerful techniques for structured/tabular data, and has become the go-to choice for many winning solutions in machine learning competitions.\nIn the previous chapter, we explored XGBoost in detail — covering its optimization objective, regularization techniques, split finding algorithms, and how it became a cornerstone in modern tabular modeling.\nIn this chapter, we turn our attention to two other powerful gradient boosting libraries and focus on their key innovations:\nDespite their architectural differences, both LightGBM and CatBoost share key foundations with XGBoost:",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#what-they-share-with-xgboost",
    "href": "LightGBM_CatBoost.html#what-they-share-with-xgboost",
    "title": "11  LightGBM and CatBoost",
    "section": "",
    "text": "They use the same objective function structure (loss + regularization)\nThey apply a second-order Taylor approximation for efficient optimization\nThey implement a histogram-based split-finding algorithm to speed up training",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#lightgbm",
    "href": "LightGBM_CatBoost.html#lightgbm",
    "title": "11  LightGBM and CatBoost",
    "section": "11.2 LightGBM",
    "text": "11.2 LightGBM\n\n11.2.1 What is LightGBM?\nLightGBM (Light Gradient Boosting Machine) is a high-performance gradient boosting framework developed by Microsoft in 2017. LightGBM outperforms XGBoost in terms of compuational speed, and provides comparable accuracy in general. It is designed for:\n\nLarge-scale datasets with many rows and features\n\nHigh speed and memory efficiency, often outperforming XGBoost in training time\n\nNative support for categorical features (Note: XGBoost added this starting in version 1.5.0)\n\nSupport for parallel, distributed, and GPU training (XGBoost offers similar capabilities)\n\nRead the LightGBM paper for more details.\n\n\n11.2.2 What Makes LightGBM Unique?\nLightGBM often outperforms XGBoost in training speed and memory efficiency, thanks to several key innovations:\n\n11.2.2.1 Leaf-Wise Tree Growth\n\nLightGBM splits the leaf with the largest potential loss reduction, unlike XGBoost’s level-wise approach.\nThis leads to lower loss per tree, making learning more efficient — though it may overfit without proper regularization.\nMain controls:\n\nnum_leaves: primary control for tree complexity\nmax_depth: optional constraint to prevent overfitting\n\n\n\n\n11.2.2.2 GOSS (Gradient-based One-Side Sampling)\n\nGOSS improves speed by:\n\nRetaining all instances with large gradients (i.e., high error)\nRandomly sampling those with small gradients\n\nThis reduces the dataset size while maintaining accurate split decisions.\n\nIn gradient boosting, the tree is fit to the negative gradient of the loss:\n\\[\nr_m = -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f = f_{m-1}}\n\\]\nObservations with larger gradients have more influence on reducing the loss — GOSS prioritizes those.\n\nHyperparameters for GOSS:\n\nboosting_type='goss': activates GOSS instead of traditional random sampling\ntop_rate: fraction of data with the largest gradients to keep (e.g., 0.2)\nother_rate: fraction of data with smaller gradients to sample (e.g., 0.1)\n\n\n\n\n11.2.2.3 EFB (Exclusive Feature Bundling)\n\nEFB compresses high-dimensional sparse feature spaces by bundling features that are mutually exclusive (i.e., rarely non-zero at the same time).\nThis is particularly effective in datasets with many categorical variables or one-hot encoded features.\n\nExample:\n\n\n\nfeature1\nfeature2\nfeature_bundle\n\n\n\n\n0\n2\n6\n\n\n0\n1\n5\n\n\n0\n2\n6\n\n\n1\n0\n1\n\n\n2\n0\n2\n\n\n3\n0\n3\n\n\n4\n0\n4\n\n\n\nHere, feature1 and feature2 never overlap in non-zero values, so they can be safely merged into a single bundled feature.\n\nHyperparameter for EFB:\n\nenable_bundle: set to true (default) to enable automatic exclusive feature bundling\n\n\nTogether, these optimizations make LightGBM especially well-suited for large-scale, sparse, tabular datasets, offering both speed and scalability without significant loss in accuracy.\n\n\n\n11.2.3 Using LightGBM\nAlthough LightGBM is not part of Scikit-learn, it provides a Scikit-learn-compatible API through the lightgbm.sklearn module. This allows you to use LightGBM models seamlessly with Scikit-learn tools such as Pipeline, GridSearchCV, and cross_val_score.\nThe main classes are:\n\nLGBMRegressor: for regression tasks\n\nLGBMClassifier: for classification tasks\n\nTo install the package:\npip install lightgbm\n\nNote: LightGBM is a separate library, not part of Scikit-learn, but it provides a Scikit-learn-compatible API via LGBMClassifier and LGBMRegressor.\nThis makes it easy to integrate LightGBM models into Scikit-learn workflows such as Pipeline, GridSearchCV, and cross_val_score.\n\n\n11.2.3.1 Core LightGBM Hyperparameters\nCore Tree Structure:\n\nnum_leaves: Maximum number of leaves (terminal nodes) per tree.\nmin_data_in_leaf: Minimum number of data points required in a leaf.\nmax_depth: Maximum depth of a tree (used to control overfitting).\n\nLearning Control and Regularization:\n\nlearning_rate (η): Shrinks the contribution of each tree.\nn_estimators: Number of boosting rounds.\nlambda_l1 / lambda_l2: L1 and L2 regularization on leaf weights.\nmin_gain_to_split: Minimum loss reduction required to make a further split (structure regularization).\n\nData Handling:\n\nfeature_fraction: Fraction of features randomly sampled for each tree (a.k.a. colsample_bytree in XGBoost).\nbagging_fraction: Fraction of data randomly sampled for each iteration.\nbagging_freq: Frequency (in iterations) to perform bagging.\ncategorical_feature: Specifies which features are categorical (enables native handling).\n\nSpeed vs. Accuracy Trade-offs:\n\nmax_bin: Number of bins used to bucket continuous features.\ndata_sample_strategy : bagging or goss\ntop_rate (goss only): Fraction of instances with the largest gradients to keep.\nother_rate (goss only): Fraction of small-gradient instances to randomly sample. -enable_bundle: set this to true to spped up the training for sparse datasets\n\nOptimization Control:\n\nboosting: Type of boosting algorithm (gbdt, dart, rf, etc.).\nearly_stopping_rounds: Stops training if the validation score doesn’t improve over a set number of rounds.\n\nImbalanced Data\n\nscale_pos_weight: Manually sets the weight for the positive class in binary classification.\nis_unbalance: Automatically adjusts class weights based on the training data distribution.\n\n\n⚠️ These two options are mutually exclusive — use only one. If both are set, scale_pos_weight takes priority.\n\nFor full details and advanced options, see the LightGBM Parameters Guide.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import root_mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\nfrom xgboost import XGBRegressor, XGBClassifier\nimport lightgbm as lgb\nimport seaborn as sns\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_convergence\nimport warnings\n\nWe’ll continue to use the same datasets that we have been using throughout the course.\n\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\ncar.head()\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\nvw\nBeetle\n2014\nManual\n55457\nDiesel\n30\n65.3266\n1.6\n7490\n\n\n1\nvauxhall\nGTC\n2017\nManual\n15630\nPetrol\n145\n47.2049\n1.4\n10998\n\n\n2\nmerc\nG Class\n2012\nAutomatic\n43000\nDiesel\n570\n25.1172\n3.0\n44990\n\n\n3\naudi\nRS5\n2019\nAutomatic\n10\nPetrol\n145\n30.5593\n2.9\n51990\n\n\n4\nmerc\nX-CLASS\n2018\nAutomatic\n14000\nDiesel\n240\n35.7168\n2.3\n28990\n\n\n\n\n\n\n\n\nX = car.drop(columns=['price'])\ny = car['price']\n\n# extract the categorical columns and put them in a list\ncategorical_feature = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumerical_feature = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# convert the categorical columns to category type\nfor col in categorical_feature:\n    X[col] = X[col].astype('category')\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n11.2.3.2 Building a Baseline Model Using LightGBM’s Native Categorical Feature Support\nLightGBM provides built-in support for handling categorical features, eliminating the need for manual encoding (like one-hot or ordinal encoding). By directly passing categorical column names or indices to the model, LightGBM can internally apply efficient encoding and optimized split finding for categorical variables.\nIn this section, we’ll use this native capability to quickly build a baseline model, taking advantage of LightGBM’s efficiency with structured data that includes categorical columns.\nThis baseline model serves as a starting point for comparison against more advanced tuning\n\n%%time\n# ===== 1. Baseline Model =====\nprint(\"\\n===== Baseline LightGBM Model =====\")\n# Initialize the LightGBM regressor\nmodel = lgb.LGBMRegressor(random_state=42)\n\n# Train the model with categorical features specified\nmodel.fit(\n    X_train, \n    y_train,\n    categorical_feature=categorical_feature\n)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Calculate evaluation metrics\nrmse = root_mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Output results\nprint(f\"Test RMSE: {rmse:.4f}\")\nprint(f\"Test R²: {r2:.4f}\")\n\n\n===== Baseline LightGBM Model =====\nTest RMSE: 3680.8999\nTest R²: 0.9538\nCPU times: total: 875 ms\nWall time: 82.6 ms\n\n\n\n\n11.2.3.3 Enabling GOSS and EFB in LightGBM\nBy default, LightGBM uses:\ndata_sample_strategy = 'bagging'\nTo enable GOSS (Gradient-based One-Side Sampling) — a faster sampling strategy that prioritizes high-gradient instances — set:\nboosting_type = 'goss'\nWhen using GOSS, you should also configure:\n\ntop_rate: Fraction of data with the largest gradients to retain (e.g., 0.2)\nother_rate: Fraction of small-gradient data to randomly sample (e.g., 0.1)\n\nLightGBM also enables EFB (Exclusive Feature Bundling) by default:\nenable_bundle = True\nThis optimization reduces dimensionality by bundling mutually exclusive sparse features, such as those resulting from one-hot encoding.\n⚠️ Note: In our car dataset, the data size is small and there are only a few categorical features, so these optimizations may not have a noticeable impact. However, for large-scale datasets with many categorical features, enabling GOSS and EFB is highly recommended to improve training efficiency and reduce memory usage.\n\n%%time\n# ===== 2. LightGBM with GOSS Sampling =====\nprint(\"\\n===== LightGBM with GOSS Sampling =====\")\n\n# Initialize the LightGBM regressor with GOSS\nmodel_goss = lgb.LGBMRegressor(\n    boosting_type='goss',\n    random_state=42\n)\n\n# Train the model with categorical features specified\nmodel_goss.fit(\n    X_train,\n    y_train,\n    categorical_feature=categorical_feature\n)\n\n# Predict on the test set\ny_pred_goss = model_goss.predict(X_test)\n\n# Calculate evaluation metrics\nrmse_goss = root_mean_squared_error(y_test, y_pred_goss)\nr2_goss = r2_score(y_test, y_pred_goss)\n\n# Output results\nprint(f\"Test RMSE (GOSS): {rmse_goss:.4f}\")\nprint(f\"Test R² (GOSS): {r2_goss:.4f}\")\n\n\n===== LightGBM with GOSS Sampling =====\nTest RMSE (GOSS): 3510.7726\nTest R² (GOSS): 0.9580\nCPU times: total: 766 ms\nWall time: 79.6 ms\n\n\n\n\n11.2.3.4 Tuning top_rate and other_rate in GOSS\nEven with this small dataset, we observed a shorter execution time and a slight improvement in performance using GOSS. Next, we’ll tune the top_rate and other_rate parameters to see if we can further boost the model’s performance.\n\n⚠️ Note: When using boosting_type='goss', LightGBM requires that\ntop_rate + other_rate ≤ 1.0\nThis constraint ensures that the combined sample used for training does not exceed the size of the full dataset.\n\n\n# tuning the top_rate and other_rate parameters\n# Initialize the LightGBM regressor with GOSS\nmodel_goss_tune = lgb.LGBMRegressor(\n    boosting_type='goss',\n    random_state=42\n)\n# Define the parameter grid for tuning\nparam_grid = {\n    'top_rate': Real(0.1, 0.6, prior='uniform'),\n    'other_rate': Real(0.1, 0.4, prior='uniform'),\n}\n# Initialize the BayesSearchCV object\nopt = BayesSearchCV(\n    model_goss_tune,\n    param_grid,\n    n_iter=10,\n    cv=3,\n    n_jobs=-1,\n    random_state=42\n)\n# Fit the model\nopt.fit(\n    X_train,\n    y_train,\n    categorical_feature=categorical_feature\n)\n# the best parameters\nprint(\"Best parameters found: \", opt.best_params_)\n\n# Predict on the test set\ny_pred_opt = opt.predict(X_test)\n\n# Calculate evaluation metrics\nrmse_opt = root_mean_squared_error(y_test, y_pred_opt)\nr2_opt = r2_score(y_test, y_pred_opt)\n# Output results\nprint(f\"Test RMSE (GOSS with tuning): {rmse_opt:.4f}\")\nprint(f\"Test R² (GOSS with tuning): {r2_opt:.4f}\")\n\nBest parameters found:  OrderedDict({'other_rate': 0.33986603248215197, 'top_rate': 0.31901459322046166})\nTest RMSE (GOSS with tuning): 3458.7664\nTest R² (GOSS with tuning): 0.9592\n\n\n\n\n11.2.3.5 Optimizing LightGBM with Categorical Features and BayesSearchCV\nBayesSearchCV from scikit-optimize provides an efficient way to tune hyperparameters. Here’s how to set this up:\n\n%%time\n# ===== 2. Hyperparameter Tuning with Bayesian Optimization =====\n# Define the parameter space for Bayesian optimization\nparam_space = {\n    'num_leaves': Integer(20, 100),\n    'max_depth': Integer(5, 50),\n    'min_data_in_leaf': Integer(1, 100),\n    'learning_rate': Real(0.01, 0.5, prior='uniform'),\n    'n_estimators': Integer(50, 500),\n    'top_rate': Real(0.1, 0.6, prior='uniform'),\n    'other_rate': Real(0.1, 0.4, prior='uniform'),\n}\n# Create the Bayesian search object\nbayes_search = BayesSearchCV(\n    # using verbose=-1 to suppress warnings\n    # using n_jobs=-1 to use all available cores\n    # using random_state=42 for reproducibility\n    estimator=lgb.LGBMRegressor( categorical_feature=categorical_feature, random_state=42, boosting_type='goss', verbose=-1),\n    # Define the parameter space for Bayesian optimization\n    search_spaces=param_space,\n    n_iter=50,\n    scoring='neg_root_mean_squared_error',\n    cv=3,\n    n_jobs=-1,\n    random_state=42\n)\n# Fit the Bayesian search object to the training data\nbayes_search.fit(X_train, y_train)\n# Get the best parameters and score\nbest_params = bayes_search.best_params_\nbest_score = bayes_search.best_score_\nprint(f\"Best Parameters: {best_params}\")\nprint(f\"Best Score: {best_score}\")\n# Get the best model\nbest_model = bayes_search.best_estimator_\n# Make predictions on the test set\ny_pred_bayes = best_model.predict(X_test)\n# Calculate RMSE and R2 score for the best model\nrmse_bayes = root_mean_squared_error(y_test, y_pred_bayes)\nr2_bayes = r2_score(y_test, y_pred_bayes)\nprint(f\"RMSE (Bayesian Optimized): {rmse_bayes}\")\nprint(f\"R2 Score (Bayesian Optimized): {r2_bayes}\")\n\nBest Parameters: OrderedDict({'learning_rate': 0.31777940485083805, 'max_depth': 5, 'min_data_in_leaf': 47, 'n_estimators': 369, 'num_leaves': 20, 'other_rate': 0.4, 'top_rate': 0.6})\nBest Score: -3361.8218393725633\nRMSE (Bayesian Optimized): 3071.418344800289\nR2 Score (Bayesian Optimized): 0.9678447743461689\nCPU times: total: 49.4 s\nWall time: 1min 35s\n\n\nUsing GOSS and Feature Estimation by Bagging (FEB) led to a slight improvement in performance compared to XGBoost, while also reducing the time required for cross-validation tuning.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#catboost",
    "href": "LightGBM_CatBoost.html#catboost",
    "title": "11  LightGBM and CatBoost",
    "section": "11.3 CatBoost",
    "text": "11.3 CatBoost\n\n11.3.1 What is CatBoost?\nCatBoost (short for Categorical Boosting) is a high-performance gradient boosting framework developed by Yandex, specifically designed to handle datasets with categorical features without requiring manual preprocessing.\nLike XGBoost and LightGBM, it is based on gradient boosting over decision trees, but CatBoost introduces key innovations that make it robust, easy to use, and effective out of the box—particularly on tabular data.\n\n\n11.3.2 What Makes CatBoost Unique?\nCatBoost offers several innovations that distinguish it from other boosting frameworks:\n\n11.3.2.1 Native Categorical Feature Encoding\nCatBoost can natively process categorical features using an approach based on ordered target statistics, which:\n\nAvoids target leakage during training\nTypically outperforms traditional encodings like one-hot or label encoding\nRequires no manual preprocessing — simply specify the categorical columns\n\n\n\n11.3.2.2 Ordered Boosting (vs. Standard Boosting)\nTraditional gradient boosting algorithms often suffer from prediction shift, a form of overfitting that occurs when the model uses the same data to compute residuals and to fit new trees.\nCatBoost addresses this with ordered boosting, a permutation-driven strategy that builds each tree on one subset of data and computes residuals on another (unseen) subset.\nRecall that gradient boosting fits trees on the gradient of the loss function:\n\\[\nr_m = -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f = f_{m-1}}\n\\]\nIn classic boosting, this gradient is calculated using the same training observations that were used to fit the model, which leads to target leakage.\nIn contrast, CatBoost:\n\nShuffles the data at each iteration\nComputes residuals for an observation only from prior observations in the permutation\nEnsures that each gradient estimate is based on unseen data\n\nThis significantly improves the model’s generalizability and reduces overfitting, especially on small or noisy datasets.\n\n\n11.3.2.3 Symmetric (Oblivious) Trees\nCatBoost builds symmetric (oblivious) decision trees, where the same splitting condition is applied across each level of the tree. This structure results in:\n\nFaster inference times\nCompact model size\nImproved regularization, due to the constrained tree structure\n\nThese trees are particularly well-suited for deployment scenarios where prediction speed matters.\nTogether, these innovations make CatBoost a strong candidate for modeling high-dimensional, categorical, and imbalanced tabular data, even with minimal feature engineering or hyperparameter tuning.\nThe authors have also shown that CatBoost performs better than XGBoost and LightGBM without tuning, i.e., with default hyperparameter settings.\nRead the CatBoost paper for more details.\nHere is a good blog listing the key features of CatBoost.\n\n\n\n11.3.3 Installing and Using CatBoost with Scikit-Learn API\nCatBoost provides a scikit-learn-compatible API through CatBoostClassifier and CatBoostRegressor, which makes it easy to integrate into pipelines and use with tools like GridSearchCV, cross_val_score, and train_test_split.\n\n\n11.3.4 Installation\nTo install CatBoost, run:\npip install catboost\n\n💡 GPU users: CatBoost automatically detects and uses GPU if available. You can explicitly enable it with task_type='GPU'.\n\n\n\n11.3.5 CatBoost for Regression\nLet us check the performance of CatBoostRegressor() without tuning, i.e., with default hyperparameter settings on our car dataset\nThe parameter cat_features will be used to specify the indices of the categorical predictors for target encoding.\n\n# build a catboostregressor model\nfrom catboost import CatBoostRegressor\n# Initialize the CatBoost regressor\nmodel_cat = CatBoostRegressor(\n    cat_features=categorical_feature,\n    random_seed=42,\n    verbose=0\n)\n\n# Train the model\nmodel_cat.fit(X_train, y_train)\n# Predict on the test set\ny_pred_cat = model_cat.predict(X_test)\n# Calculate evaluation metrics\nrmse_cat = root_mean_squared_error(y_test, y_pred_cat)\nr2_cat = r2_score(y_test, y_pred_cat)\n# Output results\nprint(f\"Test RMSE (CatBoost): {rmse_cat:.4f}\")\nprint(f\"Test R² (CatBoost): {r2_cat:.4f}\")\n\nTest RMSE (CatBoost): 3307.2604\nTest R² (CatBoost): 0.9627\n\n\nEven with default hyperparameter settings, CatBoost has outperformed both XGBoost and LightGBM in terms of test RMSE and R-squared.\n\n\n11.3.6 Tuning CatBoostRegressor with Optuna\nYou can tune the hyperparameters of CatBoostRegressor using Optuna, just as you would for XGBoost or LightGBM. However, CatBoost uses a different set of hyperparameters.\nFor example, it does not include:\n\nreg_alpha: L1 regularization on leaf weights\n\ncolsample_bytree: Subsample ratio of columns when constructing each tree\n\nThese parameters are available in XGBoost and LightGBM but are not part of CatBoost’s configuration.\n\nimport optuna\nfrom optuna import create_study\nfrom catboost import CatBoostRegressor, Pool\n\n# create a validation set for early stopping\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n\n#convert to Catboost pool\ntrain_pool = Pool(X_train, y_train, cat_features=categorical_feature)\nvalid_pool = Pool(X_valid, y_valid, cat_features=categorical_feature)\n\n# Define the objective function for Optuna\ndef objective(trial):\n    # Define the hyperparameters to tune\n    params = {\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 30),\n        'border_count': trial.suggest_int('border_count', 32, 255),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n        'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n        \n        # Fixed parameters\n        'iterations': 3000,  # Set to a high number, early stopping will determine the actual number\n        'verbose': False,\n        'random_seed': 42\n    }\n    \n    # Create and train the model with early stopping\n    model = CatBoostRegressor(**params)\n    \n    # Use early stopping to prevent overfitting\n    model.fit(\n        train_pool,\n        eval_set=valid_pool,\n        early_stopping_rounds=20,  # Stop if no improvement for 50 rounds\n        verbose=False\n    )\n    \n    # Evaluate on validation set\n    y_pred = model.predict(valid_pool)\n    val_rmse = root_mean_squared_error(y_valid, y_pred)\n    \n    # Return negative RMSE (for maximization)\n    return -val_rmse\n\n# Create and run the study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n\n[I 2025-05-14 03:37:01,074] A new study created in memory with name: no-name-782044d9-7185-49f6-a68b-ddc83fa639a4\n[I 2025-05-14 03:37:20,600] Trial 0 finished with value: -2592.5059280623623 and parameters: {'learning_rate': 0.07952470550019325, 'depth': 8, 'l2_leaf_reg': 0.001114023475261074, 'min_data_in_leaf': 27, 'border_count': 83, 'bagging_temperature': 0.5317851293779137, 'random_strength': 0.0005861214502486961, 'grow_policy': 'Depthwise'}. Best is trial 0 with value: -2592.5059280623623.\n[I 2025-05-14 03:37:31,837] Trial 1 finished with value: -2556.728680370376 and parameters: {'learning_rate': 0.245419708648389, 'depth': 7, 'l2_leaf_reg': 3.3386247434742167, 'min_data_in_leaf': 15, 'border_count': 100, 'bagging_temperature': 0.5050586506217019, 'random_strength': 0.0004718975881753086, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:37:40,883] Trial 2 finished with value: -2682.6259877001603 and parameters: {'learning_rate': 0.1520719761941552, 'depth': 8, 'l2_leaf_reg': 0.0046149514168069014, 'min_data_in_leaf': 8, 'border_count': 178, 'bagging_temperature': 0.24466088131049457, 'random_strength': 0.015640185801924847, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:38:15,606] Trial 3 finished with value: -2808.7037865332763 and parameters: {'learning_rate': 0.06614862212494911, 'depth': 10, 'l2_leaf_reg': 0.7245780669681481, 'min_data_in_leaf': 19, 'border_count': 34, 'bagging_temperature': 0.8902029685133379, 'random_strength': 0.013193287592498581, 'grow_policy': 'SymmetricTree'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:38:30,461] Trial 4 finished with value: -2740.549186918632 and parameters: {'learning_rate': 0.2585298479851385, 'depth': 7, 'l2_leaf_reg': 0.12149380817119523, 'min_data_in_leaf': 24, 'border_count': 61, 'bagging_temperature': 0.7820109206688168, 'random_strength': 0.00013909547722379054, 'grow_policy': 'SymmetricTree'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:39:21,344] Trial 5 finished with value: -2699.921784155897 and parameters: {'learning_rate': 0.1558390710067145, 'depth': 8, 'l2_leaf_reg': 2.8639962432617574e-06, 'min_data_in_leaf': 21, 'border_count': 129, 'bagging_temperature': 0.27054829342202646, 'random_strength': 1.0090519644888372, 'grow_policy': 'Lossguide'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:39:33,102] Trial 6 finished with value: -2761.634505100397 and parameters: {'learning_rate': 0.29273701564295207, 'depth': 8, 'l2_leaf_reg': 0.0001383950532512802, 'min_data_in_leaf': 29, 'border_count': 53, 'bagging_temperature': 0.543935579774364, 'random_strength': 1.3150608041342185e-05, 'grow_policy': 'SymmetricTree'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:39:42,048] Trial 7 finished with value: -2796.3445154770925 and parameters: {'learning_rate': 0.1985854921483664, 'depth': 5, 'l2_leaf_reg': 0.0017893237819096464, 'min_data_in_leaf': 24, 'border_count': 228, 'bagging_temperature': 0.6563761366663239, 'random_strength': 3.830259157708319e-05, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:40:06,522] Trial 8 finished with value: -2736.652008119944 and parameters: {'learning_rate': 0.12981791308255294, 'depth': 9, 'l2_leaf_reg': 3.6626986581511205, 'min_data_in_leaf': 14, 'border_count': 169, 'bagging_temperature': 0.26693935389565604, 'random_strength': 6.108854259908809e-08, 'grow_policy': 'SymmetricTree'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:40:33,641] Trial 9 finished with value: -2625.6156037888204 and parameters: {'learning_rate': 0.05266635206771846, 'depth': 10, 'l2_leaf_reg': 8.431301598557988e-05, 'min_data_in_leaf': 22, 'border_count': 38, 'bagging_temperature': 0.20458732239577748, 'random_strength': 0.00026802410291762685, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:40:53,462] Trial 10 finished with value: -2802.6759048814906 and parameters: {'learning_rate': 0.2321480916058081, 'depth': 5, 'l2_leaf_reg': 1.9294653593783387e-08, 'min_data_in_leaf': 2, 'border_count': 110, 'bagging_temperature': 0.024153475153251502, 'random_strength': 3.071215792269348e-07, 'grow_policy': 'Lossguide'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:41:09,188] Trial 11 finished with value: -2626.5794465595454 and parameters: {'learning_rate': 0.0931538267421292, 'depth': 7, 'l2_leaf_reg': 0.044383501183871764, 'min_data_in_leaf': 13, 'border_count': 94, 'bagging_temperature': 0.46970044709843706, 'random_strength': 0.010496613104965287, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:42:21,080] Trial 12 finished with value: -2623.594568822965 and parameters: {'learning_rate': 0.013207489266983119, 'depth': 6, 'l2_leaf_reg': 6.27769599914231e-07, 'min_data_in_leaf': 10, 'border_count': 82, 'bagging_temperature': 0.4901244457515505, 'random_strength': 2.391770871806472e-06, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:42:39,928] Trial 13 finished with value: -2840.1838885804445 and parameters: {'learning_rate': 0.20525833721959236, 'depth': 4, 'l2_leaf_reg': 5.674336337792675, 'min_data_in_leaf': 29, 'border_count': 137, 'bagging_temperature': 0.672572957300273, 'random_strength': 0.0022624464321546684, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:42:58,745] Trial 14 finished with value: -2655.490477601683 and parameters: {'learning_rate': 0.10736768228799226, 'depth': 6, 'l2_leaf_reg': 0.01831562504579733, 'min_data_in_leaf': 17, 'border_count': 82, 'bagging_temperature': 0.4256033457922353, 'random_strength': 6.775957335351946, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:43:07,127] Trial 15 finished with value: -2789.7188981990007 and parameters: {'learning_rate': 0.17513907427821254, 'depth': 9, 'l2_leaf_reg': 6.515138402249421e-06, 'min_data_in_leaf': 7, 'border_count': 169, 'bagging_temperature': 0.9865139963615146, 'random_strength': 0.14575672222120323, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:43:41,175] Trial 16 finished with value: -2621.361959827787 and parameters: {'learning_rate': 0.2821616638222643, 'depth': 6, 'l2_leaf_reg': 0.33163654681962784, 'min_data_in_leaf': 26, 'border_count': 115, 'bagging_temperature': 0.6342355083316009, 'random_strength': 0.001346200849628001, 'grow_policy': 'Lossguide'}. Best is trial 1 with value: -2556.728680370376.\n[I 2025-05-14 03:44:43,383] Trial 17 finished with value: -2556.489614367946 and parameters: {'learning_rate': 0.01940229207182844, 'depth': 9, 'l2_leaf_reg': 0.0010525861450567827, 'min_data_in_leaf': 12, 'border_count': 72, 'bagging_temperature': 0.3496687786887461, 'random_strength': 4.041308474367292e-06, 'grow_policy': 'Depthwise'}. Best is trial 17 with value: -2556.489614367946.\n[I 2025-05-14 03:45:42,592] Trial 18 finished with value: -2866.4522097973445 and parameters: {'learning_rate': 0.01596497708445574, 'depth': 9, 'l2_leaf_reg': 1.1159472661594606e-08, 'min_data_in_leaf': 4, 'border_count': 248, 'bagging_temperature': 0.3623887368574112, 'random_strength': 2.1384578490475107e-06, 'grow_policy': 'Depthwise'}. Best is trial 17 with value: -2556.489614367946.\n[I 2025-05-14 03:46:46,564] Trial 19 finished with value: -2708.191790901802 and parameters: {'learning_rate': 0.24477192572019485, 'depth': 7, 'l2_leaf_reg': 2.3098308271653418e-05, 'min_data_in_leaf': 12, 'border_count': 59, 'bagging_temperature': 0.0711240090347467, 'random_strength': 5.019623855651714e-08, 'grow_policy': 'Lossguide'}. Best is trial 17 with value: -2556.489614367946.\n[I 2025-05-14 03:47:09,708] Trial 20 finished with value: -2629.0927431412074 and parameters: {'learning_rate': 0.045281219242332915, 'depth': 10, 'l2_leaf_reg': 0.010169259979428625, 'min_data_in_leaf': 17, 'border_count': 190, 'bagging_temperature': 0.13109532595280188, 'random_strength': 5.725889350557309e-06, 'grow_policy': 'Depthwise'}. Best is trial 17 with value: -2556.489614367946.\n[I 2025-05-14 03:47:28,038] Trial 21 finished with value: -2662.8455154565036 and parameters: {'learning_rate': 0.08417126506035016, 'depth': 8, 'l2_leaf_reg': 0.0006641702867048814, 'min_data_in_leaf': 16, 'border_count': 95, 'bagging_temperature': 0.3550175252017867, 'random_strength': 6.473733730238062e-05, 'grow_policy': 'Depthwise'}. Best is trial 17 with value: -2556.489614367946.\n[I 2025-05-14 03:48:22,098] Trial 22 finished with value: -2553.5175758331197 and parameters: {'learning_rate': 0.02854304599072048, 'depth': 9, 'l2_leaf_reg': 0.0004429789647772918, 'min_data_in_leaf': 10, 'border_count': 67, 'bagging_temperature': 0.5558186913583201, 'random_strength': 0.0014086113230469067, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:49:10,065] Trial 23 finished with value: -2567.391805156711 and parameters: {'learning_rate': 0.026764961342591816, 'depth': 9, 'l2_leaf_reg': 4.0555978330521646e-07, 'min_data_in_leaf': 10, 'border_count': 68, 'bagging_temperature': 0.5904786989858626, 'random_strength': 0.003513242023709581, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:49:22,054] Trial 24 finished with value: -2622.624216994232 and parameters: {'learning_rate': 0.11762433128006189, 'depth': 7, 'l2_leaf_reg': 0.00011122236949708718, 'min_data_in_leaf': 6, 'border_count': 112, 'bagging_temperature': 0.7681535959955343, 'random_strength': 6.673067700091263e-07, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:49:53,628] Trial 25 finished with value: -2566.687493479829 and parameters: {'learning_rate': 0.0428655871308162, 'depth': 9, 'l2_leaf_reg': 1.2056415233295121, 'min_data_in_leaf': 11, 'border_count': 146, 'bagging_temperature': 0.38006611000782997, 'random_strength': 0.09046406371852381, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:50:02,251] Trial 26 finished with value: -2832.1352269234903 and parameters: {'learning_rate': 0.1970147411078323, 'depth': 10, 'l2_leaf_reg': 0.07162501984849304, 'min_data_in_leaf': 14, 'border_count': 48, 'bagging_temperature': 0.7456911509314165, 'random_strength': 1.598786064756365e-05, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:50:40,171] Trial 27 finished with value: -2601.1648427815717 and parameters: {'learning_rate': 0.038043609585293056, 'depth': 7, 'l2_leaf_reg': 1.6824470580542856e-05, 'min_data_in_leaf': 9, 'border_count': 69, 'bagging_temperature': 0.4427744483199044, 'random_strength': 0.00030247232285297106, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:51:03,817] Trial 28 finished with value: -2723.539972578103 and parameters: {'learning_rate': 0.063339552281372, 'depth': 9, 'l2_leaf_reg': 0.0033137305082140056, 'min_data_in_leaf': 5, 'border_count': 100, 'bagging_temperature': 0.5728331614449053, 'random_strength': 0.08608571674502018, 'grow_policy': 'SymmetricTree'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:51:44,354] Trial 29 finished with value: -2903.438265613962 and parameters: {'learning_rate': 0.22327494818582883, 'depth': 8, 'l2_leaf_reg': 0.00046397471558624393, 'min_data_in_leaf': 19, 'border_count': 77, 'bagging_temperature': 0.33115797120918855, 'random_strength': 0.001283308483124051, 'grow_policy': 'Lossguide'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:51:53,564] Trial 30 finished with value: -3143.416961771734 and parameters: {'learning_rate': 0.2615824515538357, 'depth': 8, 'l2_leaf_reg': 2.6721859802422237e-07, 'min_data_in_leaf': 2, 'border_count': 150, 'bagging_temperature': 0.5255403450913998, 'random_strength': 3.7142097112219726e-07, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:53:18,711] Trial 31 finished with value: -2620.1543675837006 and parameters: {'learning_rate': 0.036092954832845386, 'depth': 9, 'l2_leaf_reg': 1.6375335570564933, 'min_data_in_leaf': 11, 'border_count': 126, 'bagging_temperature': 0.4070745587934888, 'random_strength': 1.0037092538282515e-08, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:53:59,231] Trial 32 finished with value: -2566.1296831641434 and parameters: {'learning_rate': 0.06594270014769671, 'depth': 9, 'l2_leaf_reg': 0.5692095900658858, 'min_data_in_leaf': 12, 'border_count': 208, 'bagging_temperature': 0.3165260807907623, 'random_strength': 0.06397745585020524, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:54:35,567] Trial 33 finished with value: -2638.995606936495 and parameters: {'learning_rate': 0.07724237363909499, 'depth': 8, 'l2_leaf_reg': 0.2844212377732495, 'min_data_in_leaf': 15, 'border_count': 204, 'bagging_temperature': 0.16971199093261635, 'random_strength': 0.04813732436092434, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:56:06,127] Trial 34 finished with value: -2647.590494720153 and parameters: {'learning_rate': 0.06460022496756479, 'depth': 10, 'l2_leaf_reg': 6.601504952572579, 'min_data_in_leaf': 8, 'border_count': 215, 'bagging_temperature': 0.280595071181398, 'random_strength': 0.5822640982430759, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 03:56:26,769] Trial 35 finished with value: -2625.921488111509 and parameters: {'learning_rate': 0.14705058259331355, 'depth': 10, 'l2_leaf_reg': 0.017914490598987375, 'min_data_in_leaf': 13, 'border_count': 39, 'bagging_temperature': 0.3287024903377104, 'random_strength': 0.008267542278170484, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:00:07,278] Trial 36 finished with value: -2713.318053553942 and parameters: {'learning_rate': 0.011286702277088917, 'depth': 8, 'l2_leaf_reg': 0.2339927288492896, 'min_data_in_leaf': 18, 'border_count': 243, 'bagging_temperature': 0.5034843298839619, 'random_strength': 0.00055952127058293, 'grow_policy': 'SymmetricTree'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:01:52,318] Trial 37 finished with value: -2599.3830690422274 and parameters: {'learning_rate': 0.026614468934226813, 'depth': 9, 'l2_leaf_reg': 0.006370159217597417, 'min_data_in_leaf': 8, 'border_count': 97, 'bagging_temperature': 0.855566752212292, 'random_strength': 8.32305758695803e-05, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:02:36,816] Trial 38 finished with value: -2610.623762642826 and parameters: {'learning_rate': 0.09686963957512237, 'depth': 7, 'l2_leaf_reg': 0.0014074747896680212, 'min_data_in_leaf': 21, 'border_count': 71, 'bagging_temperature': 0.7156956610821732, 'random_strength': 0.018501470320959317, 'grow_policy': 'SymmetricTree'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:03:07,048] Trial 39 finished with value: -2671.015969019692 and parameters: {'learning_rate': 0.16854993390170198, 'depth': 6, 'l2_leaf_reg': 0.6496465232605684, 'min_data_in_leaf': 15, 'border_count': 53, 'bagging_temperature': 0.22219445776878718, 'random_strength': 0.7258361806395729, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:04:30,028] Trial 40 finished with value: -2575.0422520898715 and parameters: {'learning_rate': 0.13301237034987956, 'depth': 8, 'l2_leaf_reg': 0.00025678296288065243, 'min_data_in_leaf': 13, 'border_count': 129, 'bagging_temperature': 0.5756243041812479, 'random_strength': 1.949907402720845e-05, 'grow_policy': 'Lossguide'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:05:48,971] Trial 41 finished with value: -2642.556436107284 and parameters: {'learning_rate': 0.049948179063764114, 'depth': 9, 'l2_leaf_reg': 1.5618546291904019, 'min_data_in_leaf': 11, 'border_count': 158, 'bagging_temperature': 0.3964202751479039, 'random_strength': 2.393747505741706, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:06:46,627] Trial 42 finished with value: -2598.4755135822797 and parameters: {'learning_rate': 0.0580674532442787, 'depth': 9, 'l2_leaf_reg': 1.8177693207658425, 'min_data_in_leaf': 11, 'border_count': 187, 'bagging_temperature': 0.29635746209371416, 'random_strength': 0.21208291644651261, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:07:19,719] Trial 43 finished with value: -2657.7880298639225 and parameters: {'learning_rate': 0.07363937743695215, 'depth': 10, 'l2_leaf_reg': 0.04359530886331162, 'min_data_in_leaf': 9, 'border_count': 217, 'bagging_temperature': 0.44664672961449514, 'random_strength': 0.04495554133929357, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:09:46,045] Trial 44 finished with value: -2691.606927827555 and parameters: {'learning_rate': 0.029786575860281366, 'depth': 9, 'l2_leaf_reg': 9.943736562101853, 'min_data_in_leaf': 12, 'border_count': 152, 'bagging_temperature': 0.36930442821158393, 'random_strength': 0.0038218125123354175, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:10:38,605] Trial 45 finished with value: -2616.0330119726023 and parameters: {'learning_rate': 0.046671945704494774, 'depth': 8, 'l2_leaf_reg': 0.12115447142224317, 'min_data_in_leaf': 14, 'border_count': 140, 'bagging_temperature': 0.48481335723150554, 'random_strength': 0.3294399030680058, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:12:54,361] Trial 46 finished with value: -2590.002658938037 and parameters: {'learning_rate': 0.025820733438051514, 'depth': 10, 'l2_leaf_reg': 1.0279696260416582, 'min_data_in_leaf': 16, 'border_count': 85, 'bagging_temperature': 0.2455200340992076, 'random_strength': 0.00015044433051361004, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:13:57,004] Trial 47 finished with value: -2635.6488533277184 and parameters: {'learning_rate': 0.09024036605373528, 'depth': 7, 'l2_leaf_reg': 5.4022309753871646e-05, 'min_data_in_leaf': 19, 'border_count': 43, 'bagging_temperature': 0.5347005159269773, 'random_strength': 0.017468337161715447, 'grow_policy': 'SymmetricTree'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:14:16,489] Trial 48 finished with value: -2832.750001651622 and parameters: {'learning_rate': 0.29562301087772724, 'depth': 9, 'l2_leaf_reg': 4.218536936460986, 'min_data_in_leaf': 10, 'border_count': 232, 'bagging_temperature': 0.6069911018530717, 'random_strength': 1.9157498081287359, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n[I 2025-05-14 04:14:46,553] Trial 49 finished with value: -2609.0502948976673 and parameters: {'learning_rate': 0.10265216059916468, 'depth': 8, 'l2_leaf_reg': 0.6037874295139654, 'min_data_in_leaf': 7, 'border_count': 122, 'bagging_temperature': 0.39641107195397474, 'random_strength': 0.0009672456521149151, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.\n\n\n\n# Get best parameters and train final model with early stopping\nbest_params = study.best_params\nprint(\"Best parameters:\", best_params)\n\n# Get the best trial\nbest_trial = study.best_trial\nprint(\"Best trial:\", best_trial)\n\nBest parameters: {'learning_rate': 0.02854304599072048, 'depth': 9, 'l2_leaf_reg': 0.0004429789647772918, 'min_data_in_leaf': 10, 'border_count': 67, 'bagging_temperature': 0.5558186913583201, 'random_strength': 0.0014086113230469067, 'grow_policy': 'Depthwise'}\nBest trial: FrozenTrial(number=22, state=1, values=[-2553.5175758331197], datetime_start=datetime.datetime(2025, 5, 14, 3, 47, 28, 39246), datetime_complete=datetime.datetime(2025, 5, 14, 3, 48, 22, 98057), params={'learning_rate': 0.02854304599072048, 'depth': 9, 'l2_leaf_reg': 0.0004429789647772918, 'min_data_in_leaf': 10, 'border_count': 67, 'bagging_temperature': 0.5558186913583201, 'random_strength': 0.0014086113230469067, 'grow_policy': 'Depthwise'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.3, log=False, low=0.01, step=None), 'depth': IntDistribution(high=10, log=False, low=4, step=1), 'l2_leaf_reg': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'min_data_in_leaf': IntDistribution(high=30, log=False, low=1, step=1), 'border_count': IntDistribution(high=255, log=False, low=32, step=1), 'bagging_temperature': FloatDistribution(high=1.0, log=False, low=0.0, step=None), 'random_strength': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'grow_policy': CategoricalDistribution(choices=('SymmetricTree', 'Depthwise', 'Lossguide'))}, trial_id=22, value=None)\n\n\n\nnp.concatenate((y_train, y_valid))\n\narray([48750, 17949, 22995, ..., 27300,  7952, 13498], dtype=int64)\n\n\n\n# Use column indices instead of names\ncat_feature_indices = [X_train.columns.get_loc(col) for col in categorical_feature]\n\n# Add iterations parameter back for final model\nbest_params['iterations'] = 3000  # High number, early stopping will be used\n\n# create a train+validation set for final model\ntrain_val_pool = Pool(\n    np.vstack((X_train, X_valid)),\n    np.concatenate((y_train, y_valid)),\n    cat_features=cat_feature_indices\n)\n\n# Create a test pool\ntest_pool = Pool(X_test, y_test, cat_features=categorical_feature)\n\n# Train final model on combined train+validation data\nfinal_model = CatBoostRegressor(**best_params)\nfinal_model.fit(\n    train_val_pool,\n    eval_set=test_pool,\n    early_stopping_rounds=50,\n    verbose=False\n)\n\n# Get actual number of trees used after early stopping\nactual_iterations = final_model.tree_count_\nprint(f\"Actual number of trees used: {actual_iterations}\")\n\n# Evaluate on test set\ny_pred_test = final_model.predict(X_test)\ntest_rmse = root_mean_squared_error(y_test, y_pred_test)\ntest_r2 = r2_score(y_test, y_pred_test)\nprint(f\"Test RMSE: {test_rmse:.4f}\")\nprint(f\"Test R²: {test_r2:.4f}\")\n\nActual number of trees used: 466\nTest RMSE: 3042.9611\nTest R²: 0.9684\n\n\n\nfig1 = optuna.visualization.plot_optimization_history(study)\nfig1.show()\n    \nfig2 = optuna.visualization.plot_param_importances(study)\nfig2.show()\n    \n# Plot feature importance from the final model\nfeature_importance = final_model.get_feature_importance()\nsorted_idx = np.argsort(feature_importance)\nplt.figure(figsize=(6, 9))\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\nplt.yticks(range(len(sorted_idx)), np.array(range(X.shape[1]))[sorted_idx])\nplt.title('CatBoost Feature Importance')\nplt.tight_layout()\nplt.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\n\n\n\n\nIt takes 2 minutes to tune CatBoost, which is higher than LightGBM and lesser than XGBoost. CatBoost falls in between LightGBM and XGBoost in terms of speed. However, it is likely to be more accurate than XGBoost and LighGBM, and likely to require lesser tuning as compared to XGBoost.\nCheck the documentation for hyperparameter tuning\n\n\n11.3.7 When to Use CatBoost Over XGBoost\n\nWhen your dataset contains many categorical features\n\nCatBoost tends to perform well out of the box with minimal hyperparameter tuning, making it more user-friendly for quick experimentation or deployment\n\nCatBoost’s GPU implementation is optimized for handling categorical data efficiently, and can outperform XGBoost on datasets dominated by categorical variables\n&gt; While both libraries support GPU acceleration, CatBoost’s architecture is particularly well-suited for categorical-heavy tasks",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost",
    "href": "LightGBM_CatBoost.html#handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost",
    "title": "11  LightGBM and CatBoost",
    "section": "11.4 Handling Imbalanced Classification: XGBoost vs. LightGBM vs. CatBoost",
    "text": "11.4 Handling Imbalanced Classification: XGBoost vs. LightGBM vs. CatBoost\nImbalanced classification occurs when one class significantly outnumbers the other (e.g., fraud detection, disease diagnosis). Each boosting library offers tools to address this issue:\nXGBoost:\n\nParameter: scale_pos_weight\n\nFormula:\n\\[\n\\texttt{scale\\_pos\\_weight} = \\frac{\\text{Number of negative samples}}{\\text{Number of positive samples}}\n\\]\nIncreases the gradient of the positive class during training.\n\nAdditional Strategies:\n\nUse custom eval_metric (e.g., \"auc\", \"aucpr\", or \"logloss\")\nApply early stopping on validation AUC\n\n\nLightGBM:\n\nParameter: scale_pos_weight (same as in XGBoost)\nAlternative: is_unbalance = TRUE\n\nAutomatically adjusts class weights based on distribution\n\nOther Tips:\n\nUse metric = \"auc\" or \"binary_logloss\" for better guidance during training\nResampling techniques also compatible\n\n\nCatBoost:\n\nParameter: class_weights\n\nAccepts a numeric vector (e.g., class_weights = c(1, 5) for [negative, positive])\nDirectly modifies the loss function to emphasize minority class\n\nAdvantages:\n\nMore flexible than scale_pos_weight\nWorks well with default settings\n\nOther Tips:\n\nUse loss_function = \"Logloss\" and eval_metric = \"AUC\" for binary classification\n\n\nBelow is the summary table:\n\n\n\n\n\n\n\n\n\nLibrary\nImbalance Handling Parameter\nDefault Support\nRecommended Metric\n\n\n\n\nXGBoost\nscale_pos_weight\nNo\nauc, aucpr\n\n\nLightGBM\nscale_pos_weight, is_unbalance\nYes (with flag)\nauc, binary_logloss\n\n\nCatBoost\nclass_weights\nYes\nLogloss, AUC",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#summary-xgboost-vs.-lightgbm-vs.-catboost",
    "href": "LightGBM_CatBoost.html#summary-xgboost-vs.-lightgbm-vs.-catboost",
    "title": "11  LightGBM and CatBoost",
    "section": "11.5 Summary: XGBoost vs. LightGBM vs. CatBoost",
    "text": "11.5 Summary: XGBoost vs. LightGBM vs. CatBoost\nGradient boosting is a powerful ensemble technique, and XGBoost, LightGBM, and CatBoost are three of its most widely used implementations. Each has unique strengths and is well-suited to different use cases.\nXGBoost:\n\nStrengths: Robust, well-documented, strong performance on structured/tabular data\n\nSplit Finding: Level-wise tree growth\n\nRegularization: Explicit L1 and L2 regularization\n\nFlexibility: Highly customizable with many hyperparameters\n\nBest for: General-purpose tabular data, especially when you have time to tune parameters\n\nLightGBM:\n\nStrengths: Fast training, low memory usage, excellent scalability\n\nSplit Finding: Leaf-wise tree growth with depth control\n\nBinning: Uses histogram-based algorithm with max_bin to speed up training\n\nBest for: Large-scale datasets, high-dimensional features, and when training speed matters\n\nCatBoost:\n\nStrengths: Handles categorical features natively, works well with minimal tuning\n\nBoosting Innovation: Uses ordered boosting to prevent prediction shift\n\nCategorical Encoding: No need for manual preprocessing — uses target-based encoding internally\n\nBest for: Datasets with many categorical variables or limited time for tuning\n\nFinal Thoughts\nAll three libraries are powerful and battle-tested. Here’s a rough guideline:\n\nUse XGBoost if you want control, flexibility, and a well-documented standard\nUse LightGBM when training speed and large data scalability are your top priorities\nUse CatBoost when working with many categorical features or seeking strong baseline results with minimal tuning",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#references",
    "href": "LightGBM_CatBoost.html#references",
    "title": "11  LightGBM and CatBoost",
    "section": "11.6 References",
    "text": "11.6 References\n\nLightGBM Paper (Original NIPS 2017)\nLightGBM Official Website\nCatBoost Paper (arXiv)\nCatBoost Official Website",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "Lec10_Ensemble.html",
    "href": "Lec10_Ensemble.html",
    "title": "12  Ensemble modeling",
    "section": "",
    "text": "12.1 Ensembling regression models\nEnsembling models can help reduce error by leveraging the diversity and collective wisdom of multiple models. When ensembling, several individual models are trained independently and their predictions are combined to make the final prediction.\nWe have already seen examples of ensemble models in chapters 5 - 13. The ensembled models may reduce error by reducing the bias (boosting) and / or reducing the variance (bagging / random forests / boosting).\nHowever, in this chapter we’ll ensemble different types of models, instead of the same type of model. We may ensemble a linear regression model, a random forest, a gradient boosting model, and as many different types of models as we wish.\nBelow are a couple of reasons why ensembling models can be effective in reducing error:\nMathematically also, we can show the effectiveness of an ensemble model. Let’s consider the case of regression, and let the predictors be denoted as \\(X\\), and the response as \\(Y\\). Let \\(f_1, ..., f_m\\) be individual models. The expected MSE of an ensemble can be written as:\n\\[ E(MSE_{Ensemble}) = E\\bigg[\\bigg( \\frac{1}{m} \\sum_{i = 1}^{m} f_i(X) - Y \\bigg)^2 \\bigg] = \\frac{1}{m^2} \\sum_{i = 1}^{m} E \\bigg[\\big(f_i(X) - Y\\big)^2 \\bigg] + \\frac{1}{m^2} \\sum_{i \\ne j} E\\bigg[\\big(f_i(X) - Y\\big)\\big(f_j(X) - Y\\big) \\bigg]\\]\n\\[ \\implies E(MSE_{Ensemble}) = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m E \\bigg[\\big(f_i(X) - Y\\big)^2 \\bigg]\\bigg) + \\frac{1}{m^2} \\sum_{i \\ne j} E\\bigg[\\big(f_i(X) - Y\\big)\\big(f_j(X) - Y\\big) \\bigg]\\]\n\\[ \\implies E(MSE_{Ensemble}) = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i})\\bigg) + \\frac{1}{m^2} \\sum_{i \\ne j} E\\bigg[\\big(f_i(X) - Y\\big)\\big(f_j(X) - Y\\big) \\bigg]\\]\nIf \\(f_1, ..., f_m\\) are unbiased, then,\n\\[ E(MSE_{Ensemble}) = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i})\\bigg) + \\frac{1}{m^2} \\sum_{i \\ne j} Cov(f_i(X), f_j(X))\\]\nAssuming the models are uncorrelated (i.e., they have a zero correlation), the second term (covariance of \\(f_i(.)\\) and \\(f_j(.)\\)) reduces to zero, and the expected MSE of the ensemble reduces to:\n\\[\nE(MSE_{Ensemble}) = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i})\\bigg)\n\\tag{12.1}\\]\nThus, the expected MSE of an ensemble model with uncorrelated models is much smaller than the average MSE of all the models. Unless there is a model that is much better than the rest of the models, the MSE of the ensemble model is likely to be lower than the MSE of the individual models. However, there is no guarantee that the MSE of the ensemble model will be lower than the MSE of the individual models. Consider an extreme case where only one of the models have a zero MSE. The MSE of this model will be lower than the expected MSE of the ensemble model.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ensemble modeling</span>"
    ]
  },
  {
    "objectID": "Lec10_Ensemble.html#ensembling-regression-models",
    "href": "Lec10_Ensemble.html#ensembling-regression-models",
    "title": "12  Ensemble modeling",
    "section": "",
    "text": "12.1.1 Voting Regressor\nHere, we will combine the predictions of different models. The function VotingRegressor() averages the predictions of all the models.\nBelow are the individual models tuned in the previous chapters.\n\n#Tuned AdaBoost model from Section 7.2.4\nmodel_ada = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,\n                    learning_rate=1.0,  random_state=1).fit(X, y)\nprint(\"RMSE for AdaBoost = \", np.sqrt(mean_squared_error(model_ada.predict(Xtest), ytest)))\n\n#Tuned Random forest model from Section 6.1.2\nmodel_rf = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\nprint(\"RMSE for Random forest = \", np.sqrt(mean_squared_error(model_rf.predict(Xtest), ytest)))\n\n# Tuned XGBoost model from Section 9.2.6\nmodel_xgb = xgb.XGBRegressor(random_state=1,max_depth=8,n_estimators=1000, subsample = 0.75, colsample_bytree = 1.0,\n                                         learning_rate = 0.01,reg_lambda=1, gamma = 100).fit(X, y)\nprint(\"RMSE for XGBoost = \", np.sqrt(mean_squared_error(model_xgb.predict(Xtest), ytest)))\n\n#Tuned gradient boosting model from Section 8.2.5\nmodel_gb = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n                         random_state=1,loss='huber').fit(X, y)\nprint(\"RMSE for Gradient Boosting = \", np.sqrt(mean_squared_error(model_gb.predict(Xtest), ytest)))\n\n# Tuned Light GBM model from Section 13.1.1\nmodel_lgbm = LGBMRegressor(subsample = 0.5, reg_lambda = 0, reg_alpha = 100, boosting_type = 'goss',\n            num_leaves = 31, n_estimators = 500, learning_rate = 0.05, colsample_bytree = 1.0,\n                          top_rate = 0.5).fit(X, y)\nprint(\"RMSE for LightGBM = \", np.sqrt(mean_squared_error(model_lgbm.predict(Xtest), ytest)))\n\n# Tuned CatBoost model from Section 13.2.3\nmodel_cat = CatBoostRegressor(subsample=0.5, num_leaves=40, n_estimators=500, max_depth=10, \n                              verbose = False, learning_rate = 0.05, colsample_bylevel=0.75, \n                              grow_policy='Lossguide', random_state = 1).fit(X, y)\nprint(\"RMSE for CatBoost = \", np.sqrt(mean_squared_error(model_cat.predict(Xtest), ytest)))\n\nRMSE for AdaBoost =  5693.165811600585\nRMSE for Random forest =  5642.45839697972\nRMSE for XGBoost =  5497.553788113875\nRMSE for Gradient Boosting =  5405.787029062213\nRMSE for LightGBM =  5355.964600884197\nRMSE for CatBoost =  5271.104736146779\n\n\nNote that we don’t need to fit the models individually before fitting them simultaneously in the voting ensemble. If we fit them individual, it will unnecessarily waste time.\nLet us ensemble the models using the voting ensemble with equal weights.\n\n#Voting ensemble: Averaging the predictions of all models\n\n#Tuned AdaBoost model from Section 7.2.4\nmodel_ada = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=10),\n                    n_estimators=50,learning_rate=1.0,  random_state=1)\n\n#Tuned Random forest model from Section 6.1.2\nmodel_rf = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2)\n\n# Tuned XGBoost model from Section 9.2.6\nmodel_xgb = xgb.XGBRegressor(random_state=1,max_depth=8,n_estimators=1000, subsample = 0.75, \n                colsample_bytree = 1.0, learning_rate = 0.01,reg_lambda=1, gamma = 100)\n\n#Tuned gradient boosting model from Section 8.2.5\nmodel_gb = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n                         random_state=1,loss='huber')\n\n# Tuned CatBoost model from Section 13.2.3\nmodel_cat = CatBoostRegressor(subsample=0.5, num_leaves=40, n_estimators=500, max_depth=10,\n                             learning_rate = 0.05, colsample_bylevel=0.75, grow_policy='Lossguide',\n                             random_state=1, verbose = False)\n\n# Tuned Light GBM model from Section 13.1.1\nmodel_lgbm = LGBMRegressor(subsample = 0.5, reg_lambda = 0, reg_alpha = 100, boosting_type = 'goss',\n                           num_leaves = 31, n_estimators = 500, learning_rate = 0.05, \n                           colsample_bytree = 1.0, top_rate = 0.5)\n\nstart_time = time.time()\nen = VotingRegressor(estimators = [('xgb',model_xgb),('ada',model_ada),('rf',model_rf),\n                    ('gb',model_gb), ('cat', model_cat), ('lgbm', model_lgbm)], n_jobs = -1)\nen.fit(X,y)\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nEnsemble model RMSE =  5259.899392611916\nTime taken =  0.21 minutes\n\n\nAs expected, RMSE of the ensembled model is less than that of each of the individual models.\nNote that the RMSE can be further improved by removing the weaker models from the ensemble. Let us remove the three weakest models - XGBoost, Random forest, and AdaBoost.\n\n#Voting ensemble: Averaging the predictions of all models\n\nstart_time = time.time()\nen = VotingRegressor(estimators = [('gb',model_gb), ('cat', model_cat), ('lgbm', model_lgbm)], n_jobs = -1)\nen.fit(X,y)\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nEnsemble model RMSE =  5191.814866810768\nTime taken =  0.18 minutes\n\n\n\n\n12.1.2 Stacking Regressor\nStacking is a more sophisticated method of ensembling models. The method is as follows:\n\nThe training data is split into K folds. Each of the K folds serves as a test data in one of the K iterations, and the rest of the folds serve as train data.\nEach model is used to make predictions on each of the K folds, after being trained on the remaining K-1 folds. In this manner, each model predicts the response on each train data point - when that train data point was not used to train the model.\nPredictions at each training data points are generated by each model in step 2 (the above step). These predictions are now used as predictors to train a meta-model (referred by the argument final_estimator), with the original response as the response. The meta-model (or final_estimator) learns to combine predictions of different models to make a better prediction.\n\n\n12.1.2.1 Metamodel: Linear regression\n\n#Stacking using LinearRegression as the metamodel\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n                                     ('gb', model_gb), ('cat', model_cat), ('lgbm', model_lgbm)],\n                     final_estimator=LinearRegression(),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nstart_time = time.time()\nen.fit(X,y)\nprint(\"Linear regression metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nLinear regression metamodel RMSE =  5220.456280327686\nTime taken =  2.03 minutes\n\n\n\n#Co-efficients of the meta-model\nen.final_estimator_.coef_\n\narray([ 0.05502964,  0.14566665,  0.01093624,  0.30478283,  0.57403909,\n       -0.07057344])\n\n\n\nsum(en.final_estimator_.coef_)\n\n1.0198810182715363\n\n\nNote the above coefficients of the meta-model. The model gives the highest weight to the gradient boosting model (with huber loss), and the catboost model, and the lowest weight to the relatively weak random forest model.\nAlso, note that the coefficients need not sum to one.\nLet us try improving the RMSE further by removing the weaker models from the ensemble. Let us remove the three weakest models based on the size of their coefficients in the linear regression metamodel.\n\n#Stacking using LinearRegression as the metamodel\nen = StackingRegressor(estimators = [('gb', model_gb), ('cat', model_cat), ('ada', model_ada)],\n                     final_estimator=LinearRegression(),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nstart_time = time.time()\nen.fit(X,y)\nprint(\"Linear regression metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nLinear regression metamodel RMSE =  5205.225710180056\nTime taken =  1.36 minutes\n\n\nThe metamodel accuracy improves further, when strong models are ensembled.\n\n#Co-efficients of the meta-model\nen.final_estimator_.coef_\n\narray([0.31824119, 0.54231032, 0.15998634])\n\n\n\nsum(en.final_estimator_.coef_)\n\n1.020537847948332\n\n\n\n\n12.1.2.2 Metamodel: Lasso\n\n#Stacking using Lasso as the metamodel\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n                        ('gb', model_gb),('cat', model_cat), ('lgbm', model_lgbm) ],\n                     final_estimator = LassoCV(),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nstart_time = time.time()\nen.fit(X,y)\nprint(\"Lasso metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nLasso metamodel RMSE =  5206.021083501416\nTime taken =  2.05 minutes\n\n\n\n#Coefficients of the lasso metamodel\nen.final_estimator_.coef_\n\narray([ 0.03524446,  0.15077605,  0.        ,  0.30392268,  0.52946243,\n       -0.        ])\n\n\nNote that lasso reduces the weight of the weak random forest model, and light gbm model to 0. Even though light GBM is a strong model, it may be correlated or collinear with XGBoost, or other models, and hence is not needed.\nNote that as lasso performs model selection on its own, removing models with zero coefficients or weights does not make a difference, as shown below.\n\n#Stacking using Lasso as the metamodel\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),\n                        ('gb', model_gb),('cat', model_cat) ],\n                     final_estimator = LassoCV(),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nstart_time = time.time()\nen.fit(X,y)\nprint(\"Lasso metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nLasso metamodel RMSE =  5205.93233977352\nTime taken =  1.79 minutes\n\n\n\n#Coefficients of the lasso metamodel\nen.final_estimator_.coef_\n\narray([0.03415944, 0.15053122, 0.30464838, 0.53006297])\n\n\n\n\n12.1.2.3 Metamodel: Random forest\nA highly flexible model such as a random forest may not be a good choice for ensembling correlated models. However, let us tune the random forest meta model, and check its accuracy.\n\n# Tuning hyperparameter of the random forest meta-model\nstart_time = time.time()\noob_score_i = []\nfor i in range(1, 7):\n    en = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n                        ('gb', model_gb),('cat', model_cat), ('lgbm', model_lgbm)],\n                     final_estimator = RandomForestRegressor(max_features = i, oob_score = True),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1)).fit(X,y)\n    oob_score_i.append(en.final_estimator_.oob_score_)\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nTime taken =  12.08 minutes\n\n\n\nprint(\"Optimal value of max_features =\", np.array(oob_score_i).argmax() + 1)\n\nOptimal value of max_features = 1\n\n\n\n# Training the tuned random forest metamodel\nstart_time = time.time()\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),\n                    ('rf', model_rf), ('gb', model_gb),('cat', model_cat), \n                    ('lgbm', model_lgbm)],\n                final_estimator = RandomForestRegressor(max_features = 1, \n                n_estimators=500), cv = KFold(n_splits = 5, shuffle = True, \n                random_state=1)).fit(X,y)\nprint(\"Random Forest metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nRandom Forest metamodel RMSE =  5441.9155087961\nTime taken =  1.71 minutes\n\n\nNote that highly flexible models may not be needed when the predictors are highly correlated with the response. However, in some cases, they may be useful, as in the classification example in the next section.\n\n\n12.1.2.4 Metamodel: CatBoost\n\n#Stacking using MARS as the meta-model\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n                        ('gb', model_gb),('cat', model_cat), ('lgbm', model_lgbm)],\n                     final_estimator = CatBoostRegressor(verbose = False),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nstart_time = time.time()\nen.fit(X,y)\nprint(\"Random Forest metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\nprint(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")\n\nRandom Forest metamodel RMSE =  5828.803609683251\nTime taken =  1.66 minutes",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ensemble modeling</span>"
    ]
  },
  {
    "objectID": "Lec10_Ensemble.html#ensembling-classification-models",
    "href": "Lec10_Ensemble.html#ensembling-classification-models",
    "title": "12  Ensemble modeling",
    "section": "12.2 Ensembling classification models",
    "text": "12.2 Ensembling classification models\nWe’ll ensemble models for predicting accuracy of identifying people having a heart disease.\n\ndata = pd.read_csv('./Datasets/Heart.csv')\ndata.dropna(inplace = True)\n#Response variable\ny = pd.get_dummies(data['AHD'])['Yes']\n\n#Creating a dataframe for predictors with dummy variables replacing the categorical variables\nX = data.drop(columns = ['AHD','ChestPain','Thal'])\nX = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\n\n#Creating train and test datasets\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)\n\nLet us tune the individual models first.\n\nAdaBoost\n\n# Tuning Adaboost for maximizing accuracy\nmodel = AdaBoostClassifier(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['base_estimator'] = [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), \n                          DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=4)]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',refit='accuracy')\n# execute the grid search\ngrid_result = grid_search.fit(Xtrain, ytrain)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\nBest: 0.871494 using {'base_estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 0.01, 'n_estimators': 200}\n\n\n\n\nGradient Boosting\n\n# Tuning gradient boosting for maximizing accuracy\nmodel = GradientBoostingClassifier(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['max_depth'] = [1,2,3,4,5]\ngrid['subsample'] = [0.5,1.0]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',refit='accuracy')\n# execute the grid search\ngrid_result = grid_search.fit(Xtrain, ytrain)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\nBest: 0.871954 using {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 100, 'subsample': 1.0}\n\n\n\n\nXGBoost\n\n# Tuning XGBoost for maximizing accuracy\nstart_time = time.time()\nparam_grid = {'n_estimators':[25, 100,250,500],\n                'max_depth': [4, 6 ,8],\n              'learning_rate': [0.01,0.1,0.2],\n               'gamma': [0, 1, 10, 100],\n               'reg_lambda':[0, 10, 100],\n               'subsample': [0.5, 0.75, 1.0]\n                'scale_pos_weight':[1.25,1.5,1.75]#Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances).\n             }\n\ncv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = GridSearchCV(estimator=xgb.XGBClassifier(random_state=1),\n                             param_grid = param_grid,\n                             scoring = 'accuracy',\n                             verbose = 1,\n                             n_jobs=-1,\n                             cv = cv)\noptimal_params.fit(Xtrain,ytrain)\nprint(optimal_params.best_params_,optimal_params.best_score_)\nprint(\"Time taken = \", (time.time()-start_time)/60, \" minutes\")\n\nFitting 5 folds for each of 972 candidates, totalling 4860 fits\n{'gamma': 0, 'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 25, 'reg_lambda': 0, 'scale_pos_weight': 1.25} 0.872183908045977\nTime taken =  0.9524135629336039  minutes\n\n\n\n#Tuned Adaboost model\nmodel_ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=200, \n                               random_state=1,learning_rate=0.01).fit(Xtrain, ytrain)    \ntest_accuracy_ada = model_ada.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n    \n#Tuned Random forest model from Section 6.3\nmodel_rf = RandomForestClassifier(n_estimators=500, random_state=1,max_features=3,\n                        n_jobs=-1,oob_score=False).fit(Xtrain, ytrain)\ntest_accuracy_rf = model_rf.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n    \n#Tuned gradient boosting model\nmodel_gb = GradientBoostingClassifier(n_estimators=100, random_state=1,max_depth=4,learning_rate=1.0,\n                                     subsample = 1.0).fit(Xtrain, ytrain)\ntest_accuracy_gb = model_gb.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n\n#Tuned XGBoost model\nmodel_xgb = xgb.XGBClassifier(random_state=1,gamma=0,learning_rate = 0.2,max_depth=4,\n                              n_estimators = 25,reg_lambda = 0,scale_pos_weight=1.25).fit(Xtrain,ytrain)\ntest_accuracy_xgb = model_xgb.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n\nprint(\"Adaboost accuracy = \",test_accuracy_ada)\nprint(\"Random forest accuracy = \",test_accuracy_rf)\nprint(\"Gradient boost accuracy = \",test_accuracy_gb)\nprint(\"XGBoost model accuracy = \",test_accuracy_xgb)\n\nAdaboost accuracy =  0.7986577181208053\nRandom forest accuracy =  0.8120805369127517\nGradient boost accuracy =  0.7986577181208053\nXGBoost model accuracy =  0.7785234899328859\n\n\n\n\n12.2.1 Voting classifier - hard voting\nIn this type of ensembling, the predicted class is the one predicted by the majority of the classifiers.\n\nensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)])\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.825503355704698\n\n\nNote that the prediction accuracy of the ensemble is higher than the prediction accuracy of each of the individual models on unseen data.\n\n\n12.2.2 Voting classifier - soft voting\nIn this type of ensembling, the predicted class is the one based on the average predicted probabilities of all the classifiers. The threshold probability is 0.5.\n\nensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                 voting='soft')\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.7919463087248322\n\n\nNote that soft voting will be good only for well calibrated classifiers, i.e., all the classifiers must have probabilities at the same scale.\n\n\n12.2.3 Stacking classifier\nConceptually, the idea is similar to that of Stacking regressor.\n\n#Using Logistic regression as the meta model (final_estimator)\nensemble_model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                   final_estimator=LogisticRegression(random_state=1,max_iter=10000),n_jobs=-1,\n                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1))\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.7986577181208053\n\n\n\n#Coefficients of the logistic regression metamodel\nensemble_model.final_estimator_.coef_\n\narray([[0.81748051, 1.28663164, 1.64593342, 1.50947087]])\n\n\n\n#Using random forests as the meta model (final_estimator). Note that random forest will require tuning\nensemble_model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                   final_estimator=RandomForestClassifier(n_estimators=500, max_features=1,\n                                                                          random_state=1,oob_score=True),n_jobs=-1,\n                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1))\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.8322147651006712\n\n\nNote that a complex final_estimator such as random forest will require tuning. In the above case, the max_features argument of random forests has been tuned to obtain the maximum OOB score. The tuning is shown below.\n\n#Tuning the random forest parameters\nstart_time = time.time()\noob_score = {}\n\ni=0\nfor pr in range(1,5):\n    model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                   final_estimator=RandomForestClassifier(n_estimators=500, max_features=pr,\n                                    random_state=1,oob_score=True),n_jobs=-1,\n                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)).fit(Xtrain, ytrain)\n    oob_score[pr] = model.final_estimator_.oob_score_\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"max accuracy = \", np.max(list(oob_score.values())))\nprint(\"Best value of max_features= \", np.argmax(list(oob_score.values()))+1)\n\ntime taken =  0.33713538646698  minutes\nmax accuracy =  0.8445945945945946\nBest value of max_features=  1\n\n\n\n#The final predictor (metamodel) - random forest obtains the maximum oob_score for max_features = 1\noob_score\n\n{1: 0.8445945945945946,\n 2: 0.831081081081081,\n 3: 0.8378378378378378,\n 4: 0.831081081081081}\n\n\n\n\n12.2.4 Tuning all models simultaneously\nIndividual model hyperparameters can be tuned simultaneously while ensembling them with a VotingClassifier(). However, this approach can be too expensive for even moderately-sized datasets.\n\n# Create the param grid with the names of the models as prefixes\n\nmodel_ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier())\nmodel_rf = RandomForestClassifier()\nmodel_gb = GradientBoostingClassifier()\nmodel_xgb = xgb.XGBClassifier()\n\nensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)])\n\nhp_grid = dict()\n\n# XGBoost\nhp_grid['xgb__n_estimators'] = [25, 100,250,50]\nhp_grid['xgb__max_depth'] = [4, 6 ,8]\nhp_grid['xgb__learning_rate'] = [0.01, 0.1, 1.0]\nhp_grid['xgb__gamma'] = [0, 1, 10, 100]\nhp_grid['xgb__reg_lambda'] = [0, 1, 10, 100]\nhp_grid['xgb__subsample'] = [0, 1, 10, 100]\nhp_grid['xgb__scale_pos_weight'] = [1.0, 1.25, 1.5]\nhp_grid['xgb__colsample_bytree'] = [0.5, 0.75, 1.0]\n\n# AdaBoost\nhp_grid['ada__n_estimators'] = [10, 50, 100,200,500]\nhp_grid['ada__base_estimator__max_depth'] = [1, 3, 5]\nhp_grid['ada__learning_rate'] = [0.01, 0.1, 0.2]\n\n# Random Forest\nhp_grid['rf__n_estimators'] = [100]\nhp_grid['rf__max_features'] = [3, 6, 9, 12, 15]\n\n# GradBoost\nhp_grid['gb__n_estimators'] = [10, 50, 100,200,500]\nhp_grid['gb__max_depth'] = [1, 3, 5]\nhp_grid['gb__learning_rate'] = [0.01, 0.1, 0.2, 1.0]\nhp_grid['gb__subsample'] = [0.5, 0.75, 1.0]\n\nstart_time = time.time()\ngrid = RandomizedSearchCV(ensemble_model, hp_grid, cv=5, scoring='accuracy', verbose = True,\n                         n_iter = 100, n_jobs=-1).fit(Xtrain, ytrain)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\n\ngrid.best_estimator_.score(Xtest, ytest)\n\n0.8120805369127517",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ensemble modeling</span>"
    ]
  },
  {
    "objectID": "Lec10_Ensemble.html#ensembling-models-based-on-different-sets-of-predictors",
    "href": "Lec10_Ensemble.html#ensembling-models-based-on-different-sets-of-predictors",
    "title": "12  Ensemble modeling",
    "section": "12.3 Ensembling models based on different sets of predictors",
    "text": "12.3 Ensembling models based on different sets of predictors\nGenerally, tree-based models such as CatBoost, and XGBoost are the most accurate, while other models, such as bagging, random forests, KNN, and linear models, may not be as accurate. Thus, sometimes, the weaker models, despite bringing-in diversity in the model ensemble may deteriorate the ensemble accuracy due to their poor individual performance (check slides for technical details). Thus, sometimes, another approach to bring-in model diversity is to develop strong models based on different sets of predictors, and ensemble them.\nDifferent feature selection methods (such as Lasso, feature importance returned by tree-based methods, stepwise k-fold feature selection, etc.), may be used to obtain different sets of important features, strong models can be tuned on these sets, and then ensembled. Even though the models may be of the same type, the different sets of predictors will help bring-in the element of diversity in the ensemble.\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\n\n\n\n\n\n\n\n\nX = train[['mileage','mpg','year','engineSize']]\nXtest = test[['mileage','mpg','year','engineSize']]\ny = train['price']\nytest = test['price']\n\nWe will create polynomial interactions to develop two sets of predictors - first order predictors, and second order predictors.\n\npoly_set = PolynomialFeatures(2, include_bias = False)\nX_poly = poly_set.fit_transform(X)\nX_poly = pd.DataFrame(X_poly, columns=poly_set.get_feature_names_out())\nX_poly.columns = X_poly.columns.str.replace(\"^\", \"_\", regex=True)\nXtest_poly = poly_set.fit_transform(Xtest)\nXtest_poly = pd.DataFrame(Xtest_poly, columns=poly_set.get_feature_names_out())\nXtest_poly.columns = Xtest_poly.columns.str.replace(\"^\", \"_\", regex=True)\n\nLet us use 2 different sets of predictors to introduce diversity in the ensemble.\n\ncol_set1 = ['mileage','mpg', 'year','engineSize']\ncol_set2 = X_poly.columns\n\nLet us use two types of strong tree-based models.\n\ncat = CatBoostRegressor(verbose=False)\ngb = GradientBoostingRegressor(loss = 'huber')\n\nWe will use the Pipeline() function along with ColumnTransformer() to map a predictor set to each model.\n\ncat_pipe1 = Pipeline([\n    ('column_transformer', ColumnTransformer([('cat1_transform', 'passthrough', col_set1)], remainder='drop')),\n    ('cat1', cat)\n])\n\ncat_pipe2 = Pipeline([\n    ('column_transformer', ColumnTransformer([('cat2_transform', 'passthrough', col_set2)], remainder='drop')),\n    ('cat2', cat)\n])\n\ngb_pipe1 = Pipeline([\n    ('column_transformer', ColumnTransformer([('gb1_transform', 'passthrough', col_set1)], remainder='drop')),\n    ('gb1', gb)\n])\n\ngb_pipe2 = Pipeline([\n    ('column_transformer', ColumnTransformer([('gb2_transform', 'passthrough', col_set2)], remainder='drop')),\n    ('gb2', gb)\n])\n\nWe will use Linear regression to ensemble the models.\n\nen_new.final_estimator_.coef_\n\narray([ 0.30127482,  0.79242981, -0.07168258, -0.01781781])\n\n\n\nen_new = StackingRegressor(estimators = [('cat1', cat_pipe1),('cat2', cat_pipe2),\n                                        ('gb1', gb_pipe1), ('gb2', gb_pipe2)],\n                     final_estimator=LinearRegression(),                                          \n                    cv = KFold(n_splits = 15, shuffle = True, random_state=1))\n\n\nen_new.fit(X_poly, y)\n\nStackingRegressor(cv=KFold(n_splits=15, random_state=1, shuffle=True),\n                  estimators=[('cat1',\n                               Pipeline(steps=[('column_transformer',\n                                                ColumnTransformer(transformers=[('cat1_transform',\n                                                                                 'passthrough',\n                                                                                 ['mileage',\n                                                                                  'mpg',\n                                                                                  'year',\n                                                                                  'engineSize'])])),\n                                               ('cat1',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;)])),\n                              ('cat2',\n                               Pipeline(steps=[('column_transformer',...\n                               Pipeline(steps=[('column_transformer',\n                                                ColumnTransformer(transformers=[('gb2_transform',\n                                                                                 'passthrough',\n                                                                                 Index(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object'))])),\n                                               ('gb2',\n                                                GradientBoostingRegressor(loss='huber'))]))],\n                  final_estimator=LinearRegression())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StackingRegressorStackingRegressor(cv=KFold(n_splits=15, random_state=1, shuffle=True),\n                  estimators=[('cat1',\n                               Pipeline(steps=[('column_transformer',\n                                                ColumnTransformer(transformers=[('cat1_transform',\n                                                                                 'passthrough',\n                                                                                 ['mileage',\n                                                                                  'mpg',\n                                                                                  'year',\n                                                                                  'engineSize'])])),\n                                               ('cat1',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;)])),\n                              ('cat2',\n                               Pipeline(steps=[('column_transformer',...\n                               Pipeline(steps=[('column_transformer',\n                                                ColumnTransformer(transformers=[('gb2_transform',\n                                                                                 'passthrough',\n                                                                                 Index(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object'))])),\n                                               ('gb2',\n                                                GradientBoostingRegressor(loss='huber'))]))],\n                  final_estimator=LinearRegression())cat1column_transformer: ColumnTransformerColumnTransformer(transformers=[('cat1_transform', 'passthrough',\n                                 ['mileage', 'mpg', 'year', 'engineSize'])])cat1_transform['mileage', 'mpg', 'year', 'engineSize']passthroughpassthroughCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;cat2column_transformer: ColumnTransformerColumnTransformer(transformers=[('cat2_transform', 'passthrough',\n                                 Index(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object'))])cat2_transformIndex(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object')passthroughpassthroughCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;gb1column_transformer: ColumnTransformerColumnTransformer(transformers=[('gb1_transform', 'passthrough',\n                                 ['mileage', 'mpg', 'year', 'engineSize'])])gb1_transform['mileage', 'mpg', 'year', 'engineSize']passthroughpassthroughGradientBoostingRegressorGradientBoostingRegressor(loss='huber')gb2column_transformer: ColumnTransformerColumnTransformer(transformers=[('gb2_transform', 'passthrough',\n                                 Index(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object'))])gb2_transformIndex(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n      dtype='object')passthroughpassthroughGradientBoostingRegressorGradientBoostingRegressor(loss='huber')final_estimatorLinearRegressionLinearRegression()\n\n\n\nmean_squared_error(en_new.predict(Xtest_poly), ytest, squared = False)\n\n5185.376722607323\n\n\nNote that the above model does better on test data than all the models developed so far. Using different sets of predictors introduces diversity in the ensemble, as an alternative to including “weaker” models in the ensemble to add diversity.\nCheck the idea being used in the Spring 2023 prediction problem in the appendix.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ensemble modeling</span>"
    ]
  },
  {
    "objectID": "Assignment1_sp25.html",
    "href": "Assignment1_sp25.html",
    "title": "Appendix A — Assignment 1",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1_sp25.html#instructions",
    "href": "Assignment1_sp25.html#instructions",
    "title": "Appendix A — Assignment 1",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Thursday, 11th April 2025 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (2 points). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1_sp25.html#bias-variance-trade-off-for-regression-50-points",
    "href": "Assignment1_sp25.html#bias-variance-trade-off-for-regression-50-points",
    "title": "Appendix A — Assignment 1",
    "section": "1) Bias-Variance Trade-off for Regression (50 points)",
    "text": "1) Bias-Variance Trade-off for Regression (50 points)\nThe main goal of this question is to understand and visualize the bias-variance trade-off in a regression model by performing repetitive simulations.\nThe conceptual clarity about bias and variance will help with the main logic behind creating many models that will come up later in the course.\n\na) Define the True Relationship (Signal)\nFirst, you need to implement the underlying true relationship (Signal) you want to sample data from. Assume that the function is the Bukin function. Implement it as a user-defined function and run it with the test cases below to make sure it is implemented correctly. (5 points)\nNote: It would be more useful to have only one input to the function. You can treat the input as an array of two elements.\n\nprint(Bukin(np.array([1,2]))) # The output should be 141.177\nprint(Bukin(np.array([6,-4]))) # The output should be 208.966\nprint(Bukin(np.array([0,1]))) # The output should be 100.1\n\n\n\nb) Generate Test Set (No Noise)\nGenerate a noiseless test set with 100 observations sampled from the true underlying function. This test set will be used to evaluate bias and variance, so make sure it follows the correct data generation process.\n(5 points)\nInstructions:\n\nDo not use loops for this question.\n.apply will be especially helpful (and often simpler).\n\nData generation assumptions:\n\nUse np.random.seed(100) for reproducibility.\nThe first predictor, \\(x_1\\), should be drawn from a uniform distribution over the interval \\([-15, -5]\\), i.e., \\(x_1 \\sim U[-15, -5]\\).\nThe second predictor, \\(x_2\\), should be drawn from a uniform distribution over the interval \\([-3, 3]\\), i.e., \\(x_2 \\sim U[-3, 3]\\).\nCompute the true function values using the underlying model as your response \\(y\\)\n\n\n\nc) Initialize Results DataFrame\nCreate an empty DataFrame with the following columns:\n\ndegree: the degree of the polynomial model\n\nbias_sq: estimated squared bias (averaged over test points)\n\nvar: estimated variance of predictions\n\nbias_var_sum: sum of bias squared and variance\n\nempirical_mse: mean squared error calculated using sklearn’s mean_squared_error() on model predictions vs. true function values\n\nThis DataFrame will be used to store the results of your bias–variance tradeoff analysis and for generating comparison plots.\n(3 points)\n\n\nd) Generate Training Sets (With Noise)\nTo estimate the bias, variance, and total error (MSE) of a Linear Regression model trained on noisy data from the underlying Bukin function, follow the steps below.\n🔁 Step 1: Generate 100 Training Sets\n\nCreate 100 independent training datasets, each with 100 observations (same size as the test set).\nFor each training dataset:\n\nUse np.random.seed(i) to ensure reproducibility, where i is the dataset index (0 to 99).\nSample predictors from the same distributions used to generate the test set.\nAdd Gaussian noise with mean 0 and standard deviation 10:\n\\(\\varepsilon \\sim \\mathcal{N}(0, 10)\\)\n\n\n🧠 Step 2: Train Polynomial Models (Degrees 1 to 7)\n\nFor each training dataset, train polynomial models with degrees 1 through 7.\nUse polynomial feature transformations that include both:\n\nHigher-order terms (e.g., \\(x_1^2\\), \\(x_2^3\\))\nInteraction terms (e.g., \\(x_1 \\cdot x_2\\))\n\nMake predictions on the fixed, noiseless test set for each trained model.\n\n📊 Step 3: Estimate Bias², Variance, and MSE\n\nFor each degree, and each test point, collect the 100 predicted values from the models trained on the different training sets.\nUsing these predictions, compute:\n\nBias squared: squared difference between the mean prediction and the true value.\nVariance: variance of the predictions.\nTheoretical MSE: sum of bias squared and variance.\nEmpirical MSE: compute using sklearn.metrics.mean_squared_error between each model’s prediction and the true values, then average over the 100 training runs.\n\nStore all four quantities for each degree in your results DataFrame:\n\ndegree\nbias_sq\nvar\nbias_var_sum (bias squared + variance)\nempirical_mse\n\n\n(25 points)\n💡 Reminder: Comparing Theoretical vs. Empirical MSE\nWhen evaluating model performance on the noiseless test set:\n\nThe irreducible error (i.e., noise in training data) does not affect the test targets.\nTherefore, the test error (MSE) can be decomposed as:\n\\(MSE\\) = \\({Bias}^2\\) + \\({Variance}\\)\nThe empirical MSE (from sklearn) should closely match the sum of bias² and variance, since the test data contains no noise.\n\n\n\ne) Visualize Bias–Variance Decomposition\nUsing the results stored in your DataFrame, create a plot with four lines, each plotted against the polynomial degree:\n\nBias squared\nVariance\nBias squared + Variance (i.e., the theoretical decomposition of MSE)\nEmpirical MSE calculated using sklearn.metrics.mean_squared_error()\n(computed from the predicted values vs. true function values on the noiseless test set)\n\nPlot requirements: - Use a single line plot with the polynomial degree on the x-axis and error values on the y-axis. - Include a legend to clearly label each line. - Use different line styles or markers for easy visual comparison.\nGoal: - Compare the empirical MSE to the sum of bias squared and variance. - If everything is implemented correctly, the two lines should be very close (or even identical, up to numerical precision).\n\n\nf) Identify the Optimal Model\n\nWhat is the optimal polynomial degree based on the lowest empirical MSE (calculated using sklearn)?\n(2 points)\nReport the corresponding values of:\n\nBias squared\n\nVariance\n\nBias squared + Variance\n\nEmpirical MSE\nfor that degree.\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1_sp25.html#building-a-low-bias-low-variance-model-via-regularization-50-points",
    "href": "Assignment1_sp25.html#building-a-low-bias-low-variance-model-via-regularization-50-points",
    "title": "Appendix A — Assignment 1",
    "section": "2) Building a Low-Bias, Low-Variance Model via Regularization (50 points)",
    "text": "2) Building a Low-Bias, Low-Variance Model via Regularization (50 points)\nThe main goal of this question is to further reduce the total prediction error by applying regularization.\nSpecifically, you’ll use Ridge regression to build a low-bias, low-variance model for data generated from the underlying Bukin function with noise.\n\na) Why Regularization?\nExplain why the model with the optimal polynomial degree (as identified in Question 1) is not guaranteed to be the true low-bias, low-variance model.\nWhy might regularization still be necessary to improve generalization performance, even after selecting the degree that minimizes MSE?\n(5 points)\n\n\nb) Which Degrees to Exclude?\nBased on your plot and results from 1e and 1f, identify which polynomial degrees should be excluded from regularization experiments because they are already too simple (high bias) or too complex (high variance).\nExplain which degrees you will exclude and why, using your understanding of how regularization affects bias and variance.\n(10 points)\n\n\nc) Apply Ridge Regularization\nRepeat the steps from 1c and 1d, but this time use Ridge regression instead of ordinary least squares.\n\nUse only the degrees not excluded in 2b (and also exclude degree 7 to avoid extreme overfitting).\nUse 5-fold cross-validation to tune the Ridge regularization strength.\nUse neg_root_mean_squared_error as the scoring metric for cross-validation.\nTune over a range of regularization strengths (e.g., from 1 to 100).\nFor each retained degree, compute:\n\nBias squared\nVariance\nBias squared + Variance\nEmpirical MSE (from sklearn.metrics.mean_squared_error)\n\n\nStore your results in a new DataFrame with the same structure as in Question 1.\n(10 points)\n\n\nd) Visualize Regularized Results\nRepeat the visualization from 1e, but using the results from 2c (Ridge regression).\nYour plot should include four lines plotted against polynomial degree:\n\nBias squared\nVariance\nBias squared + Variance\nEmpirical MSE (computed using sklearn)\n\nInclude a clear legend and label your axes.\nThis will help you visually assess how regularization impacts bias, variance, and overall model error.\n(10 points)\n\n\ne) Evaluate the Regularized Model\n\nWhat is the optimal polynomial degree for the Ridge Regression model, based on the lowest empirical MSE?\n(3 points)\nReport the corresponding values of:\n\nBias squared\n\nVariance\n\nEmpirical MSE\nfor that optimal Ridge model.\n(3 points)\n\nCompare these results to those of the optimal Linear Regression model from Question 1.\nDiscuss how regularization influenced the bias, variance, and overall prediction error (MSE).\n(4 points)\n\n\n\nf) Interpreting the Impact of Regularization\n\nWas regularization successful in reducing the total prediction error (MSE) compared to the unregularized model?\n(2 points)\nBased on your results from 2e, explain how bias and variance changed as a result of regularization.\nHow did these changes affect the final total error?\nSupport your explanation with values or observations from your analysis.\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment2_sp25.html",
    "href": "Assignment2_sp25.html",
    "title": "Appendix B — Assignment 2",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment2_sp25.html#instructions",
    "href": "Assignment2_sp25.html#instructions",
    "title": "Appendix B — Assignment 2",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Monday, 18th April 2025 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (1 point). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission. (1 point)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)\n\nThe maximum possible score in the assigment is 103+5 = 108 out of 100.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment2_sp25.html#optimizing-knn-for-classification-71-points",
    "href": "Assignment2_sp25.html#optimizing-knn-for-classification-71-points",
    "title": "Appendix B — Assignment 2",
    "section": "B.1 Optimizing KNN for Classification (71 points)",
    "text": "B.1 Optimizing KNN for Classification (71 points)\nIn this question, you will use classification_data.csv. Each row is a loan and the each column represents some financial information as follows:\n\nhi_int_prncp_pd: Indicates if a high percentage of the repayments went to interest rather than principal. This is the classification response.\nout_prncp_inv: Remaining outstanding principal for portion of total amount funded by investors\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\nint_rate: Interest Rate on the loan\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60.\n\nAs indicated above, hi_int_prncp_pd is the response and all the remaining columns are predictors. You will tune and train a K-Nearest Neighbors (KNN) classifier throughout this question.\n\nB.1.1 a) Load the Dataset (1 point)\nRead the dataset into your notebook.\n\n\nB.1.2 b) Define Predictor and Response Variables (1 point)\nCreate the predictor (features) and response (target) variables from the dataset.\n\n\nB.1.3 c) Split the Data into Training and Test Sets (1 points)\nCreate the training and test datasets using the following specifications:\n\nUse a 75%-25% split.\nEnsure that the class ratio is preserved in both training and test sets (i.e., stratify the split).\nSet random_state=45 for reproducibility.\n\n\n\nB.1.4 d) Check Class Ratios (2 points)\nPrint the class distribution (ratios) for:\n\nThe entire dataset\n\nThe training set\n\nThe test set\n\nThis is to verify that the class ratio is preserved after splitting.\n\n\nB.1.5 e) Scale the Dataset (2 points)\nUse StandardScaler to scale the dataset in order to prepare it for KNN modeling.\nScaling ensures that all features contribute equally to the distance calculations used by the KNN algorithm.\n\n\nB.1.6 f) Set Up Cross-Validation (2 points)\nBefore creating and tuning your model, you need to define cross-validation settings to ensure consistent and accurate evaluation across folds.\nPlease follow these specifications:\n\nUse 5 stratified folds to preserve class distributions in each split.\nShuffle the data before splitting to introduce randomness.\nSet random_state=14 for reproducibility.\n\nNote: You must use these exact cross-validation settings throughout the rest of this question to maintain consistency.\n\n\nB.1.7 g) Tune K for KNN Using Cross-Validation (12 points)\nTune a KNN Classifier using cross-validation with the following specifications:\n\nUse every odd K value from 1 to 50 (inclusive).\nKeep all other model settings at their defaults.\nUse the cross-validation settings defined in part (f).\nEvaluate performance using the F1 score as the metric.\n\n(4 points)\nThen, complete the following tasks:\n\nCreate a plot of K values vs. cross-validation F1 scores to visualize how K balances overfitting and underfitting. (2 points)\nPrint the best average cross-validation F1 score. (1 points)\nReport the K value corresponding to the best F1 score. (1 points)\nDetermine whether this is the only K value that results in the best F1 score. Use code to justify your answer. (2 points)\nReflect on whether accuracy is a good metric for tuning the model in this case. Explain your reasoning. (2 points)\n\n💡 Hint:\nIn addition to reporting the best K and best F1 score, you may also want to examine the full cross-validation results to check if other K values achieved the same F1 score.\n\n\nB.1.8 h) Optimize the Classification Threshold (4 points)\nUsing the optimal K value you identified in part (g), optimize the classification threshold to maximize the cross-validation F1 score.\n\nB.1.8.1 Specifications:\n\nSearch across all possible threshold values using a step size of 0.05.\nUse the cross-validation settings defined in part (f).\nEvaluate performance using the F1 score, consistent with part (g).\n\n\n\nB.1.8.2 Tasks:\n\nVisualize the F1 score vs. different threshold values. (2 points)\nIdentify and report the best threshold that yields the highest F1 score. (1 points)\nOutput the best cross-validation F1 score. (1 points)\n\n\n\n\nB.1.9 i) Evaluate the Tuning Method (2 points)\nIs the method we used in parts (g) and (h) guaranteed to find the best combination of K and threshold, i.e., to tune the classifier to its optimal values?\n(1 point)\nJustify your answer.\n(1 point)\n\n\nB.1.10 j) Evaluate Tuned Classifier on Test Set (3 points)\nUsing the tuned KNN classifier and the optimal threshold you identified, evaluate the model on the test set. Report the following metrics:\n\nF1 Score\n\nAccuracy\n\nPrecision\n\nRecall\n\nAUC\n\n\n\nB.1.11 k) Jointly Tune K and Threshold (6 points)\nNow, tune K and the classification threshold simultaneously, rather than sequentially.\n\nUse the same settings from the previous parts (i.e., odd K values from 1 to 50, threshold step size of 0.05, F1 score as the metric, and the same cross-validation strategy).\nIdentify the best F1 score, along with the K value and threshold that produce it.\n\n\n\nB.1.12 l) Visualize Cross-Validation Results with a Heatmap (3 points)\nCreate a heatmap to visualize the cross-validation results in two dimensions.\n\nThe x-axis should represent the K values.\nThe y-axis should represent the threshold values.\nThe color should represent the F1 score.\n\nNote: This question only requires one line of code. You’ll need to recall a data visualization function and a data reshaping method from 303-1.\n\n\nB.1.13 m) Compare Joint vs. Sequential Tuning Results (4 points)\n\nHow does the best cross-validation F1 score from part (k) compare to the scores from parts (g) and (h)? (1 point)\nDid the optimal K value and threshold change when tuning them jointly? (1 point)\nExplain why or why not. Consider how tuning the two parameters together might impact the result. (2 points)\n\n\n\nB.1.14 n) Evaluate Final Tuned Model on Test Set (3 points)\nUsing the tuned classifier and threshold from part (k), evaluate the model on the test set. Report the following metrics:\n\nF1 Score\n\nAccuracy\n\nPrecision\n\nRecall\n\nAUC\n\n\n\nB.1.15 o) Compare Tuning Strategies and Computational Cost (3 points)\nCompare the tuning approach used in parts (g) & (h) (separate tuning of K and threshold) with the approach in part (k) (joint tuning of K and threshold) in terms of computational cost.\n\nHow many K and threshold combinations did you evaluate in each approach? (2 points)\nBased on this comparison and your answer from part (l), explain the main trade-off involved in model tuning (e.g., between computation and performance). (2 points)\n\n\n\nB.1.16 p) Tune K Using Multiple Metrics (5 points)\nGridSearchCV and cross_val_score are designed to optimize based on a single metric.\nIn this section, you’ll practice tuning hyperparameters while evaluating multiple metrics simultaneously using cross_validate.\nFor this imbalanced classification task, instead of optimizing the F1 score directly, we’ll focus on precision and recall together.\nKeep in mind that the F1 score is the harmonic mean of precision and recall—it balances the trade-off between the two.\nCross-validate a KNN classifier using the following specifications:\n\nUse the same cross-validation setting and hyperparameter grid as before\nEvaluate the model using precision, recall, and f1-score, as metrics at the same time.\n\nSave the cross-validation results into a DataFrame, and compute the average score for each metric, and visualize how these metrics change with different values of K.\n\n\nB.1.17 q) Optimize for Recall with Precision Constraint (4 point)\nIdentify the K value that yields the highest recall, while maintaining a precision of at least 75%.\n(3 points)\nThen, print the average cross-validation metrics (f1-score, precision, recall) for that K value.\n(1 point)\n\n\nB.1.18 r) Tune Threshold for Maximum Recall (3 point)\nUsing the optimal K value identified in part (q), find the threshold that maximizes cross-validation Recall, following the specifications below:\n\nEvaluate all possible threshold values with a step size of 0.05.\nUse the cross-validation settings from part (f).\n\nThen: - Print the best cross-validation recall. - Report the threshold value that achieves this recall.\nNote: This task is very similar to part (h), but it’s important for the next part.\n\n\nB.1.19 s) Evaluate Precision-Optimized Model on Test Set (2 points)\nUsing the tuned classifier and threshold from parts (q) and (r), evaluate the model on the test set. Report the following metrics:\n\nF1 Score\n\nAccuracy\n\nPrecision\n\nRecall\n\nAUC\n\n\n\nB.1.20 t) Final Reflection: Comparing Tuning Strategies (3 points)\nYou have now tuned your KNN classifier using three different strategies:\n\nSequential tuning of K and threshold based on F1 score (parts g–h)\nJoint tuning of K and threshold using F1 score (part k)\nTuning based on multiple metrics, selecting the K with the highest recall while maintaining precision ≥ 75% (parts p–r)\n\nReflect on the following:\n\nWhich tuning strategy led to the best overall performance on the test set, based on the metrics you care about most?\nWhich strategy would you choose in a real-world application, and why?\nWhat are the trade-offs between tuning for F1 score versus prioritizing precision or recall individually?\n\nNote: This is an open-ended question. As long as your reasoning makes sense, you will receive full credit.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment2_sp25.html#tuning-a-knn-regressor-on-bank-loan-data-32-points",
    "href": "Assignment2_sp25.html#tuning-a-knn-regressor-on-bank-loan-data-32-points",
    "title": "Appendix B — Assignment 2",
    "section": "B.2 Tuning a KNN Regressor on Bank Loan Data (32 points)",
    "text": "B.2 Tuning a KNN Regressor on Bank Loan Data (32 points)\nIn this question, you will use bank_loan_train_data.csv to tune (the model hyperparameters) and train the model. Each row is a loan and the each column represents some financial information as follows:\n\nmoney_made_inv: Indicates the amount of money made by the bank on the loan. This is the regression response.\nout_prncp_inv: Remaining outstanding principal for portion of total amount funded by investors\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\nint_rate: Interest Rate on the loan\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60\nmort_acc: The number of mortgage accounts\napplication_type_Individual: 1 if the loan is an individual application or a joint application with two co-borrowers\ntot_cur_bal: Total current balance of all accounts\npub_rec: Number of derogatory public records\n\nAs indicated above, money_made_inv is the response and all the remaining columns are predictors. You will tune and train a K-Nearest Neighbors (KNN) regressor throughout this question.\n\nB.2.1 a) Split, Scale, and Tune a KNN Regressor (15 point)\nCreate the training and test datasets using the following specifications:\n\nUse an 80%-20% split.\nSet random_state=1 for reproducibility.\n\nThen, scale all the predictors, as KNN is sensitive to the scale of input features.\nNext, you will tune a KNN Regressor by searching for the optimal hyperparameters using three search approaches: Grid Search, Random Search, and Bayesian Search.\n\nB.2.1.1 Cross-Validation Setting\nYou should use 5-fold cross-validation, with the following specifications:\n\nThe data should be shuffled before splitting\n\nUse random_state=1 to ensure reproducibility\n\n\n\nB.2.1.2 Hyperparameters to Tune:\nYou will tune the following hyperparameters for the KNN Regressor:\nYou will tune the following hyperparameters for the K-Nearest Neighbors Regressor, using Minkowski as the distance metric:\n\nn_neighbors: Number of nearest neighbors\n\nTune over the range: np.arange(1, 25, 1)\n\np: Power parameter for the Minkowski distance\n\nUse values: np.arange(1, 4, 1)\np = 1 corresponds to Manhattan distance\n\np = 2 corresponds to Euclidean distance\n\nNote: Set the distance metric to \"minkowski\"\n\nweights: Weight function used in prediction\nYou must consider the following 5 types of weights:\n\n'uniform': All neighbors are weighted equally\n\n'distance': Weight is inversely proportional to distance\n\nCustom weight functions:\n\n∝1distance2\\propto \\frac{1}{\\text{distance}^2}\n∝1distance3\\propto \\frac{1}{\\text{distance}^3}\n∝1distance4\\propto \\frac{1}{\\text{distance}^4}\n\n\n\nFor each search method (Grid Search, Random Search, Bayesian Search), report the following:\n\nbest_params_: The best combination of hyperparameters\n\nbest_score_: Cross-validated RMSE on the training set\n\nTest RMSE obtained from the best model\n\nExecution time for the search process\n\nHint:\nDefine three custom weight functions as shown below:\ndef dist_power_2(distance):\n    return 1 / (1e-10 + distance**2)\n\ndef dist_power_3(distance):\n    return 1 / (1e-10 + distance**3)\n\ndef dist_power_4(distance):\n    return 1 / (1e-10 + distance**4)\nNote the small constant 1e-10 helps avoid division by zero and numerical instability.\n\n\n\nB.2.2 b) Compare Tuning Approaches (1 point)\nCompare the results from part (2a) in terms of execution time and model performance.\nBriefly discuss the main trade-offs among the three hyperparameter tuning approaches: Grid Search, Random Search, and Bayesian Search.\n\n\nB.2.3 c) Feature Selection and Hyperparameter Tuning with GridSearchCV (15 point)\nKNN performance can deteriorate significantly if irrelevant or noisy predictors are included. In this part, you will explore feature selection to improve model performance, followed by hyperparameter tuning using GridSearchCV (with refit=True).\nTry the following three different feature selection approaches:\n\nCorrelation-based filtering:\n\nSelect features with an absolute correlation of at least 0.1 with the target variable.\n\nLasso regression for feature selection:\n\nUse Lasso(alpha=50) to select important features based on non-zero coefficients.\n\nSelectKBest:\n\nUse SelectKBest with f_regression, selecting the top 4 features.\n\n\nFor each approach, perform hyperparameter tuning using GridSearchCV, and report:\n\nThe best score (cross-validated RMSE) on the training set\nThe test RMSE from the best model\nThe best hyperparameters\n\n\n\nB.2.4 d) Compare Feature Selection Approaches (1 point)\nCreate a DataFrame that summarizes the model performance from each feature selection method, including:\n\nTraining RMSE\nTest RMSE\n\nBe sure to also include the results from the model trained without any feature selection for comparison.\nThen, briefly explain what you learned from this experiment.\nFor example: Did feature selection improve performance? Which method worked best?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment3_sp25.html",
    "href": "Assignment3_sp25.html",
    "title": "Appendix C — Assignment 3",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment3_sp25.html#instructions",
    "href": "Assignment3_sp25.html#instructions",
    "title": "Appendix C — Assignment 3",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Friday, 2th May 2025 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (2 pts). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file. If your issue doesn’t seem genuine, you will lose points.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment3_sp25.html#regression-problem---miami-housing",
    "href": "Assignment3_sp25.html#regression-problem---miami-housing",
    "title": "Appendix C — Assignment 3",
    "section": "C.1 Regression Problem - Miami housing",
    "text": "C.1 Regression Problem - Miami housing\n\nC.1.1 a) Data preparation\nRead the data miami-housing.csv. Check the description of the variables here. Split the data into 60% train and 40% test. Use random_state = 45. The response is SALE_PRC, and the rest of the columns are predictors, except PARCELNO. Print the shape of the predictors dataframe of the train data.\n(2 points)\n\n\nC.1.2 b) Baseline Decision Tree Model\nTrain a Decision Tree Regressor to predict SALE_PRC using all available predictors.\n\nUse random_state=45 and keep all other hyperparameters at their default values.\nAfter training the model, evaluate and report the following on both the training and test sets:\n\nMean Absolute Error (MAE)\nR² Score\n\n\n(3 points)\n\n\nC.1.3 c) Tune the Decision Tree Model\nTune the hyperparameters of the Decision Tree Regressor developed in the previous question and evaluate its performance.\nYour goal is to achieve a test set MAE (Mean Absolute Error) below $68,000.\n\nYou must display the optimal hyperparameter values obtained from the tuning process.\nCompute and report the test MAE and R² Score using the tuned model.\n\nHints: 1. BayesSearchCV() with max_depth and max_features can often complete in under a minute. 2. You may use any hyperparameter tuning method (e.g., GridSearchCV, RandomizedSearchCV, BayesSearchCV). 3. You are free to choose which hyperparameters to tune and define your own search space.\n(9 points)\n\n\nC.1.4 d) Bagged Decision Trees with Out-of-Bag Evaluation\nTrain a Bagging Regressor using Decision Trees as base estimators to predict SALE_PRC.\n\nUse only the n_estimators hyperparameter for tuning; keep all other parameters at their default values.\nIncrease the number of trees (n_estimators) until the out-of-bag (OOB) MAE stabilizes.\nReport the final OOB MAE, test MAE, and R² Score, and ensure that the OOB MAE is less than $48,000.\n\n(4 points)\n\n\nC.1.5 e) Bagged Decision Trees Without Bootstrapping\nTrain a Bagging Regressor using Decision Trees, but this time disable bootstrapping by setting bootstrap=False.\n\nUse the same n_estimators value as in the previous question.\nCompute and report the following on the test set:\n\nMean Absolute Error (MAE)\nR² Score\n\n\nExplain why the test MAE in this case is:\n\nMuch higher than the MAE obtained when bootstrapping was enabled (previous question).\nLower than the MAE obtained from a single untuned decision tree (as in Question 1(b)).\n\n\n💡 Hint: Consider the impact of bootstrap sampling on variance reduction and the benefits of aggregation in ensemble methods.\n\n(2 point for code, 3 + 3 points for reasoning)\n\n\nC.1.6 f) Bagged Decision Trees with Feature Bootstrapping Only\nTrain a Bagging Regressor using Decision Trees, with the following configuration: - Disable sample bootstrapping by setting bootstrap=False - Enable feature bootstrapping by setting bootstrap_features=True\nUse the same number of estimators (n_estimators) as in the previous bagging experiments.\n\nCompute and report the following on the test set:\n\nMean Absolute Error (MAE)\nR² Score\n\n\nExplain why the test MAE obtained in this setting is much lower than the one in the previous question, where neither bootstrapping samples nor features was used.\n(2 point for code, 3 points for reasoning)\n\n\nC.1.7 g) Tuning a Bagged Tree Model\n\nC.1.7.1 i) Approaches\nThere are two common approaches for tuning a bagged tree model:\n\nOut-of-Bag (OOB) Prediction\nKK-fold Cross-Validation using GridSearchCV\n\nWhat is the advantage of each approach over the other? Specifically:\n\nWhat is the advantage of the out-of-bag approach compared to KK-fold cross-validation?\nWhat is the advantage of KK-fold cross-validation compared to the out-of-bag approach?\n\n(3 + 3 points)\n\n\nC.1.7.2 ii) Tuning the hyperparameters\nTune the hyperparameters of the bagged tree model developed in 1(d). You may use either of the approaches mentioned in the previous question. Show the optimal values of the hyperparameters obtained. Compute the MAE and R² Score on test data with the tuned model. Your test MAE must be less than the test MAE ontained in the previous question.\nIt is up to you to pick the hyperparameters and their values in the grid.\nHint:\nGridSearchCV() may work better than BayesSearchCV() in this case.\n(9 points)\n\n\n\nC.1.8 h) Random Forest\n\nC.1.8.1 i) Tuning a Random Forest Model\nTrain and tune a Random Forest Regressor to predict SALE_PRC.\n\nSelect hyperparameters and define your own tuning grid.\nUse any tuning approach (e.g., Out-of-Bag (OOB) evaluation or KK-fold cross-validation).\nReport the following performance metrics on the test set:\n\nMean Absolute Error (MAE)\nR² Score\n\n\n\n✅ Your goal is to achieve a test MAE below $46,000.\n\nHint:\nThe OOB approach is efficient and can complete in under a minute.\n(9 points)\n\n\nC.1.8.2 ii) Feature Importance\nAfter fitting the tuned Random Forest Regressor, extract and display the feature importances.\n\nPrint the predictors in decreasing order of importance based on the trained model.\nThis helps identify which features contribute most to predicting SALE_PRC.\n\n(4 points)\n\n\nC.1.8.3 iii) Feature Selection\nDrop the least important predictor identified in the previous step, and re-train the tuned Random Forest model.\n\nCompute the test MAE and R² Score after dropping the feature.\nYou may need to adjust the max_features hyperparameter to reflect the reduced number of predictors.\nCompare the new test MAE with the previous one.\n\n\n❓ Did the test MAE decrease after removing the least important feature?\n\n(4 points)\n\n\nC.1.8.4 iv) Random Forest vs. Bagging: max_features\nThe max_features hyperparameter is available in both RandomForestRegressor() and BaggingRegressor().\nDoes max_features have the same meaning in both models?\nIf not, explain the difference in how it is interpreted and applied.\n\n💡 Hint: Refer to the scikit-learn documentation for both estimators to understand how max_features affects feature selection during training.\n\n(1 + 3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment3_sp25.html#classification---term-deposit",
    "href": "Assignment3_sp25.html#classification---term-deposit",
    "title": "Appendix C — Assignment 3",
    "section": "C.2 Classification - Term deposit",
    "text": "C.2 Classification - Term deposit\nThe data for this question is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls, where bank clients were called to subscribe for a term deposit.\nThere is a train data - train.csv, which you will use to develop a model. There is a test data - test.csv, which you will use to test your model. Each dataset has the following attributes about the clients called in the marketing campaign:\n\nage: Age of the client\neducation: Education level of the client\nday: Day of the month the call is made\nmonth: Month of the call\ny: did the client subscribe to a term deposit?\nduration: Call duration, in seconds. This attribute highly affects the output target (e.g., if duration=0 then y=‘no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for inference purposes and should be discarded if the intention is to have a realistic predictive model.\n\n(Raw data source: Source. Do not use the raw data source for this assignment. It is just for reference.)\n\nC.2.1 a) Data Preparation\nBegin by examining the distribution of the target variable in both the training and test sets. This will help you assess whether there is any significant class imbalance.\nNext, consider the two available approaches for hyperparameter tuning:\n\nCross-validation (CV)\nOut-of-bag (OOB) evaluation\n\n\nC.2.1.1 ❓ Which method do you prefer for this dataset, and why?\nDiscuss your choice based on:\n\nThe size of the dataset\nThe class imbalance in the target variable\nThe reliability and interpretability of each method\nWhether you need stratified sampling to preserve class distribution during evaluation\n\n(2 points)\n\n\n\nC.2.2 b) Random Forest for Term Deposit Subscription Prediction\nDevelop and tune a Random Forest Classifier to predict whether a client will subscribe to a term deposit using the following predictors:\n\nage\neducation\nday\nmonth\n\nThe model must satisfy the following performance criteria:\n\nC.2.2.1 ✅ Requirements:\n\nMinimum overall classification accuracy of 75%, across both train.csv and test.csv.\nMinimum recall of 60%, across both train.csv and test.csv.\n\nYou must:\n\nPrint the accuracy and recall for both datasets (train.csv and test.csv).\nUse cross-validation on the training data to optimize the model hyperparameters.\nSelect a threshold probability for classification and apply it consistently across both datasets.\n\n\n\nC.2.2.2 ⚠️ Important Notes:\n\nDo not use duration as a predictor. Its value is determined after the marketing call ends, so using it would leak information about the outcome.\nYou are free to choose any decision threshold for classification, but the same threshold must be used consistently for both training and test evaluation.\nUse cross-validation to tune hyperparameters such as max_features, max_depth, and max_leaf_nodes.\n- You may use StratifiedKFold or any appropriate CV method that respects class imbalance.\nAfter tuning the model, plot cross-validated accuracy and recall across a range of threshold values (e.g., 0.1 to 0.9). Use this plot to select a threshold that satisfies the required trade-off between accuracy and recall.\nEvaluate the final tuned model (with the chosen threshold) on the test dataset. Do not use the test data to guide any part of the tuning or threshold selection.\n\n\n\nC.2.2.3 💡 Hints:\n\nRestrict the search space to:\n\nmax_depth ≤ 25\n\nmax_leaf_nodes ≤ 45\nThese limits encourage generalization and help balance recall and accuracy.\n\nConsider using cross-validation scores to compute predicted probabilities when plotting recall/accuracy curves.\n\n\n\nC.2.2.4 📝 Scoring Breakdown (22 points total):\n\n8 points – Hyperparameter tuning via cross-validation\n\n5 points – Plotting accuracy and recall across thresholds\n\n5 points – Threshold selection based on the plot\n\n4 points – Reporting accuracy and recall on both datasets",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment3_sp25.html#predictor-transformations-in-trees",
    "href": "Assignment3_sp25.html#predictor-transformations-in-trees",
    "title": "Appendix C — Assignment 3",
    "section": "C.3 Predictor Transformations in Trees",
    "text": "C.3 Predictor Transformations in Trees\nCan a non-linear monotonic transformation of predictors (such as log(), sqrt(), etc.) be useful in improving the accuracy of decision tree models?\nProvide a brief explanation based on your understanding of how decision trees split data and handle predictor scales.\n(4 points for answer)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment4_sp25.html",
    "href": "Assignment4_sp25.html",
    "title": "Appendix D — Assignment 4",
    "section": "",
    "text": "Instructions\nFeel free to add data visualizations of your hyperparameter tuning process. Visualizing and analyzing tuning results is important—even if it’s not explicitly required in the instructions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Assignment4_sp25.html#instructions",
    "href": "Assignment4_sp25.html#instructions",
    "title": "Appendix D — Assignment 4",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Friday, 23th May 2025 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (1 point). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission. (1 point)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)\n\nPlease make sure your code results are clearly incorporated in your submitted HTML file.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Assignment4_sp25.html#adaboost-vs-bagging-4-points",
    "href": "Assignment4_sp25.html#adaboost-vs-bagging-4-points",
    "title": "Appendix D — Assignment 4",
    "section": "D.1 AdaBoost vs Bagging (4 points)",
    "text": "D.1 AdaBoost vs Bagging (4 points)\nWhich model among AdaBoost and Random Forest is more sensitive to outliers? (1 point) Explain your reasoning with the theory you learned on the training process of both models. (3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Assignment4_sp25.html#regression-with-boosting-54-points",
    "href": "Assignment4_sp25.html#regression-with-boosting-54-points",
    "title": "Appendix D — Assignment 4",
    "section": "D.2 Regression with Boosting (54 points)",
    "text": "D.2 Regression with Boosting (54 points)\nFor this question, you will use the miami_housing.csv file. You can find the description for the variables here.\nThe SALE_PRC variable is the regression response and the rest of the variables, except PARCELNO, are the predictors.\n\nD.2.1 a): Preprocessing\nRead the dataset. Create the training and test sets with a 60%-40% split and random_state = 1. (1 point)\n\n\nD.2.2 b) AdaBoost\nTune an AdaBoost Regressor to achieve a test MAE below $47,000.\n\nYou must set random_state=1 for all components (e.g., base estimator, AdaBoost model, etc.).\nSubmissions that meet the MAE cutoff using any other random_state will receive zero credit.\n\nScoring: - 5 points for achieving test MAE &lt; $47,000 - 1 point for reporting the training MAE of your tuned model to evaluate generalization\n\n\nD.2.3 c) Loss Functions in Gradient Boosting\nGradient Boosting supports multiple loss functions, including squared_error, absolute_error, and huber.\n\n(1 point) Which loss function performs best on this dataset?\n(3 points) What are the advantages of this loss function compared to the other two?\n\n\n\nD.2.4 Task: Tune a Gradient Boosting Model\nYour goal is to tune a Gradient Boosting Regressor to achieve a cross-validation MAE below $45,000.\n\nYou must keep all random_state values set to 1.\n\nSubmissions using any other random_state will receive zero credit, even if the MAE cutoff is met.\n\nScoring (10 points total):\n- 5 points for using a well-reasoned hyperparameter search strategy\n- 5 points for achieving MAE &lt; $45,000 - 1 point for reporting the training MAE of your tuned model to evaluate generalization\nHints\n\nParallel processing is not supported in the vanilla GradientBoostingRegressor.\nBayesSearchCV, like gradient boosting itself, performs a sequential search—each trial depends on the result of the previous one—so it does not support parallel exploration.\nOptuna is generally faster and more efficient than both BayesSearchCV and GridSearchCV. It supports parallel execution of trials and includes several built-in performance enhancements.\n\n\n\nD.2.5 d) XGBoost vs. Gradient Boosting\nXGBoost Enhancements:\n\nWhat improvements make XGBoost superior to vanilla Gradient Boosting in terms of performance and runtime?\n\nExplain the enhancements (1 point)\n\nProvide the reasons behind the improvements (1 point)\n\nIdentify relevant hyperparameters and describe how they influence model behavior (2 points)\n\n\nXGBoost Limitations:\n\nWhat important feature or behavior is missing in XGBoost but well-implemented in vanilla Gradient Boosting? (1 point)\n\n\n\nD.2.6 e) Tuning XGBoost with Different Search Strategies\nTune an XGBoost Regressor to achieve a cross-validation MAE below $42,500.\n\nYou must keep random_state=1 in all components (e.g., XGBoost model, CV splits, search objects).\nSubmissions that meet the cutoff using any other random_state will receive zero credit.\n\nScoring (10 points total): - 5 points for a well-designed and appropriate hyperparameter search strategy - 5 using 3 different search strategies - 5 points for achieving MAE &lt; $42,500\nSearch Strategies (Required Comparison)\nYou must tune the model using three different search settings:\n\nBayesSearchCV\n\nUnlike vanilla GradientBoostingRegressor, XGBoost supports parallel training and can benefit from multi-core processing (n_jobs=-1), so BayesSearchCV is practica with it.\n\nOptuna (with n_jobs=-1)\nOptuna (default single-threaded)\n\nExecution Time\nYou must report the execution time for each tuning strategy.\n\nYou can measure this using:\n\nA Jupyter magic command like %%time, or\nPython’s time.time() (end - start)\n\n\nFor a fair comparison, use the same search space across all methods.\nOnly one of the tuned models needs to meet the performance cutoff, but you should still report times for all three.\n\n\nD.2.7 f) Feature Importance\nUsing the best hyperparameter settings, fit the final model and output the feature importances.\n\nUse the .feature_importances_ attribute or equivalent method from your model.\nVisualize the importances if possible (e.g., with a bar plot).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Assignment4_sp25.html#imbalanced-classification-with-regularized-gradient-boosting-42-points",
    "href": "Assignment4_sp25.html#imbalanced-classification-with-regularized-gradient-boosting-42-points",
    "title": "Appendix D — Assignment 4",
    "section": "D.3 Imbalanced Classification with Regularized Gradient Boosting (42 points)",
    "text": "D.3 Imbalanced Classification with Regularized Gradient Boosting (42 points)\nIn this question, you will use the train.csv and test.csv datasets. Each observation represents a marketing call made by a banking institution. The target variable y indicates whether the client subscribed to a term deposit (1) or not (0), making this a binary classification task.\nThe predictors you should use are: age, day, month, and education.\n⚠️ Note: As discussed last quarter, the variable duration must not be used as a predictor.\nNo credit will be given for models that include it.\n\nD.3.1 a) Data Preprocessing\nPerform the following preprocessing steps:\n\nRead in the training and testing datasets.\nCreate a new season feature by mapping each month to its corresponding season.\nDefine the predictor and response variables.\nConvert all categorical predictors to pandas.Categorical dtype before passing them to the models.\nConvert the response variable y to binary values (0 and 1).\n\n(5 points)\nWe will rely on the native categorical feature support provided by each library (XGBoost, LightGBM, and CatBoost), so explicit one-hot encoding is not required.\n\n\nD.3.2 b) Target Exploration\nFor classification tasks, it’s important to examine the distribution of the target variable to determine whether the classes are imbalanced. This helps you avoid common pitfalls when dealing with imbalanced classification.\n\nExplore the class distribution in both the training and test sets.\n\n(2 points)\n\n\nD.3.3 c) LightGBM and CatBoost\nLightGBM and CatBoost are gradient boosting frameworks, like XGBoost, but each introduces unique innovations.\n\nWhat do LightGBM and CatBoost have in common with XGBoost? (2 points)\n\nWhat advantages do they offer over XGBoost? (2 points)\n\nHow are these advantages implemented in each model? (3 points)\n\nAll three libraries support native categorical feature handling.\nDo they use the same approach? If not, explain the differences. (3 points)\n\n\n\nD.3.4 c) Handling Imbalanced Classification in Gradient Boosting Extensions\nFor all extensions of Gradient Boosting (XGBoost, LightGBM, and CatBoost):\n\nAre there additional inputs or hyperparameters available to handle imbalanced classification? (1 point)\n\nIf yes, describe how the method works. (1 point)\n\nHow should the value of this hyperparameter be set or tuned for best results? (1 point)\n\n\n\nD.3.5 d) Model Evaluation: XGBoost, LightGBM, and CatBoost\nEvaluate the performance of the following models: XGBoost, LightGBM, and CatBoost, using the metrics below:\n\nRecall\nPrecision\nF1 Score\nAUPRC (Area Under the Precision-Recall Curve)\nROC AUC\n\nFor each model, build and compare two versions:\n\nBaseline model: using default settings with random_state=1, without addressing class imbalance.\nImbalance-aware model: with scale_pos_weight enabled to handle class imbalance.\n\n\nCompare the performance of both versions for each model.\nSummarize which model and approach performed best for imbalanced classification, and try to explain why.\n\n\n\nD.3.6 d) Tuning LightGBM for Classification\nTune a LightGBM classifier to achieve:\n\nCross-validation accuracy ≥ 70%\nCross-validation recall ≥ 65%\n\nYou must set random_state=1 in all components (e.g., model, cross-validation, search objects).\nSubmissions that exceed the cutoffs using any other random_state will receive zero credit.\nScoring (15 points total):\n- 7.5 points for a well-designed and justified search strategy\n- 7.5 points for meeting both performance thresholds\nHints:\n\nFor classification, you may also tune the decision threshold (not just model hyperparameters).\n\n\n\nD.3.7 e) Test Set Evaluation\nEvaluate the tuned LightGBM model on the test set:\n\nReport the test accuracy and test recall.\nInclude the threshold used for classification.\n\nThis will help assess how well the model generalizes beyond the training data.\n(2 points)\n\n\nD.3.8 f) Tuning CatBoost for Classification\nTune a CatBoost classifier to achieve:\n\nCross-validation accuracy ≥ 75%\nCross-validation recall ≥ 65%\n\nYou must set random_state=1 in all components (e.g., model, cross-validation, search objects).\nSubmissions that exceed the cutoffs using any other random_state will receive zero credit.\nScoring (15 points total):\n- 7.5 points for a well-structured and appropriate hyperparameter search\n- 7.5 points for meeting both performance thresholds\nHints:\n\nYou are free to use any tuning strategy and define any reasonable search space.\nIn addition to tuning hyperparameters, you may also need to tune the decision threshold to meet the classification performance criteria.\n\n\n\nD.3.9 g) Test Set Evaluation\nEvaluate the tuned CatBoost model on the test set:\n\nReport the test accuracy and test recall.\nInclude the classification threshold used.\n\nThis will help assess whether the model generalizes well beyond the training data.\n(1 point)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Assignment4_sp25.html#bonus-extra-credit-20-points",
    "href": "Assignment4_sp25.html#bonus-extra-credit-20-points",
    "title": "Appendix D — Assignment 4",
    "section": "D.4 🎁 Bonus (Extra Credit) – 20 Points",
    "text": "D.4 🎁 Bonus (Extra Credit) – 20 Points\nTo help you prepare for your upcoming prediction project involving hyperparameter tuning, I’ve created the following optional tasks.\nFeel free to skip them if time does not permit.\n\nD.4.1 a) Comparing Tuning Strategies\nCompare the tuning time and results of GridSearchCV and RandomizedSearchCV using the same search space you used in Task 2e (BayesSearchCV and Optuna).\n\nWhat are the trade-offs between exhaustive search, random search, and smarter strategies like Bayesian optimization and Optuna?\nAre the differences in runtime justified by improvements in model performance?\n\n\n\nD.4.2 b) Resumable Tuning Strategies\nDo your own research: Among all the tuning strategies you have used, which ones allow you to continue tuning without starting from scratch when increasing n_trials or n_iter?\n\nIdentify the methods that support incremental or resumable search.\nExplain how they work and why they are efficient.\nProvide code to demonstrate how these strategies reuse previous results rather",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix E — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Datasets, assignment and project files</span>"
    ]
  }
]