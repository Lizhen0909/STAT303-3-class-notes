\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\newlabel{preface}{{}{3}{}{chapter*.2}{}}
\@writefile{toc}{\contentsline {chapter}{Preface}{3}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{\numberline {I}Bias \& Variance; KNN}{4}{part.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Bias-variance tradeoff}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{bias-variance-tradeoff}{{1}{5}{Bias-variance tradeoff}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Simple model (Less flexible)}{5}{section.1.1}\protected@file@percent }
\newlabel{simple-model-less-flexible}{{1.1}{5}{Simple model (Less flexible)}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Complex model (more flexible)}{8}{section.1.2}\protected@file@percent }
\newlabel{complex-model-more-flexible}{{1.2}{8}{Complex model (more flexible)}{section.1.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}KNN}{11}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{knn}{{2}{11}{KNN}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}KNN for regression}{11}{section.2.1}\protected@file@percent }
\newlabel{knn-for-regression}{{2.1}{11}{KNN for regression}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Feature Scaling in KNN}{13}{section.2.2}\protected@file@percent }
\newlabel{feature-scaling-in-knn}{{2.2}{13}{Feature Scaling in KNN}{section.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Hyperparameters in KNN}{14}{section.2.3}\protected@file@percent }
\newlabel{hyperparameters-in-knn}{{2.3}{14}{Hyperparameters in KNN}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Tuning \emph  {k} in KNN}{15}{subsection.2.3.1}\protected@file@percent }
\newlabel{tuning-k-in-knn}{{2.3.1}{15}{\texorpdfstring {Tuning \emph {k} in KNN}{Tuning k in KNN}}{subsection.2.3.1}{}}
\gdef \LT@i {\LT@entry 
    {3}{27.90001pt}\LT@entry 
    {1}{40.03201pt}\LT@entry 
    {3}{56.12851pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{55.27501pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Tuning Other KNN Hyperparameters}{18}{subsection.2.3.2}\protected@file@percent }
\newlabel{tuning-other-knn-hyperparameters}{{2.3.2}{18}{Tuning Other KNN Hyperparameters}{subsection.2.3.2}{}}
\gdef \LT@ii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{115.48843pt}\LT@entry 
    {1}{145.07533pt}\LT@entry 
    {1}{91.13565pt}\LT@entry 
    {1}{120.38309pt}\LT@entry 
    {3}{260.0722pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\gdef \LT@iii {\LT@entry 
    {3}{16.95001pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{115.48843pt}\LT@entry 
    {1}{145.07533pt}\LT@entry 
    {1}{91.13565pt}\LT@entry 
    {1}{120.38309pt}\LT@entry 
    {3}{267.98904pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Hyperparameter Tuning}{22}{section.2.4}\protected@file@percent }
\newlabel{hyperparameter-tuning}{{2.4}{22}{Hyperparameter Tuning}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.0.1}RandomizedSearchCV}{22}{subsubsection.2.4.0.1}\protected@file@percent }
\newlabel{randomizedsearchcv}{{2.4.0.1}{22}{RandomizedSearchCV}{subsubsection.2.4.0.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.0.2}BayesSearchCV}{25}{subsubsection.2.4.0.2}\protected@file@percent }
\newlabel{bayessearchcv}{{2.4.0.2}{25}{BayesSearchCV}{subsubsection.2.4.0.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Hyperparameter tuning}{29}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{hyperparameter-tuning-1}{{3}{29}{Hyperparameter tuning}{chapter.3}{}}
\gdef \LT@iv {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{39.37502pt}\LT@entry 
    {1}{40.03201pt}\LT@entry 
    {3}{48.58395pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}\href  {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{\texttt  {GridSearchCV}}}{30}{section.3.1}\protected@file@percent }
\newlabel{gridsearchcv}{{3.1}{30}{\texorpdfstring {\href {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{\texttt {GridSearchCV}}}{GridSearchCV}}{section.3.1}{}}
\gdef \LT@v {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{81.10545pt}\LT@entry 
    {1}{110.69234pt}\LT@entry 
    {1}{86.00009pt}\LT@entry 
    {3}{243.31866pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\gdef \LT@vi {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{81.10545pt}\LT@entry 
    {1}{110.69234pt}\LT@entry 
    {1}{86.00009pt}\LT@entry 
    {3}{243.31866pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}\href  {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html}{\texttt  {RandomizedSearchCV()}}}{33}{section.3.2}\protected@file@percent }
\newlabel{randomizedsearchcv-1}{{3.2}{33}{\texorpdfstring {\href {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html}{\texttt {RandomizedSearchCV()}}}{RandomizedSearchCV()}}{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}\href  {https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html}{\texttt  {BayesSearchCV()}}}{35}{section.3.3}\protected@file@percent }
\newlabel{bayessearchcv-1}{{3.3}{35}{\texorpdfstring {\href {https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html}{\texttt {BayesSearchCV()}}}{BayesSearchCV()}}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Diagonosis of cross-validated score optimization}{38}{subsection.3.3.1}\protected@file@percent }
\newlabel{diagonosis-of-cross-validated-score-optimization}{{3.3.1}{38}{Diagonosis of cross-validated score optimization}{subsection.3.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Live monitoring of cross-validated score}{43}{subsection.3.3.2}\protected@file@percent }
\newlabel{live-monitoring-of-cross-validated-score}{{3.3.2}{43}{Live monitoring of cross-validated score}{subsection.3.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}\href  {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html}{\texttt  {cross\_validate()}}}{44}{section.3.4}\protected@file@percent }
\newlabel{cross_validate}{{3.4}{44}{\texorpdfstring {\href {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html}{\texttt {cross\_validate()}}}{cross\_validate()}}{section.3.4}{}}
\gdef \LT@vii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{27.81181pt}\LT@entry 
    {1}{26.95772pt}\LT@entry 
    {1}{22.95001pt}\LT@entry 
    {1}{50.4783pt}\LT@entry 
    {1}{31.1625pt}\LT@entry 
    {1}{25.75322pt}\LT@entry 
    {1}{44.92665pt}\LT@entry 
    {1}{46.98526pt}\LT@entry 
    {1}{39.68161pt}\LT@entry 
    {1}{49.1205pt}\LT@entry 
    {1}{36.09001pt}\LT@entry 
    {1}{22.3368pt}\LT@entry 
    {1}{30.86687pt}\LT@entry 
    {1}{34.6233pt}}
\@writefile{toc}{\contentsline {part}{\numberline {II}Tree based models}{48}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Regression trees}{49}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{regression-trees}{{4}{49}{Regression trees}{chapter.4}{}}
\gdef \LT@viii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Native Support for Missing Values}{50}{section.4.1}\protected@file@percent }
\newlabel{native-support-for-missing-values}{{4.1}{50}{Native Support for Missing Values}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Build a regression tree using mileage as the solo predictor}{51}{subsection.4.1.1}\protected@file@percent }
\newlabel{build-a-regression-tree-using-mileage-as-the-solo-predictor}{{4.1.1}{51}{Build a regression tree using mileage as the solo predictor}{subsection.4.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Building regression trees}{52}{section.4.2}\protected@file@percent }
\newlabel{building-regression-trees}{{4.2}{52}{Building regression trees}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Using only mileage feature}{52}{subsection.4.2.1}\protected@file@percent }
\newlabel{using-only-mileage-feature}{{4.2.1}{52}{Using only mileage feature}{subsection.4.2.1}{}}
\gdef \LT@ix {\LT@entry 
    {3}{27.90001pt}\LT@entry 
    {3}{44.8719pt}\LT@entry 
    {3}{61.5378pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{55.27501pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Using mileage and brand as predictors}{54}{subsection.4.2.2}\protected@file@percent }
\newlabel{using-mileage-and-brand-as-predictors}{{4.2.2}{54}{Using mileage and brand as predictors}{subsection.4.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Using all predictors}{56}{subsection.4.2.3}\protected@file@percent }
\newlabel{using-all-predictors}{{4.2.3}{56}{Using all predictors}{subsection.4.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Key Hyperparameters in Decision Tree}{58}{section.4.3}\protected@file@percent }
\newlabel{key-hyperparameters-in-decision-tree}{{4.3}{58}{Key Hyperparameters in Decision Tree}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Underfitting}{58}{subsection.4.3.1}\protected@file@percent }
\newlabel{underfitting}{{4.3.1}{58}{Underfitting}{subsection.4.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Overfitting}{58}{subsection.4.3.2}\protected@file@percent }
\newlabel{overfitting}{{4.3.2}{58}{Overfitting}{subsection.4.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}\texttt  {max\_depth}}{58}{subsection.4.3.3}\protected@file@percent }
\newlabel{max_depth}{{4.3.3}{58}{\texorpdfstring {\texttt {max\_depth}}{max\_depth}}{subsection.4.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}\texttt  {min\_samples\_split}}{58}{subsection.4.3.4}\protected@file@percent }
\newlabel{min_samples_split}{{4.3.4}{58}{\texorpdfstring {\texttt {min\_samples\_split}}{min\_samples\_split}}{subsection.4.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}\texttt  {min\_samples\_leaf}}{59}{subsection.4.3.5}\protected@file@percent }
\newlabel{min_samples_leaf}{{4.3.5}{59}{\texorpdfstring {\texttt {min\_samples\_leaf}}{min\_samples\_leaf}}{subsection.4.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}\texttt  {max\_features}}{59}{subsection.4.3.6}\protected@file@percent }
\newlabel{max_features}{{4.3.6}{59}{\texorpdfstring {\texttt {max\_features}}{max\_features}}{subsection.4.3.6}{}}
\gdef \LT@x {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{159.46362pt}\LT@entry 
    {1}{165.24522pt}\LT@entry 
    {1}{175.36302pt}\LT@entry 
    {1}{197.8981pt}\LT@entry 
    {1}{201.91676pt}\LT@entry 
    {3}{257.27994pt}\LT@entry 
    {1}{21.13231pt}\LT@entry 
    {1}{92.42775pt}\LT@entry 
    {1}{99.4029pt}\LT@entry 
    {1}{85.87965pt}\LT@entry 
    {1}{85.87965pt}\LT@entry 
    {1}{85.87965pt}\LT@entry 
    {1}{85.87965pt}\LT@entry 
    {1}{85.87965pt}\LT@entry 
    {1}{85.20074pt}\LT@entry 
    {1}{74.31645pt}\LT@entry 
    {1}{75.2916pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}Output feature importance}{62}{subsection.4.3.7}\protected@file@percent }
\newlabel{output-feature-importance}{{4.3.7}{62}{Output feature importance}{subsection.4.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Cost-Complexity Pruning (\texttt  {ccp\_alpha})}{64}{section.4.4}\protected@file@percent }
\newlabel{cost-complexity-pruning-ccp_alpha}{{4.4}{64}{\texorpdfstring {Cost-Complexity Pruning (\texttt {ccp\_alpha})}{Cost-Complexity Pruning (ccp\_alpha)}}{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Key Idea}{64}{subsection.4.4.1}\protected@file@percent }
\newlabel{key-idea}{{4.4.1}{64}{Key Idea}{subsection.4.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Parameter: \texttt  {ccp\_alpha} in scikit-learn}{64}{subsection.4.4.2}\protected@file@percent }
\newlabel{parameter-ccp_alpha-in-scikit-learn}{{4.4.2}{64}{\texorpdfstring {Parameter: \texttt {ccp\_alpha} in scikit-learn}{Parameter: ccp\_alpha in scikit-learn}}{subsection.4.4.2}{}}
\gdef \LT@xi {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{68.94pt}\LT@entry 
    {1}{71.05334pt}\LT@entry 
    {1}{67.4508pt}\LT@entry 
    {1}{81.11641pt}\LT@entry 
    {1}{71.38185pt}\LT@entry 
    {1}{75.3786pt}\LT@entry 
    {1}{78.35701pt}\LT@entry 
    {1}{88.4091pt}\LT@entry 
    {1}{61.932pt}\LT@entry 
    {1}{89.33984pt}\LT@entry 
    {1}{21.13231pt}\LT@entry 
    {1}{90.68669pt}\LT@entry 
    {1}{98.87729pt}\LT@entry 
    {1}{95.7018pt}\LT@entry 
    {1}{90.21585pt}\LT@entry 
    {1}{91.278pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{55.27501pt}}
\gdef \LT@xii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{27.81181pt}\LT@entry 
    {1}{26.95772pt}\LT@entry 
    {1}{22.95001pt}\LT@entry 
    {1}{50.4783pt}\LT@entry 
    {1}{31.1625pt}\LT@entry 
    {1}{25.75322pt}\LT@entry 
    {1}{44.92665pt}\LT@entry 
    {1}{46.98526pt}\LT@entry 
    {1}{39.68161pt}\LT@entry 
    {1}{49.1205pt}\LT@entry 
    {1}{36.09001pt}\LT@entry 
    {1}{22.3368pt}\LT@entry 
    {1}{30.86687pt}\LT@entry 
    {1}{34.6233pt}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Classification trees}{69}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{classification-trees}{{5}{69}{Classification trees}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Building a Classification Tree}{70}{section.5.1}\protected@file@percent }
\newlabel{building-a-classification-tree}{{5.1}{70}{Building a Classification Tree}{section.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Pre-pruning: Hyperparameters Tuning}{72}{section.5.2}\protected@file@percent }
\newlabel{pre-pruning-hyperparameters-tuning}{{5.2}{72}{Pre-pruning: Hyperparameters Tuning}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Gini or entropy}{74}{subsection.5.2.1}\protected@file@percent }
\newlabel{gini-or-entropy}{{5.2.1}{74}{Gini or entropy}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Post-pruning: Cost complexity pruning}{76}{section.5.3}\protected@file@percent }
\newlabel{post-pruning-cost-complexity-pruning}{{5.3}{76}{Post-pruning: Cost complexity pruning}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}step 1: calculate the cost complexity pruning path}{77}{subsection.5.3.1}\protected@file@percent }
\newlabel{step-1-calculate-the-cost-complexity-pruning-path}{{5.3.1}{77}{step 1: calculate the cost complexity pruning path}{subsection.5.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}step 2: Create trees with different ccp\_alpha values and evaluate their performance}{77}{subsection.5.3.2}\protected@file@percent }
\newlabel{step-2-create-trees-with-different-ccp_alpha-values-and-evaluate-their-performance}{{5.3.2}{77}{step 2: Create trees with different ccp\_alpha values and evaluate their performance}{subsection.5.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Step 3: Visualize the results}{77}{subsection.5.3.3}\protected@file@percent }
\newlabel{step-3-visualize-the-results}{{5.3.3}{77}{Step 3: Visualize the results}{subsection.5.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Step 4: Create the final model with the optimal alpha}{80}{subsection.5.3.4}\protected@file@percent }
\newlabel{step-4-create-the-final-model-with-the-optimal-alpha}{{5.3.4}{80}{Step 4: Create the final model with the optimal alpha}{subsection.5.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Feature Importance in Decision Trees}{81}{section.5.4}\protected@file@percent }
\newlabel{feature-importance-in-decision-trees}{{5.4}{81}{Feature Importance in Decision Trees}{section.5.4}{}}
\gdef \LT@xiii {\LT@entry 
    {3}{16.95001pt}\LT@entry 
    {3}{50.4783pt}\LT@entry 
    {1}{60.78285pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Do We Need Feature Selection or Regularization with Tree Models?}{83}{subsection.5.4.1}\protected@file@percent }
\newlabel{do-we-need-feature-selection-or-regularization-with-tree-models}{{5.4.1}{83}{Do We Need Feature Selection or Regularization with Tree Models?}{subsection.5.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Bottom Line}{84}{subsection.5.4.2}\protected@file@percent }
\newlabel{bottom-line}{{5.4.2}{84}{Bottom Line}{subsection.5.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Next Lecture}{84}{section.5.5}\protected@file@percent }
\newlabel{next-lecture}{{5.5}{84}{Next Lecture}{section.5.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bagging}{85}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{bagging}{{6}{85}{Bagging}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Bagging: A Variance Reduction Technique}{85}{section.6.1}\protected@file@percent }
\newlabel{bagging-a-variance-reduction-technique}{{6.1}{85}{Bagging: A Variance Reduction Technique}{section.6.1}{}}
\gdef \LT@xiv {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Bagging Regression Trees}{86}{section.6.2}\protected@file@percent }
\newlabel{bagging-regression-trees}{{6.2}{86}{Bagging Regression Trees}{section.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Bagging Doesn't Reduce Bias}{90}{section.6.3}\protected@file@percent }
\newlabel{bagging-doesnt-reduce-bias}{{6.3}{90}{Bagging Doesn't Reduce Bias}{section.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Model Performance vs.\nobreakspace  {}Number of Trees}{92}{section.6.4}\protected@file@percent }
\newlabel{model-performance-vs.-number-of-trees}{{6.4}{92}{Model Performance vs.~Number of Trees}{section.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}OOB Sample and OOB Score in Bagging}{94}{section.6.5}\protected@file@percent }
\newlabel{oob-sample-and-oob-score-in-bagging}{{6.5}{94}{OOB Sample and OOB Score in Bagging}{section.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}What is an OOB Sample?}{94}{subsection.6.5.1}\protected@file@percent }
\newlabel{what-is-an-oob-sample}{{6.5.1}{94}{What is an OOB Sample?}{subsection.6.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}What is OOB Score?}{95}{subsection.6.5.2}\protected@file@percent }
\newlabel{what-is-oob-score}{{6.5.2}{95}{What is OOB Score?}{subsection.6.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Bagging Hyperparameter Tuning}{97}{section.6.6}\protected@file@percent }
\newlabel{bagging-hyperparameter-tuning}{{6.6}{97}{Bagging Hyperparameter Tuning}{section.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Tuning with Cross-Validation}{98}{subsection.6.6.1}\protected@file@percent }
\newlabel{tuning-with-cross-validation}{{6.6.1}{98}{Tuning with Cross-Validation}{subsection.6.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Tuning with Out-of-Bag (OOB) Score}{99}{subsection.6.6.2}\protected@file@percent }
\newlabel{tuning-with-out-of-bag-oob-score}{{6.6.2}{99}{Tuning with Out-of-Bag (OOB) Score}{subsection.6.6.2}{}}
\gdef \LT@xv {\LT@entry 
    {1}{92.415pt}\LT@entry 
    {1}{181.74005pt}\LT@entry 
    {1}{172.65012pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.3}Comparing Hyperparameter Tuning: Cross-Validation vs.\nobreakspace  {}OOB Score}{102}{subsection.6.6.3}\protected@file@percent }
\newlabel{comparing-hyperparameter-tuning-cross-validation-vs.-oob-score}{{6.6.3}{102}{Comparing Hyperparameter Tuning: Cross-Validation vs.~OOB Score}{subsection.6.6.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.3.1}✅ Best Practices for Imbalanced Classification}{102}{subsubsection.6.6.3.1}\protected@file@percent }
\newlabel{best-practices-for-imbalanced-classification}{{6.6.3.1}{102}{✅ Best Practices for Imbalanced Classification}{subsubsection.6.6.3.1}{}}
\gdef \LT@xvi {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{27.81181pt}\LT@entry 
    {1}{26.95772pt}\LT@entry 
    {1}{22.95001pt}\LT@entry 
    {1}{50.4783pt}\LT@entry 
    {1}{31.1625pt}\LT@entry 
    {1}{25.75322pt}\LT@entry 
    {1}{44.92665pt}\LT@entry 
    {1}{46.98526pt}\LT@entry 
    {1}{39.68161pt}\LT@entry 
    {1}{49.1205pt}\LT@entry 
    {1}{36.09001pt}\LT@entry 
    {1}{22.3368pt}\LT@entry 
    {1}{30.86687pt}\LT@entry 
    {1}{34.6233pt}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Bagging Classification Trees}{103}{section.6.7}\protected@file@percent }
\newlabel{bagging-classification-trees}{{6.7}{103}{Bagging Classification Trees}{section.6.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Random Forests}{105}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{random-forests}{{7}{105}{Random Forests}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Motivation: Bagging Revisited}{105}{section.7.1}\protected@file@percent }
\newlabel{motivation-bagging-revisited}{{7.1}{105}{Motivation: Bagging Revisited}{section.7.1}{}}
\gdef \LT@xvii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Let's build a single decision tree and output its performance}{106}{subsection.7.1.1}\protected@file@percent }
\newlabel{lets-build-a-single-decision-tree-and-output-its-performance}{{7.1.1}{106}{Let's build a single decision tree and output its performance}{subsection.7.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Let's Build a Bagging Tree with Bootstrap Sampling to Reduce Variance}{107}{subsection.7.1.2}\protected@file@percent }
\newlabel{lets-build-a-bagging-tree-with-bootstrap-sampling-to-reduce-variance}{{7.1.2}{107}{Let's Build a Bagging Tree with Bootstrap Sampling to Reduce Variance}{subsection.7.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Let's Build a Bagging Tree Without Bootstrap Sampling}{108}{subsection.7.1.3}\protected@file@percent }
\newlabel{lets-build-a-bagging-tree-without-bootstrap-sampling}{{7.1.3}{108}{Let's Build a Bagging Tree Without Bootstrap Sampling}{subsection.7.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.4}❓ Why Does Bagging Without Bootstrap Perform Worse?}{110}{subsection.7.1.4}\protected@file@percent }
\newlabel{why-does-bagging-without-bootstrap-perform-worse}{{7.1.4}{110}{❓ Why Does Bagging Without Bootstrap Perform Worse?}{subsection.7.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.5}❓ Why Can Bagging Without Bootstrap Still Show Slight Improvement?}{110}{subsection.7.1.5}\protected@file@percent }
\newlabel{why-can-bagging-without-bootstrap-still-show-slight-improvement}{{7.1.5}{110}{❓ Why Can Bagging Without Bootstrap Still Show Slight Improvement?}{subsection.7.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Random Forest}{110}{section.7.2}\protected@file@percent }
\newlabel{random-forest}{{7.2}{110}{Random Forest}{section.7.2}{}}
\gdef \LT@xviii {\LT@entry 
    {1}{78.71349pt}\LT@entry 
    {1}{180.17252pt}\LT@entry 
    {1}{187.82887pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Idea Behind Random Forest}{111}{subsection.7.2.1}\protected@file@percent }
\newlabel{idea-behind-random-forest}{{7.2.1}{111}{Idea Behind Random Forest}{subsection.7.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Key Hyperparameter Comparison}{111}{subsection.7.2.2}\protected@file@percent }
\newlabel{key-hyperparameter-comparison}{{7.2.2}{111}{Key Hyperparameter Comparison}{subsection.7.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Let's Build a Random Forest Model Using the Default Settings}{111}{subsection.7.2.3}\protected@file@percent }
\newlabel{lets-build-a-random-forest-model-using-the-default-settings}{{7.2.3}{111}{Let's Build a Random Forest Model Using the Default Settings}{subsection.7.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Let's Build a Random Forest Model with \texttt  {sqrt} max\_features}{112}{subsection.7.2.4}\protected@file@percent }
\newlabel{lets-build-a-random-forest-model-with-sqrt-max_features}{{7.2.4}{112}{\texorpdfstring {Let's Build a Random Forest Model with \texttt {sqrt} max\_features}{Let's Build a Random Forest Model with sqrt max\_features}}{subsection.7.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Let's Explore How \texttt  {max\_features} Affects Performance}{114}{section.7.3}\protected@file@percent }
\newlabel{lets-explore-how-max_features-affects-performance}{{7.3}{114}{\texorpdfstring {Let's Explore How \texttt {max\_features} Affects Performance}{Let's Explore How max\_features Affects Performance}}{section.7.3}{}}
\gdef \LT@xix {\LT@entry 
    {1}{145.21484pt}\LT@entry 
    {1}{197.58969pt}\LT@entry 
    {1}{103.95549pt}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Other Hyperparameters in Random Forest}{116}{section.7.4}\protected@file@percent }
\newlabel{other-hyperparameters-in-random-forest}{{7.4}{116}{Other Hyperparameters in Random Forest}{section.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Why Bagging Uses Unpruned Trees}{116}{subsection.7.4.1}\protected@file@percent }
\newlabel{why-bagging-uses-unpruned-trees}{{7.4.1}{116}{Why Bagging Uses Unpruned Trees}{subsection.7.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Hyperparameters That Control Tree Complexity in Random Forest}{116}{subsection.7.4.2}\protected@file@percent }
\newlabel{hyperparameters-that-control-tree-complexity-in-random-forest}{{7.4.2}{116}{Hyperparameters That Control Tree Complexity in Random Forest}{subsection.7.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Why Random Forest Often Limits Tree Depth}{117}{subsection.7.4.3}\protected@file@percent }
\newlabel{why-random-forest-often-limits-tree-depth}{{7.4.3}{117}{Why Random Forest Often Limits Tree Depth}{subsection.7.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.4}Let's Tune Multiple Hyperparameters Simultaneously Using Cross-Validation}{117}{subsection.7.4.4}\protected@file@percent }
\newlabel{lets-tune-multiple-hyperparameters-simultaneously-using-cross-validation}{{7.4.4}{117}{Let's Tune Multiple Hyperparameters Simultaneously Using Cross-Validation}{subsection.7.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Feature Importance in Random Forest}{119}{section.7.5}\protected@file@percent }
\newlabel{feature-importance-in-random-forest}{{7.5}{119}{Feature Importance in Random Forest}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}How Feature Importance Is Calculated}{119}{subsection.7.5.1}\protected@file@percent }
\newlabel{how-feature-importance-is-calculated}{{7.5.1}{119}{How Feature Importance Is Calculated}{subsection.7.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Accessing Feature Importances}{119}{subsection.7.5.2}\protected@file@percent }
\newlabel{accessing-feature-importances}{{7.5.2}{119}{Accessing Feature Importances}{subsection.7.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}In Summary}{120}{section.7.6}\protected@file@percent }
\newlabel{in-summary}{{7.6}{120}{In Summary}{section.7.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Next Step}{121}{section.7.7}\protected@file@percent }
\newlabel{next-step}{{7.7}{121}{Next Step}{section.7.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Adaptive Boosting}{122}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{adaptive-boosting}{{8}{122}{Adaptive Boosting}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}What is AdaBoost?}{123}{section.8.1}\protected@file@percent }
\newlabel{what-is-adaboost}{{8.1}{123}{What is AdaBoost?}{section.8.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}AdaBoost Intuition}{123}{section.8.2}\protected@file@percent }
\newlabel{adaboost-intuition}{{8.2}{123}{AdaBoost Intuition}{section.8.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}How AdaBoost Works (High-Level Steps)}{123}{section.8.3}\protected@file@percent }
\newlabel{how-adaboost-works-high-level-steps}{{8.3}{123}{How AdaBoost Works (High-Level Steps)}{section.8.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Key Hyperparameters in AdaBoost}{123}{section.8.4}\protected@file@percent }
\newlabel{key-hyperparameters-in-adaboost}{{8.4}{123}{Key Hyperparameters in AdaBoost}{section.8.4}{}}
\gdef \LT@xx {\LT@entry 
    {1}{90.5507pt}\LT@entry 
    {1}{265.6586pt}\LT@entry 
    {1}{90.5507pt}}
\gdef \LT@xxi {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}AdaBoost for Regression}{124}{section.8.5}\protected@file@percent }
\newlabel{adaboost-for-regression}{{8.5}{124}{AdaBoost for Regression}{section.8.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}Let's build a adaboost regressor model with default settings}{125}{subsection.8.5.1}\protected@file@percent }
\newlabel{lets-build-a-adaboost-regressor-model-with-default-settings}{{8.5.1}{125}{Let's build a adaboost regressor model with default settings}{subsection.8.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.1.1}❓ Why AdaBoost Perform Worse Here}{126}{subsubsection.8.5.1.1}\protected@file@percent }
\newlabel{why-adaboost-perform-worse-here}{{8.5.1.1}{126}{❓ Why AdaBoost Perform Worse Here}{subsubsection.8.5.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}Impact of Tree Depth}{127}{subsection.8.5.2}\protected@file@percent }
\newlabel{impact-of-tree-depth}{{8.5.2}{127}{Impact of Tree Depth}{subsection.8.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}Impact of Learning Rate}{129}{subsection.8.5.3}\protected@file@percent }
\newlabel{impact-of-learning-rate}{{8.5.3}{129}{Impact of Learning Rate}{subsection.8.5.3}{}}
\gdef \LT@xxii {\LT@entry 
    {1}{124.37228pt}\LT@entry 
    {1}{168.41914pt}\LT@entry 
    {1}{153.96858pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.3.1}Effect on Performance}{130}{subsubsection.8.5.3.1}\protected@file@percent }
\newlabel{effect-on-performance}{{8.5.3.1}{130}{Effect on Performance}{subsubsection.8.5.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.4}Impact of Number of Trees in Boosting}{132}{subsection.8.5.4}\protected@file@percent }
\newlabel{impact-of-number-of-trees-in-boosting}{{8.5.4}{132}{Impact of Number of Trees in Boosting}{subsection.8.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.5}Tuning Hyperparameters Simultaneously}{135}{subsection.8.5.5}\protected@file@percent }
\newlabel{tuning-hyperparameters-simultaneously}{{8.5.5}{135}{Tuning Hyperparameters Simultaneously}{subsection.8.5.5}{}}
\gdef \LT@xxiii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{85.6059pt}\LT@entry 
    {1}{74.7216pt}\LT@entry 
    {1}{99.06345pt}\LT@entry 
    {1}{88.17914pt}\LT@entry 
    {1}{168.16887pt}\LT@entry 
    {1}{116.13449pt}\LT@entry 
    {1}{114.38249pt}\LT@entry 
    {3}{255.21037pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{21.13231pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{96.15074pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.5.1}Analyzing \texttt  {BayesSearchCV} Results}{139}{subsubsection.8.5.5.1}\protected@file@percent }
\newlabel{analyzing-bayessearchcv-results}{{8.5.5.1}{139}{\texorpdfstring {Analyzing \texttt {BayesSearchCV} Results}{Analyzing BayesSearchCV Results}}{subsubsection.8.5.5.1}{}}
\gdef \LT@xxiv {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{116.13449pt}\LT@entry 
    {1}{168.16887pt}\LT@entry 
    {1}{114.38249pt}\LT@entry 
    {1}{95.47185pt}\LT@entry 
    {1}{84.58755pt}\LT@entry 
    {1}{85.5627pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.6}Using Optuna for Hyperparameter Tuning}{144}{subsection.8.5.6}\protected@file@percent }
\newlabel{using-optuna-for-hyperparameter-tuning}{{8.5.6}{144}{Using Optuna for Hyperparameter Tuning}{subsection.8.5.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Gradient Boosting}{149}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{gradient-boosting}{{9}{149}{Gradient Boosting}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}What is Gradient Boosting?}{149}{section.9.1}\protected@file@percent }
\newlabel{what-is-gradient-boosting}{{9.1}{149}{What is Gradient Boosting?}{section.9.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Gradient Boosting Intuition}{149}{section.9.2}\protected@file@percent }
\newlabel{gradient-boosting-intuition}{{9.2}{149}{Gradient Boosting Intuition}{section.9.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}How Gradient Boosting Works (Regression Example)}{150}{section.9.3}\protected@file@percent }
\newlabel{how-gradient-boosting-works-regression-example}{{9.3}{150}{How Gradient Boosting Works (Regression Example)}{section.9.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Gradient Boosting in Scikit-Learn}{151}{section.9.4}\protected@file@percent }
\newlabel{gradient-boosting-in-scikit-learn}{{9.4}{151}{Gradient Boosting in Scikit-Learn}{section.9.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Core Hyperparameters Categories}{151}{section.9.5}\protected@file@percent }
\newlabel{core-hyperparameters-categories}{{9.5}{151}{Core Hyperparameters Categories}{section.9.5}{}}
\gdef \LT@xxv {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Hyperparameter Tuning}{153}{section.9.6}\protected@file@percent }
\newlabel{hyperparameter-tuning-2}{{9.6}{153}{Hyperparameter Tuning}{section.9.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.1}Individual Hyperparameter Impact Analysis}{154}{subsection.9.6.1}\protected@file@percent }
\newlabel{individual-hyperparameter-impact-analysis}{{9.6.1}{154}{Individual Hyperparameter Impact Analysis}{subsection.9.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.1.1}Effect of Number of Trees on Cross-Validation Error}{154}{subsubsection.9.6.1.1}\protected@file@percent }
\newlabel{effect-of-number-of-trees-on-cross-validation-error}{{9.6.1.1}{154}{Effect of Number of Trees on Cross-Validation Error}{subsubsection.9.6.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.1.2}Early stopping in Gradient Boosting}{156}{subsubsection.9.6.1.2}\protected@file@percent }
\newlabel{early-stopping-in-gradient-boosting}{{9.6.1.2}{156}{Early stopping in Gradient Boosting}{subsubsection.9.6.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.1.3}Effect of Learning Rate on Cross-Validation Error}{160}{subsubsection.9.6.1.3}\protected@file@percent }
\newlabel{effect-of-learning-rate-on-cross-validation-error}{{9.6.1.3}{160}{Effect of Learning Rate on Cross-Validation Error}{subsubsection.9.6.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.1.4}Learning Rate and Number of Trees Are Closely Linked}{162}{subsubsection.9.6.1.4}\protected@file@percent }
\newlabel{learning-rate-and-number-of-trees-are-closely-linked}{{9.6.1.4}{162}{Learning Rate and Number of Trees Are Closely Linked}{subsubsection.9.6.1.4}{}}
\gdef \LT@xxvi {\LT@entry 
    {1}{66.41179pt}\LT@entry 
    {1}{98.83429pt}\LT@entry 
    {1}{281.55263pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.1.5}Effect of Stochastic Gradient Boosting on Cross-Validation Error}{163}{subsubsection.9.6.1.5}\protected@file@percent }
\newlabel{effect-of-stochastic-gradient-boosting-on-cross-validation-error}{{9.6.1.5}{163}{Effect of Stochastic Gradient Boosting on Cross-Validation Error}{subsubsection.9.6.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.1.6}Effect of Tree Complexity on Cross-Validation Error (Not Tuned Here)}{165}{subsubsection.9.6.1.6}\protected@file@percent }
\newlabel{effect-of-tree-complexity-on-cross-validation-error-not-tuned-here}{{9.6.1.6}{165}{Effect of Tree Complexity on Cross-Validation Error (Not Tuned Here)}{subsubsection.9.6.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.1.7}Loss Function (\texttt  {loss})}{166}{subsubsection.9.6.1.7}\protected@file@percent }
\newlabel{loss-function-loss}{{9.6.1.7}{166}{\texorpdfstring {Loss Function (\texttt {loss})}{Loss Function (loss)}}{subsubsection.9.6.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.2}Joint Hyperparameter Optimization}{166}{subsection.9.6.2}\protected@file@percent }
\newlabel{joint-hyperparameter-optimization}{{9.6.2}{166}{Joint Hyperparameter Optimization}{subsection.9.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.2.1}Using \texttt  {BayesSearchCV} for Hyperparameter Tuning}{167}{subsubsection.9.6.2.1}\protected@file@percent }
\newlabel{using-bayessearchcv-for-hyperparameter-tuning}{{9.6.2.1}{167}{\texorpdfstring {Using \texttt {BayesSearchCV} for Hyperparameter Tuning}{Using BayesSearchCV for Hyperparameter Tuning}}{subsubsection.9.6.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.2.2}Hyperparameter Optimization with Optuna}{171}{subsubsection.9.6.2.2}\protected@file@percent }
\newlabel{hyperparameter-optimization-with-optuna}{{9.6.2.2}{171}{Hyperparameter Optimization with Optuna}{subsubsection.9.6.2.2}{}}
\gdef \LT@xxvii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{68.81955pt}\LT@entry 
    {1}{49.24095pt}\LT@entry 
    {1}{80.93025pt}\LT@entry 
    {1}{79.9557pt}\LT@entry 
    {1}{44.62006pt}\LT@entry 
    {1}{33.7467pt}\LT@entry 
    {1}{136.35913pt}\LT@entry 
    {1}{30.54932pt}\LT@entry 
    {1}{49.18681pt}}
\@writefile{toc}{\contentsline {section}{\numberline {9.7}Independent Study}{175}{section.9.7}\protected@file@percent }
\newlabel{independent-study}{{9.7}{175}{Independent Study}{section.9.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.8}Foundational Paper}{176}{section.9.8}\protected@file@percent }
\newlabel{foundational-paper}{{9.8}{176}{Foundational Paper}{section.9.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}XGBoost}{177}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{xgboost}{{10}{177}{XGBoost}{chapter.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}What is XGBoost?}{177}{section.10.1}\protected@file@percent }
\newlabel{what-is-xgboost}{{10.1}{177}{What is XGBoost?}{section.10.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}XGBoost Intuition}{177}{section.10.2}\protected@file@percent }
\newlabel{xgboost-intuition}{{10.2}{177}{XGBoost Intuition}{section.10.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}How XGBoost Works (Regression Example)}{177}{section.10.3}\protected@file@percent }
\newlabel{how-xgboost-works-regression-example}{{10.3}{177}{How XGBoost Works (Regression Example)}{section.10.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.4}Using XGBoost}{178}{section.10.4}\protected@file@percent }
\newlabel{using-xgboost}{{10.4}{178}{Using XGBoost}{section.10.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.5}Core Hyperparameter Categories}{179}{section.10.5}\protected@file@percent }
\newlabel{core-hyperparameter-categories}{{10.5}{179}{Core Hyperparameter Categories}{section.10.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.1}Model Complexity}{179}{subsection.10.5.1}\protected@file@percent }
\newlabel{model-complexity}{{10.5.1}{179}{Model Complexity}{subsection.10.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.2}Learning and Regularization}{179}{subsection.10.5.2}\protected@file@percent }
\newlabel{learning-and-regularization}{{10.5.2}{179}{Learning and Regularization}{subsection.10.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.3}Regularization}{179}{subsection.10.5.3}\protected@file@percent }
\newlabel{regularization}{{10.5.3}{179}{Regularization}{subsection.10.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.4}Optimization Control}{179}{subsection.10.5.4}\protected@file@percent }
\newlabel{optimization-control}{{10.5.4}{179}{Optimization Control}{subsection.10.5.4}{}}
\gdef \LT@xxviii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.5}Baseline Model}{182}{subsection.10.5.5}\protected@file@percent }
\newlabel{baseline-model}{{10.5.5}{182}{Baseline Model}{subsection.10.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.6}Early Stopping in XGBoost}{182}{subsection.10.5.6}\protected@file@percent }
\newlabel{early-stopping-in-xgboost}{{10.5.6}{182}{Early Stopping in XGBoost}{subsection.10.5.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.6.1}How It Works}{182}{subsubsection.10.5.6.1}\protected@file@percent }
\newlabel{how-it-works}{{10.5.6.1}{182}{How It Works}{subsubsection.10.5.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.6.2}Requirements}{183}{subsubsection.10.5.6.2}\protected@file@percent }
\newlabel{requirements}{{10.5.6.2}{183}{Requirements}{subsubsection.10.5.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.7}\texttt  {gamma} in XGBoost}{184}{subsection.10.5.7}\protected@file@percent }
\newlabel{gamma-in-xgboost}{{10.5.7}{184}{\texorpdfstring {\texttt {gamma} in XGBoost}{gamma in XGBoost}}{subsection.10.5.7}{}}
\gdef \LT@xxix {\LT@entry 
    {1}{97.91109pt}\LT@entry 
    {1}{195.81572pt}\LT@entry 
    {1}{153.03322pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.8}\texttt  {reg\_lambda} and \texttt  {reg\_alpha} in XGBoost}{187}{subsection.10.5.8}\protected@file@percent }
\newlabel{reg_lambda-and-reg_alpha-in-xgboost}{{10.5.8}{187}{\texorpdfstring {\texttt {reg\_lambda} and \texttt {reg\_alpha} in XGBoost}{reg\_lambda and reg\_alpha in XGBoost}}{subsection.10.5.8}{}}
\gdef \LT@xxx {\LT@entry 
    {1}{172.25246pt}\LT@entry 
    {1}{274.50757pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.9}Exploring Regularization Hyperparameters Simultaneously}{191}{subsection.10.5.9}\protected@file@percent }
\newlabel{exploring-regularization-hyperparameters-simultaneously}{{10.5.9}{191}{Exploring Regularization Hyperparameters Simultaneously}{subsection.10.5.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.10}Comprehensive Hyperparameter Tuning}{193}{subsection.10.5.10}\protected@file@percent }
\newlabel{comprehensive-hyperparameter-tuning}{{10.5.10}{193}{Comprehensive Hyperparameter Tuning}{subsection.10.5.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.10.1}Why \texttt  {GridSearchCV} Is Not a Practical Option}{194}{subsubsection.10.5.10.1}\protected@file@percent }
\newlabel{why-gridsearchcv-is-not-a-practical-option}{{10.5.10.1}{194}{\texorpdfstring {Why \texttt {GridSearchCV} Is Not a Practical Option}{Why GridSearchCV Is Not a Practical Option}}{subsubsection.10.5.10.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.10.2}Smarter Tuning with Optuna or BayesSearchCV}{195}{subsubsection.10.5.10.2}\protected@file@percent }
\newlabel{smarter-tuning-with-optuna-or-bayessearchcv}{{10.5.10.2}{195}{Smarter Tuning with Optuna or BayesSearchCV}{subsubsection.10.5.10.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {10.5.10.2.1}\texttt  {BayesSearchCV} (from \texttt  {skopt})}{196}{paragraph.10.5.10.2.1}\protected@file@percent }
\newlabel{bayessearchcv-from-skopt}{{10.5.10.2.1}{196}{\texorpdfstring {\texttt {BayesSearchCV} (from \texttt {skopt})}{BayesSearchCV (from skopt)}}{paragraph.10.5.10.2.1}{}}
\gdef \LT@xxxi {\LT@entry 
    {3}{16.95001pt}\LT@entry 
    {3}{115.94833pt}\LT@entry 
    {1}{60.78285pt}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {10.5.10.2.2}Tuning with Optuna}{203}{paragraph.10.5.10.2.2}\protected@file@percent }
\newlabel{tuning-with-optuna}{{10.5.10.2.2}{203}{Tuning with Optuna}{paragraph.10.5.10.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.10.3}After Training: Analyze and Refine}{206}{subsubsection.10.5.10.3}\protected@file@percent }
\newlabel{after-training-analyze-and-refine}{{10.5.10.3}{206}{After Training: Analyze and Refine}{subsubsection.10.5.10.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.6}XGBoost for Imbalanced Classification}{206}{section.10.6}\protected@file@percent }
\newlabel{xgboost-for-imbalanced-classification}{{10.6}{206}{XGBoost for Imbalanced Classification}{section.10.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.1}Common Strategies Across Libraries}{206}{subsection.10.6.1}\protected@file@percent }
\newlabel{common-strategies-across-libraries}{{10.6.1}{206}{Common Strategies Across Libraries}{subsection.10.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.2}Handling Class Imbalance with \texttt  {scale\_pos\_weight} in XGBoost}{207}{subsection.10.6.2}\protected@file@percent }
\newlabel{handling-class-imbalance-with-scale_pos_weight-in-xgboost}{{10.6.2}{207}{\texorpdfstring {Handling Class Imbalance with \texttt {scale\_pos\_weight} in XGBoost}{Handling Class Imbalance with scale\_pos\_weight in XGBoost}}{subsection.10.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.6.2.1}What Does \texttt  {scale\_pos\_weight} Do?}{207}{subsubsection.10.6.2.1}\protected@file@percent }
\newlabel{what-does-scale_pos_weight-do}{{10.6.2.1}{207}{\texorpdfstring {What Does \texttt {scale\_pos\_weight} Do?}{What Does scale\_pos\_weight Do?}}{subsubsection.10.6.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.6.2.2}When to Use It}{207}{subsubsection.10.6.2.2}\protected@file@percent }
\newlabel{when-to-use-it}{{10.6.2.2}{207}{When to Use It}{subsubsection.10.6.2.2}{}}
\gdef \LT@xxxii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{68.81955pt}\LT@entry 
    {1}{49.24095pt}\LT@entry 
    {1}{80.93025pt}\LT@entry 
    {1}{79.9557pt}\LT@entry 
    {1}{44.62006pt}\LT@entry 
    {1}{33.7467pt}\LT@entry 
    {1}{136.35913pt}\LT@entry 
    {1}{30.54932pt}\LT@entry 
    {1}{49.18681pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.3}How to Set It}{208}{subsection.10.6.3}\protected@file@percent }
\newlabel{how-to-set-it}{{10.6.3}{208}{How to Set It}{subsection.10.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.4}Using \texttt  {scale\_pos\_weight}}{210}{subsection.10.6.4}\protected@file@percent }
\newlabel{using-scale_pos_weight}{{10.6.4}{210}{\texorpdfstring {Using \texttt {scale\_pos\_weight}}{Using scale\_pos\_weight}}{subsection.10.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.5}Threshold adjustment}{214}{subsection.10.6.5}\protected@file@percent }
\newlabel{threshold-adjustment}{{10.6.5}{214}{Threshold adjustment}{subsection.10.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.6}Alternative Method: Custom Instance Weights (\texttt  {sample\_weight})}{215}{subsection.10.6.6}\protected@file@percent }
\newlabel{alternative-method-custom-instance-weights-sample_weight}{{10.6.6}{215}{\texorpdfstring {Alternative Method: Custom Instance Weights (\texttt {sample\_weight})}{Alternative Method: Custom Instance Weights (sample\_weight)}}{subsection.10.6.6}{}}
\gdef \LT@xxxiii {\LT@entry 
    {1}{56.64528pt}\LT@entry 
    {1}{201.86012pt}\LT@entry 
    {1}{188.25462pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.7}\texttt  {scale\_pos\_weight} vs.\nobreakspace  {}\texttt  {sample\_weight}}{216}{subsection.10.6.7}\protected@file@percent }
\newlabel{scale_pos_weight-vs.-sample_weight}{{10.6.7}{216}{\texorpdfstring {\texttt {scale\_pos\_weight} vs.~\texttt {sample\_weight}}{scale\_pos\_weight vs.~sample\_weight}}{subsection.10.6.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.7}Resources for Learning XGBoost}{216}{section.10.7}\protected@file@percent }
\newlabel{resources-for-learning-xgboost}{{10.7}{216}{Resources for Learning XGBoost}{section.10.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}LightGBM and CatBoost}{217}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{lightgbm-and-catboost}{{11}{217}{LightGBM and CatBoost}{chapter.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}What They Share with XGBoost}{217}{section.11.1}\protected@file@percent }
\newlabel{what-they-share-with-xgboost}{{11.1}{217}{What They Share with XGBoost}{section.11.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}LightGBM}{218}{section.11.2}\protected@file@percent }
\newlabel{lightgbm}{{11.2}{218}{LightGBM}{section.11.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}What is LightGBM?}{218}{subsection.11.2.1}\protected@file@percent }
\newlabel{what-is-lightgbm}{{11.2.1}{218}{What is LightGBM?}{subsection.11.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.2}What Makes LightGBM Lighting Fast?}{218}{subsection.11.2.2}\protected@file@percent }
\newlabel{what-makes-lightgbm-lighting-fast}{{11.2.2}{218}{What Makes LightGBM Lighting Fast?}{subsection.11.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.2.1}Leaf-Wise Tree Growth}{218}{subsubsection.11.2.2.1}\protected@file@percent }
\newlabel{leaf-wise-tree-growth}{{11.2.2.1}{218}{Leaf-Wise Tree Growth}{subsubsection.11.2.2.1}{}}
\gdef \LT@xxxiv {\LT@entry 
    {1}{51.99pt}\LT@entry 
    {1}{57.99pt}\LT@entry 
    {1}{86.4825pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.2.2}GOSS (Gradient-based One-Side Sampling)}{219}{subsubsection.11.2.2.2}\protected@file@percent }
\newlabel{goss-gradient-based-one-side-sampling}{{11.2.2.2}{219}{GOSS (Gradient-based One-Side Sampling)}{subsubsection.11.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.2.3}Exclusive Feature Bundling (EFB)}{219}{subsubsection.11.2.2.3}\protected@file@percent }
\newlabel{exclusive-feature-bundling-efb}{{11.2.2.3}{219}{Exclusive Feature Bundling (EFB)}{subsubsection.11.2.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.3}Using LightGBM}{220}{subsection.11.2.3}\protected@file@percent }
\newlabel{using-lightgbm}{{11.2.3}{220}{Using LightGBM}{subsection.11.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3.1}Core LightGBM Hyperparameters}{220}{subsubsection.11.2.3.1}\protected@file@percent }
\newlabel{core-lightgbm-hyperparameters}{{11.2.3.1}{220}{Core LightGBM Hyperparameters}{subsubsection.11.2.3.1}{}}
\gdef \LT@xxxv {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3.2}Building a Baseline Model Using LightGBM's Native Categorical Feature Support}{223}{subsubsection.11.2.3.2}\protected@file@percent }
\newlabel{building-a-baseline-model-using-lightgbms-native-categorical-feature-support}{{11.2.3.2}{223}{Building a Baseline Model Using LightGBM's Native Categorical Feature Support}{subsubsection.11.2.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3.3}Enabling GOSS and EFB in LightGBM}{224}{subsubsection.11.2.3.3}\protected@file@percent }
\newlabel{enabling-goss-and-efb-in-lightgbm}{{11.2.3.3}{224}{Enabling GOSS and EFB in LightGBM}{subsubsection.11.2.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {11.2.3.3.1}GOSS is \textbf  {not enabled by default}.}{224}{paragraph.11.2.3.3.1}\protected@file@percent }
\newlabel{goss-is-not-enabled-by-default.}{{11.2.3.3.1}{224}{\texorpdfstring {GOSS is \textbf {not enabled by default}.}{GOSS is not enabled by default.}}{paragraph.11.2.3.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {11.2.3.3.2}EFB is \textbf  {enabled by default}}{224}{paragraph.11.2.3.3.2}\protected@file@percent }
\newlabel{efb-is-enabled-by-default}{{11.2.3.3.2}{224}{\texorpdfstring {EFB is \textbf {enabled by default}}{EFB is enabled by default}}{paragraph.11.2.3.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3.4}Tuning \texttt  {top\_rate} and \texttt  {other\_rate} in GOSS}{225}{subsubsection.11.2.3.4}\protected@file@percent }
\newlabel{tuning-top_rate-and-other_rate-in-goss}{{11.2.3.4}{225}{\texorpdfstring {Tuning \texttt {top\_rate} and \texttt {other\_rate} in GOSS}{Tuning top\_rate and other\_rate in GOSS}}{subsubsection.11.2.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3.5}Optimizing LightGBM with \texttt  {BayesSearchCV}}{226}{subsubsection.11.2.3.5}\protected@file@percent }
\newlabel{optimizing-lightgbm-with-bayessearchcv}{{11.2.3.5}{226}{\texorpdfstring {Optimizing LightGBM with \texttt {BayesSearchCV}}{Optimizing LightGBM with BayesSearchCV}}{subsubsection.11.2.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.3}CatBoost}{228}{section.11.3}\protected@file@percent }
\newlabel{catboost}{{11.3}{228}{CatBoost}{section.11.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.1}What is CatBoost?}{228}{subsection.11.3.1}\protected@file@percent }
\newlabel{what-is-catboost}{{11.3.1}{228}{What is CatBoost?}{subsection.11.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.2}What Makes CatBoost Unique?}{228}{subsection.11.3.2}\protected@file@percent }
\newlabel{what-makes-catboost-unique}{{11.3.2}{228}{What Makes CatBoost Unique?}{subsection.11.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2.1}Symmetric (Oblivious) Trees}{228}{subsubsection.11.3.2.1}\protected@file@percent }
\newlabel{symmetric-oblivious-trees}{{11.3.2.1}{228}{Symmetric (Oblivious) Trees}{subsubsection.11.3.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2.2}Advanced Categorical Feature Handling}{229}{subsubsection.11.3.2.2}\protected@file@percent }
\newlabel{advanced-categorical-feature-handling}{{11.3.2.2}{229}{Advanced Categorical Feature Handling}{subsubsection.11.3.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2.3}Ordered Boosting (vs.\nobreakspace  {}Standard Boosting)}{229}{subsubsection.11.3.2.3}\protected@file@percent }
\newlabel{ordered-boosting-vs.-standard-boosting}{{11.3.2.3}{229}{Ordered Boosting (vs.~Standard Boosting)}{subsubsection.11.3.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2.4}Handling of Text and Embedding Features}{229}{subsubsection.11.3.2.4}\protected@file@percent }
\newlabel{handling-of-text-and-embedding-features}{{11.3.2.4}{229}{Handling of Text and Embedding Features}{subsubsection.11.3.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2.5}Ease of Use and Defaults}{230}{subsubsection.11.3.2.5}\protected@file@percent }
\newlabel{ease-of-use-and-defaults}{{11.3.2.5}{230}{Ease of Use and Defaults}{subsubsection.11.3.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.3}Using CatBoost}{230}{subsection.11.3.3}\protected@file@percent }
\newlabel{using-catboost}{{11.3.3}{230}{Using CatBoost}{subsection.11.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.4}Installation}{230}{subsection.11.3.4}\protected@file@percent }
\newlabel{installation}{{11.3.4}{230}{Installation}{subsection.11.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.5}CatBoost for Regression}{230}{subsection.11.3.5}\protected@file@percent }
\newlabel{catboost-for-regression}{{11.3.5}{230}{CatBoost for Regression}{subsection.11.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.6}Tuning \texttt  {CatBoostRegressor}}{231}{subsection.11.3.6}\protected@file@percent }
\newlabel{tuning-catboostregressor}{{11.3.6}{231}{\texorpdfstring {Tuning \texttt {CatBoostRegressor}}{Tuning CatBoostRegressor}}{subsection.11.3.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.6.1}❌ Hyperparameters \textbf  {not used} in CatBoost:}{231}{subsubsection.11.3.6.1}\protected@file@percent }
\newlabel{hyperparameters-not-used-in-catboost}{{11.3.6.1}{231}{\texorpdfstring {❌ Hyperparameters \textbf {not used} in CatBoost:}{❌ Hyperparameters not used in CatBoost:}}{subsubsection.11.3.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.6.2}✅ Unique Hyperparameters in CatBoost}{232}{subsubsection.11.3.6.2}\protected@file@percent }
\newlabel{unique-hyperparameters-in-catboost}{{11.3.6.2}{232}{✅ Unique Hyperparameters in CatBoost}{subsubsection.11.3.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.7}When to Use \textbf  {CatBoost} Over \textbf  {XGBoost}}{238}{subsection.11.3.7}\protected@file@percent }
\newlabel{when-to-use-catboost-over-xgboost}{{11.3.7}{238}{\texorpdfstring {When to Use \textbf {CatBoost} Over \textbf {XGBoost}}{When to Use CatBoost Over XGBoost}}{subsection.11.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Handling Imbalanced Classification: XGBoost vs.\nobreakspace  {}LightGBM vs.\nobreakspace  {}CatBoost}{238}{section.11.4}\protected@file@percent }
\newlabel{handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost}{{11.4}{238}{Handling Imbalanced Classification: XGBoost vs.~LightGBM vs.~CatBoost}{section.11.4}{}}
\gdef \LT@xxxvi {\LT@entry 
    {1}{49.04655pt}\LT@entry 
    {1}{168.49821pt}\LT@entry 
    {1}{109.8012pt}\LT@entry 
    {1}{119.45164pt}}
\@writefile{toc}{\contentsline {section}{\numberline {11.5}Summary: XGBoost vs.\nobreakspace  {}LightGBM vs.\nobreakspace  {}CatBoost}{239}{section.11.5}\protected@file@percent }
\newlabel{summary-xgboost-vs.-lightgbm-vs.-catboost}{{11.5}{239}{Summary: XGBoost vs.~LightGBM vs.~CatBoost}{section.11.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.6}References}{240}{section.11.6}\protected@file@percent }
\newlabel{references}{{11.6}{240}{References}{section.11.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Smarter Hyperparameter Optimization for Tree-Based Models}{241}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{smarter-hyperparameter-optimization-for-tree-based-models}{{12}{241}{Smarter Hyperparameter Optimization for Tree-Based Models}{chapter.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Cross-Validation Basics}{241}{section.12.1}\protected@file@percent }
\newlabel{cross-validation-basics}{{12.1}{241}{Cross-Validation Basics}{section.12.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}\texttt  {GridSearchCV}: Exhaustive but Limited}{241}{section.12.2}\protected@file@percent }
\newlabel{gridsearchcv-exhaustive-but-limited}{{12.2}{241}{\texorpdfstring {\texttt {GridSearchCV}: Exhaustive but Limited}{GridSearchCV: Exhaustive but Limited}}{section.12.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}\texttt  {RandomizedSearchCV}: Efficient but Random}{242}{section.12.3}\protected@file@percent }
\newlabel{randomizedsearchcv-efficient-but-random}{{12.3}{242}{\texorpdfstring {\texttt {RandomizedSearchCV}: Efficient but Random}{RandomizedSearchCV: Efficient but Random}}{section.12.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.4}Challenges with Boosting Models}{242}{section.12.4}\protected@file@percent }
\newlabel{challenges-with-boosting-models}{{12.4}{242}{Challenges with Boosting Models}{section.12.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.5}Smarter Alternatives for Complex Models}{243}{section.12.5}\protected@file@percent }
\newlabel{smarter-alternatives-for-complex-models}{{12.5}{243}{Smarter Alternatives for Complex Models}{section.12.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.1}1. \texttt  {BayesSearchCV} (Bayesian Optimization)}{243}{subsection.12.5.1}\protected@file@percent }
\newlabel{bayessearchcv-bayesian-optimization}{{12.5.1}{243}{\texorpdfstring {1. \texttt {BayesSearchCV} (Bayesian Optimization)}{1. BayesSearchCV (Bayesian Optimization)}}{subsection.12.5.1}{}}
\gdef \LT@xxxvii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {3}{52.16461pt}\LT@entry 
    {3}{58.9974pt}\LT@entry 
    {3}{33.90001pt}\LT@entry 
    {1}{71.8308pt}\LT@entry 
    {1}{47.88316pt}\LT@entry 
    {1}{53.9823pt}\LT@entry 
    {3}{28.42502pt}\LT@entry 
    {3}{47.8941pt}\LT@entry 
    {1}{61.27501pt}\LT@entry 
    {3}{33.37502pt}}
