[
  {
    "objectID": "Assignment4_sp25.html",
    "href": "Assignment4_sp25.html",
    "title": "Appendix D ‚Äî Assignment 4",
    "section": "",
    "text": "Instructions\nFeel free to add data visualizations of your hyperparameter tuning process. Visualizing and analyzing tuning results is important‚Äîeven if it‚Äôs not explicitly required in the instructions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>¬† <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Assignment4_sp25.html#instructions",
    "href": "Assignment4_sp25.html#instructions",
    "title": "Appendix D ‚Äî Assignment 4",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Friday, 23th May 2025 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (1 point). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student‚Äôs identity‚Äîe.g.¬†printouts of the working directory should not be included in the final submission. (1 point)\nThere aren‚Äôt excessively long outputs of extraneous information (e.g.¬†no printouts of entire data frames without good reason, there aren‚Äôt long printouts of which iteration a loop is on, there aren‚Äôt long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)\n\nPlease make sure your code results are clearly incorporated in your submitted HTML file.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>¬† <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Assignment4_sp25.html#adaboost-vs-bagging-4-points",
    "href": "Assignment4_sp25.html#adaboost-vs-bagging-4-points",
    "title": "Appendix D ‚Äî Assignment 4",
    "section": "D.1 AdaBoost vs Bagging (4 points)",
    "text": "D.1 AdaBoost vs Bagging (4 points)\nWhich model among AdaBoost and Random Forest is more sensitive to outliers? (1 point) Explain your reasoning with the theory you learned on the training process of both models. (3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>¬† <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Assignment4_sp25.html#regression-with-boosting-54-points",
    "href": "Assignment4_sp25.html#regression-with-boosting-54-points",
    "title": "Appendix D ‚Äî Assignment 4",
    "section": "D.2 Regression with Boosting (54 points)",
    "text": "D.2 Regression with Boosting (54 points)\nFor this question, you will use the miami_housing.csv file. You can find the description for the variables here.\nThe SALE_PRC variable is the regression response and the rest of the variables, except PARCELNO, are the predictors.\n\nD.2.1 a): Preprocessing\nRead the dataset. Create the training and test sets with a 60%-40% split and random_state = 1. (1 point)\n\n\nD.2.2 b) AdaBoost\nTune an AdaBoost Regressor to achieve a test MAE below $47,000.\n\nYou must set random_state=1 for all components (e.g., base estimator, AdaBoost model, etc.).\nSubmissions that meet the MAE cutoff using any other random_state will receive zero credit.\n\nScoring: - 5 points for achieving test MAE &lt; $47,000 - 1 point for reporting the training MAE of your tuned model to evaluate generalization\n\n\nD.2.3 c) Loss Functions in Gradient Boosting\nGradient Boosting supports multiple loss functions, including squared_error, absolute_error, and huber.\n\n(1 point) Which loss function performs best on this dataset?\n(3 points) What are the advantages of this loss function compared to the other two?\n\n\n\nD.2.4 Task: Tune a Gradient Boosting Model\nYour goal is to tune a Gradient Boosting Regressor to achieve a cross-validation MAE below $45,000.\n\nYou must keep all random_state values set to 1.\n\nSubmissions using any other random_state will receive zero credit, even if the MAE cutoff is met.\n\nScoring (10 points total):\n- 5 points for using a well-reasoned hyperparameter search strategy\n- 5 points for achieving MAE &lt; $45,000 - 1 point for reporting the training MAE of your tuned model to evaluate generalization\nHints\n\nParallel processing is not supported in the vanilla GradientBoostingRegressor.\nBayesSearchCV, like gradient boosting itself, performs a sequential search‚Äîeach trial depends on the result of the previous one‚Äîso it does not support parallel exploration.\nOptuna is generally faster and more efficient than both BayesSearchCV and GridSearchCV. It supports parallel execution of trials and includes several built-in performance enhancements.\n\n\n\nD.2.5 d) XGBoost vs.¬†Gradient Boosting\nXGBoost Enhancements:\n\nWhat improvements make XGBoost superior to vanilla Gradient Boosting in terms of performance and runtime?\n\nExplain the enhancements (1 point)\n\nProvide the reasons behind the improvements (1 point)\n\nIdentify relevant hyperparameters and describe how they influence model behavior (2 points)\n\n\nXGBoost Limitations:\n\nWhat important feature or behavior is missing in XGBoost but well-implemented in vanilla Gradient Boosting? (1 point)\n\n\n\nD.2.6 e) Tuning XGBoost with Different Search Strategies\nTune an XGBoost Regressor to achieve a cross-validation MAE below $43,000.\n\nYou must keep random_state=1 in all components (e.g., XGBoost model, CV splits, search objects).\nSubmissions that meet the cutoff using any other random_state will receive zero credit.\n\nScoring (10 points total):\n\n5 points for a well-designed and appropriate hyperparameter search strategy\n5 using 3 different search strategies\n5 points for achieving MAE &lt; $42,500\n\nSearch Strategies (Required Comparison)\nYou must tune the model using three different search settings:\n\nBayesSearchCV\n\nUnlike vanilla GradientBoostingRegressor, XGBoost supports parallel training and can benefit from multi-core processing (n_jobs=-1), so BayesSearchCV is practica with it.\n\nOptuna (with n_jobs=-1)\nOptuna (default single-threaded)\n\nExecution Time\nYou must report the execution time for each tuning strategy.\n\nYou can measure this using:\n\nA Jupyter magic command like %%time, or\nPython‚Äôs time.time() (end - start)\n\n\nFor a fair comparison, use the same search space across all methods.\nOnly one of the tuned models needs to meet the performance cutoff, but you should still report times for all three.\n\n\nD.2.7 f) Feature Importance\nUsing the best hyperparameter settings, fit the final model and output the feature importances.\n\nUse the .feature_importances_ attribute or equivalent method from your model.\nVisualize the importances if possible (e.g., with a bar plot).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>¬† <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Assignment4_sp25.html#imbalanced-classification-with-regularized-gradient-boosting-42-points",
    "href": "Assignment4_sp25.html#imbalanced-classification-with-regularized-gradient-boosting-42-points",
    "title": "Appendix D ‚Äî Assignment 4",
    "section": "D.3 Imbalanced Classification with Regularized Gradient Boosting (42 points)",
    "text": "D.3 Imbalanced Classification with Regularized Gradient Boosting (42 points)\nIn this question, you will use the train.csv and test.csv datasets. Each observation represents a marketing call made by a banking institution. The target variable y indicates whether the client subscribed to a term deposit (1) or not (0), making this a binary classification task.\nThe predictors you should use are: age, day, month, and education.\n‚ö†Ô∏è Note: As discussed last quarter, the variable duration must not be used as a predictor.\nNo credit will be given for models that include it.\n\nD.3.1 a) Data Preprocessing\nPerform the following preprocessing steps:\n\nRead in the training and testing datasets.\nCreate a new season feature by mapping each month to its corresponding season.\nDefine the predictor and response variables.\nConvert all categorical predictors to pandas.Categorical dtype before passing them to the models.\nConvert the response variable y to binary values (0 and 1).\n\n(5 points)\nWe will rely on the native categorical feature support provided by each library (XGBoost, LightGBM, and CatBoost), so explicit one-hot encoding is not required.\n\n\nD.3.2 b) Target Exploration\nFor classification tasks, it‚Äôs important to examine the distribution of the target variable to determine whether the classes are imbalanced. This helps you avoid common pitfalls when dealing with imbalanced classification.\n\nExplore the class distribution in both the training and test sets.\n\n(2 points)\n\n\nD.3.3 c) LightGBM and CatBoost\nLightGBM and CatBoost are gradient boosting frameworks, like XGBoost, but each introduces unique innovations.\n\nWhat do LightGBM and CatBoost have in common with XGBoost? (2 points)\n\nWhat advantages do they offer over XGBoost? (2 points)\n\nHow are these advantages implemented in each model? (3 points)\n\nAll three libraries support native categorical feature handling.\nDo they use the same approach? If not, explain the differences. (3 points)\n\n\n\nD.3.4 c) Handling Imbalanced Classification in Gradient Boosting Extensions\nFor all extensions of Gradient Boosting (XGBoost, LightGBM, and CatBoost):\n\nAre there additional inputs or hyperparameters available to handle imbalanced classification? (1 point)\n\nIf yes, describe how the method works. (1 point)\n\nHow should the value of this hyperparameter be set or tuned for best results? (1 point)\n\n\n\nD.3.5 d) Model Evaluation: XGBoost, LightGBM, and CatBoost\nEvaluate the performance of the following models: XGBoost, LightGBM, and CatBoost, using the metrics on test set below:\n\nRecall\nPrecision\nF1 Score\nAUPRC (Area Under the Precision-Recall Curve)\nROC AUC\n\nFor each model, build and compare two versions:\n\nBaseline model: using default settings with random_state=1, without addressing class imbalance.\nImbalance-aware model: with scale_pos_weight enabled to handle class imbalance.\n\n\nCompare the performance of both versions for each model.\nSummarize which model and approach performed best for imbalanced classification, and try to explain why.\n\n\n\nD.3.6 d) Tuning LightGBM for Classification\nTune a LightGBM classifier to achieve:\n\nCross-validation accuracy ‚â• 70%\nCross-validation recall ‚â• 65%\n\nYou must set random_state=1 in all components (e.g., model, cross-validation, search objects).\nSubmissions that exceed the cutoffs using any other random_state will receive zero credit.\nScoring (15 points total):\n- 7.5 points for a well-designed and justified search strategy\n- 7.5 points for meeting both performance thresholds\nHints:\n\nFor classification, you may also tune the decision threshold (not just model hyperparameters).\n\n\n\nD.3.7 e) Test Set Evaluation\nEvaluate the tuned LightGBM model on the test set:\n\nReport the test accuracy and test recall.\nInclude the threshold used for classification.\n\nThis will help assess how well the model generalizes beyond the training data.\n(2 points)\n\n\nD.3.8 f) Tuning CatBoost for Classification\nTune a CatBoost classifier to achieve:\n\nCross-validation accuracy ‚â• 70%\nCross-validation recall ‚â• 65%\n\nYou must set random_state=1 in all components (e.g., model, cross-validation, search objects).\nSubmissions that exceed the cutoffs using any other random_state will receive zero credit.\nScoring (15 points total):\n- 7.5 points for a well-structured and appropriate hyperparameter search\n- 7.5 points for meeting both performance thresholds\nHints:\n\nYou are free to use any tuning strategy and define any reasonable search space.\nIn addition to tuning hyperparameters, you may also need to tune the decision threshold to meet the classification performance criteria.\n\n\n\nD.3.9 g) Test Set Evaluation\nEvaluate the tuned CatBoost model on the test set:\n\nReport the test accuracy and test recall.\nInclude the classification threshold used.\n\nThis will help assess whether the model generalizes well beyond the training data.\n(1 point)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>¬† <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Assignment4_sp25.html#bonus-extra-credit-20-points",
    "href": "Assignment4_sp25.html#bonus-extra-credit-20-points",
    "title": "Appendix D ‚Äî Assignment 4",
    "section": "D.4 üéÅ Bonus (Extra Credit) ‚Äì 20 Points",
    "text": "D.4 üéÅ Bonus (Extra Credit) ‚Äì 20 Points\nTo help you prepare for your upcoming prediction project involving hyperparameter tuning, I‚Äôve created the following optional tasks.\nFeel free to skip them if time does not permit.\n\nD.4.1 a) Comparing Tuning Strategies\nCompare the tuning time and results of GridSearchCV and RandomizedSearchCV using the same search space you used in Task 2e (BayesSearchCV and Optuna).\n\nWhat are the trade-offs between exhaustive search, random search, and smarter strategies like Bayesian optimization and Optuna?\nAre the differences in runtime justified by improvements in model performance?\n\n\n\nD.4.2 b) Resumable Tuning Strategies\nDo your own research: Among all the tuning strategies you have used, which ones allow you to continue tuning without starting from scratch when increasing n_trials or n_iter?\n\nIdentify the methods that support incremental or resumable search.\nExplain how they work and why they are efficient.\nProvide code to demonstrate how these strategies reuse previous results rather",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>¬† <span class='chapter-title'>Assignment 4</span>"
    ]
  }
]