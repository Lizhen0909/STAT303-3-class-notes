[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science III with python (Class notes)",
    "section": "",
    "text": "Preface\nThese are class notes for the course STAT303-3. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThe course notes are currently being written, and will continue to being developed as the course progresses (just like the class notes last quarter). Please report any typos / mistakes / inconsistencies / issues with the class notes / class presentations in your comments here. Thank you!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html",
    "href": "L1_Scikit-learn.html",
    "title": "1  Introduction to scikit-learn",
    "section": "",
    "text": "1.1 Splitting data into train and test\nLet us create train and test datasets for developing a model to predict if a person has diabetes.\n# Creating training and test data\n    # 80-20 split, which is usual - 70-30 split is also fine, 90-10 is fine if the dataset is large\n    # random_state to set a random seed for the splitting - reproducible results\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the complete dataset.\n#Proportion of 0s and 1s in the complete data\ny.value_counts()/y.shape\n\n0    0.651042\n1    0.348958\nName: Outcome, dtype: float64\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the train dataset.\n#Proportion of 0s and 1s in train data\ny_train.value_counts()/y_train.shape\n\n0    0.644951\n1    0.355049\nName: Outcome, dtype: float64\n#Proportion of 0s and 1s in test data\ny_test.value_counts()/y_test.shape\n\n0    0.675325\n1    0.324675\nName: Outcome, dtype: float64\nWe observe that the proportion of 0s and 1s in the train and test dataset are slightly different from that in the complete data. In order for these datasets to be more representative of the population, they should have a proportion of 0s and 1s similar to that in the complete dataset. This is especially critical in case of imbalanced datasets, where one class is represented by a significantly smaller number of instances than the other(s).\nWhen training a classification model on an imbalanced dataset, the model might not learn enough about the minority class, which can lead to poor generalization performance on new data. This happens because the model is biased towards the majority class, and it might even predict all instances as belonging to the majority class.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "href": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "title": "1  Introduction to scikit-learn",
    "section": "",
    "text": "1.1.1 Stratified splitting\nWe will use the argument stratify to obtain a proportion of 0s and 1s in the train and test datasets that is similar to the proportion in the complete `data.\n\n#Stratified train-test split\nX_train_stratified, X_test_stratified, y_train_stratified,\\\ny_test_stratified = train_test_split(X, y, test_size = 0.2, random_state = 45, stratify=y)\n\n\n#Proportion of 0s and 1s in train data with stratified split\ny_train_stratified.value_counts()/y_train.shape\n\n0    0.651466\n1    0.348534\nName: Outcome, dtype: float64\n\n\n\n#Proportion of 0s and 1s in test data with stratified split\ny_test_stratified.value_counts()/y_test.shape\n\n0    0.649351\n1    0.350649\nName: Outcome, dtype: float64\n\n\nThe proportion of the classes in the stratified split mimics the proportion in the complete dataset more closely.\nBy using stratified splitting, we ensure that both the train and test data sets have the same proportion of instances from each class, which means that the model will see enough instances from the minority class during training. This, in turn, helps the model learn to distinguish between the classes better, leading to better performance on new data.\nThus, stratified splitting helps to ensure that the model sees enough instances from each class during training, which can improve the model’s ability to generalize to new data, particularly in cases where one class is underrepresented in the dataset.\nLet us develop a logistic regression model for predicting if a person has diabetes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#scaling-data",
    "href": "L1_Scikit-learn.html#scaling-data",
    "title": "1  Introduction to scikit-learn",
    "section": "1.2 Scaling data",
    "text": "1.2 Scaling data\nIn certain models, it may be important to scale data for various reasons. In a logistic regression model, scaling can help with model convergence. Scikit-learn uses a method known as gradient-descent (not in scope of the syllabus of this course) to obtain a solution. In case the predictors have different orders of magnitude, the algorithm may fail to converge. In such cases, it is useful to standardize the predictors so that all of them are at the same scale.\n\n# With linear/logistic regression in scikit-learn, especially when the predictors have different orders \n# of magn., scaling is necessary. This is to enable the training algo. which we did not cover. (Gradient Descent)\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test) # Do NOT refit the scaler with the test data, just transform it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#fitting-a-model",
    "href": "L1_Scikit-learn.html#fitting-a-model",
    "title": "1  Introduction to scikit-learn",
    "section": "1.3 Fitting a model",
    "text": "1.3 Fitting a model\nLet us fit a logistic regression model for predicting if a person has diabetes. Let us try fitting a model with the un-scaled data.\n\n# Create a model object - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train, y_train)\n\nC:\\Users\\akl0407\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\nNote that the model with the un-scaled predictors fails to converge. Check out the data X_train to see that this may be probably due to the predictors have different orders of magnitude. For example, the predictor DiabetesPedigreeFunction has values in [0.078, 2.42], while the predictor Insulin has values in [0, 800].\nLet us fit the model to the scaled data.\n\n# Create a model - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train_scaled, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\nThe model converges to a solution with the scaled data!\nThe coefficients of the model can be returned with the coef_ attribute of the LogisticRegression() object. However, the output is not as well formatted as in the case of the statsmodels library since sklearn is developed primarily for the purpose of prediction, and not inference.\n\n# Use coef_ to return the coefficients - only log reg inference you can do with sklearn\nprint(logreg.coef_) \n\n[[ 0.32572891  1.20110566 -0.32046591  0.06849882 -0.21727131  0.72619528\n   0.40088897  0.29698818]]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#computing-performance-metrics",
    "href": "L1_Scikit-learn.html#computing-performance-metrics",
    "title": "1  Introduction to scikit-learn",
    "section": "1.4 Computing performance metrics",
    "text": "1.4 Computing performance metrics\n\n1.4.1 Accuracy\nLet us test the model prediction accuracy on the test data. We’ll demonstrate two different functions that can be used to compute model accuracy - accuracy_score(), and score().\nThe accuracy_score() function from the metrics module of the sklearn library is general, and can be used for any classification model. We’ll use it along with the predict() method of the LogisticRegression() object, which returns the predicted class based on a threshold probability of 0.5.\n\n# Get the predicted classes first\ny_pred = logreg.predict(X_test_scaled)\n\n# Use the predicted and true classes for accuracy\nprint(accuracy_score(y_pred, y_test)*100) \n\n73.37662337662337\n\n\nThe score() method of the LogisticRegression() object can be used to compute the accuracy only for a logistic regression model. Note that for a LinearRegression() object, the score() method will return the model \\(R\\)-squared.\n\n# Use .score with test predictors and response to get the accuracy\n# Implements the same thing under the hood\nprint(logreg.score(X_test_scaled, y_test)*100)  \n\n73.37662337662337\n\n\n\n\n1.4.2 ROC-AUC\nThe roc_curve() and auc() functions from the metrics module of the sklearn library can be used to compute the ROC-AUC, or the area under the ROC curve. Note that for computing ROC-AUC, we need the predicted probability, instead of the predicted class. Thus, we’ll use the predict_proba() method of the LogisticRegression() object, which returns the predicted probability for the observation to belong to each of the classes, instead of using the predict() method, which returns the predicted class based on threshold probability of 0.5.\n\n#Computing the predicted probability for the observation to belong to the positive class (y=1);\n#The 2nd column in the output of predict_proba() consists of the probability of the observation to \n#belong to the positive class (y=1)\ny_pred_prob = logreg.predict_proba(X_test_scaled)[:,1] \n\n#Using the predicted probability computed above to find ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test, y_pred_prob)\nprint(auc(fpr, tpr))# AUC of ROC\n\n0.7923076923076922\n\n\n\n\n1.4.3 Confusion matrix & precision-recall\nThe confusion_matrix(), precision_score(), and recall_score() functions from the metrics module of the sklearn library can be used to compute the confusion matrix, precision, and recall respectively.\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\n\n\n\n\n\n\n\n\nprint(\"Precision: \", precision_score(y_test, y_pred))\nprint(\"Recall: \", recall_score(y_test, y_pred))\n\nPrecision:  0.6046511627906976\nRecall:  0.52\n\n\nLet us compute the performance metrics if we develop the model using stratified splitting.\n\n# Developing the model with stratified splitting\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n#Computing the accuracy\ny_pred_stratified = logreg.predict(X_test_stratified_scaled)\nprint(\"Accuracy: \",accuracy_score(y_pred_stratified, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_stratified))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_stratified))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_stratified), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  78.57142857142857\nROC-AUC:  0.8505555555555556\nPrecision:  0.7692307692307693\nRecall:  0.5555555555555556\n\n\n\n\n\n\n\n\n\nThe model with the stratified train-test split has a better performance as compared to the other model on all the performance metrics!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#tuning-the-model-hyperparameters",
    "href": "L1_Scikit-learn.html#tuning-the-model-hyperparameters",
    "title": "1  Introduction to scikit-learn",
    "section": "1.5 Tuning the model hyperparameters",
    "text": "1.5 Tuning the model hyperparameters\nA hyperparameter (among others) that can be trained in a logistic regression model is the regularization parameter.\nWe may also wish to tune the decision threshold probability. Note that the decision threshold probability is not considered a hyperparameter of the model. Hyperparameters are model parameters that are set prior to training and cannot be directly adjusted by the model during training. Examples of hyperparameters in a logistic regression model include the regularization parameter, and the type of shrinkage penalty - lasso / ridge. These hyperparameters are typically optimized through a separate tuning process, such as cross-validation or grid search, before training the final model.\nThe performance metrics can be computed using a desired value of the threshold probability. Let us compute the performance metrics for a desired threshold probability of 0.3.\n\n# Performance metrics computation for a desired threshold probability of 0.3\ndesired_threshold = 0.3\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob &gt; desired_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  75.32467532467533\nROC-AUC:  0.8505555555555556\nPrecision:  0.6111111111111112\nRecall:  0.8148148148148148\n\n\n\n\n\n\n\n\n\n\n1.5.1 Tuning decision threshold probability\nSuppose we wish to find the optimal decision threshold probability to maximize accuracy. Note that we cannot use the test dataset to optimize model hyperparameters, as that may lead to overfitting on the test data. We’ll use \\(K\\)-fold cross validation on train data to find the optimal decision threshold probability.\nWe’ll use the cross_val_predict() function from the model_selection module of sklearn to compute the \\(K\\)-fold cross validated predicted probabilities. Note that this function simplifies the task of manually creating the \\(K\\)-folds, training the model \\(K\\)-times, and computing the predicted probabilities on each of the \\(K\\)-folds. Thereafter, the predicted probabilities will be used to find the optimal threshold probability that maximizes the classification accuracy.\n\nhyperparam_vals = np.arange(0,1.01,0.01)\naccuracy_iter = []\n\npredicted_probability = cross_val_predict(LogisticRegression(), X_train_stratified_scaled, \n                                              y_train_stratified, cv = 5, method = 'predict_proba')\n\nfor threshold_prob in hyperparam_vals:\n    predicted_class = predicted_probability[:,1] &gt; threshold_prob\n    predicted_class = predicted_class.astype(int)\n\n    #Computing the accuracy\n    accuracy = accuracy_score(predicted_class, y_train_stratified)*100\n    accuracy_iter.append(accuracy)\n\nLet us visualize the accuracy with change in decision threshold probability.\n\n# Accuracy vs decision threshold probability\nsns.scatterplot(x = hyperparam_vals, y = accuracy_iter)\nplt.xlabel('Decision threshold probability')\nplt.ylabel('Average 5-fold CV accuracy');\n\n\n\n\n\n\n\n\nThe optimal decision threshold probability is the one that maximizes the \\(K\\)-fold cross validation accuracy.\n\n# Optimal decision threshold probability\nhyperparam_vals[accuracy_iter.index(max(accuracy_iter))]\n\n0.46\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.46\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob &gt; desired_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8505555555555556\nPrecision:  0.7804878048780488\nRecall:  0.5925925925925926\n\n\n\n\n\n\n\n\n\nModel performance on test data has improved with the optimal decision threshold probability.\n\n\n1.5.2 Tuning the regularization parameter\nThe LogisticRegression() method has a default L2 regularization penalty, which means ridge regression.C is \\(1/\\lambda\\), where \\(\\lambda\\) is the hyperparameter that is multiplied with the ridge penalty. C is 1 by default.\n\naccuracy_iter = []\nhyperparam_vals = 10**np.linspace(-3.5, 1)\n\nfor c_val in hyperparam_vals: # For each possible C value in your grid\n    logreg_model = LogisticRegression(C=c_val) # Create a model with the C value\n    \n    accuracy_iter.append(cross_val_score(logreg_model, X_train_stratified_scaled, y_train_stratified,\n                                      scoring='accuracy', cv=5)) # Find the cv results\n\n\nplt.plot(hyperparam_vals, np.mean(np.array(accuracy_iter), axis=1))\nplt.xlabel('C')\nplt.ylabel('Average 5-fold CV accuracy')\nplt.xscale('log')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Optimal value of the regularization parameter 'C'\noptimal_C = hyperparam_vals[np.argmax(np.array(accuracy_iter).mean(axis=1))]\noptimal_C\n\n0.11787686347935879\n\n\n\n# Developing the model with stratified splitting and optimal 'C'\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg = LogisticRegression(C = optimal_C)\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n#Computing the accuracy\ny_pred_stratified = logreg.predict(X_test_stratified_scaled)\nprint(\"Accuracy: \",accuracy_score(y_pred_stratified, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_stratified))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_stratified))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_stratified), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  78.57142857142857\nROC-AUC:  0.8516666666666666\nPrecision:  0.7837837837837838\nRecall:  0.5370370370370371\n\n\n\n\n\n\n\n\n\n\n\n1.5.3 Tuning the decision threshold probability and the regularization parameter simultaneously\n\nthreshold_hyperparam_vals = np.arange(0,1.01,0.01)\nC_hyperparam_vals = 10**np.linspace(-3.5, 1)\naccuracy_iter = pd.DataFrame({'threshold':[], 'C':[], 'accuracy':[]})\niter_number = 0\n\nfor c_val in C_hyperparam_vals:\n    predicted_probability = cross_val_predict(LogisticRegression(C = c_val), X_train_stratified_scaled, \n                                                  y_train_stratified, cv = 5, method = 'predict_proba')\n\n    for threshold_prob in threshold_hyperparam_vals:\n        predicted_class = predicted_probability[:,1] &gt; threshold_prob\n        predicted_class = predicted_class.astype(int)\n\n        #Computing the accuracy\n        accuracy = accuracy_score(predicted_class, y_train_stratified)*100\n        accuracy_iter.loc[iter_number, 'threshold'] = threshold_prob\n        accuracy_iter.loc[iter_number, 'C'] = c_val\n        accuracy_iter.loc[iter_number, 'accuracy'] = accuracy\n        iter_number = iter_number + 1\n\n\n# Parameters for highest accuracy\noptimal_C = accuracy_iter.sort_values(by = 'accuracy', ascending = False).iloc[0,:]['C']\noptimal_threshold = accuracy_iter.sort_values(by = 'accuracy', ascending = False).iloc[0, :]['threshold']\n\n#Optimal decision threshold probability\nprint(\"Optimal decision threshold = \", optimal_threshold)\n\n#Optimal C\nprint(\"Optimal C = \", optimal_C)\n\nOptimal decision threshold =  0.46\nOptimal C =  4.291934260128778\n\n\n\n# Developing the model with stratified splitting, optimal decision threshold probability, and optimal 'C'\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg = LogisticRegression(C = optimal_C)\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n# Performance metrics computation for the optimal threshold probability\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob &gt; optimal_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8509259259259259\nPrecision:  0.7804878048780488\nRecall:  0.5925925925925926\n\n\n\n\n\n\n\n\n\nLater in the course, we’ll see the sklearn function GridSearchCV, which is used to optimize several model hyperparameters simultaneously with \\(K\\)-fold cross validation, while avoiding for loops.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "Lec3_RegressionTrees.html",
    "href": "Lec3_RegressionTrees.html",
    "title": "2  Regression trees",
    "section": "",
    "text": "2.1 Building a regression tree\nDevelop a regression tree to predict car price based on mileage\nX = train['mileage']\ny = train['price']\n#Defining the object to build a regression tree\nmodel = DecisionTreeRegressor(random_state=1, max_depth=3) \n\n#Fitting the regression tree to the data\nmodel.fit(X.values.reshape(-1,1), y)\n\nDecisionTreeRegressor(max_depth=3, random_state=1)\n#Visualizing the regression tree\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('car_price_tree.png')\nImage(graph.create_png())\n#prediction on test data\npred=model.predict(test[['mileage']])\n#RMSE on test data\nnp.sqrt(mean_squared_error(test.price, pred))\n\n13764.798425410803\n#Visualizing the model fit\nXtest = np.linspace(min(X), max(X), 100)\npred_test = model.predict(Xtest.reshape(-1,1))\nsns.scatterplot(x = 'mileage', y = 'price', data = train, color = 'orange')\nsns.lineplot(x = Xtest, y = pred_test, color = 'blue')\nAll cars falling within the same terminal node have the same predicted price, which is seen as flat line segments in the above model curve.\nDevelop a regression tree to predict car price based on mileage, mpg, engineSize and year\nX = train[['mileage','mpg','year','engineSize']]\nmodel = DecisionTreeRegressor(random_state=1, max_depth=3) \nmodel.fit(X, y)\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage','mpg','year','engineSize'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('car_price_tree.png')\nImage(graph.create_png())",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "Lec3_RegressionTrees.html#optimizing-parameters-to-improve-the-regression-tree",
    "href": "Lec3_RegressionTrees.html#optimizing-parameters-to-improve-the-regression-tree",
    "title": "2  Regression trees",
    "section": "2.2 Optimizing parameters to improve the regression tree",
    "text": "2.2 Optimizing parameters to improve the regression tree\nLet us find the optimal depth of the tree and the number of terminal nodes (leaves) by cross validation.\n\n2.2.1 Range of hyperparameter values\nFirst, we’ll find the minimum and maximum possible values of the depth and leaves, and then find the optimal value in that range.\n\nmodel = DecisionTreeRegressor(random_state=1) \nmodel.fit(X, y)\n\nprint(\"Maximum tree depth =\", model.get_depth())\n\nprint(\"Maximum leaves =\", model.get_n_leaves())\n\nMaximum tree depth = 29\nMaximum leaves = 4845\n\n\n\n\n2.2.2 Cross validation: Coarse grid\nWe’ll use the sklearn function GridSearchCV to find the optimal hyperparameter values over a grid of possible values. By default, GridSearchCV returns the optimal hyperparameter values based on the coefficient of determination \\(R^2\\). However, the scoring argument of the function can be used to find the optimal parameters based on several different criteria as mentioned in the scoring-parameter documentation.\n\n#Finding cross-validation error for trees \nparameters = {'max_depth':range(2,30, 3),'max_leaf_nodes':range(2,4900, 100)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv)\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \n\nFitting 5 folds for each of 490 candidates, totalling 2450 fits\n0.8433100904754441 {'max_depth': 11, 'max_leaf_nodes': 302}\n\n\nLet us find the optimal hyperparameters based on the mean squared error, instead of \\(R^2\\). Let us compute \\(R^2\\) as well during cross validation, as we can compute multiple performance metrics using the scoring argument. However, when computing multiple performance metrics, we will need to specify the performance metric used to find the optimal hyperparameters with the refit argument.\n\n#Finding cross-validation error for trees \nparameters = {'max_depth':range(2,30, 3),'max_leaf_nodes':range(2,4900, 100)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv,\n                    scoring=['neg_mean_squared_error', 'r2'], refit = 'neg_mean_squared_error')\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \n\nFitting 5 folds for each of 490 candidates, totalling 2450 fits\n-42064467.15261547 {'max_depth': 11, 'max_leaf_nodes': 302}\n\n\nNote that as the GridSearchCV function maximizes the performance metric to find the optimal hyperparameters, we are maximizing the negative mean squared error (neg_mean_squared_error), and the function returns the optimal negative mean squared error.\nLet us visualize the mean squared error based on the hyperparameter values. We’ll use the cross validation results stored in the cv_results_ attribute of the GridSearchCV fit() object.\n\n#Detailed results of k-fold cross validation\ncv_results = pd.DataFrame(model.cv_results_)\ncv_results.head()\n\n\nfig, axes = plt.subplots(1,2,figsize=(14,5))\nplt.subplots_adjust(wspace=0.2)\naxes[0].plot(cv_results.param_max_depth, np.sqrt(-cv_results.mean_test_neg_mean_squared_error), 'o')\naxes[0].set_ylim([6200, 7500])\naxes[0].set_xlabel('Depth')\naxes[0].set_ylabel('K-fold RMSE')\naxes[1].plot(cv_results.param_max_leaf_nodes, np.sqrt(-cv_results.mean_test_neg_mean_squared_error), 'o')\naxes[1].set_ylim([6200, 7500])\naxes[1].set_xlabel('Leaves')\naxes[1].set_ylabel('K-fold RMSE');\n\n\n\n\n\n\n\n\nWe observe that for a depth of around 8-14, and number of leaves within 1000, we get the lowest \\(K\\)-fold RMSE. So, we should do a finer search in that region to obtain more precise hyperparameter values.\n\n\n2.2.3 Cross validation: Finer grid\n\n#Finding cross-validation error for trees\nstart_time = tm.time()\nparameters = {'max_depth':range(8,15),'max_leaf_nodes':range(2,1000)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv)\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \nprint(\"Time taken =\", round((tm.time() - start_time)/60), \"minutes\")\n\nFitting 5 folds for each of 6986 candidates, totalling 34930 fits\n0.8465176078797111 {'max_depth': 10, 'max_leaf_nodes': 262}\nTime taken = 1 minutes\n\n\nFrom the above cross-validation, the optimal hyperparameter values are max_depth = 10 and max_leaf_nodes = 262.\n\n#Developing the tree based on optimal hyperparameters found by cross-validation\nmodel = DecisionTreeRegressor(random_state=1, max_depth=10,max_leaf_nodes=262) \nmodel.fit(X, y)\n\nDecisionTreeRegressor(max_depth=10, max_leaf_nodes=262, random_state=1)\n\n\n\n#RMSE on test data\nXtest = test[['mileage','mpg','year','engineSize']]\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n6921.0404660552895\n\n\nThe RMSE for the decision tree is lower than that of linear regression models and spline regression models (including MARS), with these four predictors. This may be probably due to car price having a highly non-linear association with the predictors.\nPredictor importance: The importance of a predictor is computed as the (normalized) total reduction of the criterion (SSE in case of regression trees) brought by that predictor.\nWarning: impurity-based feature importances can be misleading for high cardinality features (many unique values) Source: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor.feature_importances_\nWhy?\nBecause high cardinality predictors will tend to overfit. When the predictors have high cardinality, it means they form little groups (in the leaf nodes) and then the model “learns” the individuals, instead of “learning” the general trend. The higher the cardinality of the predictor, the more prone is the model to overfitting.\n\nmodel.feature_importances_\n\narray([0.04490344, 0.15882336, 0.29739951, 0.49887369])\n\n\nEngine size is the most important predictor, followed by year, which is followed by mpg, and mileage is the least important predictor.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "Lec3_RegressionTrees.html#cost-complexity-pruning",
    "href": "Lec3_RegressionTrees.html#cost-complexity-pruning",
    "title": "2  Regression trees",
    "section": "2.3 Cost complexity pruning",
    "text": "2.3 Cost complexity pruning\nWhile optimizing parameters above, we optimized them within a range that we thought was reasonable. While doing so, we restricted ourselves to considering only a subset of the unpruned tree. Thus, we could have missed out on finding the optimal tree (or the best model).\nWith cost complexity pruning, we first develop an unpruned tree without any restrictions. Then, using cross validation, we find the optimal value of the tuning parameter \\(\\alpha\\). All the non-terminal nodes for which \\(\\alpha_{eff}\\) is smaller that the optimal \\(\\alpha\\) will be pruned. You will need to check out the link below to understand this better.\nCheck out a detailed explanation of how cost complexity pruning is implemented in sklearn at: https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning\nHere are some informative visualizations that will help you understand what is happening in cost complexity pruning: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n\nmodel = DecisionTreeRegressor(random_state = 1)#model without any restrictions\npath= model.cost_complexity_pruning_path(X,y)# Compute the pruning path during Minimal Cost-Complexity Pruning.\n\n\nalphas=path['ccp_alphas']\n\n\nlen(alphas)\n\n4126\n\n\n\nstart_time = tm.time()\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\ntree = GridSearchCV(DecisionTreeRegressor(random_state=1), param_grid = {'ccp_alpha':alphas}, \n                     scoring = 'neg_mean_squared_error',n_jobs=-1,verbose=1,cv=cv)\ntree.fit(X, y)\nprint (tree.best_score_, tree.best_params_)\nprint(\"Time taken =\",round((tm.time()-start_time)/60), \"minutes\")\n\nFitting 5 folds for each of 4126 candidates, totalling 20630 fits\n-44150619.209031895 {'ccp_alpha': 143722.94076639024}\nTime taken = 2 minutes\n\n\nThe code took 2 minutes to run on a dataset of about 5000 observations and 4 predictors.\n\nmodel = DecisionTreeRegressor(ccp_alpha=143722.94076639024,random_state=1)\nmodel.fit(X, y)\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(test.price, pred))\n\n7306.592294294368\n\n\nThe RMSE for the decision tree with cost complexity pruning is lower than that of linear regression models and spline regression models (including MARS), with these four predictors. However, it is higher than the one obtained with tuning tree parameters using grid search (shown previously). Cost complexity pruning considers a completely unpruned tree unlike the ‘grid search’ method of searching over a grid of hyperparameters such as max_depth and max_leaf_nodes, and thus may seem to be more comprehensive than the ‘grid search’ approach. However, both the approaches may consider trees that are not considered by the other approach, and thus either one may provide a more accurate model. Depending on the grid of parameters chosen for cross validation, the grid search method may be more or less comprehensive than cost complexity pruning.\n\ngridcv_results = pd.DataFrame(tree.cv_results_)\ncv_error = -gridcv_results['mean_test_score']\n\n\n#Visualizing the 5-fold cross validation error vs alpha\nplt.plot(alphas,cv_error)\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('K-fold MSE');\n\n\n\n\n\n\n\n\n\n#Zooming in the above visualization to see the alpha where the 5-fold cross validation error is minimizing\nplt.plot(alphas[0:4093],cv_error[0:4093])\nplt.xlabel('alpha')\nplt.ylabel('K-fold MSE');\n\n\n\n\n\n\n\n\n\n2.3.1 Depth vs alpha; Node counts vs alpha\n\nstime = time.time()\ntrees=[]\nfor i in alphas:\n    tree = DecisionTreeRegressor(ccp_alpha=i,random_state=1)\n    tree.fit(X, train['price'])\n    trees.append(tree)\nprint(time.time()-stime)\n\n268.10325384140015\n\n\nThis code takes 4.5 minutes to run\n\nnode_counts = [clf.tree_.node_count for clf in trees]\ndepth = [clf.tree_.max_depth for clf in trees]\n\n\nfig, ax = plt.subplots(1, 2,figsize=(10,6))\nax[0].plot(alphas[0:4093], node_counts[0:4093], marker=\"o\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(alphas[0:4093], depth[0:4093], marker=\"o\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\n#fig.tight_layout()\n\nText(0.5, 1.0, 'Depth vs alpha')\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Train and test accuracies (R-squared) vs alpha\n\ntrain_scores = [clf.score(X, y) for clf in trees]\ntest_scores = [clf.score(Xtest, test.price) for clf in trees]\n\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(alphas[0:4093], train_scores[0:4093], marker=\"o\", label=\"train\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax.plot(alphas[0:4093], test_scores[0:4093], marker=\"o\", label=\"test\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix A — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Datasets, assignment and project files</span>"
    ]
  }
]