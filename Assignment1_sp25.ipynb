{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7fc45304",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Assignment 1\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    self-contained: true\n",
    "    number-sections: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672791b9",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity. \n",
    "\n",
    "2. Write your code in the **Code cells** and your answers in the **Markdown cells** of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\n",
    "\n",
    "3. Use [Quarto](https://quarto.org/docs/output-formats/html-basics.html) to render the **.ipynb** file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: `quarto render filename.ipynb --to html`. Submit the HTML file.\n",
    "\n",
    "4. The assignment is worth 100 points, and is due on **Thursday, 11th April 2025 at 11:59 pm**. \n",
    "\n",
    "5. **Five points are properly formatting the assignment**. The breakdown is as follows:\n",
    "    - Must be an HTML file rendered using Quarto **(2 points)**. *If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.* \n",
    "    - There aren‚Äôt excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren‚Äôt long printouts of which iteration a loop is on, there aren‚Äôt long sections of commented-out code, etc.) **(1 point)**\n",
    "    - Final answers to each question are written in the Markdown cells. **(1 point)**\n",
    "    - There is no piece of unnecessary / redundant code, and no unnecessary / redundant text. **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae5120",
   "metadata": {},
   "source": [
    "## 1) Bias-Variance Trade-off for Regression **(50 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988fbe55",
   "metadata": {},
   "source": [
    "The main goal of this question is to understand and visualize the bias-variance trade-off in a regression model by performing repetitive simulations.\n",
    "\n",
    "The conceptual clarity about bias and variance will help with the main logic behind creating many models that will come up later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dce7cd",
   "metadata": {},
   "source": [
    "### a) Define the True Relationship (Signal)\n",
    "\n",
    "First, you need to implement the underlying true relationship (Signal) you want to sample data from. Assume that the function is the [Bukin function](https://www.sfu.ca/~ssurjano/bukin6.html). Implement it as a user-defined function and run it with the test cases below to make sure it is implemented correctly. **(5 points)**\n",
    "\n",
    "**Note:** It would be more useful to have only one input to the function. You can treat the input as an array of two elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Bukin(np.array([1,2]))) # The output should be 141.177\n",
    "print(Bukin(np.array([6,-4]))) # The output should be 208.966\n",
    "print(Bukin(np.array([0,1]))) # The output should be 100.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8788939",
   "metadata": {},
   "source": [
    "### b) Generate Test Set (No Noise)\n",
    "\n",
    "Generate a **noiseless** test set with **100 observations** sampled from the true underlying function. This test set will be used to evaluate **bias and variance**, so make sure it follows the correct data generation process. \n",
    "\n",
    "**(5 points)**\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- **Do not use loops** for this question.\n",
    "- `.apply` will be especially helpful (and often simpler).\n",
    "\n",
    "**Data generation assumptions:**\n",
    "\n",
    "- Use `np.random.seed(100)` for reproducibility.\n",
    "- The first predictor, $x_1$, should be drawn from a **uniform distribution** over the interval $[-15, -5]$, i.e., $x_1 \\sim U[-15, -5]$.\n",
    "- The second predictor, $x_2$, should be drawn from a **uniform distribution** over the interval $[-3, 3]$, i.e., $x_2 \\sim U[-3, 3]$.\n",
    "- Compute the true function values using the underlying model as your response $y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561b204",
   "metadata": {},
   "source": [
    "### c) Initialize Results DataFrame\n",
    "\n",
    "Create an empty DataFrame with the following columns:\n",
    "\n",
    "- **degree**: the degree of the polynomial model  \n",
    "- **bias_sq**: estimated squared bias (averaged over test points)  \n",
    "- **var**: estimated variance of predictions  \n",
    "- **bias_var_sum**: sum of bias squared and variance  \n",
    "- **empirical_mse**: mean squared error calculated using sklearn‚Äôs `mean_squared_error()` on model predictions vs. true function values\n",
    "\n",
    "This DataFrame will be used to store the results of your bias‚Äìvariance tradeoff analysis and for generating comparison plots.\n",
    "\n",
    "**(3 points)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a532d",
   "metadata": {},
   "source": [
    "### d) Generate Training Sets (With Noise)\n",
    "\n",
    "To estimate the **bias**, **variance**, and **total error (MSE)** of a Linear Regression model trained on noisy data from the underlying Bukin function, follow the steps below.\n",
    "\n",
    "\n",
    "**üîÅ Step 1: Generate 100 Training Sets**\n",
    "\n",
    "- Create **100 independent training datasets**, each with **100 observations** (same size as the test set).\n",
    "- For each training dataset:\n",
    "  - Use `np.random.seed(i)` to ensure reproducibility, where `i` is the dataset index (0 to 99).\n",
    "  - Sample predictors from the **same distributions** used to generate the test set.\n",
    "  - Add **Gaussian noise** with mean 0 and standard deviation 10:  \n",
    "    $\\varepsilon \\sim \\mathcal{N}(0, 10)$\n",
    "\n",
    "\n",
    "**üß† Step 2: Train Polynomial Models (Degrees 1 to 7)**\n",
    "\n",
    "- For each training dataset, train polynomial models with degrees **1 through 7**.\n",
    "- Use polynomial feature transformations that include both:\n",
    "  - **Higher-order terms** (e.g., $x_1^2$, $x_2^3$)\n",
    "  - **Interaction terms** (e.g., $x_1 \\cdot x_2$)\n",
    "- Make predictions on the **fixed, noiseless test set** for each trained model.\n",
    "\n",
    "\n",
    "**üìä Step 3: Estimate Bias¬≤, Variance, and MSE**\n",
    "\n",
    "- For each **degree**, and each **test point**, collect the 100 predicted values from the models trained on the different training sets.\n",
    "- Using these predictions, compute:\n",
    "  - **Bias squared**: squared difference between the mean prediction and the true value.\n",
    "  - **Variance**: variance of the predictions.\n",
    "  - **Theoretical MSE**: sum of bias squared and variance.\n",
    "  - **Empirical MSE**: compute using `sklearn.metrics.mean_squared_error` between each model‚Äôs prediction and the true values, then average over the 100 training runs.\n",
    "\n",
    "- Store all four quantities for each degree in your results DataFrame:\n",
    "  - `degree`\n",
    "  - `bias_sq`\n",
    "  - `var`\n",
    "  - `bias_var_sum` (bias squared + variance)\n",
    "  - `empirical_mse`\n",
    "\n",
    "\n",
    "**(25 points)**\n",
    "\n",
    "\n",
    "üí° **Reminder: Comparing Theoretical vs. Empirical MSE**\n",
    "\n",
    "When evaluating model performance on the **noiseless test set**:\n",
    "\n",
    "- The **irreducible error** (i.e., noise in training data) does **not** affect the test targets.\n",
    "- Therefore, the test error (MSE) can be decomposed as:\n",
    "\n",
    "  $MSE$ = ${Bias}^2$ + ${Variance}$\n",
    "\n",
    "- The **empirical MSE** (from sklearn) should closely match the **sum of bias¬≤ and variance**, since the test data contains no noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73415572",
   "metadata": {},
   "source": [
    "### e) Visualize Bias‚ÄìVariance Decomposition\n",
    "\n",
    "Using the results stored in your DataFrame, create a plot with **four lines**, each plotted against the polynomial **degree**:\n",
    "\n",
    "1. **Bias squared**\n",
    "2. **Variance**\n",
    "3. **Bias squared + Variance** (i.e., the theoretical decomposition of MSE)\n",
    "4. **Empirical MSE** calculated using `sklearn.metrics.mean_squared_error()`  \n",
    "   (computed from the predicted values vs. true function values on the noiseless test set)\n",
    "\n",
    "\n",
    "**Plot requirements:**\n",
    "- Use a single line plot with the polynomial degree on the x-axis and error values on the y-axis.\n",
    "- Include a **legend** to clearly label each line.\n",
    "- Use different line styles or markers for easy visual comparison.\n",
    "\n",
    "\n",
    "**Goal:**\n",
    "- Compare the **empirical MSE** to the **sum of bias squared and variance**.\n",
    "- If everything is implemented correctly, the two lines should be very close (or even identical, up to numerical precision).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4133fe",
   "metadata": {},
   "source": [
    "### f) Identify the Optimal Model\n",
    "\n",
    "- What is the **optimal polynomial degree** based on the **lowest empirical MSE** (calculated using sklearn)?  \n",
    "  **(2 points)**\n",
    "\n",
    "- Report the corresponding values of:  \n",
    "  - **Bias squared**  \n",
    "  - **Variance**  \n",
    "  - **Bias squared + Variance**  \n",
    "  - **Empirical MSE**  \n",
    "  for that degree.  \n",
    "  **(3 points)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901566b",
   "metadata": {},
   "source": [
    "## 2) Building a Low-Bias, Low-Variance Model via Regularization (50 points)\n",
    "\n",
    "The main goal of this question is to further reduce the **total prediction error** by applying **regularization**.  \n",
    "Specifically, you‚Äôll use **Ridge regression** to build a **low-bias, low-variance** model for data generated from the underlying Bukin function with noise.\n",
    "\n",
    "\n",
    "\n",
    "### a) Why Regularization?\n",
    "\n",
    "Explain why the model with the optimal polynomial degree (as identified in Question 1) is **not guaranteed** to be the true low-bias, low-variance model.\n",
    "\n",
    "Why might **regularization** still be necessary to improve generalization performance, even after selecting the degree that minimizes MSE?\n",
    "\n",
    "**(5 points)**\n",
    "\n",
    "\n",
    "\n",
    "### b) Which Degrees to Exclude?\n",
    "\n",
    "Based on your plot and results from **1e** and **1f**, identify which polynomial degrees should be **excluded** from regularization experiments because they are already too simple (high bias) or too complex (high variance).\n",
    "\n",
    "Explain which degrees you will exclude and **why**, using your understanding of how **regularization affects bias and variance**.\n",
    "\n",
    "**(10 points)**\n",
    "\n",
    "\n",
    "\n",
    "### c) Apply Ridge Regularization\n",
    "\n",
    "Repeat the steps from **1c** and **1d**, but this time use **Ridge regression** instead of ordinary least squares.\n",
    "\n",
    "- Use only the degrees **not excluded** in 2b (and also exclude degree 7 to avoid extreme overfitting).\n",
    "- Use **5-fold cross-validation** to tune the Ridge regularization strength.\n",
    "- Use `neg_root_mean_squared_error` as the scoring metric for cross-validation.\n",
    "- Tune over a range of regularization strengths (e.g., from 1 to 100).\n",
    "- For each retained degree, compute:\n",
    "  - **Bias squared**\n",
    "  - **Variance**\n",
    "  - **Bias squared + Variance**\n",
    "  - **Empirical MSE** (from `sklearn.metrics.mean_squared_error`)\n",
    "\n",
    "Store your results in a new DataFrame with the same structure as in Question 1.\n",
    "\n",
    "**(10 points)**\n",
    "\n",
    "\n",
    "\n",
    "### d) Visualize Regularized Results\n",
    "\n",
    "Repeat the visualization from **1e**, but using the results from **2c** (Ridge regression).\n",
    "\n",
    "Your plot should include **four lines** plotted against polynomial degree:\n",
    "\n",
    "1. **Bias squared**\n",
    "2. **Variance**\n",
    "3. **Bias squared + Variance**\n",
    "4. **Empirical MSE** (computed using sklearn)\n",
    "\n",
    "Include a clear **legend** and label your axes.  \n",
    "This will help you visually assess how regularization impacts bias, variance, and overall model error.\n",
    "\n",
    "**(10 points)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb923103",
   "metadata": {},
   "source": [
    "### e) Evaluate the Regularized Model\n",
    "\n",
    "- What is the **optimal polynomial degree** for the Ridge Regression model, based on the **lowest empirical MSE**?  \n",
    "  **(3 points)**\n",
    "\n",
    "- Report the corresponding values of:  \n",
    "  - **Bias squared**  \n",
    "  - **Variance**  \n",
    "  - **Empirical MSE**  \n",
    "  for that optimal Ridge model.  \n",
    "  **(3 points)**\n",
    "\n",
    "- Compare these results to those of the optimal **Linear Regression** model from Question 1.  \n",
    "  Discuss how **regularization** influenced the **bias**, **variance**, and **overall prediction error (MSE)**.  \n",
    "  **(4 points)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139496ef",
   "metadata": {},
   "source": [
    "### f) Interpreting the Impact of Regularization\n",
    "\n",
    "- Was **regularization successful** in reducing the **total prediction error (MSE)** compared to the unregularized model?  \n",
    "  **(2 points)**\n",
    "\n",
    "- Based on your results from **2e**, explain how **bias** and **variance** changed as a result of regularization.  \n",
    "  How did these changes affect the final total error?  \n",
    "  Support your explanation with values or observations from your analysis.  \n",
    "  **(3 points)**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
