{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6c947150",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Best Kaggle submission (Spring 2023)\"\n",
    "author: Jack O' Keefe\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e36d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz \n",
    "from six import StringIO\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "import time as tm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.ensemble import BaggingRegressor,BaggingClassifier\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, \\\n",
    "recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.ensemble import BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afcc92d",
   "metadata": {},
   "source": [
    "## How this model works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ad31b",
   "metadata": {},
   "source": [
    "This model was created in order to introduce bias to the base models of the stacked model. Each of these models individually will not offer a great solution, but a stacked model with these as the base will allow the overall model to work off the strengths of the individual models. \n",
    "\n",
    "Four feature sets were used: \n",
    "The top predictors from a random forest model, with the number of features being decided by a forward step-wise 5-fold cv process (not in this code), where the most significant model was added first and the RMSE on the train data was calculated from there. \n",
    "\n",
    "The top predictors from a MARS model:\n",
    "These were the significant predictors in a MARS model where X predicts log_y (no cv and the code is not present as I could only get it to work on Google Collab). \n",
    "\n",
    "Top 40 predictors from kbest:\n",
    "I wanted to add another predictor subset, but was sttuggling to think of other ways. K-best is a function from sklearn, and provided different features from the MARS and RF best predictors so I decided to use it. \n",
    "\n",
    "The entire predictor set:\n",
    "This was used to train a catboost regressor on the entire dataset, which is relatively quick and provided higher accuracy for the meta model to learn from. \n",
    "\n",
    "## How to use the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2906d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('rf_transform', 'passthrough', X_top_kbest.columns)], remainder='drop')),\n",
    "    ('rf', rf_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee770c",
   "metadata": {},
   "source": [
    "The above code is used in the actual model, and is used as an example here. Pipeline is also a feature from sklearn that allows multiple functions to be executed in order. This can be used to streamline many processes. \n",
    "\n",
    "In this case, there are two functions. The first is the 'column_transformer,' which looking back, I probably should have created a unique name for every transformation. This works by using ColumnTransformer, which takes the subset of columns specified, in this case X_top_kbest.columns. The 'passthrough' indicates that these are the columns to be worked on, with the remainder being dropped. 'rf' is the name in the pipeline I gave the rf_model. I gave unique names to every model depending on what subset they were working on. e.g. 'rf1' corresponds with the MARS predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895a285",
   "metadata": {},
   "source": [
    "## What the model does overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcd96f3",
   "metadata": {},
   "source": [
    "As mentioned, the purpose of the model is to introduce bias to the base models, which will be overcome by the meta model. A good way to introduct bias is by using different predictors, and different models, which is what I did. I suspect that using other, more different models could work well, but I was not succesful in my attempts. This is a stacking regressor, which is different from a voting one. The difference is that there is a 'meta model' that corrects the error for the base models, by learning from the RMSE (or another specified loss function) on the base models. The meta model regressor I used was catboost, as it is the most accurate (I tried many other ways, and I thought that maybe ridge regression would work well, but it did not). \n",
    "\n",
    "The specifics of the model are as follows: (Note, I have lightboost set up to be used in the model, but removing it actually made the model better. I suspect this is because catboost is a very similar model, but provides a lower RMSE)\n",
    "\n",
    "The same, untuned models are run on each predictor subset. The models are random forest, catboost, and adaboost. These are all ran on the MARS subset, the rf subset, and the kbest subset. The catboost model is the only model ran on the entire dataset. 15 fold cross validation is used, which takes around 20-30 minutes to run. This model is not too slow because most of the models are run only on 20-40 predictors. It seems that the higher the k-fold, the better the models, but this is not true as the model gets worse after 15-fold cv. I suspect this is because the model will be testing on very little data for each split, which will not help RMSE, after a point. \n",
    "\n",
    "The exact same model was also ran on a 13-fold cv. The purpose of this was to once again introduce bias. This introduces bias because this will create two different models, each with innacuracies of their own. By themselves, these models are still extremely useful. \n",
    "\n",
    "These models were then combined by simply averaging the output, giving a low RMSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe39ac5",
   "metadata": {},
   "source": [
    "## These models are not precisely reproducable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ca216",
   "metadata": {},
   "source": [
    "Because these models are untuned, they will give slightly different output every time they are run. To combat this, the models could be tuned for reproducibility, but that could take a very long time. On my first try I got my best RMSE, so it does not take very long. In my case, I saved the model by using pickle, a library to save models. The exact model I used can be uploaded at the very bottom of this under the 'upload' section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59adc13",
   "metadata": {},
   "source": [
    "## Train Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "y = train.y\n",
    "x = train.drop(\"y\", axis=1)\n",
    "x = x.drop(\"id\", axis=1)\n",
    "\n",
    "x = x.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "y = y.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(x), columns=x.columns)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=7)\n",
    "\n",
    "X = pd.DataFrame(imputer.fit_transform(X_scaled), columns=x.columns)\n",
    "\n",
    "log_y = np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13efd944",
   "metadata": {},
   "source": [
    "## Get top predictors from untuned random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502366ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "model = RandomForestRegressor(n_estimators = 100)\n",
    "model.fit(X,log_y)\n",
    "\n",
    "top_50_rf = model.feature_importances_.argsort()[-50:][::-1]\n",
    "\n",
    "top_50_rf = pd.DataFrame(top_50_rf)\n",
    "\n",
    "top_50_rf.columns = ['predictor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144623e0",
   "metadata": {},
   "source": [
    "## Computed the best MARS features on google collab and am importing it as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "MARS_features = pd.read_csv('features.csv')\n",
    "\n",
    "filtered_features = top_50_rf[~top_50_rf['predictor'].isin(MARS_features)]\n",
    "new_df = filtered_features[['predictor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5b7b5",
   "metadata": {},
   "source": [
    "## Above code should only take predictors from the random forest subset that are not present in MARS, I am not sure if it worked, however, so there might be some overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c2771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "rf_top_30 = new_df[-30:][::-1]\n",
    "\n",
    "top_predictors = rf_top_30['predictor']\n",
    "X_top_rf = X.iloc[:,top_predictors]\n",
    "\n",
    "top_predictors_MARS = MARS_features['predictor']\n",
    "X_top_MARS = X[top_predictors_MARS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63278c18",
   "metadata": {},
   "source": [
    "## Get further subset of features by selecting kbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Assuming X is your feature set and log_y is your target variable\n",
    "selector = SelectKBest(f_regression, k=40)\n",
    "X_new = selector.fit_transform(X, log_y)\n",
    "\n",
    "# Get the indices sorted by most important to least important\n",
    "indices = np.argsort(selector.scores_)[::-1]\n",
    "\n",
    "# Get the names of the top 40 features\n",
    "X_top_kbest = []\n",
    "for i in range(40):\n",
    "    X_top_kbest.append(X.columns[indices[i]])\n",
    "    \n",
    "X_top_kbest = X[X_top_kbest]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e82a1",
   "metadata": {},
   "source": [
    "## Test Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49553340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "test_x = test.drop(\"id\", axis=1)\n",
    "test_x = test_x.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "test_X_scaled = pd.DataFrame(scaler.fit_transform(test_x), columns=test_x.columns)\n",
    "\n",
    "imputer_KNN = KNNImputer(n_neighbors=7)\n",
    "test_x = pd.DataFrame(imputer_KNN.fit_transform(test_X_scaled), columns=test_x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a81671",
   "metadata": {},
   "source": [
    "## Create Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5fd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "cat = CatBoostRegressor(verbose=False, random_seed=403)\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "light_model = LGBMRegressor(verbose=-1)\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor,BaggingClassifier,AdaBoostRegressor,AdaBoostClassifier\n",
    "ada_model = AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0657b4",
   "metadata": {},
   "source": [
    "## First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8722f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "X_train, X_test, y_train_log, y_test_log = train_test_split(X, log_y, test_size = 0.2, random_state = 8)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "#kbest\n",
    "cat_pipe = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('cat_transform', 'passthrough', X_top_kbest.columns)], remainder='drop')),\n",
    "    ('cat', cat)\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('rf_transform', 'passthrough', X_top_kbest.columns)], remainder='drop')),\n",
    "    ('rf', rf_model)\n",
    "])\n",
    "\n",
    "ada_pipe = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('ada_transform', 'passthrough', X_top_kbest.columns)], remainder='drop')),\n",
    "    ('ada', ada_model)\n",
    "])\n",
    "\n",
    "light_pipe = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('light_transform', 'passthrough', X_top_kbest.columns)], remainder='drop')),\n",
    "    ('light', light_model)\n",
    "])\n",
    "\n",
    "cat_pipe1 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('cat_transform1', 'passthrough', X_top_MARS.columns)], remainder='drop')),\n",
    "    ('cat1', cat)\n",
    "])\n",
    "\n",
    "rf_pipe1 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('rf_transform1', 'passthrough', X_top_MARS.columns)], remainder='drop')),\n",
    "    ('rf1', rf_model)\n",
    "])\n",
    "\n",
    "ada_pipe1 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('ada_transform1', 'passthrough', X_top_MARS.columns)], remainder='drop')),\n",
    "    ('ada1', ada_model)\n",
    "])\n",
    "\n",
    "light_pipe1 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('light_transform1', 'passthrough', X_top_MARS.columns)], remainder='drop')),\n",
    "    ('light1', light_model)\n",
    "])\n",
    "\n",
    "cat_pipe2 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('cat_transform2', 'passthrough', X_top_rf.columns)], remainder='drop')),\n",
    "    ('cat2', cat)\n",
    "])\n",
    "\n",
    "rf_pipe2 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('rf_transform2', 'passthrough', X_top_rf.columns)], remainder='drop')),\n",
    "    ('rf2', rf_model)\n",
    "])\n",
    "\n",
    "ada_pipe2 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('ada_transform2', 'passthrough', X_top_rf.columns)], remainder='drop')),\n",
    "    ('ada2', ada_model)\n",
    "])\n",
    "\n",
    "light_pipe2 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('light_transform2', 'passthrough', X_top_rf.columns)], remainder='drop')),\n",
    "    ('light2', light_model)\n",
    "])\n",
    "\n",
    "cat_pipe3 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('cat_transform3', 'passthrough', X.columns)], remainder='drop')),\n",
    "    ('cat3', cat)\n",
    "])\n",
    "\n",
    "en_new = StackingRegressor(estimators = [('cat', cat_pipe),('rf', rf_pipe),('ada', ada_pipe), \n",
    "                                     ('cat1', cat_pipe1),('rf1', rf_pipe1),('ada1', ada_pipe1),\n",
    "                                     ('cat2', cat_pipe2),('rf2', rf_pipe2),('ada2', ada_pipe2),\n",
    "                                     ('cat3', cat_pipe3)],\n",
    "                     final_estimator=CatBoostRegressor(),                                          \n",
    "                    cv = KFold(n_splits = 15, shuffle = True, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "en_new.fit(X_train, y_train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16930209",
   "metadata": {},
   "source": [
    "## Add intercept because model underestimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a327a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "new_intercept = np.mean(np.exp(y_test_log) - np.exp(en_new.predict(X_test)))\n",
    "\n",
    "en_new.fit(X, log_y)\n",
    "\n",
    "s = pd.DataFrame({'id':test.iloc[:, 0], \"y\":np.exp(en_new.predict(test_x)) + new_intercept})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf391615",
   "metadata": {},
   "source": [
    "## Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "X_train, X_test, y_train_log, y_test_log = train_test_split(X, log_y, test_size = 0.2, random_state = 8)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "#kbest\n",
    "cat_pipe = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('cat_transform', 'passthrough', X_top_kbest.columns)], remainder='drop')),\n",
    "    ('cat', cat)\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('rf_transform', 'passthrough', X_top_kbest.columns)], remainder='drop')),\n",
    "    ('rf', rf_model)\n",
    "])\n",
    "\n",
    "ada_pipe = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('ada_transform', 'passthrough', X_top_kbest.columns)], remainder='drop')),\n",
    "    ('ada', ada_model)\n",
    "])\n",
    "\n",
    "light_pipe = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('light_transform', 'passthrough', X_top_kbest.columns)], remainder='drop')),\n",
    "    ('light', light_model)\n",
    "])\n",
    "\n",
    "cat_pipe1 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('cat_transform1', 'passthrough', X_top_MARS.columns)], remainder='drop')),\n",
    "    ('cat1', cat)\n",
    "])\n",
    "\n",
    "rf_pipe1 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('rf_transform1', 'passthrough', X_top_MARS.columns)], remainder='drop')),\n",
    "    ('rf1', rf_model)\n",
    "])\n",
    "\n",
    "ada_pipe1 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('ada_transform1', 'passthrough', X_top_MARS.columns)], remainder='drop')),\n",
    "    ('ada1', ada_model)\n",
    "])\n",
    "\n",
    "light_pipe1 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('light_transform1', 'passthrough', X_top_MARS.columns)], remainder='drop')),\n",
    "    ('light1', light_model)\n",
    "])\n",
    "\n",
    "cat_pipe2 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('cat_transform2', 'passthrough', X_top_rf.columns)], remainder='drop')),\n",
    "    ('cat2', cat)\n",
    "])\n",
    "\n",
    "rf_pipe2 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('rf_transform2', 'passthrough', X_top_rf.columns)], remainder='drop')),\n",
    "    ('rf2', rf_model)\n",
    "])\n",
    "\n",
    "ada_pipe2 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('ada_transform2', 'passthrough', X_top_rf.columns)], remainder='drop')),\n",
    "    ('ada2', ada_model)\n",
    "])\n",
    "\n",
    "light_pipe2 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('light_transform2', 'passthrough', X_top_rf.columns)], remainder='drop')),\n",
    "    ('light2', light_model)\n",
    "])\n",
    "\n",
    "cat_pipe3 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('cat_transform3', 'passthrough', X.columns)], remainder='drop')),\n",
    "    ('cat3', cat)\n",
    "])\n",
    "\n",
    "winner = StackingRegressor(estimators = [('cat', cat_pipe),('rf', rf_pipe),('ada', ada_pipe), \n",
    "                                     ('cat1', cat_pipe1),('rf1', rf_pipe1),('ada1', ada_pipe1),\n",
    "                                     ('cat2', cat_pipe2),('rf2', rf_pipe2),('ada2', ada_pipe2),\n",
    "                                     ('cat3', cat_pipe3)],\n",
    "                     final_estimator=CatBoostRegressor(),                                          \n",
    "                    cv = KFold(n_splits = 13, shuffle = True, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4cf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "winner.fit(X_train, y_train_log)\n",
    "\n",
    "winner_intercept = np.mean(np.exp(y_test_log) - np.exp(winner.predict(X_test)))\n",
    "\n",
    "winner.fit(X, log_y)\n",
    "\n",
    "game = pd.DataFrame({'id':test.iloc[:, 0], \"y\":np.exp(winner.predict(test_x)) + winner_intercept})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2181b50e",
   "metadata": {},
   "source": [
    "## Create en ensemble by combining the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2175d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "x = game.copy()\n",
    "x.y = s.y*.5 + game.y*.5\n",
    "\n",
    "x.to_csv('xgame1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
