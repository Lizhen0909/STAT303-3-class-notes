[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science III with python (Class notes)",
    "section": "",
    "text": "Preface\nThese are class notes for the course STAT303-3. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThe course notes are currently being written, and will continue to being developed as the course progresses (just like the class notes last quarter). Please report any typos / mistakes / inconsistencies / issues with the class notes / class presentations in your comments here. Thank you!"
  },
  {
    "objectID": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "href": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "title": "1  Introduction to Scikit-learn",
    "section": "1.1 Splitting data into train and test",
    "text": "1.1 Splitting data into train and test\nLet us create train and test datasets for developing a model to predict if a person has diabetes.\n\n# Creating training and test data\n    # 80-20 split, which is usual - 70-30 split is also fine, 90-10 is fine if the dataset is large\n    # random_state to set a random seed for the splitting - reproducible results\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)\n\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the complete dataset.\n\n#Proportion of 0s and 1s in the complete data\ny.value_counts()/y.shape\n\n0    0.651042\n1    0.348958\nName: Outcome, dtype: float64\n\n\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the train dataset.\n\n#Proportion of 0s and 1s in train data\ny_train.value_counts()/y_train.shape\n\n0    0.644951\n1    0.355049\nName: Outcome, dtype: float64\n\n\n\n#Proportion of 0s and 1s in test data\ny_test.value_counts()/y_test.shape\n\n0    0.675325\n1    0.324675\nName: Outcome, dtype: float64\n\n\nWe observe that the proportion of 0s and 1s in the train and test dataset are slightly different from that in the complete data. In order for these datasets to be more representative of the population, they should have a proportion of 0s and 1s similar to that in the complete dataset. This is especially critical in case of imbalanced datasets, where one class is represented by a significantly smaller number of instances than the other(s).\n\n1.1.1 \n\n\n1.1.2 Stratified splitting\nWe will use the argument stratify to obtain a proportion of 0s and 1s in the train and test datasets that is similar to the proportion in the complete `data.\n\n#Stratified train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45, stratify=y)\n\n\n#Proportion of 0s and 1s in train data with stratified split\ny_train.value_counts()/y_train.shape\n\n0    0.651466\n1    0.348534\nName: Outcome, dtype: float64\n\n\n\n#Proportion of 0s and 1s in test data with stratified split\ny_test.value_counts()/y_test.shape\n\n0    0.649351\n1    0.350649\nName: Outcome, dtype: float64\n\n\nThe proportion of the classes in the stratified split mimics the proportion in the complete dataset more closely.\nStratified splitting is particularly useful when dealing with imbalanced datasets, where one class is represented by a significantly smaller number of instances than the other(s).\nWhen training a classification model on an imbalanced dataset, the model might not learn enough about the minority class, which can lead to poor generalization performance on new data. This happens because the model is biased towards the majority class, and it might even predict all instances as belonging to the majority class.\nBy using stratified splitting, we ensure that both the train and test data sets have the same proportion of instances from each class, which means that the model will see enough instances from the minority class during training. This, in turn, helps the model learn to distinguish between the classes better, leading to better performance on new data.\nIn summary, stratified splitting helps to ensure that the model sees enough instances from each class during training, which can improve the model’s ability to generalize to new data, particularly in cases where one class is underrepresented in the dataset.\n\n# With linear/logistic regression in scikit-learn, especially when the predictors have different orders \n# of magn., scaling is necessary. This is to enable the training algo. which we did not cover. (Gradient Descent)\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test) # Do NOT refit the scaler with the test data, just transform it.\n\nX_train = X_train_scaled \nX_test = X_test_scaled\n\n\n# Create a model - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train, y_train)\n\n# Test the model - prediction - two ways to go\ny_pred = logreg.predict(X_test) # Get the predicted classes first\nprint(accuracy_score(y_pred, y_test)*100) # Use the predicted and true classes for accuracy\n\n73.37662337662337\n\n\n\nprint(logreg.score(X_test, y_test)*100) # Use .score with test predictors and response to get the accuracy\n                                            # Implements the same thing under the hood\n\n73.37662337662337\n\n\n\nprint(logreg.coef_) # Use coef_ to return the coefficients - only log reg inference you can do with sklearn\n\n[[ 0.32572891  1.20110566 -0.32046591  0.06849882 -0.21727131  0.72619528\n   0.40088897  0.29698818]]\n\n\n\n# all metrics exist in sklearn\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred))\nprint(precision_score(y_test, y_pred))\nprint(recall_score(y_test, y_pred))\n\n# What we covered today:\n    # A recap of Log. Reg. with sklearn\n        # Separate the predictors and response (if necessary)\n        # Split the data into train and test (if necessary)\n        \n        # Create a model\n        # Train with .fit\n        # Predict with .predict or get the accuracy with .score\n        # Use sklearn metrics with y_pred and y_test\n\n\n        # Same idea with LinearRegression()\n        # .score returns r-squared by default\n        # use the appropriate metrics!\n\n\n###################################################################################################\n\n[[87 17]\n [24 26]]\n0.6046511627906976\n0.52\n\n\n\n# More details on the LogisticRegression model:\n    # Inputs - for regularization and \n    # prediction prob.s instead of classes - so we can change the thresholds\n    \n# .predict_proba returns the prob.s for both classes\n    # Two cols for two classes\n    # Apply your threshold to y_pred_probs[1] (second col)\ny_pred_probs = logreg.predict_proba(X_test)    \n\ncutoff = 0.3\n\ny_pred2 = y_pred_probs[:,1] > cutoff\ny_pred2 = y_pred2.astype(int)\n\nprint(confusion_matrix(y_test, y_pred2))\nprint(precision_score(y_test, y_pred2))\nprint(recall_score(y_test, y_pred2))\n\n[[78 26]\n [15 35]]\n0.5737704918032787\n0.7\n\n\n\n# Throughout the course, we will create many different models in sklearn, all of them will have\n    # .fit\n    # .predict\n    # .score\n    # .predict_proba\n# methods, among others that are specific to them. Make sure how to use there 4 basic ones.\n\n# Let's take a look at the documentation - always do this for the other models we will see\n# Log Reg model has default regularization\n    # Default (regularization) penalty is \"l2\" - this means Ridge \n    # C is 1/lambda - remember that lambda is the hyperparameter that is multiplied with the ridge penalty\n        # C is 1 by default\n        \n# Let's take away the regularization\nlogreg2 = LogisticRegression(C=1e10)\nlogreg2.fit(X_train, y_train)\ny_pred = logreg2.predict(X_test) # Get the predicted classes first\nprint(accuracy_score(y_pred, y_test)*100)\n\n73.37662337662337\n\n\n\n# Test accuracy stayed the same - reg was not very necessary\n\n# Too much reg\nlogreg2 = LogisticRegression(C=1e-10)\nlogreg2.fit(X_train, y_train)\ny_pred = logreg2.predict(X_test) # Get the predicted classes first\nprint(accuracy_score(y_pred, y_test)*100)\n\n# Test accuracy is even lower - too much reg caused underfitting\n\n67.53246753246754\n\n\n\n# The key to take full advantage of sklearn models is their inputs\n    # Always read the doc\n    # In Log Reg, you can switch to Lasso with penalty = 'l1' - for variable selection\n    # For no regression, besides what we did above, you can use penalty = None\n    \n# Recall that C, or lambda, is a hyperparameter, which is optimized with cross-validation\n    # There is LogisticRegressionCV, just like LassoCV and RidgeCV\n    # Works the exact same way - check LassoCV and RidgeCV notes\n    \n# For all the sklearn models we will create in this course, there will be hyperparameters.\n    # Mostly more than one for each model\n    # These hyperparameters will determine how much regularization the model will have\n    # These models will not have a CV version\n    # So, we need to use two sklearn tools that implement cross-validation\n        # cross_val_score - now\n        # GridSearchCV - later when we get to trees and tree-based models\n\nfrom sklearn.model_selection import cross_val_score\n\nval_scores = []\n\nhyperparam_vals = 10**np.linspace(-5, 10)\n\nfor c_val in hyperparam_vals: # For each possible C value in your grid\n    logreg_model = LogisticRegression(C=c_val) # Create a model with the C value\n    \n    val_scores.append(cross_val_score(logreg_model, X_train, y_train, scoring='accuracy', cv=5)) # Find the cv results\n    \n    \nimport matplotlib.pyplot as plt\n\nplt.plot(hyperparam_vals, np.mean(np.array(val_scores), axis=1))\nplt.xlabel('possible C value')\nplt.ylabel('avg 5-fold CV value')\nplt.xscale('log')\nplt.show()\n\n\n# Train the best model with the hyperparam val that returns the highest average accuracy\nlogreg_model_best = LogisticRegression(C=hyperparam_vals[np.argmax(np.mean(np.array(val_scores), axis=1))])\n\n# .fit\n# .predict & .predict_proba\n# .score\n# ...\n\n# Log. reg. has one hyperparameter - C.\n# More complex models will have more\n# If we have two hyperparams - we can use a nested loop and cross_val_score\n    # or we can use GridSearchCV - more on that when we get to trees and tree-based models."
  },
  {
    "objectID": "Lec2_Regression_splines.html#polynomial-regression-vs-regression-splines",
    "href": "Lec2_Regression_splines.html#polynomial-regression-vs-regression-splines",
    "title": "2  Regression splines",
    "section": "2.1 Polynomial regression vs Regression splines",
    "text": "2.1 Polynomial regression vs Regression splines\n\n2.1.1 Model of degree 1\n\nols_object = smf.ols(formula = 'price~mileage', data = train)\nlr_model = ols_object.fit()\n\n\n#Regression spline of degree 1\n\n#Creating basis functions for splines of degree 1\ntransformed_x = dmatrix(\"bs(mileage , knots=(33000,66000,100000), degree = 1, include_intercept = False)\",\n                        data = {'mileage':train['mileage']},return_type = 'dataframe')\n\n#Developing a linear regression model on the spline basis functions - this is the regression splines model\nreg_spline_model = sm.OLS(train['price'], transformed_x).fit()\n\n\n#Visualizing polynomial model and the regression spline model of degree 1\n\nknots = [33000,66000,100000] #Knots for the spline\nd=1 #Degree of predictor in the model\n#Writing a function to visualize polynomial model and the regression spline model of degree d\ndef viz_models():\n    fig, axes = plt.subplots(1,2,figsize = (15,5))\n    plt.subplots_adjust(wspace=0.2)\n\n    #Visualizing the linear regression model\n    pred_price = lr_model.predict(train)\n    sns.scatterplot(ax = axes[0],x = 'mileage', y = 'price', data = train, color = 'orange')\n    sns.lineplot(ax = axes[0],x = train.mileage, y = pred_price, color = 'blue')\n    axes[0].set_title('Polynomial regression model of degree '+str(d))\n    \n    #Visualizing the regression splines model of degree 'd'    \n    axes[1].set_title('Regression splines model of degree '+ str(d))\n    sns.scatterplot(ax=axes[1],x = 'mileage', y = 'price', data = train, color = 'orange')\n    sns.lineplot(ax=axes[1],x = train.mileage, y = reg_spline_model.predict(), color = 'blue')\n    for i in range(3):\n        plt.axvline(knots[i], 0,100,color='red')\nviz_models()\n\n\n\n\nWe observe the regression splines model better fits the data as compared to the polynomial regression model. This is because regression splines of degree 1 fit piecewise polynomials, or linear models on sub-sections of the predictor, which helps better capture the trend. However, this added flexibility may also lead to overfitting. Hence, one must be careful to check for overfitting when using splines. Overfitting may be checked by k-fold cross validation or comparing test and train errors.\nThe red lines in the plot on the right denote the position of knots. Knots separate distinct splines.\n\n#Creating basis functions for test data for prediction\ntest_x = dmatrix(\"bs(mileage , knots=(33000,66000,100000), degree = 1, include_intercept = False)\",data = {'mileage':test['mileage']},\n                                                                                                  return_type = 'dataframe')\n\n\n#Function to compute RMSE (root mean squared error on train and test datasets)\ndef rmse():\n    #Error on train data for the linear regression model\n    print(\"RMSE on train data:\")\n    print(\"Linear regression:\", np.sqrt(mean_squared_error(lr_model.predict(),train.price)))\n\n    #Error on train data for the regression spline model\n    print(\"Regression splines:\", np.sqrt(mean_squared_error(reg_spline_model.predict(),train.price)))\n    \n    #Error on test data for the linear regression model\n    print(\"\\nRMSE on test data:\")\n    print(\"Linear regression:\",np.sqrt(mean_squared_error(lr_model.predict(test),test.price)))\n\n    #Error on test data for the regression spline model\n    print(\"Regression splines:\",np.sqrt(mean_squared_error(reg_spline_model.predict(test_x),test.price)))    \nrmse()\n\nRMSE on train data:\nLinear regression: 14403.250083261853\nRegression splines: 13859.640716531134\n\nRMSE on test data:\nLinear regression: 14370.94086395544\nRegression splines: 13770.133025694666\n\n\n\n\n2.1.2 Model of degree 2\nA higher degree model will lead to additional flexibility for both polynomial and regression splines models.\n\n#Including mileage squared as a predictor and developing the model\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)', data = train)\nlr_model = ols_object.fit()\n\n\n#Regression spline of degree 2\n\n#Creating basis functions for splines of degree 2\ntransformed_x = dmatrix(\"bs(mileage , knots=(33000,66000,100000), degree = 2, include_intercept = False)\",\n                        data = {'mileage':train['mileage']},return_type = 'dataframe')\n\n#Developing a linear regression model on the spline basis functions - this is the regression splines model\nreg_spline_model = sm.OLS(train['price'], transformed_x).fit()\n\n\nd=2\nviz_models()\n\n\n\n\nUnlike polynomial regression, splines functions avoid imposing a global structure on the non-linear function of X. This provides a better local fit to the data.\n\n#Creating basis functions for test data for prediction\ntest_x = dmatrix(\"bs(mileage , knots=(33000,66000,100000), degree = 2, include_intercept = False)\",data = {'mileage':test['mileage']},\n                                                                                                  return_type = 'dataframe')\n\n\nrmse()\n\nRMSE on train data:\nLinear regression: 14403.250083261853\nRegression splines: 13859.640716531134\n\nRMSE on test data:\nLinear regression: 14370.94086395544\nRegression splines: 13770.133025694666\n\n\n\n\n2.1.3 Model of degree 3\n\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)+I(mileage**3)', data = train)\nlr_model = ols_object.fit()\n\n\n#Regression spline of degree 3\n\n#Creating basis functions for splines of degree 3\ntransformed_x = dmatrix(\"bs(mileage , knots=(20000,40000,80000), degree = 3, include_intercept = False)\",\n                        data = {'mileage':train['mileage']},return_type = 'dataframe')\n\n#Developing a linear regression model on the spline basis functions - this is the regression splines model\nreg_spline_model = sm.OLS(train['price'], transformed_x).fit()\n\n\ntransformed_x\n\n\n\n\n\n  \n    \n      \n      Intercept\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[0]\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[1]\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[2]\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[3]\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[4]\n      bs(mileage, knots=(20000, 40000, 80000), degree=3, include_intercept=False)[5]\n    \n  \n  \n    \n      0\n      1.0\n      0.001499\n      3.749187e-07\n      1.562637e-11\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      1\n      1.0\n      0.583162\n      3.001491e-01\n      1.975041e-02\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      2\n      1.0\n      0.000750\n      9.374336e-08\n      1.953296e-12\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      3\n      1.0\n      0.293446\n      6.009875e-01\n      1.053974e-01\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      4\n      1.0\n      0.000000\n      2.580169e-02\n      7.669068e-01\n      0.200988\n      0.006303\n      0.000000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4955\n      1.0\n      0.005441\n      4.824519e-01\n      5.016606e-01\n      0.010446\n      0.000000\n      0.000000\n    \n    \n      4956\n      1.0\n      0.206763\n      6.438755e-01\n      1.493551e-01\n      0.000006\n      0.000000\n      0.000000\n    \n    \n      4957\n      1.0\n      0.000000\n      0.000000e+00\n      2.783832e-01\n      0.496164\n      0.213126\n      0.012326\n    \n    \n      4958\n      1.0\n      0.198162\n      6.468919e-01\n      1.549344e-01\n      0.000012\n      0.000000\n      0.000000\n    \n    \n      4959\n      1.0\n      0.000000\n      2.233101e-01\n      7.229229e-01\n      0.053702\n      0.000065\n      0.000000\n    \n  \n\n4960 rows × 7 columns\n\n\n\n\nd=3\nknots=[20000,40000,80000]\nviz_models()\n\n\n\n\nUnlike polynomial regression, splines functions avoid imposing a global structure on the non-linear function of X. This provides a better local fit to the data.\n\n#Creating basis functions for test data for prediction\ntest_x = dmatrix(\"bs(mileage , knots=(20000,40000,80000), degree = 3, include_intercept = False)\",data = {'mileage':test['mileage']},\n                                                                                                  return_type = 'dataframe')\n\n\nrmse()\n\nRMSE on train data:\nLinear regression: 13891.962447594644\nRegression splines: 13792.371446327243\n\nRMSE on test data:\nLinear regression: 13789.708418357186\nRegression splines: 13651.288965905529"
  },
  {
    "objectID": "Lec2_Regression_splines.html#regression-splines-with-knots-at-uniform-quantiles-of-data",
    "href": "Lec2_Regression_splines.html#regression-splines-with-knots-at-uniform-quantiles-of-data",
    "title": "2  Regression splines",
    "section": "2.2 Regression splines with knots at uniform quantiles of data",
    "text": "2.2 Regression splines with knots at uniform quantiles of data\nIf degrees of freedom are provided instead of knots, the knots are by default chosen at uniform quantiles of data. For example if there are 7 degrees of freedom (including the intercept), then there will be 7-4 = 3 knots. These knots will be chosen at the 255h, 50th and 75th quantiles of the data.\n\n#Regression spline of degree 3\n\n#Creating basis functions for splines of degree 3\ntransformed_x = dmatrix(\"bs(mileage , df=6, degree = 3, include_intercept = False)\",\n                        data = {'mileage':train['mileage']},return_type = 'dataframe')\n\n#Developing a linear regression model on the spline basis functions - this is the regression splines model\nreg_spline_model = sm.OLS(train['price'], transformed_x).fit()\n\n\nd=3\nunif_knots = pd.qcut(train.mileage,4,retbins=True)[1][1:4]\nknots=unif_knots\nviz_models()\n\n\n\n\nSplines can be unstable at the outer range of predictors. In the figure (on the right), the left-most spline may be overfitting.\n\n#Creating basis functions for test data for prediction\ntest_x = dmatrix(\"bs(mileage , knots=\" +str(tuple(unif_knots)) + \", degree = 3, include_intercept = False)\",data = {'mileage':test['mileage']},\n                                                                                                  return_type = 'dataframe')\n\n\nrmse()\n\nRMSE on train data:\nLinear regression: 13891.962447594644\nRegression splines: 13781.79102252679\n\nRMSE on test data:\nLinear regression: 13789.708418357186\nRegression splines: 13676.271829882426"
  },
  {
    "objectID": "Lec2_Regression_splines.html#natural-cubic-splines",
    "href": "Lec2_Regression_splines.html#natural-cubic-splines",
    "title": "2  Regression splines",
    "section": "2.3 Natural cubic splines",
    "text": "2.3 Natural cubic splines\nPage 298: “A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This additional constraint means that natural splines generally produce more stable estimates at the boundaries.”\n\n#Natural cubic spline\n\n#Creating basis functions for the natural cubic spline\ntransformed_x = dmatrix(\"cr(mileage , df=4,constraints='center')\",\n                        data = {'mileage':train['mileage']},return_type = 'dataframe')\nreg_spline_model = sm.GLM(train['price'], transformed_x).fit()\n\n\nd=3;\nunif_knots = pd.qcut(train.mileage,4,retbins=True)[1][1:4]\nknots=unif_knots\nviz_models()\n\n\n\n\nNote that the natural cubic spline is more stable than a cubic splines with knots at uniformly distributed quantiles.\n\n#Creating basis functions for test data for prediction\ntest_x = dmatrix(\"cr(mileage , knots=\"+str(tuple(unif_knots))+\",constraints='center')\",data = {'mileage':test['mileage']},\n                                                                                                  return_type = 'dataframe')\n\n\nrmse()\n\nRMSE on train data:\nLinear regression: 13891.962447594644\nRegression splines: 13805.022189679756\n\nRMSE on test data:\nLinear regression: 13789.708418357186\nRegression splines: 13666.943224268975"
  },
  {
    "objectID": "Lec2_Regression_splines.html#generalized-additive-model-gam",
    "href": "Lec2_Regression_splines.html#generalized-additive-model-gam",
    "title": "2  Regression splines",
    "section": "2.4 Generalized additive model (GAM)",
    "text": "2.4 Generalized additive model (GAM)\nGAM allow for flexible nonlinearities in several variables, but retain the additive structure of linear models. In a GAM, non-linear basis functions of predictors can be used as predictors of a linear regression model. For example, \\[y = f_1(X_1) + f_2(X_2) + \\epsilon\\] is a GAM, where \\(f_1(.)\\) may be a cubic spline based on the predictor \\(X_1\\), and \\(f_2(.)\\) may be a step function based on the predictor \\(X_2\\).\n\nsns.distplot(train.year)\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n<AxesSubplot:xlabel='year', ylabel='Density'>\n\n\n\n\n\n\n#GAM\n#GAM includes cubic splines for mileage. Other predictors are year, engineSize, mpg, mileage and their interactions\nX_transformed = dmatrix('bs(mileage,df=6,degree = 3)+year*engineSize*mpg*mileage', \n                data = {'year':train['year'],'engineSize':train['engineSize'],'mpg':train['mpg'],'mileage':train['mileage']} ,\n                                                                            return_type = 'dataframe')\n# fit the model\nmodel_gam = sm.OLS(train['price'],X_transformed).fit()\n\n#Creating basis functions for test data for prediction\nX_test = dmatrix('bs(mileage,df=6,degree = 3, include_intercept = False)+year*engineSize*mpg*mileage', \n                data = {'year':test['year'],'engineSize':test['engineSize'],'mpg':test['mpg'],'mileage':test['mileage']} ,\n                                                                            return_type = 'dataframe')\n\npreds = model_gam.predict(X_test)\nnp.sqrt(mean_squared_error(preds,test.price))\n\n8434.756663328963\n\n\n\n#GAM\n#GAM includes cubic splines for mileage, year, engineSize, mpg, and interactions of all predictors\nX_transformed = dmatrix('bs(mileage,df=6,degree = 3)+bs(mpg,df=6,degree = 3)+bs(engineSize,df=6,degree = 3)+year*engineSize*mpg*mileage', \n                data = {'year':train['year'],'engineSize':train['engineSize'],'mpg':train['mpg'],'mileage':train['mileage']} ,\n                                                                            return_type = 'dataframe')\n# fit the model\nmodel_gam = sm.OLS(train['price'],X_transformed).fit()\n\n#Creating basis functions for test data for prediction\nX_test = dmatrix('bs(mileage,df=6,degree = 3, include_intercept = False)+bs(mpg,df=6,degree = 3)+bs(engineSize,df=6,degree = 3)+year*engineSize*mpg*mileage', \n                data = {'year':test['year'],'engineSize':test['engineSize'],'mpg':test['mpg'],'mileage':test['mileage']} ,\n                                                                            return_type = 'dataframe')\n\npreds = model_gam.predict(X_test)\nnp.sqrt(mean_squared_error(preds,test.price))\n\n7997.325718841729\n\n\n\nols_object = smf.ols(formula = 'price~(year+engineSize+mileage+mpg)**2+I(mileage**2)+I(mileage**3)', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.704 \n\n\n  Model:                   OLS         Adj. R-squared:        0.703 \n\n\n  Method:             Least Squares    F-statistic:           1308. \n\n\n  Date:             Sun, 27 Mar 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:08:50       Log-Likelihood:      -52157. \n\n\n  No. Observations:        4960        AIC:                1.043e+05\n\n\n  Df Residuals:            4950        BIC:                1.044e+05\n\n\n  Df Model:                   9                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                        coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept             -0.0009     0.000    -2.752  0.006    -0.002    -0.000\n\n\n  year                  -1.1470     0.664    -1.728  0.084    -2.448     0.154\n\n\n  engineSize             0.0052     0.000    17.419  0.000     0.005     0.006\n\n\n  mileage              -31.4751     2.621   -12.010  0.000   -36.613   -26.337\n\n\n  mpg                   -0.0201     0.002   -13.019  0.000    -0.023    -0.017\n\n\n  year:engineSize        9.5957     0.254    37.790  0.000     9.098    10.094\n\n\n  year:mileage           0.0154     0.001    11.816  0.000     0.013     0.018\n\n\n  year:mpg               0.0572     0.013     4.348  0.000     0.031     0.083\n\n\n  engineSize:mileage    -0.1453     0.008   -18.070  0.000    -0.161    -0.130\n\n\n  engineSize:mpg       -98.9062    11.832    -8.359  0.000  -122.102   -75.710\n\n\n  mileage:mpg            0.0011     0.000     2.432  0.015     0.000     0.002\n\n\n  I(mileage ** 2)     7.713e-06  3.75e-07    20.586  0.000  6.98e-06  8.45e-06\n\n\n  I(mileage ** 3)    -1.867e-11  1.43e-12   -13.077  0.000 -2.15e-11 -1.59e-11\n\n\n\n\n  Omnibus:       1830.457   Durbin-Watson:         0.634 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   34927.811\n\n\n  Skew:            1.276    Prob(JB):               0.00 \n\n\n  Kurtosis:       15.747    Cond. No.           2.50e+18 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.5e+18. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nnp.sqrt(mean_squared_error(model.predict(test),test.price))\n\n9026.775740000594\n\n\nNote the RMSE with GAM that includes regression splines for mileage is lesser than that of the linear regression model, indicating a better fit."
  },
  {
    "objectID": "Lec2_Regression_splines.html#mars-multivariate-adaptive-regression-splines",
    "href": "Lec2_Regression_splines.html#mars-multivariate-adaptive-regression-splines",
    "title": "2  Regression splines",
    "section": "2.5 MARS (Multivariate Adaptive Regression Splines)",
    "text": "2.5 MARS (Multivariate Adaptive Regression Splines)\n\nX=train['mileage']\ny=train['price']\n\n\n2.5.1 MARS of degree 1\n\nmodel = Earth(max_terms=500, max_degree=1) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\nEarth(max_degree=1, max_terms=500)\n\n\n\nprint(model.summary())\n\nEarth Model\n-------------------------------------\nBasis Function  Pruned  Coefficient  \n-------------------------------------\n(Intercept)     No      -553155      \nh(x0-22141)     Yes     None         \nh(22141-x0)     Yes     None         \nh(x0-3354)      No      -6.23571     \nh(3354-x0)      Yes     None         \nh(x0-15413)     No      -36.9613     \nh(15413-x0)     No      38.167       \nh(x0-106800)    Yes     None         \nh(106800-x0)    No      0.221844     \nh(x0-500)       No      170.039      \nh(500-x0)       Yes     None         \nh(x0-741)       Yes     None         \nh(741-x0)       No      -54.5265     \nh(x0-375)       No      -126.804     \nh(375-x0)       Yes     None         \nh(x0-2456)      Yes     None         \nh(2456-x0)      No      7.04609      \n-------------------------------------\nMSE: 188429705.7549, GCV: 190035470.5664, RSQ: 0.2998, GRSQ: 0.2942\n\n\nModel equation: \\[-553155 -6.23(h(x0-3354)) -36.96(h(x0-15413) + .......... -7.04(h(2456-x0)\\]\n\npred = model.predict(test.mileage)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n13650.2113154515\n\n\n\nsns.scatterplot(x = 'mileage', y = 'price', data = train, color = 'orange')\nsns.lineplot(x = train.mileage, y = model.predict(train.mileage), color = 'blue')\n\n<AxesSubplot:xlabel='mileage', ylabel='price'>\n\n\n\n\n\n\n\n2.5.2 MARS of degree 2\n\nmodel = Earth(max_terms=500, max_degree=2) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\nprint(model.summary())\n\nEarth Model\n-----------------------------------------------\nBasis Function           Pruned  Coefficient   \n-----------------------------------------------\n(Intercept)              No      19369.7       \nh(x0-22141)              Yes     None          \nh(22141-x0)              Yes     None          \nh(x0-7531)*h(22141-x0)   No      3.74934e-05   \nh(7531-x0)*h(22141-x0)   No      -6.74252e-05  \nx0*h(x0-22141)           No      -8.0703e-06   \nh(x0-15012)              Yes     None          \nh(15012-x0)              No      1.79813       \nh(x0-26311)*h(x0-22141)  No      8.85097e-06   \nh(26311-x0)*h(x0-22141)  Yes     None          \n-----------------------------------------------\nMSE: 189264421.5682, GCV: 190298913.1652, RSQ: 0.2967, GRSQ: 0.2932\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\n\npred = model.predict(test.mileage)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n13590.995419204985\n\n\n\nsns.scatterplot(x = 'mileage', y = 'price', data = train, color = 'orange')\nsns.lineplot(x = train.mileage, y = model.predict(train.mileage), color = 'blue')\n\n<AxesSubplot:xlabel='mileage', ylabel='price'>\n\n\n\n\n\nMARS provides a better fit than the splines that we used above. This is because MARS tunes the positions of the knots and considers interactions (also with tuned knots) to improve the model fit. Tuning of knots may improve the fit of splines as well.\n\n\n2.5.3 MARS including categorical variables\n\n#A categorical variable can be turned to dummy variables to use the Earth package for fitting MARS model\ntrain_cat = pd.concat([train,pd.get_dummies(train.fuelType)],axis=1)\ntest_cat = pd.concat([test,pd.get_dummies(test.fuelType)],axis=1)\n\n\ntrain_cat.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n      Diesel\n      Electric\n      Hybrid\n      Other\n      Petrol\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n      1\n      0\n      0\n      0\n      0\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n      1\n      0\n      0\n      0\n      0\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n      1\n      0\n      0\n      0\n      0\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n      1\n      0\n      0\n      0\n      0\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\n\nX = train_cat[['mileage','mpg','engineSize','year','Diesel','Electric','Hybrid','Petrol']]\nXtest = test_cat[['mileage','mpg','engineSize','year','Diesel','Electric','Hybrid','Petrol']]\n\n\nmodel = Earth(max_terms=500, max_degree=2) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\nprint(model.summary())\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\n\n\nEarth Model\n---------------------------------------------------------\nBasis Function                      Pruned  Coefficient  \n---------------------------------------------------------\n(Intercept)                         No      2.17604e+06  \nh(engineSize-5.5)                   No      9.80752e+06  \nh(5.5-engineSize)                   No      1.92817e+06  \nh(mileage-21050)                    No      18.687       \nh(21050-mileage)                    No      -177.871     \nh(mileage-21050)*h(5.5-engineSize)  Yes     None         \nh(21050-mileage)*h(5.5-engineSize)  No      -0.224909    \nyear                                No      4126.41      \nh(mpg-53.3495)                      No      344595       \nh(53.3495-mpg)                      Yes     None         \nHybrid*h(5.5-engineSize)            No      6124.34      \nh(mileage-21050)*year               No      -0.00930239  \nh(21050-mileage)*year               No      0.0886455    \nh(engineSize-5.5)*year              No      -4864.84     \nh(5.5-engineSize)*year              No      -952.92      \nh(mileage-1422)*h(53.3495-mpg)      No      -16.62       \nh(1422-mileage)*h(53.3495-mpg)      No      16.4306      \nHybrid                              No      -89090.6     \nh(mpg-21.1063)*h(53.3495-mpg)       Yes     None         \nh(21.1063-mpg)*h(53.3495-mpg)       No      -8815.99     \nh(mpg-23.4808)*h(5.5-engineSize)    No      -3649.97     \nh(23.4808-mpg)*h(5.5-engineSize)    Yes     None         \nh(mpg-20.5188)*year                 No      31.7341      \nh(20.5188-mpg)*year                 Yes     None         \nh(mpg-22.2566)*h(53.3495-mpg)       No      -52.2531     \nh(22.2566-mpg)*h(53.3495-mpg)       No      7916.19      \nh(mpg-22.6767)                      No      7.56432e+06  \nh(22.6767-mpg)                      Yes     None         \nh(mpg-23.9595)*h(mpg-22.6767)       Yes     None         \nh(23.9595-mpg)*h(mpg-22.6767)       No      -63225.4     \nh(mpg-21.4904)*h(22.6767-mpg)       No      -149055      \nh(21.4904-mpg)*h(22.6767-mpg)       Yes     None         \nh(mpg-21.1063)                      No      -887098      \nh(21.1063-mpg)                      Yes     None         \nh(mpg-29.5303)*h(mpg-22.6767)       No      -3028.87     \nh(29.5303-mpg)*h(mpg-22.6767)       Yes     None         \nh(mpg-28.0681)*h(5.5-engineSize)    No      3572.89      \nh(28.0681-mpg)*h(5.5-engineSize)    Yes     None         \nengineSize*h(5.5-engineSize)        No      -2952.65     \nh(mpg-25.3175)*h(mpg-21.1063)       No      -332551      \nh(25.3175-mpg)*h(mpg-21.1063)       No      324298       \nPetrol*year                         No      -1.37031     \nh(mpg-68.9279)*Hybrid               No      -4087.9      \nh(68.9279-mpg)*Hybrid               Yes     None         \nh(mpg-31.5043)*h(5.5-engineSize)    Yes     None         \nh(31.5043-mpg)*h(5.5-engineSize)    No      3691.82      \nh(mpg-32.7011)*h(5.5-engineSize)    Yes     None         \nh(32.7011-mpg)*h(5.5-engineSize)    No      -2262.78     \nh(mpg-44.9122)*h(mpg-22.6767)       No      335577       \nh(44.9122-mpg)*h(mpg-22.6767)       No      -335623      \nh(engineSize-5.5)*h(mpg-21.1063)    No      27815        \nh(5.5-engineSize)*h(mpg-21.1063)    Yes     None         \nh(mpg-78.1907)*Hybrid               Yes     None         \nh(78.1907-mpg)*Hybrid               No      2221.49      \nh(mpg-63.1632)*h(mpg-22.6767)       Yes     None         \nh(63.1632-mpg)*h(mpg-22.6767)       No      21.0093      \nHybrid*h(mpg-53.3495)               No      4121.91      \nh(mileage-22058)*h(53.3495-mpg)     No      16.6177      \nh(22058-mileage)*h(53.3495-mpg)     No      -16.6044     \nh(mpg-21.8985)                      Yes     None         \nh(21.8985-mpg)                      No      371659       \n---------------------------------------------------------\nMSE: 45859836.5623, GCV: 47884649.3622, RSQ: 0.8296, GRSQ: 0.8221\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\n\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(pred,test2.price))\n\n7499.709075454322\n\n\nLet us compare the RMSE of a MARS model with mileage, mpg, engineSize and year with a linear regression model having the same predictors.\n\nX = train[['mileage','mpg','engineSize','year']]\n\n\nmodel = Earth(max_terms=500, max_degree=2) # note, terms in brackets are the hyperparameters \nmodel.fit(X,y)\nprint(model.summary())\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\n\n\nEarth Model\n----------------------------------------------------------\nBasis Function                      Pruned  Coefficient   \n----------------------------------------------------------\n(Intercept)                         No      -8.13682e+06  \nh(engineSize-5.5)                   No      9.53908e+06   \nh(5.5-engineSize)                   Yes     None          \nh(mileage-21050)                    No      23.4448       \nh(21050-mileage)                    No      -215.861      \nh(mileage-21050)*h(5.5-engineSize)  Yes     None          \nh(21050-mileage)*h(5.5-engineSize)  No      -0.278562     \nyear                                No      4125.85       \nh(mpg-53.3495)                      Yes     None          \nh(53.3495-mpg)                      Yes     None          \nh(mileage-21050)*year               No      -0.0116601    \nh(21050-mileage)*year               No      0.107624      \nh(mpg-53.2957)*h(5.5-engineSize)    No      -59801.3      \nh(53.2957-mpg)*h(5.5-engineSize)    No      59950.5       \nh(engineSize-5.5)*year              No      -4713.74      \nh(5.5-engineSize)*year              No      -755.742      \nh(mileage-1766)*h(53.3495-mpg)      No      -0.00337072   \nh(1766-mileage)*h(53.3495-mpg)      No      -0.144905     \nh(mpg-19.1277)*h(53.3495-mpg)       No      161.153       \nh(19.1277-mpg)*h(53.3495-mpg)       Yes     None          \nh(mpg-23.4808)*h(5.5-engineSize)    Yes     None          \nh(23.4808-mpg)*h(5.5-engineSize)    Yes     None          \nh(mpg-21.4971)*h(5.5-engineSize)    Yes     None          \nh(21.4971-mpg)*h(5.5-engineSize)    Yes     None          \nh(mpg-40.224)*h(5.5-engineSize)     Yes     None          \nh(40.224-mpg)*h(5.5-engineSize)     No      298.139       \nengineSize*h(5.5-engineSize)        No      -2553.17      \nh(mpg-22.2566)                      Yes     None          \nh(22.2566-mpg)                      No      29257.3       \nh(mpg-20.7712)*h(22.2566-mpg)       No      143796        \nh(20.7712-mpg)*h(22.2566-mpg)       No      -1249.17      \nh(mpg-21.4971)*h(22.2566-mpg)       No      -315486       \nh(21.4971-mpg)*h(22.2566-mpg)       Yes     None          \nh(mpg-27.0995)*h(mpg-22.2566)       No      3855.71       \nh(27.0995-mpg)*h(mpg-22.2566)       Yes     None          \nh(mpg-29.3902)*year                 No      6.05449       \nh(29.3902-mpg)*year                 No      -20.176       \nh(mpg-28.0681)*h(5.5-engineSize)    No      59901.6       \nh(28.0681-mpg)*h(5.5-engineSize)    No      -55502.2      \nh(mpg-23.2962)*h(mpg-22.2566)       No      -56126        \nh(23.2962-mpg)*h(mpg-22.2566)       No      73153.9       \nh(mpg-69.0719)*h(mpg-53.3495)       Yes     None          \nh(69.0719-mpg)*h(mpg-53.3495)       No      -124.847      \nh(engineSize-5.5)*h(22.2566-mpg)    No      -20955.8      \nh(5.5-engineSize)*h(22.2566-mpg)    No      -8336.23      \nh(mpg-23.9595)*h(mpg-22.2566)       No      -62983        \nh(23.9595-mpg)*h(mpg-22.2566)       Yes     None          \nh(mpg-23.6406)*h(mpg-22.2566)       No      115253        \nh(23.6406-mpg)*h(mpg-22.2566)       Yes     None          \nh(mpg-56.1908)                      Yes     None          \nh(56.1908-mpg)                      No      -2239.85      \nh(mpg-29.7993)*h(53.3495-mpg)       No      -139.61       \nh(29.7993-mpg)*h(53.3495-mpg)       No      788.756       \n----------------------------------------------------------\nMSE: 49704412.0771, GCV: 51526765.3943, RSQ: 0.8153, GRSQ: 0.8086\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\n\nXtest = test[['mileage','mpg','engineSize','year']]\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n7614.158359050244\n\n\n\nols_object = smf.ols(formula = 'price~(year+engineSize+mileage+mpg)**2', data = train)\nmodel = ols_object.fit()\npred = model.predict(test)\nnp.sqrt(mean_squared_error(pred,test.price))\n\n8729.912066822455\n\n\nThe RMSE for the MARS model is lesser than that of the linear regression model, as expected."
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix A — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here"
  }
]