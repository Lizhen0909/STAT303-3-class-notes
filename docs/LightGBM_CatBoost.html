<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; LightGBM and CatBoost – Data Science III with python (Class notes)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Lec10_Ensemble.html" rel="next">
<link href="./XGBoost.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8da5b4427184b79ecddefad3d342027e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regression_tree_sp25.html">Tree based models</a></li><li class="breadcrumb-item"><a href="./LightGBM_CatBoost.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">LightGBM and CatBoost</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="https://statistics.northwestern.edu/" class="sidebar-logo-link">
      <img src="./NU_Stat_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science III with python (Class notes)</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bias &amp; Variance; KNN</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Bias_variance_code.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Bias-variance tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./KNN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">KNN</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Hyperparameter tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hyperparameter tuning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Tree based models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression_tree_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Regression trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Classification _Tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Classification trees</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bagging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bagging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./random_forest.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Random Forests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./adaboost.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Adaptive Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Gradient_Boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Gradient Boosting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./XGBoost.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">XGBoost</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LightGBM_CatBoost.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">LightGBM and CatBoost</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Lec10_Ensemble.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ensemble modeling</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Assignment1_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Assignment 1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Assignment2_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Assignment 2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Assignment3_sp25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Assignment 3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Datasets, assignment and project files</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#what-they-share-with-xgboost" id="toc-what-they-share-with-xgboost" class="nav-link active" data-scroll-target="#what-they-share-with-xgboost"><span class="header-section-number">11.1</span> What They Share with XGBoost</a></li>
  <li><a href="#lightgbm" id="toc-lightgbm" class="nav-link" data-scroll-target="#lightgbm"><span class="header-section-number">11.2</span> LightGBM</a>
  <ul>
  <li><a href="#what-is-lightgbm" id="toc-what-is-lightgbm" class="nav-link" data-scroll-target="#what-is-lightgbm"><span class="header-section-number">11.2.1</span> What is LightGBM?</a></li>
  <li><a href="#what-makes-lightgbm-unique" id="toc-what-makes-lightgbm-unique" class="nav-link" data-scroll-target="#what-makes-lightgbm-unique"><span class="header-section-number">11.2.2</span> What Makes LightGBM Unique?</a>
  <ul class="collapse">
  <li><a href="#leaf-wise-tree-growth" id="toc-leaf-wise-tree-growth" class="nav-link" data-scroll-target="#leaf-wise-tree-growth"><span class="header-section-number">11.2.2.1</span> Leaf-Wise Tree Growth</a></li>
  <li><a href="#goss-gradient-based-one-side-sampling" id="toc-goss-gradient-based-one-side-sampling" class="nav-link" data-scroll-target="#goss-gradient-based-one-side-sampling"><span class="header-section-number">11.2.2.2</span> GOSS (Gradient-based One-Side Sampling)</a></li>
  <li><a href="#efb-exclusive-feature-bundling" id="toc-efb-exclusive-feature-bundling" class="nav-link" data-scroll-target="#efb-exclusive-feature-bundling"><span class="header-section-number">11.2.2.3</span> EFB (Exclusive Feature Bundling)</a></li>
  </ul></li>
  <li><a href="#using-lightgbm" id="toc-using-lightgbm" class="nav-link" data-scroll-target="#using-lightgbm"><span class="header-section-number">11.2.3</span> Using LightGBM</a>
  <ul class="collapse">
  <li><a href="#core-lightgbm-hyperparameters" id="toc-core-lightgbm-hyperparameters" class="nav-link" data-scroll-target="#core-lightgbm-hyperparameters"><span class="header-section-number">11.2.3.1</span> Core LightGBM Hyperparameters</a></li>
  <li><a href="#building-a-baseline-model-using-lightgbms-native-categorical-feature-support" id="toc-building-a-baseline-model-using-lightgbms-native-categorical-feature-support" class="nav-link" data-scroll-target="#building-a-baseline-model-using-lightgbms-native-categorical-feature-support"><span class="header-section-number">11.2.3.2</span> Building a Baseline Model Using LightGBM’s Native Categorical Feature Support</a></li>
  <li><a href="#enabling-goss-and-efb-in-lightgbm" id="toc-enabling-goss-and-efb-in-lightgbm" class="nav-link" data-scroll-target="#enabling-goss-and-efb-in-lightgbm"><span class="header-section-number">11.2.3.3</span> Enabling GOSS and EFB in LightGBM</a></li>
  <li><a href="#tuning-top_rate-and-other_rate-in-goss" id="toc-tuning-top_rate-and-other_rate-in-goss" class="nav-link" data-scroll-target="#tuning-top_rate-and-other_rate-in-goss"><span class="header-section-number">11.2.3.4</span> Tuning <code>top_rate</code> and <code>other_rate</code> in GOSS</a></li>
  <li><a href="#optimizing-lightgbm-with-categorical-features-and-bayessearchcv" id="toc-optimizing-lightgbm-with-categorical-features-and-bayessearchcv" class="nav-link" data-scroll-target="#optimizing-lightgbm-with-categorical-features-and-bayessearchcv"><span class="header-section-number">11.2.3.5</span> Optimizing LightGBM with Categorical Features and BayesSearchCV</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#catboost" id="toc-catboost" class="nav-link" data-scroll-target="#catboost"><span class="header-section-number">11.3</span> CatBoost</a>
  <ul>
  <li><a href="#what-is-catboost" id="toc-what-is-catboost" class="nav-link" data-scroll-target="#what-is-catboost"><span class="header-section-number">11.3.1</span> What is CatBoost?</a></li>
  <li><a href="#what-makes-catboost-unique" id="toc-what-makes-catboost-unique" class="nav-link" data-scroll-target="#what-makes-catboost-unique"><span class="header-section-number">11.3.2</span> What Makes CatBoost Unique?</a>
  <ul class="collapse">
  <li><a href="#native-categorical-feature-encoding" id="toc-native-categorical-feature-encoding" class="nav-link" data-scroll-target="#native-categorical-feature-encoding"><span class="header-section-number">11.3.2.1</span> Native Categorical Feature Encoding</a></li>
  <li><a href="#ordered-boosting-vs.-standard-boosting" id="toc-ordered-boosting-vs.-standard-boosting" class="nav-link" data-scroll-target="#ordered-boosting-vs.-standard-boosting"><span class="header-section-number">11.3.2.2</span> Ordered Boosting (vs.&nbsp;Standard Boosting)</a></li>
  <li><a href="#symmetric-oblivious-trees" id="toc-symmetric-oblivious-trees" class="nav-link" data-scroll-target="#symmetric-oblivious-trees"><span class="header-section-number">11.3.2.3</span> Symmetric (Oblivious) Trees</a></li>
  </ul></li>
  <li><a href="#installing-and-using-catboost-with-scikit-learn-api" id="toc-installing-and-using-catboost-with-scikit-learn-api" class="nav-link" data-scroll-target="#installing-and-using-catboost-with-scikit-learn-api"><span class="header-section-number">11.3.3</span> Installing and Using CatBoost with Scikit-Learn API</a></li>
  <li><a href="#installation" id="toc-installation" class="nav-link" data-scroll-target="#installation"><span class="header-section-number">11.3.4</span> Installation</a></li>
  <li><a href="#catboost-for-regression" id="toc-catboost-for-regression" class="nav-link" data-scroll-target="#catboost-for-regression"><span class="header-section-number">11.3.5</span> CatBoost for Regression</a></li>
  <li><a href="#tuning-catboostregressor-with-optuna" id="toc-tuning-catboostregressor-with-optuna" class="nav-link" data-scroll-target="#tuning-catboostregressor-with-optuna"><span class="header-section-number">11.3.6</span> Tuning <code>CatBoostRegressor</code> with Optuna</a></li>
  <li><a href="#when-to-use-catboost-over-xgboost" id="toc-when-to-use-catboost-over-xgboost" class="nav-link" data-scroll-target="#when-to-use-catboost-over-xgboost"><span class="header-section-number">11.3.7</span> When to Use <strong>CatBoost</strong> Over <strong>XGBoost</strong></a></li>
  </ul></li>
  <li><a href="#handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost" id="toc-handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost" class="nav-link" data-scroll-target="#handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost"><span class="header-section-number">11.4</span> Handling Imbalanced Classification: XGBoost vs.&nbsp;LightGBM vs.&nbsp;CatBoost</a></li>
  <li><a href="#summary-xgboost-vs.-lightgbm-vs.-catboost" id="toc-summary-xgboost-vs.-lightgbm-vs.-catboost" class="nav-link" data-scroll-target="#summary-xgboost-vs.-lightgbm-vs.-catboost"><span class="header-section-number">11.5</span> Summary: XGBoost vs.&nbsp;LightGBM vs.&nbsp;CatBoost</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">11.6</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regression_tree_sp25.html">Tree based models</a></li><li class="breadcrumb-item"><a href="./LightGBM_CatBoost.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">LightGBM and CatBoost</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">LightGBM and CatBoost</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Gradient boosting is one of the most powerful techniques for structured/tabular data, and has become the <strong>go-to choice for many winning solutions in machine learning competitions</strong>.</p>
<p>In the previous chapter, we explored <strong>XGBoost</strong> in detail — covering its optimization objective, regularization techniques, split finding algorithms, and how it became a cornerstone in modern tabular modeling.</p>
<p>In this chapter, we turn our attention to two other powerful gradient boosting libraries and focus on their <strong>key innovations</strong>:</p>
<ul>
<li><p><strong>LightGBM</strong>: Developed by Microsoft, LightGBM is designed for <strong>speed and scalability</strong>. It introduces innovations like <strong>leaf-wise tree growth</strong> and <strong>histogram-based split finding</strong>, making it ideal for <strong>large datasets and high-dimensional features</strong>.</p></li>
<li><p><strong>CatBoost</strong>: Created by Yandex, CatBoost stands out for its <strong>native support for categorical features</strong> and its use of <strong>ordered boosting</strong> to prevent prediction shift. It often performs well <strong>with minimal tuning</strong>, especially on datasets rich in categorical variables.</p></li>
</ul>
<section id="what-they-share-with-xgboost" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="what-they-share-with-xgboost"><span class="header-section-number">11.1</span> What They Share with XGBoost</h2>
<p>Despite their architectural differences, both LightGBM and CatBoost share key foundations with XGBoost:</p>
<ul>
<li>They use the <strong>same objective function structure</strong> (loss + regularization)</li>
<li>They apply a <strong>second-order Taylor approximation</strong> for efficient optimization</li>
<li>They implement a <strong>histogram-based split-finding algorithm</strong> to speed up training</li>
</ul>
</section>
<section id="lightgbm" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="lightgbm"><span class="header-section-number">11.2</span> LightGBM</h2>
<section id="what-is-lightgbm" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="what-is-lightgbm"><span class="header-section-number">11.2.1</span> What is LightGBM?</h3>
<p><strong>LightGBM</strong> (Light Gradient Boosting Machine) is a high-performance gradient boosting framework developed by Microsoft in 2017. LightGBM outperforms XGBoost in terms of compuational speed, and provides comparable accuracy in general. It is designed for:</p>
<ul>
<li><strong>Large-scale datasets</strong> with many rows and features<br>
</li>
<li><strong>High speed and memory efficiency</strong>, often outperforming XGBoost in training time<br>
</li>
<li><strong>Native support for categorical features</strong> (Note: XGBoost added this starting in version 1.5.0)<br>
</li>
<li><strong>Support for parallel, distributed, and GPU training</strong> (XGBoost offers similar capabilities)</li>
</ul>
<p>Read the <a href="https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf">LightGBM paper</a> for more details.</p>
</section>
<section id="what-makes-lightgbm-unique" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="what-makes-lightgbm-unique"><span class="header-section-number">11.2.2</span> What Makes LightGBM Unique?</h3>
<p>LightGBM often outperforms XGBoost in <strong>training speed</strong> and <strong>memory efficiency</strong>, thanks to several key innovations:</p>
<section id="leaf-wise-tree-growth" class="level4" data-number="11.2.2.1">
<h4 data-number="11.2.2.1" class="anchored" data-anchor-id="leaf-wise-tree-growth"><span class="header-section-number">11.2.2.1</span> Leaf-Wise Tree Growth</h4>
<ul>
<li>LightGBM splits the <strong>leaf with the largest potential loss reduction</strong>, unlike XGBoost’s <strong>level-wise</strong> approach.</li>
<li>This leads to <strong>lower loss per tree</strong>, making learning more efficient — though it may <strong>overfit</strong> without proper regularization.</li>
<li>Main controls:
<ul>
<li><code>num_leaves</code>: primary control for tree complexity</li>
<li><code>max_depth</code>: optional constraint to prevent overfitting</li>
</ul></li>
</ul>
</section>
<section id="goss-gradient-based-one-side-sampling" class="level4" data-number="11.2.2.2">
<h4 data-number="11.2.2.2" class="anchored" data-anchor-id="goss-gradient-based-one-side-sampling"><span class="header-section-number">11.2.2.2</span> GOSS (Gradient-based One-Side Sampling)</h4>
<ul>
<li>GOSS improves speed by:
<ul>
<li><strong>Retaining all instances with large gradients</strong> (i.e., high error)</li>
<li><strong>Randomly sampling those with small gradients</strong></li>
</ul></li>
<li>This reduces the dataset size while maintaining accurate split decisions.</li>
</ul>
<p>In gradient boosting, the tree is fit to the <strong>negative gradient</strong> of the loss:</p>
<p><span class="math display">\[
r_m = -\left[ \frac{\partial L(y_i, f(x_i))}{\partial f(x_i)} \right]_{f = f_{m-1}}
\]</span></p>
<p>Observations with larger gradients have more influence on reducing the loss — GOSS prioritizes those.</p>
<ul>
<li>Hyperparameters for GOSS:
<ul>
<li><code>boosting_type='goss'</code>: activates GOSS instead of traditional random sampling</li>
<li><code>top_rate</code>: fraction of data with the largest gradients to keep (e.g., <code>0.2</code>)</li>
<li><code>other_rate</code>: fraction of data with smaller gradients to sample (e.g., <code>0.1</code>)</li>
</ul></li>
</ul>
</section>
<section id="efb-exclusive-feature-bundling" class="level4" data-number="11.2.2.3">
<h4 data-number="11.2.2.3" class="anchored" data-anchor-id="efb-exclusive-feature-bundling"><span class="header-section-number">11.2.2.3</span> EFB (Exclusive Feature Bundling)</h4>
<ul>
<li>EFB compresses <strong>high-dimensional sparse feature spaces</strong> by bundling features that are <strong>mutually exclusive</strong> (i.e., rarely non-zero at the same time).</li>
<li>This is particularly effective in datasets with <strong>many categorical variables</strong> or <strong>one-hot encoded features</strong>.</li>
</ul>
<p>Example:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>feature1</th>
<th>feature2</th>
<th>feature_bundle</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>2</td>
<td>6</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>5</td>
</tr>
<tr class="odd">
<td>0</td>
<td>2</td>
<td>6</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0</td>
<td>2</td>
</tr>
<tr class="even">
<td>3</td>
<td>0</td>
<td>3</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>Here, <code>feature1</code> and <code>feature2</code> never overlap in non-zero values, so they can be safely merged into a single bundled feature.</p>
<ul>
<li>Hyperparameter for EFB:
<ul>
<li><code>enable_bundle</code>: set to <code>true</code> (default) to enable automatic exclusive feature bundling</li>
</ul></li>
</ul>
<p>Together, these optimizations make LightGBM especially well-suited for <strong>large-scale, sparse, tabular datasets</strong>, offering both <strong>speed and scalability</strong> without significant loss in accuracy.</p>
</section>
</section>
<section id="using-lightgbm" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="using-lightgbm"><span class="header-section-number">11.2.3</span> Using LightGBM</h3>
<p>Although <strong>LightGBM is not part of Scikit-learn</strong>, it provides a <strong>Scikit-learn-compatible API</strong> through the <code>lightgbm.sklearn</code> module. This allows you to use LightGBM models seamlessly with Scikit-learn tools such as <code>Pipeline</code>, <code>GridSearchCV</code>, and <code>cross_val_score</code>.</p>
<p>The main classes are:</p>
<ul>
<li><a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html"><code>LGBMRegressor</code></a>: for regression tasks<br>
</li>
<li><a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html"><code>LGBMClassifier</code></a>: for classification tasks</li>
</ul>
<p>To install the package:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>pip install lightgbm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p><strong>Note:</strong> LightGBM is a separate library, not part of Scikit-learn, but it provides a <strong>Scikit-learn-compatible API</strong> via <code>LGBMClassifier</code> and <code>LGBMRegressor</code>.<br>
This makes it easy to integrate LightGBM models into Scikit-learn workflows such as <code>Pipeline</code>, <code>GridSearchCV</code>, and <code>cross_val_score</code>.</p>
</blockquote>
<section id="core-lightgbm-hyperparameters" class="level4" data-number="11.2.3.1">
<h4 data-number="11.2.3.1" class="anchored" data-anchor-id="core-lightgbm-hyperparameters"><span class="header-section-number">11.2.3.1</span> Core LightGBM Hyperparameters</h4>
<p><strong>Core Tree Structure</strong>:</p>
<ul>
<li><code>num_leaves</code>: Maximum number of leaves (terminal nodes) per tree.</li>
<li><code>min_data_in_leaf</code>: Minimum number of data points required in a leaf.</li>
<li><code>max_depth</code>: Maximum depth of a tree (used to control overfitting).</li>
</ul>
<p><strong>Learning Control and Regularization</strong>:</p>
<ul>
<li><code>learning_rate (η)</code>: Shrinks the contribution of each tree.</li>
<li><code>n_estimators</code>: Number of boosting rounds.</li>
<li><code>lambda_l1</code> / <code>lambda_l2</code>: L1 and L2 regularization on leaf weights.</li>
<li><code>min_gain_to_split</code>: Minimum loss reduction required to make a further split (structure regularization).</li>
</ul>
<p><strong>Data Handling</strong>:</p>
<ul>
<li><code>feature_fraction</code>: Fraction of features randomly sampled for each tree (a.k.a. <code>colsample_bytree</code> in XGBoost).</li>
<li><code>bagging_fraction</code>: Fraction of data randomly sampled for each iteration.</li>
<li><code>bagging_freq</code>: Frequency (in iterations) to perform bagging.</li>
<li><code>categorical_feature</code>: Specifies which features are categorical (enables native handling).</li>
</ul>
<p><strong>Speed vs.&nbsp;Accuracy Trade-offs</strong>:</p>
<ul>
<li><code>max_bin</code>: Number of bins used to bucket continuous features.</li>
<li><code>data_sample_strategy</code> : <code>bagging</code> or <code>goss</code></li>
<li><code>top_rate</code> <em>(<code>goss</code> only)</em>: Fraction of instances with the largest gradients to keep.</li>
<li><code>other_rate</code> <em>(<code>goss</code> only)</em>: Fraction of small-gradient instances to randomly sample. -<code>enable_bundle</code>: set this to true to spped up the training for sparse datasets</li>
</ul>
<p><strong>Optimization Control</strong>:</p>
<ul>
<li><code>boosting</code>: Type of boosting algorithm (<code>gbdt</code>, <code>dart</code>, <code>rf</code>, etc.).</li>
<li><code>early_stopping_rounds</code>: Stops training if the validation score doesn’t improve over a set number of rounds.</li>
</ul>
<p><strong>Imbalanced Data</strong></p>
<ul>
<li><code>scale_pos_weight</code>: Manually sets the weight for the positive class in binary classification.</li>
<li><code>is_unbalance</code>: Automatically adjusts class weights based on the training data distribution.</li>
</ul>
<blockquote class="blockquote">
<p>⚠️ These two options are <strong>mutually exclusive</strong> — use <strong>only one</strong>. If both are set, <code>scale_pos_weight</code> takes priority.</p>
</blockquote>
<p>For full details and advanced options, see the <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html">LightGBM Parameters Guide</a>.</p>
<div id="f819f995" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, GridSearchCV</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder, StandardScaler</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBRegressor, XGBClassifier</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> lightgbm <span class="im">as</span> lgb</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt <span class="im">import</span> BayesSearchCV</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt.space <span class="im">import</span> Real, Categorical, Integer</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skopt.plots <span class="im">import</span> plot_objective, plot_histogram, plot_convergence</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll continue to use the same datasets that we have been using throughout the course.</p>
<div id="a9036ef3" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>car <span class="op">=</span> pd.read_csv(<span class="st">'Datasets/car.csv'</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>car.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">brand</th>
<th data-quarto-table-cell-role="th">model</th>
<th data-quarto-table-cell-role="th">year</th>
<th data-quarto-table-cell-role="th">transmission</th>
<th data-quarto-table-cell-role="th">mileage</th>
<th data-quarto-table-cell-role="th">fuelType</th>
<th data-quarto-table-cell-role="th">tax</th>
<th data-quarto-table-cell-role="th">mpg</th>
<th data-quarto-table-cell-role="th">engineSize</th>
<th data-quarto-table-cell-role="th">price</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>vw</td>
<td>Beetle</td>
<td>2014</td>
<td>Manual</td>
<td>55457</td>
<td>Diesel</td>
<td>30</td>
<td>65.3266</td>
<td>1.6</td>
<td>7490</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>vauxhall</td>
<td>GTC</td>
<td>2017</td>
<td>Manual</td>
<td>15630</td>
<td>Petrol</td>
<td>145</td>
<td>47.2049</td>
<td>1.4</td>
<td>10998</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>merc</td>
<td>G Class</td>
<td>2012</td>
<td>Automatic</td>
<td>43000</td>
<td>Diesel</td>
<td>570</td>
<td>25.1172</td>
<td>3.0</td>
<td>44990</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>audi</td>
<td>RS5</td>
<td>2019</td>
<td>Automatic</td>
<td>10</td>
<td>Petrol</td>
<td>145</td>
<td>30.5593</td>
<td>2.9</td>
<td>51990</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>merc</td>
<td>X-CLASS</td>
<td>2018</td>
<td>Automatic</td>
<td>14000</td>
<td>Diesel</td>
<td>240</td>
<td>35.7168</td>
<td>2.3</td>
<td>28990</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="db6b5f99" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> car.drop(columns<span class="op">=</span>[<span class="st">'price'</span>])</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> car[<span class="st">'price'</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the categorical columns and put them in a list</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>categorical_feature <span class="op">=</span> X.select_dtypes(include<span class="op">=</span>[<span class="st">'object'</span>]).columns.tolist()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the numerical columns and put them in a list</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>numerical_feature <span class="op">=</span> X.select_dtypes(include<span class="op">=</span>[<span class="st">'int64'</span>, <span class="st">'float64'</span>]).columns.tolist()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the categorical columns to category type</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> categorical_feature:</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    X[col] <span class="op">=</span> X[col].astype(<span class="st">'category'</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="building-a-baseline-model-using-lightgbms-native-categorical-feature-support" class="level4" data-number="11.2.3.2">
<h4 data-number="11.2.3.2" class="anchored" data-anchor-id="building-a-baseline-model-using-lightgbms-native-categorical-feature-support"><span class="header-section-number">11.2.3.2</span> Building a Baseline Model Using LightGBM’s Native Categorical Feature Support</h4>
<p>LightGBM provides <strong>built-in support for handling categorical features</strong>, eliminating the need for manual encoding (like one-hot or ordinal encoding). By directly passing categorical column names or indices to the model, LightGBM can internally apply efficient encoding and optimized split finding for categorical variables.</p>
<p>In this section, we’ll use this native capability to <strong>quickly build a baseline model</strong>, taking advantage of LightGBM’s efficiency with structured data that includes categorical columns.</p>
<p>This baseline model serves as a <strong>starting point</strong> for comparison against more advanced tuning</p>
<div id="38736472" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== 1. Baseline Model =====</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">===== Baseline LightGBM Model ====="</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the LightGBM regressor</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lgb.LGBMRegressor(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with categorical features specified</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>model.fit(</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    X_train, </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    y_train,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    categorical_feature<span class="op">=</span>categorical_feature</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the test set</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate evaluation metrics</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> root_mean_squared_error(y_test, y_pred)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r2_score(y_test, y_pred)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Output results</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE: </span><span class="sc">{</span>rmse<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R²: </span><span class="sc">{</span>r2<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
===== Baseline LightGBM Model =====
Test RMSE: 3680.8999
Test R²: 0.9538
CPU times: total: 875 ms
Wall time: 82.6 ms</code></pre>
</div>
</div>
</section>
<section id="enabling-goss-and-efb-in-lightgbm" class="level4" data-number="11.2.3.3">
<h4 data-number="11.2.3.3" class="anchored" data-anchor-id="enabling-goss-and-efb-in-lightgbm"><span class="header-section-number">11.2.3.3</span> Enabling GOSS and EFB in LightGBM</h4>
<p>By default, LightGBM uses:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>data_sample_strategy <span class="op">=</span> <span class="st">'bagging'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To enable <strong>GOSS (Gradient-based One-Side Sampling)</strong> — a faster sampling strategy that prioritizes high-gradient instances — set:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>boosting_type <span class="op">=</span> <span class="st">'goss'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>When using GOSS, you should also configure:</p>
<ul>
<li><p><code>top_rate</code>: Fraction of data with the largest gradients to retain (e.g., 0.2)</p></li>
<li><p><code>other_rate</code>: Fraction of small-gradient data to randomly sample (e.g., 0.1)</p></li>
</ul>
<p>LightGBM also enables <strong>EFB (Exclusive Feature Bundling)</strong> by default:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>enable_bundle <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This optimization reduces dimensionality by bundling mutually exclusive sparse features, such as those resulting from one-hot encoding.</p>
<p>⚠️ Note: In our car dataset, the data size is small and there are only a few categorical features, so these optimizations may not have a noticeable impact. However, for large-scale datasets with many categorical features, enabling GOSS and EFB is highly recommended to improve training efficiency and reduce memory usage.</p>
<div id="e1677750" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== 2. LightGBM with GOSS Sampling =====</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">===== LightGBM with GOSS Sampling ====="</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the LightGBM regressor with GOSS</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>model_goss <span class="op">=</span> lgb.LGBMRegressor(</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    boosting_type<span class="op">=</span><span class="st">'goss'</span>,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with categorical features specified</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>model_goss.fit(</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    X_train,</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    y_train,</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    categorical_feature<span class="op">=</span>categorical_feature</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the test set</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>y_pred_goss <span class="op">=</span> model_goss.predict(X_test)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate evaluation metrics</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>rmse_goss <span class="op">=</span> root_mean_squared_error(y_test, y_pred_goss)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>r2_goss <span class="op">=</span> r2_score(y_test, y_pred_goss)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Output results</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE (GOSS): </span><span class="sc">{</span>rmse_goss<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R² (GOSS): </span><span class="sc">{</span>r2_goss<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
===== LightGBM with GOSS Sampling =====
Test RMSE (GOSS): 3510.7726
Test R² (GOSS): 0.9580
CPU times: total: 766 ms
Wall time: 79.6 ms</code></pre>
</div>
</div>
</section>
<section id="tuning-top_rate-and-other_rate-in-goss" class="level4" data-number="11.2.3.4">
<h4 data-number="11.2.3.4" class="anchored" data-anchor-id="tuning-top_rate-and-other_rate-in-goss"><span class="header-section-number">11.2.3.4</span> Tuning <code>top_rate</code> and <code>other_rate</code> in GOSS</h4>
<p>Even with this small dataset, we observed a <strong>shorter execution time</strong> and a <strong>slight improvement in performance</strong> using GOSS. Next, we’ll tune the <code>top_rate</code> and <code>other_rate</code> parameters to see if we can further boost the model’s performance.</p>
<blockquote class="blockquote">
<p>⚠️ <strong>Note:</strong> When using <code>boosting_type='goss'</code>, LightGBM requires that<br>
<strong><code>top_rate + other_rate ≤ 1.0</code></strong><br>
This constraint ensures that the combined sample used for training does not exceed the size of the full dataset.</p>
</blockquote>
<div id="09e449da" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tuning the top_rate and other_rate parameters</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the LightGBM regressor with GOSS</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>model_goss_tune <span class="op">=</span> lgb.LGBMRegressor(</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    boosting_type<span class="op">=</span><span class="st">'goss'</span>,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameter grid for tuning</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'top_rate'</span>: Real(<span class="fl">0.1</span>, <span class="fl">0.6</span>, prior<span class="op">=</span><span class="st">'uniform'</span>),</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'other_rate'</span>: Real(<span class="fl">0.1</span>, <span class="fl">0.4</span>, prior<span class="op">=</span><span class="st">'uniform'</span>),</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the BayesSearchCV object</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> BayesSearchCV(</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    model_goss_tune,</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    param_grid,</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>opt.fit(</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    X_train,</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    y_train,</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    categorical_feature<span class="op">=</span>categorical_feature</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="co"># the best parameters</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters found: "</span>, opt.best_params_)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the test set</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>y_pred_opt <span class="op">=</span> opt.predict(X_test)</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate evaluation metrics</span></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>rmse_opt <span class="op">=</span> root_mean_squared_error(y_test, y_pred_opt)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>r2_opt <span class="op">=</span> r2_score(y_test, y_pred_opt)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Output results</span></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE (GOSS with tuning): </span><span class="sc">{</span>rmse_opt<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R² (GOSS with tuning): </span><span class="sc">{</span>r2_opt<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best parameters found:  OrderedDict({'other_rate': 0.33986603248215197, 'top_rate': 0.31901459322046166})
Test RMSE (GOSS with tuning): 3458.7664
Test R² (GOSS with tuning): 0.9592</code></pre>
</div>
</div>
</section>
<section id="optimizing-lightgbm-with-categorical-features-and-bayessearchcv" class="level4" data-number="11.2.3.5">
<h4 data-number="11.2.3.5" class="anchored" data-anchor-id="optimizing-lightgbm-with-categorical-features-and-bayessearchcv"><span class="header-section-number">11.2.3.5</span> Optimizing LightGBM with Categorical Features and BayesSearchCV</h4>
<p><code>BayesSearchCV</code> from <code>scikit-optimize</code> provides an efficient way to tune hyperparameters. Here’s how to set this up:</p>
<div id="35e60b3b" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ===== 2. Hyperparameter Tuning with Bayesian Optimization =====</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameter space for Bayesian optimization</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>param_space <span class="op">=</span> {</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'num_leaves'</span>: Integer(<span class="dv">20</span>, <span class="dv">100</span>),</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: Integer(<span class="dv">5</span>, <span class="dv">50</span>),</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_data_in_leaf'</span>: Integer(<span class="dv">1</span>, <span class="dv">100</span>),</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: Real(<span class="fl">0.01</span>, <span class="fl">0.5</span>, prior<span class="op">=</span><span class="st">'uniform'</span>),</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: Integer(<span class="dv">50</span>, <span class="dv">500</span>),</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'top_rate'</span>: Real(<span class="fl">0.1</span>, <span class="fl">0.6</span>, prior<span class="op">=</span><span class="st">'uniform'</span>),</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'other_rate'</span>: Real(<span class="fl">0.1</span>, <span class="fl">0.4</span>, prior<span class="op">=</span><span class="st">'uniform'</span>),</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the Bayesian search object</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>bayes_search <span class="op">=</span> BayesSearchCV(</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># using verbose=-1 to suppress warnings</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># using n_jobs=-1 to use all available cores</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># using random_state=42 for reproducibility</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>lgb.LGBMRegressor( categorical_feature<span class="op">=</span>categorical_feature, random_state<span class="op">=</span><span class="dv">42</span>, boosting_type<span class="op">=</span><span class="st">'goss'</span>, verbose<span class="op">=-</span><span class="dv">1</span>),</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define the parameter space for Bayesian optimization</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    search_spaces<span class="op">=</span>param_space,</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_root_mean_squared_error'</span>,</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the Bayesian search object to the training data</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>bayes_search.fit(X_train, y_train)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best parameters and score</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> bayes_search.best_params_</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> bayes_search.best_score_</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best Parameters: </span><span class="sc">{</span>best_params<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best Score: </span><span class="sc">{</span>best_score<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best model</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> bayes_search.best_estimator_</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test set</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>y_pred_bayes <span class="op">=</span> best_model.predict(X_test)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate RMSE and R2 score for the best model</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>rmse_bayes <span class="op">=</span> root_mean_squared_error(y_test, y_pred_bayes)</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>r2_bayes <span class="op">=</span> r2_score(y_test, y_pred_bayes)</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE (Bayesian Optimized): </span><span class="sc">{</span>rmse_bayes<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R2 Score (Bayesian Optimized): </span><span class="sc">{</span>r2_bayes<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best Parameters: OrderedDict({'learning_rate': 0.31777940485083805, 'max_depth': 5, 'min_data_in_leaf': 47, 'n_estimators': 369, 'num_leaves': 20, 'other_rate': 0.4, 'top_rate': 0.6})
Best Score: -3361.8218393725633
RMSE (Bayesian Optimized): 3071.418344800289
R2 Score (Bayesian Optimized): 0.9678447743461689
CPU times: total: 49.4 s
Wall time: 1min 35s</code></pre>
</div>
</div>
<p>Using GOSS and Feature Estimation by Bagging (FEB) led to a slight improvement in performance compared to XGBoost, while also reducing the time required for cross-validation tuning.</p>
</section>
</section>
</section>
<section id="catboost" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="catboost"><span class="header-section-number">11.3</span> CatBoost</h2>
<section id="what-is-catboost" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="what-is-catboost"><span class="header-section-number">11.3.1</span> What is CatBoost?</h3>
<p><strong>CatBoost</strong> (short for <em>Categorical Boosting</em>) is a high-performance gradient boosting framework developed by <strong>Yandex</strong>, specifically designed to handle datasets with <strong>categorical features</strong> without requiring manual preprocessing.</p>
<p>Like XGBoost and LightGBM, it is based on gradient boosting over decision trees, but CatBoost introduces <strong>key innovations</strong> that make it robust, easy to use, and effective out of the box—particularly on tabular data.</p>
</section>
<section id="what-makes-catboost-unique" class="level3" data-number="11.3.2">
<h3 data-number="11.3.2" class="anchored" data-anchor-id="what-makes-catboost-unique"><span class="header-section-number">11.3.2</span> What Makes CatBoost Unique?</h3>
<p>CatBoost offers several innovations that distinguish it from other boosting frameworks:</p>
<section id="native-categorical-feature-encoding" class="level4" data-number="11.3.2.1">
<h4 data-number="11.3.2.1" class="anchored" data-anchor-id="native-categorical-feature-encoding"><span class="header-section-number">11.3.2.1</span> Native Categorical Feature Encoding</h4>
<p>CatBoost can <strong>natively process categorical features</strong> using an approach based on <strong>ordered target statistics</strong>, which:</p>
<ul>
<li>Avoids target leakage during training</li>
<li>Typically outperforms traditional encodings like one-hot or label encoding</li>
<li>Requires <strong>no manual preprocessing</strong> — simply specify the categorical columns</li>
</ul>
</section>
<section id="ordered-boosting-vs.-standard-boosting" class="level4" data-number="11.3.2.2">
<h4 data-number="11.3.2.2" class="anchored" data-anchor-id="ordered-boosting-vs.-standard-boosting"><span class="header-section-number">11.3.2.2</span> Ordered Boosting (vs.&nbsp;Standard Boosting)</h4>
<p>Traditional gradient boosting algorithms often suffer from <strong>prediction shift</strong>, a form of overfitting that occurs when the model uses the same data to compute residuals and to fit new trees.</p>
<p>CatBoost addresses this with <strong>ordered boosting</strong>, a permutation-driven strategy that builds each tree on one subset of data and computes residuals on another (unseen) subset.</p>
<p>Recall that gradient boosting fits trees on the gradient of the loss function:</p>
<p><span class="math display">\[
r_m = -\left[ \frac{\partial L(y_i, f(x_i))}{\partial f(x_i)} \right]_{f = f_{m-1}}
\]</span></p>
<p>In classic boosting, this gradient is calculated using the same training observations that were used to fit the model, which leads to target leakage.</p>
<p>In contrast, CatBoost:</p>
<ul>
<li>Shuffles the data at each iteration</li>
<li>Computes residuals for an observation <strong>only from prior observations</strong> in the permutation</li>
<li>Ensures that <strong>each gradient estimate is based on unseen data</strong></li>
</ul>
<p>This significantly improves the model’s <strong>generalizability</strong> and reduces overfitting, especially on <strong>small or noisy datasets</strong>.</p>
</section>
<section id="symmetric-oblivious-trees" class="level4" data-number="11.3.2.3">
<h4 data-number="11.3.2.3" class="anchored" data-anchor-id="symmetric-oblivious-trees"><span class="header-section-number">11.3.2.3</span> Symmetric (Oblivious) Trees</h4>
<p>CatBoost builds <strong>symmetric (oblivious) decision trees</strong>, where the same splitting condition is applied across each level of the tree. This structure results in:</p>
<ul>
<li><strong>Faster inference times</strong></li>
<li><strong>Compact model size</strong></li>
<li><strong>Improved regularization</strong>, due to the constrained tree structure</li>
</ul>
<p>These trees are particularly well-suited for deployment scenarios where prediction speed matters.</p>
<p>Together, these innovations make CatBoost a strong candidate for modeling <strong>high-dimensional, categorical, and imbalanced tabular data</strong>, even with minimal feature engineering or hyperparameter tuning.</p>
<p>The authors have also shown that CatBoost performs better than XGBoost and LightGBM without tuning, i.e., with default hyperparameter settings.</p>
<p>Read the <a href="https://proceedings.neurips.cc/paper_files/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf">CatBoost paper</a> for more details.</p>
<p>Here is a good <a href="https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm">blog</a> listing the key features of CatBoost.</p>
</section>
</section>
<section id="installing-and-using-catboost-with-scikit-learn-api" class="level3" data-number="11.3.3">
<h3 data-number="11.3.3" class="anchored" data-anchor-id="installing-and-using-catboost-with-scikit-learn-api"><span class="header-section-number">11.3.3</span> Installing and Using CatBoost with Scikit-Learn API</h3>
<p>CatBoost provides a <strong>scikit-learn-compatible API</strong> through <code>CatBoostClassifier</code> and <code>CatBoostRegressor</code>, which makes it easy to integrate into pipelines and use with tools like <code>GridSearchCV</code>, <code>cross_val_score</code>, and <code>train_test_split</code>.</p>
</section>
<section id="installation" class="level3" data-number="11.3.4">
<h3 data-number="11.3.4" class="anchored" data-anchor-id="installation"><span class="header-section-number">11.3.4</span> Installation</h3>
<p>To install CatBoost, run:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>pip install catboost</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>💡 GPU users: CatBoost automatically detects and uses GPU if available. You can explicitly enable it with <code>task_type='GPU'</code>.</p>
</blockquote>
</section>
<section id="catboost-for-regression" class="level3" data-number="11.3.5">
<h3 data-number="11.3.5" class="anchored" data-anchor-id="catboost-for-regression"><span class="header-section-number">11.3.5</span> CatBoost for Regression</h3>
<p>Let us check the performance of <code>CatBoostRegressor()</code> without tuning, i.e., with default hyperparameter settings on our car dataset</p>
<p>The parameter <code>cat_features</code> will be used to specify the indices of the categorical predictors for target encoding.</p>
<div id="bf0a6c1d" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># build a catboostregressor model</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> catboost <span class="im">import</span> CatBoostRegressor</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the CatBoost regressor</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>model_cat <span class="op">=</span> CatBoostRegressor(</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    cat_features<span class="op">=</span>categorical_feature,</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    random_seed<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>model_cat.fit(X_train, y_train)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the test set</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>y_pred_cat <span class="op">=</span> model_cat.predict(X_test)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate evaluation metrics</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>rmse_cat <span class="op">=</span> root_mean_squared_error(y_test, y_pred_cat)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>r2_cat <span class="op">=</span> r2_score(y_test, y_pred_cat)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Output results</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE (CatBoost): </span><span class="sc">{</span>rmse_cat<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R² (CatBoost): </span><span class="sc">{</span>r2_cat<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test RMSE (CatBoost): 3307.2604
Test R² (CatBoost): 0.9627</code></pre>
</div>
</div>
<p>Even with default hyperparameter settings, CatBoost has outperformed both XGBoost and LightGBM in terms of test RMSE and R-squared.</p>
</section>
<section id="tuning-catboostregressor-with-optuna" class="level3" data-number="11.3.6">
<h3 data-number="11.3.6" class="anchored" data-anchor-id="tuning-catboostregressor-with-optuna"><span class="header-section-number">11.3.6</span> Tuning <code>CatBoostRegressor</code> with Optuna</h3>
<p>You can tune the hyperparameters of <code>CatBoostRegressor</code> using Optuna, just as you would for XGBoost or LightGBM. However, CatBoost uses a different set of hyperparameters.</p>
<p>For example, it does <strong>not</strong> include:</p>
<ul>
<li><code>reg_alpha</code>: L1 regularization on leaf weights<br>
</li>
<li><code>colsample_bytree</code>: Subsample ratio of columns when constructing each tree</li>
</ul>
<p>These parameters are available in XGBoost and LightGBM but are not part of CatBoost’s configuration.</p>
<div id="a6403528" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> optuna <span class="im">import</span> create_study</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> catboost <span class="im">import</span> CatBoostRegressor, Pool</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create a validation set for early stopping</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>X_train, X_valid, y_train, y_valid <span class="op">=</span> train_test_split(X_train, y_train, test_size<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">#convert to Catboost pool</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>train_pool <span class="op">=</span> Pool(X_train, y_train, cat_features<span class="op">=</span>categorical_feature)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>valid_pool <span class="op">=</span> Pool(X_valid, y_valid, cat_features<span class="op">=</span>categorical_feature)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the objective function for Optuna</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(trial):</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define the hyperparameters to tune</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> {</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">'learning_rate'</span>: trial.suggest_float(<span class="st">'learning_rate'</span>, <span class="fl">0.01</span>, <span class="fl">0.3</span>),</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">'depth'</span>: trial.suggest_int(<span class="st">'depth'</span>, <span class="dv">4</span>, <span class="dv">10</span>),</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">'l2_leaf_reg'</span>: trial.suggest_float(<span class="st">'l2_leaf_reg'</span>, <span class="fl">1e-8</span>, <span class="fl">10.0</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">'min_data_in_leaf'</span>: trial.suggest_int(<span class="st">'min_data_in_leaf'</span>, <span class="dv">1</span>, <span class="dv">30</span>),</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        <span class="st">'border_count'</span>: trial.suggest_int(<span class="st">'border_count'</span>, <span class="dv">32</span>, <span class="dv">255</span>),</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">'bagging_temperature'</span>: trial.suggest_float(<span class="st">'bagging_temperature'</span>, <span class="fl">0.0</span>, <span class="fl">1.0</span>),</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">'random_strength'</span>: trial.suggest_float(<span class="st">'random_strength'</span>, <span class="fl">1e-8</span>, <span class="fl">10.0</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">'grow_policy'</span>: trial.suggest_categorical(<span class="st">'grow_policy'</span>, [<span class="st">'SymmetricTree'</span>, <span class="st">'Depthwise'</span>, <span class="st">'Lossguide'</span>]),</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fixed parameters</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        <span class="st">'iterations'</span>: <span class="dv">3000</span>,  <span class="co"># Set to a high number, early stopping will determine the actual number</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">'verbose'</span>: <span class="va">False</span>,</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="st">'random_seed'</span>: <span class="dv">42</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create and train the model with early stopping</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> CatBoostRegressor(<span class="op">**</span>params)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use early stopping to prevent overfitting</span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>    model.fit(</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        train_pool,</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>        eval_set<span class="op">=</span>valid_pool,</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>        early_stopping_rounds<span class="op">=</span><span class="dv">20</span>,  <span class="co"># Stop if no improvement for 50 rounds</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate on validation set</span></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(valid_pool)</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>    val_rmse <span class="op">=</span> root_mean_squared_error(y_valid, y_pred)</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return negative RMSE (for maximization)</span></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>val_rmse</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and run the study</span></span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study(direction<span class="op">=</span><span class="st">'maximize'</span>)</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>study.optimize(objective, n_trials<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[I 2025-05-14 03:37:01,074] A new study created in memory with name: no-name-782044d9-7185-49f6-a68b-ddc83fa639a4
[I 2025-05-14 03:37:20,600] Trial 0 finished with value: -2592.5059280623623 and parameters: {'learning_rate': 0.07952470550019325, 'depth': 8, 'l2_leaf_reg': 0.001114023475261074, 'min_data_in_leaf': 27, 'border_count': 83, 'bagging_temperature': 0.5317851293779137, 'random_strength': 0.0005861214502486961, 'grow_policy': 'Depthwise'}. Best is trial 0 with value: -2592.5059280623623.
[I 2025-05-14 03:37:31,837] Trial 1 finished with value: -2556.728680370376 and parameters: {'learning_rate': 0.245419708648389, 'depth': 7, 'l2_leaf_reg': 3.3386247434742167, 'min_data_in_leaf': 15, 'border_count': 100, 'bagging_temperature': 0.5050586506217019, 'random_strength': 0.0004718975881753086, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:37:40,883] Trial 2 finished with value: -2682.6259877001603 and parameters: {'learning_rate': 0.1520719761941552, 'depth': 8, 'l2_leaf_reg': 0.0046149514168069014, 'min_data_in_leaf': 8, 'border_count': 178, 'bagging_temperature': 0.24466088131049457, 'random_strength': 0.015640185801924847, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:38:15,606] Trial 3 finished with value: -2808.7037865332763 and parameters: {'learning_rate': 0.06614862212494911, 'depth': 10, 'l2_leaf_reg': 0.7245780669681481, 'min_data_in_leaf': 19, 'border_count': 34, 'bagging_temperature': 0.8902029685133379, 'random_strength': 0.013193287592498581, 'grow_policy': 'SymmetricTree'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:38:30,461] Trial 4 finished with value: -2740.549186918632 and parameters: {'learning_rate': 0.2585298479851385, 'depth': 7, 'l2_leaf_reg': 0.12149380817119523, 'min_data_in_leaf': 24, 'border_count': 61, 'bagging_temperature': 0.7820109206688168, 'random_strength': 0.00013909547722379054, 'grow_policy': 'SymmetricTree'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:39:21,344] Trial 5 finished with value: -2699.921784155897 and parameters: {'learning_rate': 0.1558390710067145, 'depth': 8, 'l2_leaf_reg': 2.8639962432617574e-06, 'min_data_in_leaf': 21, 'border_count': 129, 'bagging_temperature': 0.27054829342202646, 'random_strength': 1.0090519644888372, 'grow_policy': 'Lossguide'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:39:33,102] Trial 6 finished with value: -2761.634505100397 and parameters: {'learning_rate': 0.29273701564295207, 'depth': 8, 'l2_leaf_reg': 0.0001383950532512802, 'min_data_in_leaf': 29, 'border_count': 53, 'bagging_temperature': 0.543935579774364, 'random_strength': 1.3150608041342185e-05, 'grow_policy': 'SymmetricTree'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:39:42,048] Trial 7 finished with value: -2796.3445154770925 and parameters: {'learning_rate': 0.1985854921483664, 'depth': 5, 'l2_leaf_reg': 0.0017893237819096464, 'min_data_in_leaf': 24, 'border_count': 228, 'bagging_temperature': 0.6563761366663239, 'random_strength': 3.830259157708319e-05, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:40:06,522] Trial 8 finished with value: -2736.652008119944 and parameters: {'learning_rate': 0.12981791308255294, 'depth': 9, 'l2_leaf_reg': 3.6626986581511205, 'min_data_in_leaf': 14, 'border_count': 169, 'bagging_temperature': 0.26693935389565604, 'random_strength': 6.108854259908809e-08, 'grow_policy': 'SymmetricTree'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:40:33,641] Trial 9 finished with value: -2625.6156037888204 and parameters: {'learning_rate': 0.05266635206771846, 'depth': 10, 'l2_leaf_reg': 8.431301598557988e-05, 'min_data_in_leaf': 22, 'border_count': 38, 'bagging_temperature': 0.20458732239577748, 'random_strength': 0.00026802410291762685, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:40:53,462] Trial 10 finished with value: -2802.6759048814906 and parameters: {'learning_rate': 0.2321480916058081, 'depth': 5, 'l2_leaf_reg': 1.9294653593783387e-08, 'min_data_in_leaf': 2, 'border_count': 110, 'bagging_temperature': 0.024153475153251502, 'random_strength': 3.071215792269348e-07, 'grow_policy': 'Lossguide'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:41:09,188] Trial 11 finished with value: -2626.5794465595454 and parameters: {'learning_rate': 0.0931538267421292, 'depth': 7, 'l2_leaf_reg': 0.044383501183871764, 'min_data_in_leaf': 13, 'border_count': 94, 'bagging_temperature': 0.46970044709843706, 'random_strength': 0.010496613104965287, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:42:21,080] Trial 12 finished with value: -2623.594568822965 and parameters: {'learning_rate': 0.013207489266983119, 'depth': 6, 'l2_leaf_reg': 6.27769599914231e-07, 'min_data_in_leaf': 10, 'border_count': 82, 'bagging_temperature': 0.4901244457515505, 'random_strength': 2.391770871806472e-06, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:42:39,928] Trial 13 finished with value: -2840.1838885804445 and parameters: {'learning_rate': 0.20525833721959236, 'depth': 4, 'l2_leaf_reg': 5.674336337792675, 'min_data_in_leaf': 29, 'border_count': 137, 'bagging_temperature': 0.672572957300273, 'random_strength': 0.0022624464321546684, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:42:58,745] Trial 14 finished with value: -2655.490477601683 and parameters: {'learning_rate': 0.10736768228799226, 'depth': 6, 'l2_leaf_reg': 0.01831562504579733, 'min_data_in_leaf': 17, 'border_count': 82, 'bagging_temperature': 0.4256033457922353, 'random_strength': 6.775957335351946, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:43:07,127] Trial 15 finished with value: -2789.7188981990007 and parameters: {'learning_rate': 0.17513907427821254, 'depth': 9, 'l2_leaf_reg': 6.515138402249421e-06, 'min_data_in_leaf': 7, 'border_count': 169, 'bagging_temperature': 0.9865139963615146, 'random_strength': 0.14575672222120323, 'grow_policy': 'Depthwise'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:43:41,175] Trial 16 finished with value: -2621.361959827787 and parameters: {'learning_rate': 0.2821616638222643, 'depth': 6, 'l2_leaf_reg': 0.33163654681962784, 'min_data_in_leaf': 26, 'border_count': 115, 'bagging_temperature': 0.6342355083316009, 'random_strength': 0.001346200849628001, 'grow_policy': 'Lossguide'}. Best is trial 1 with value: -2556.728680370376.
[I 2025-05-14 03:44:43,383] Trial 17 finished with value: -2556.489614367946 and parameters: {'learning_rate': 0.01940229207182844, 'depth': 9, 'l2_leaf_reg': 0.0010525861450567827, 'min_data_in_leaf': 12, 'border_count': 72, 'bagging_temperature': 0.3496687786887461, 'random_strength': 4.041308474367292e-06, 'grow_policy': 'Depthwise'}. Best is trial 17 with value: -2556.489614367946.
[I 2025-05-14 03:45:42,592] Trial 18 finished with value: -2866.4522097973445 and parameters: {'learning_rate': 0.01596497708445574, 'depth': 9, 'l2_leaf_reg': 1.1159472661594606e-08, 'min_data_in_leaf': 4, 'border_count': 248, 'bagging_temperature': 0.3623887368574112, 'random_strength': 2.1384578490475107e-06, 'grow_policy': 'Depthwise'}. Best is trial 17 with value: -2556.489614367946.
[I 2025-05-14 03:46:46,564] Trial 19 finished with value: -2708.191790901802 and parameters: {'learning_rate': 0.24477192572019485, 'depth': 7, 'l2_leaf_reg': 2.3098308271653418e-05, 'min_data_in_leaf': 12, 'border_count': 59, 'bagging_temperature': 0.0711240090347467, 'random_strength': 5.019623855651714e-08, 'grow_policy': 'Lossguide'}. Best is trial 17 with value: -2556.489614367946.
[I 2025-05-14 03:47:09,708] Trial 20 finished with value: -2629.0927431412074 and parameters: {'learning_rate': 0.045281219242332915, 'depth': 10, 'l2_leaf_reg': 0.010169259979428625, 'min_data_in_leaf': 17, 'border_count': 190, 'bagging_temperature': 0.13109532595280188, 'random_strength': 5.725889350557309e-06, 'grow_policy': 'Depthwise'}. Best is trial 17 with value: -2556.489614367946.
[I 2025-05-14 03:47:28,038] Trial 21 finished with value: -2662.8455154565036 and parameters: {'learning_rate': 0.08417126506035016, 'depth': 8, 'l2_leaf_reg': 0.0006641702867048814, 'min_data_in_leaf': 16, 'border_count': 95, 'bagging_temperature': 0.3550175252017867, 'random_strength': 6.473733730238062e-05, 'grow_policy': 'Depthwise'}. Best is trial 17 with value: -2556.489614367946.
[I 2025-05-14 03:48:22,098] Trial 22 finished with value: -2553.5175758331197 and parameters: {'learning_rate': 0.02854304599072048, 'depth': 9, 'l2_leaf_reg': 0.0004429789647772918, 'min_data_in_leaf': 10, 'border_count': 67, 'bagging_temperature': 0.5558186913583201, 'random_strength': 0.0014086113230469067, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:49:10,065] Trial 23 finished with value: -2567.391805156711 and parameters: {'learning_rate': 0.026764961342591816, 'depth': 9, 'l2_leaf_reg': 4.0555978330521646e-07, 'min_data_in_leaf': 10, 'border_count': 68, 'bagging_temperature': 0.5904786989858626, 'random_strength': 0.003513242023709581, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:49:22,054] Trial 24 finished with value: -2622.624216994232 and parameters: {'learning_rate': 0.11762433128006189, 'depth': 7, 'l2_leaf_reg': 0.00011122236949708718, 'min_data_in_leaf': 6, 'border_count': 112, 'bagging_temperature': 0.7681535959955343, 'random_strength': 6.673067700091263e-07, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:49:53,628] Trial 25 finished with value: -2566.687493479829 and parameters: {'learning_rate': 0.0428655871308162, 'depth': 9, 'l2_leaf_reg': 1.2056415233295121, 'min_data_in_leaf': 11, 'border_count': 146, 'bagging_temperature': 0.38006611000782997, 'random_strength': 0.09046406371852381, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:50:02,251] Trial 26 finished with value: -2832.1352269234903 and parameters: {'learning_rate': 0.1970147411078323, 'depth': 10, 'l2_leaf_reg': 0.07162501984849304, 'min_data_in_leaf': 14, 'border_count': 48, 'bagging_temperature': 0.7456911509314165, 'random_strength': 1.598786064756365e-05, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:50:40,171] Trial 27 finished with value: -2601.1648427815717 and parameters: {'learning_rate': 0.038043609585293056, 'depth': 7, 'l2_leaf_reg': 1.6824470580542856e-05, 'min_data_in_leaf': 9, 'border_count': 69, 'bagging_temperature': 0.4427744483199044, 'random_strength': 0.00030247232285297106, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:51:03,817] Trial 28 finished with value: -2723.539972578103 and parameters: {'learning_rate': 0.063339552281372, 'depth': 9, 'l2_leaf_reg': 0.0033137305082140056, 'min_data_in_leaf': 5, 'border_count': 100, 'bagging_temperature': 0.5728331614449053, 'random_strength': 0.08608571674502018, 'grow_policy': 'SymmetricTree'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:51:44,354] Trial 29 finished with value: -2903.438265613962 and parameters: {'learning_rate': 0.22327494818582883, 'depth': 8, 'l2_leaf_reg': 0.00046397471558624393, 'min_data_in_leaf': 19, 'border_count': 77, 'bagging_temperature': 0.33115797120918855, 'random_strength': 0.001283308483124051, 'grow_policy': 'Lossguide'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:51:53,564] Trial 30 finished with value: -3143.416961771734 and parameters: {'learning_rate': 0.2615824515538357, 'depth': 8, 'l2_leaf_reg': 2.6721859802422237e-07, 'min_data_in_leaf': 2, 'border_count': 150, 'bagging_temperature': 0.5255403450913998, 'random_strength': 3.7142097112219726e-07, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:53:18,711] Trial 31 finished with value: -2620.1543675837006 and parameters: {'learning_rate': 0.036092954832845386, 'depth': 9, 'l2_leaf_reg': 1.6375335570564933, 'min_data_in_leaf': 11, 'border_count': 126, 'bagging_temperature': 0.4070745587934888, 'random_strength': 1.0037092538282515e-08, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:53:59,231] Trial 32 finished with value: -2566.1296831641434 and parameters: {'learning_rate': 0.06594270014769671, 'depth': 9, 'l2_leaf_reg': 0.5692095900658858, 'min_data_in_leaf': 12, 'border_count': 208, 'bagging_temperature': 0.3165260807907623, 'random_strength': 0.06397745585020524, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:54:35,567] Trial 33 finished with value: -2638.995606936495 and parameters: {'learning_rate': 0.07724237363909499, 'depth': 8, 'l2_leaf_reg': 0.2844212377732495, 'min_data_in_leaf': 15, 'border_count': 204, 'bagging_temperature': 0.16971199093261635, 'random_strength': 0.04813732436092434, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:56:06,127] Trial 34 finished with value: -2647.590494720153 and parameters: {'learning_rate': 0.06460022496756479, 'depth': 10, 'l2_leaf_reg': 6.601504952572579, 'min_data_in_leaf': 8, 'border_count': 215, 'bagging_temperature': 0.280595071181398, 'random_strength': 0.5822640982430759, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 03:56:26,769] Trial 35 finished with value: -2625.921488111509 and parameters: {'learning_rate': 0.14705058259331355, 'depth': 10, 'l2_leaf_reg': 0.017914490598987375, 'min_data_in_leaf': 13, 'border_count': 39, 'bagging_temperature': 0.3287024903377104, 'random_strength': 0.008267542278170484, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:00:07,278] Trial 36 finished with value: -2713.318053553942 and parameters: {'learning_rate': 0.011286702277088917, 'depth': 8, 'l2_leaf_reg': 0.2339927288492896, 'min_data_in_leaf': 18, 'border_count': 243, 'bagging_temperature': 0.5034843298839619, 'random_strength': 0.00055952127058293, 'grow_policy': 'SymmetricTree'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:01:52,318] Trial 37 finished with value: -2599.3830690422274 and parameters: {'learning_rate': 0.026614468934226813, 'depth': 9, 'l2_leaf_reg': 0.006370159217597417, 'min_data_in_leaf': 8, 'border_count': 97, 'bagging_temperature': 0.855566752212292, 'random_strength': 8.32305758695803e-05, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:02:36,816] Trial 38 finished with value: -2610.623762642826 and parameters: {'learning_rate': 0.09686963957512237, 'depth': 7, 'l2_leaf_reg': 0.0014074747896680212, 'min_data_in_leaf': 21, 'border_count': 71, 'bagging_temperature': 0.7156956610821732, 'random_strength': 0.018501470320959317, 'grow_policy': 'SymmetricTree'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:03:07,048] Trial 39 finished with value: -2671.015969019692 and parameters: {'learning_rate': 0.16854993390170198, 'depth': 6, 'l2_leaf_reg': 0.6496465232605684, 'min_data_in_leaf': 15, 'border_count': 53, 'bagging_temperature': 0.22219445776878718, 'random_strength': 0.7258361806395729, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:04:30,028] Trial 40 finished with value: -2575.0422520898715 and parameters: {'learning_rate': 0.13301237034987956, 'depth': 8, 'l2_leaf_reg': 0.00025678296288065243, 'min_data_in_leaf': 13, 'border_count': 129, 'bagging_temperature': 0.5756243041812479, 'random_strength': 1.949907402720845e-05, 'grow_policy': 'Lossguide'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:05:48,971] Trial 41 finished with value: -2642.556436107284 and parameters: {'learning_rate': 0.049948179063764114, 'depth': 9, 'l2_leaf_reg': 1.5618546291904019, 'min_data_in_leaf': 11, 'border_count': 158, 'bagging_temperature': 0.3964202751479039, 'random_strength': 2.393747505741706, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:06:46,627] Trial 42 finished with value: -2598.4755135822797 and parameters: {'learning_rate': 0.0580674532442787, 'depth': 9, 'l2_leaf_reg': 1.8177693207658425, 'min_data_in_leaf': 11, 'border_count': 187, 'bagging_temperature': 0.29635746209371416, 'random_strength': 0.21208291644651261, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:07:19,719] Trial 43 finished with value: -2657.7880298639225 and parameters: {'learning_rate': 0.07363937743695215, 'depth': 10, 'l2_leaf_reg': 0.04359530886331162, 'min_data_in_leaf': 9, 'border_count': 217, 'bagging_temperature': 0.44664672961449514, 'random_strength': 0.04495554133929357, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:09:46,045] Trial 44 finished with value: -2691.606927827555 and parameters: {'learning_rate': 0.029786575860281366, 'depth': 9, 'l2_leaf_reg': 9.943736562101853, 'min_data_in_leaf': 12, 'border_count': 152, 'bagging_temperature': 0.36930442821158393, 'random_strength': 0.0038218125123354175, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:10:38,605] Trial 45 finished with value: -2616.0330119726023 and parameters: {'learning_rate': 0.046671945704494774, 'depth': 8, 'l2_leaf_reg': 0.12115447142224317, 'min_data_in_leaf': 14, 'border_count': 140, 'bagging_temperature': 0.48481335723150554, 'random_strength': 0.3294399030680058, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:12:54,361] Trial 46 finished with value: -2590.002658938037 and parameters: {'learning_rate': 0.025820733438051514, 'depth': 10, 'l2_leaf_reg': 1.0279696260416582, 'min_data_in_leaf': 16, 'border_count': 85, 'bagging_temperature': 0.2455200340992076, 'random_strength': 0.00015044433051361004, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:13:57,004] Trial 47 finished with value: -2635.6488533277184 and parameters: {'learning_rate': 0.09024036605373528, 'depth': 7, 'l2_leaf_reg': 5.4022309753871646e-05, 'min_data_in_leaf': 19, 'border_count': 43, 'bagging_temperature': 0.5347005159269773, 'random_strength': 0.017468337161715447, 'grow_policy': 'SymmetricTree'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:14:16,489] Trial 48 finished with value: -2832.750001651622 and parameters: {'learning_rate': 0.29562301087772724, 'depth': 9, 'l2_leaf_reg': 4.218536936460986, 'min_data_in_leaf': 10, 'border_count': 232, 'bagging_temperature': 0.6069911018530717, 'random_strength': 1.9157498081287359, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.
[I 2025-05-14 04:14:46,553] Trial 49 finished with value: -2609.0502948976673 and parameters: {'learning_rate': 0.10265216059916468, 'depth': 8, 'l2_leaf_reg': 0.6037874295139654, 'min_data_in_leaf': 7, 'border_count': 122, 'bagging_temperature': 0.39641107195397474, 'random_strength': 0.0009672456521149151, 'grow_policy': 'Depthwise'}. Best is trial 22 with value: -2553.5175758331197.</code></pre>
</div>
</div>
<div id="8e347a2b" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get best parameters and train final model with early stopping</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> study.best_params</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters:"</span>, best_params)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best trial</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>best_trial <span class="op">=</span> study.best_trial</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best trial:"</span>, best_trial)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best parameters: {'learning_rate': 0.02854304599072048, 'depth': 9, 'l2_leaf_reg': 0.0004429789647772918, 'min_data_in_leaf': 10, 'border_count': 67, 'bagging_temperature': 0.5558186913583201, 'random_strength': 0.0014086113230469067, 'grow_policy': 'Depthwise'}
Best trial: FrozenTrial(number=22, state=1, values=[-2553.5175758331197], datetime_start=datetime.datetime(2025, 5, 14, 3, 47, 28, 39246), datetime_complete=datetime.datetime(2025, 5, 14, 3, 48, 22, 98057), params={'learning_rate': 0.02854304599072048, 'depth': 9, 'l2_leaf_reg': 0.0004429789647772918, 'min_data_in_leaf': 10, 'border_count': 67, 'bagging_temperature': 0.5558186913583201, 'random_strength': 0.0014086113230469067, 'grow_policy': 'Depthwise'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.3, log=False, low=0.01, step=None), 'depth': IntDistribution(high=10, log=False, low=4, step=1), 'l2_leaf_reg': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'min_data_in_leaf': IntDistribution(high=30, log=False, low=1, step=1), 'border_count': IntDistribution(high=255, log=False, low=32, step=1), 'bagging_temperature': FloatDistribution(high=1.0, log=False, low=0.0, step=None), 'random_strength': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'grow_policy': CategoricalDistribution(choices=('SymmetricTree', 'Depthwise', 'Lossguide'))}, trial_id=22, value=None)</code></pre>
</div>
</div>
<div id="58e1c2e5" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>np.concatenate((y_train, y_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>array([48750, 17949, 22995, ..., 27300,  7952, 13498], dtype=int64)</code></pre>
</div>
</div>
<div id="700beb75" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use column indices instead of names</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>cat_feature_indices <span class="op">=</span> [X_train.columns.get_loc(col) <span class="cf">for</span> col <span class="kw">in</span> categorical_feature]</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Add iterations parameter back for final model</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>best_params[<span class="st">'iterations'</span>] <span class="op">=</span> <span class="dv">3000</span>  <span class="co"># High number, early stopping will be used</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># create a train+validation set for final model</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>train_val_pool <span class="op">=</span> Pool(</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    np.vstack((X_train, X_valid)),</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    np.concatenate((y_train, y_valid)),</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    cat_features<span class="op">=</span>cat_feature_indices</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a test pool</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>test_pool <span class="op">=</span> Pool(X_test, y_test, cat_features<span class="op">=</span>categorical_feature)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Train final model on combined train+validation data</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> CatBoostRegressor(<span class="op">**</span>best_params)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>final_model.fit(</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    train_val_pool,</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    eval_set<span class="op">=</span>test_pool,</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    early_stopping_rounds<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Get actual number of trees used after early stopping</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>actual_iterations <span class="op">=</span> final_model.tree_count_</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Actual number of trees used: </span><span class="sc">{</span>actual_iterations<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate on test set</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> final_model.predict(X_test)</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>test_rmse <span class="op">=</span> root_mean_squared_error(y_test, y_pred_test)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>test_r2 <span class="op">=</span> r2_score(y_test, y_pred_test)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE: </span><span class="sc">{</span>test_rmse<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R²: </span><span class="sc">{</span>test_r2<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Actual number of trees used: 466
Test RMSE: 3042.9611
Test R²: 0.9684</code></pre>
</div>
</div>
<div id="c35df952" class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>fig1 <span class="op">=</span> optuna.visualization.plot_optimization_history(study)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>fig1.show()</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>fig2 <span class="op">=</span> optuna.visualization.plot_param_importances(study)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>fig2.show()</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot feature importance from the final model</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>feature_importance <span class="op">=</span> final_model.get_feature_importance()</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>sorted_idx <span class="op">=</span> np.argsort(feature_importance)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">9</span>))</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>plt.barh(<span class="bu">range</span>(<span class="bu">len</span>(sorted_idx)), feature_importance[sorted_idx])</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>plt.yticks(<span class="bu">range</span>(<span class="bu">len</span>(sorted_idx)), np.array(<span class="bu">range</span>(X.shape[<span class="dv">1</span>]))[sorted_idx])</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'CatBoost Feature Importance'</span>)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LightGBM_CatBoost_files/figure-html/cell-14-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It takes 2 minutes to tune CatBoost, which is higher than LightGBM and lesser than XGBoost. CatBoost falls in between LightGBM and XGBoost in terms of speed. However, it is likely to be more accurate than XGBoost and LighGBM, and likely to require lesser tuning as compared to XGBoost.</p>
<p>Check the <a href="https://catboost.ai/en/docs/references/training-parameters/common">documentation</a> for hyperparameter tuning</p>
</section>
<section id="when-to-use-catboost-over-xgboost" class="level3" data-number="11.3.7">
<h3 data-number="11.3.7" class="anchored" data-anchor-id="when-to-use-catboost-over-xgboost"><span class="header-section-number">11.3.7</span> When to Use <strong>CatBoost</strong> Over <strong>XGBoost</strong></h3>
<ul>
<li>When your dataset contains <strong>many categorical features</strong><br>
</li>
<li><strong>CatBoost</strong> tends to perform well <strong>out of the box</strong> with minimal hyperparameter tuning, making it more user-friendly for quick experimentation or deployment<br>
</li>
<li>CatBoost’s <strong>GPU implementation</strong> is optimized for handling categorical data efficiently, and can <strong>outperform XGBoost</strong> on datasets dominated by categorical variables<br>
&gt; While both libraries support GPU acceleration, CatBoost’s architecture is particularly well-suited for categorical-heavy tasks</li>
</ul>
</section>
</section>
<section id="handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost"><span class="header-section-number">11.4</span> Handling Imbalanced Classification: XGBoost vs.&nbsp;LightGBM vs.&nbsp;CatBoost</h2>
<p>Imbalanced classification occurs when one class significantly outnumbers the other (e.g., fraud detection, disease diagnosis). Each boosting library offers tools to address this issue:</p>
<p><strong>XGBoost</strong>:</p>
<ul>
<li><strong>Parameter</strong>: <code>scale_pos_weight</code>
<ul>
<li>Formula:<br>
<span class="math display">\[
\texttt{scale\_pos\_weight} = \frac{\text{Number of negative samples}}{\text{Number of positive samples}}
\]</span></li>
<li>Increases the gradient of the positive class during training.</li>
</ul></li>
<li><strong>Additional Strategies</strong>:
<ul>
<li>Use custom <code>eval_metric</code> (e.g., <code>"auc"</code>, <code>"aucpr"</code>, or <code>"logloss"</code>)</li>
<li>Apply early stopping on validation AUC</li>
</ul></li>
</ul>
<p><strong>LightGBM</strong>:</p>
<ul>
<li><strong>Parameter</strong>: <code>scale_pos_weight</code> (same as in XGBoost)</li>
<li><strong>Alternative</strong>: <code>is_unbalance = TRUE</code>
<ul>
<li>Automatically adjusts class weights based on distribution</li>
</ul></li>
<li><strong>Other Tips</strong>:
<ul>
<li>Use <code>metric = "auc"</code> or <code>"binary_logloss"</code> for better guidance during training</li>
<li>Resampling techniques also compatible</li>
</ul></li>
</ul>
<p><strong>CatBoost</strong>:</p>
<ul>
<li><strong>Parameter</strong>: <code>class_weights</code>
<ul>
<li>Accepts a numeric vector (e.g., <code>class_weights = c(1, 5)</code> for [negative, positive])</li>
<li>Directly modifies the loss function to emphasize minority class</li>
</ul></li>
<li><strong>Advantages</strong>:
<ul>
<li>More flexible than <code>scale_pos_weight</code></li>
<li>Works well with default settings</li>
</ul></li>
<li><strong>Other Tips</strong>:
<ul>
<li>Use <code>loss_function = "Logloss"</code> and <code>eval_metric = "AUC"</code> for binary classification</li>
</ul></li>
</ul>
<p>Below is the summary table:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 38%">
<col style="width: 23%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Library</th>
<th>Imbalance Handling Parameter</th>
<th>Default Support</th>
<th>Recommended Metric</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>XGBoost</td>
<td><code>scale_pos_weight</code></td>
<td>No</td>
<td><code>auc</code>, <code>aucpr</code></td>
</tr>
<tr class="even">
<td>LightGBM</td>
<td><code>scale_pos_weight</code>, <code>is_unbalance</code></td>
<td>Yes (with flag)</td>
<td><code>auc</code>, <code>binary_logloss</code></td>
</tr>
<tr class="odd">
<td>CatBoost</td>
<td><code>class_weights</code></td>
<td>Yes</td>
<td><code>Logloss</code>, <code>AUC</code></td>
</tr>
</tbody>
</table>
</section>
<section id="summary-xgboost-vs.-lightgbm-vs.-catboost" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="summary-xgboost-vs.-lightgbm-vs.-catboost"><span class="header-section-number">11.5</span> Summary: XGBoost vs.&nbsp;LightGBM vs.&nbsp;CatBoost</h2>
<p>Gradient boosting is a powerful ensemble technique, and XGBoost, LightGBM, and CatBoost are three of its most widely used implementations. Each has unique strengths and is well-suited to different use cases.</p>
<p><strong>XGBoost</strong>:</p>
<ul>
<li><strong>Strengths</strong>: Robust, well-documented, strong performance on structured/tabular data<br>
</li>
<li><strong>Split Finding</strong>: Level-wise tree growth<br>
</li>
<li><strong>Regularization</strong>: Explicit L1 and L2 regularization<br>
</li>
<li><strong>Flexibility</strong>: Highly customizable with many hyperparameters<br>
</li>
<li><strong>Best for</strong>: General-purpose tabular data, especially when you have time to tune parameters</li>
</ul>
<p><strong>LightGBM</strong>:</p>
<ul>
<li><strong>Strengths</strong>: Fast training, low memory usage, excellent scalability<br>
</li>
<li><strong>Split Finding</strong>: Leaf-wise tree growth with depth control<br>
</li>
<li><strong>Binning</strong>: Uses histogram-based algorithm with <code>max_bin</code> to speed up training<br>
</li>
<li><strong>Best for</strong>: Large-scale datasets, high-dimensional features, and when training speed matters</li>
</ul>
<p><strong>CatBoost</strong>:</p>
<ul>
<li><strong>Strengths</strong>: Handles categorical features natively, works well with minimal tuning<br>
</li>
<li><strong>Boosting Innovation</strong>: Uses <em>ordered boosting</em> to prevent prediction shift<br>
</li>
<li><strong>Categorical Encoding</strong>: No need for manual preprocessing — uses target-based encoding internally<br>
</li>
<li><strong>Best for</strong>: Datasets with many categorical variables or limited time for tuning</li>
</ul>
<p><strong>Final Thoughts</strong></p>
<p>All three libraries are powerful and battle-tested. Here’s a rough guideline:</p>
<ul>
<li><strong>Use XGBoost</strong> if you want control, flexibility, and a well-documented standard</li>
<li><strong>Use LightGBM</strong> when training speed and large data scalability are your top priorities</li>
<li><strong>Use CatBoost</strong> when working with many categorical features or seeking strong baseline results with minimal tuning</li>
</ul>
</section>
<section id="references" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="references"><span class="header-section-number">11.6</span> References</h2>
<ul>
<li><a href="https://papers.nips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf">LightGBM Paper (Original NIPS 2017)</a></li>
<li><a href="https://lightgbm.readthedocs.io/">LightGBM Official Website</a></li>
<li><a href="https://arxiv.org/abs/1810.11363">CatBoost Paper (arXiv)</a></li>
<li><a href="https://catboost.ai/">CatBoost Official Website</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./XGBoost.html" class="pagination-link" aria-label="XGBoost">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">XGBoost</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Lec10_Ensemble.html" class="pagination-link" aria-label="Ensemble modeling">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ensemble modeling</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>