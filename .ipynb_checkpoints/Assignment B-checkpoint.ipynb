{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ecb4feaf",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Assignment B\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb8be6",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity. \n",
    "\n",
    "2. Do not write your name on the assignment.\n",
    "\n",
    "3. Write your code in the *Code* cells and your answer in the *Markdown* cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\n",
    "\n",
    "4. Use [Quarto](https://quarto.org/docs/output-formats/html-basics.html) to print the *.ipynb* file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: `quarto render filename.ipynb --to html`. Submit the HTML file.\n",
    "\n",
    "5. The assignment is worth 100 points, and is due on **Sunday, 23rd April 2023 at 11:59 pm**. \n",
    "\n",
    "6. **Five points are properly formatting the assignment**. The breakdown is as follows:\n",
    "- Must be an HTML file rendered using Quarto (2 pts). *If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file. If your issue doesn't seem genuine, you will lose points.* \n",
    "- There aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\n",
    "- Final answers of each question are written in Markdown cells (1 pt).\n",
    "- There is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)\n",
    "\n",
    "7. For all questions on cross-validation, you must use `sklearn` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba7b6ed",
   "metadata": {},
   "source": [
    "## Degrees of freedom\n",
    "Find the number of degrees of freedom of the following models. Exclude the intercept when counting the degrees of freedom. You may either show your calculation, or explain briefly how you are computing the degrees of freedom.\n",
    "\n",
    "### Quadratic spline\n",
    "A model with one predictor, where the predictor is transformed into a quadratic spline with 5 knots\n",
    "\n",
    "*(2 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18510ecb",
   "metadata": {},
   "source": [
    "### Natural cubic splines\n",
    "A model with one predictor, where the predictor is transformed into a natural cubic spline with 4 knots\n",
    "\n",
    "*(2 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a974f7",
   "metadata": {},
   "source": [
    "### Generalized additive model\n",
    "A model with four predictors, where the transformations of the respective predictors are (i) cubic spline transformation with 3 knots, (ii) log transformation, (iii) linear spline transformation with 2 knots, (iv) polynomial transformation of degree 4.\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e00821",
   "metadata": {},
   "source": [
    "## Number of knots\n",
    "Find the number of knots in the following spline transformations, if each of the transformations corresponds to 7 degrees of freedom (excluding the intercept).\n",
    "\n",
    "### Cubic splines\n",
    "Cubic spline transformation \n",
    "\n",
    "*(1 point)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83842fa",
   "metadata": {},
   "source": [
    "### Natural cubic splines\n",
    "Natural cubic spline transformation \n",
    "\n",
    "*(1 point)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cc4fb",
   "metadata": {},
   "source": [
    "### Degree 4 spline\n",
    "Spline transformation of degree 4\n",
    "\n",
    "*(1 point)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b794c96",
   "metadata": {},
   "source": [
    "## Regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec019de5",
   "metadata": {},
   "source": [
    "Read the file *investment_clean_data.csv*. This data is a cleaned version of the file *train.csv* in last quarter's regression [prediction problem](https://www.kaggle.com/competitions/data-science-2-linear-regression-2023-bank-loans). Refer to the link for description of variables. It required some effort to get a RMSE of less than 650 with linear regression. In this question, we'll use MARS / natural cubic splines to get a RMSE of less than 350 with relatively less effort. Use mean squared error as the performance metric in cross validation.\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "Prepare the data for modeling as follows:\n",
    "\n",
    "1. Use the Pandas function `get_dummies()` to convert all the categorical predictors to dummy variables. \n",
    "\n",
    "2. Using the `sklearn` function `train_test_split`, split the data into 20% test and 80% train. Use `random_state = 45`.\n",
    "\n",
    "*Note:*\n",
    "\n",
    "*A. The function `get_dummies()` can be used over the entire DataFrame. Don't convert the categorical variables individually.*\n",
    "\n",
    "*B. The MARS model does not accept categorical predictors, which is why the conversion is done.*\n",
    "\n",
    "*C. The response is `money_made_inv`*\n",
    "\n",
    "*(2 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d87dd",
   "metadata": {},
   "source": [
    "### Optimal MARS degree\n",
    "Use $5$-fold cross validation to find the optimal degree of the MARS model to predict `money_made_inv` based on all the predictors in the dataset.\n",
    "\n",
    "**Hint:** Start from degree 1, and keep going until it doesn't benefit.\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0d821",
   "metadata": {},
   "source": [
    "### Fitting MARS model\n",
    "\n",
    "With the optimal degree identified in the previous question, fit a MARS model. Print the model summary. What is the degree of freedom of the model (excluding the intercept)? \n",
    "\n",
    "*(1 + 1 + 2 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845cb640",
   "metadata": {},
   "source": [
    "### Interpreting MARS basis functions\n",
    "Based on the model summary in the previous question, answer the following question. Holding all other predictors constant, what will be the mean increase in `money_made_inv` for a unit increase in `out_prncp_inv`, given that `out_prncp_inv` is in [500, 600], `term` = 36 (months), `loan_amnt` = 1000, and `int_rate` = 0.1?\n",
    "\n",
    "First, write the basis functions being used to answer the question, and then substitute the values.\n",
    "\n",
    "Also, which basis function is non-zero for the smallest domain space of `out_prncp_inv`? Also, specify the domain space in which it is non-zero.\n",
    "\n",
    "*(3 + 2 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494dcb95",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "Find the relative importance of each predictor in the MARS model developed in B.3.3. You may choose any criterion for finding feature importance based on the [MARS documentation](https://contrib.scikit-learn.org/py-earth/content.html#multivariate-adaptive-regression-splines). Print a DataFrame with 2 columns - one column consisting of predictors arranged in descending order of relative importance, and the second column quantifying their relative importance. Exclude predictors rejected by the model developed in B.3.3. \n",
    "\n",
    "*Note the forward pass and backward passes of the algorithm perform feature selection without manual intervention.*\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540ff82",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Using the model developed in B.3.3, compute the RMSE on test data.\n",
    "\n",
    "*(2 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4105932",
   "metadata": {},
   "source": [
    "### Non-trivial train data {-}\n",
    "Let us call the part of the dataset where `out_prncp_inv = 0` as a trivial subset of data. For this subset, we can directly predict the response without developing a model *(recall the EDA last quarter)*. For all the questions below, fit / tune the  model only on the non-trivial part of the train data. However, when making predictions, and computing RMSE, consider the entire test data. Combine the predictions of the model on the non-trivial subset of test data with the predictions on the trivial subset of test data to make predictions on the entire test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d13379",
   "metadata": {},
   "source": [
    "### Prediction with non-trivial train data\n",
    "Find the optimal degree of the MARS model based on the non-trivial train data, fit the model, and re-compute the RMSE on test data. \n",
    "\n",
    "*Note: You should get a lesser RMSE as compared to what you got in B.3.6.*\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53eafd",
   "metadata": {},
   "source": [
    "### Reducing model variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb06ec",
   "metadata": {},
   "source": [
    "The MARS model is highly flexible, which makes it a low bias-high variance model. However, high prediction variance increases the expected mean squared error on test data *(see **equation 2.7 on page 34** of the book)*. How can you reduce the prediction variance of the model without increasing the bias? Check slide 12 of the [bias-variance presentation](https://nuwildcat-my.sharepoint.com/personal/akl0407_ads_northwestern_edu/_layouts/15/onedrive.aspx?ga=1&id=%2Fpersonal%2Fakl0407%5Fads%5Fnorthwestern%5Fedu%2FDocuments%2FSTAT303%2D3%20presentations%2FWeek1%5FBiasVariance%2Epdf&parent=%2Fpersonal%2Fakl0407%5Fads%5Fnorthwestern%5Fedu%2FDocuments%2FSTAT303%2D3%20presentations). The MARS model, in general, corresponds to case B. You can see that by averaging the predictions of multiple models, you will reduce prediction variance without increasing the bias.\n",
    "\n",
    "Take 10 samples of train data of the same size as the train data, with replacement. For each sample, fit a MARS model with the optimal degree identified earlier. Use the $i^{th}$ model, say $\\hat{f}_i$ to make prediction $\\hat{f_i}(\\mathbf{x}_{test})$  on each test data point $\\mathbf{x}_{test}$ *(Note that predictions will be made using the model on the non-trivial test data, and without the model on the trivial test data)*. Compute the average prediction on each test data point based on the 10 models as follows: \n",
    "\n",
    "$$\\hat{f}(\\mathbf{x}_{test}) = \\frac{1}{10}\\Sigma_{1=1}^{10} \\hat{f_i}(\\mathbf{x}_{test})$$\n",
    "\n",
    "Consider $\\hat{f}(\\mathbf{x}_{test})$ as the prediction at the test data point $\\mathbf{x}_{test}$. Compute the RMSE based on this model, which is the average prediction of 10 models. You should get a lesser RMSE as compared to the previous question (B.3.7).\n",
    "\n",
    "*Note: For ease in grading, use the Pandas DataFrame method [`sample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html) to take samples with replacement, and put `random_state` for the ith sample as i, where i goes from 0 to 9.*\n",
    "\n",
    "*(6 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645de987",
   "metadata": {},
   "source": [
    "### Generalized additive model (GAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33eb74",
   "metadata": {},
   "source": [
    "Develop a Generalized linear model $\\hat{f}_{GLM}(.)$ to predict `money_made_inv` as follows:\n",
    "\n",
    "$$\\hat{f}_{GLM}(\\mathbf{x}) = \\hat{\\beta}_0 + \\Sigma_{i=1}^{4} \\hat{\\beta}_i{f}_i(\\mathbf{x}),$$\n",
    "\n",
    "where ${f}_i(\\mathbf{x})$ is a MARS model of degree $i$.\n",
    "\n",
    "Print the estimated beta coefficients ($\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\beta}_3, \\hat{\\beta}_4$) of the developed model.\n",
    "\n",
    "*Note: The model is developed on the non-trivial train data*\n",
    "\n",
    "*(8 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962613d9",
   "metadata": {},
   "source": [
    "### Prediction with GAM\n",
    "\n",
    "Use the GAM developed in the previous question to compute RMSE on test data.\n",
    "\n",
    "*Note: Predictions will be made using the model on the non-trivial test data, and without the model on the trivial test data*\n",
    "\n",
    "*(5 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80489568",
   "metadata": {},
   "source": [
    "### Reducing GAM prediction variance\n",
    "\n",
    "As we reduced the variance of the MARS model in B.3.8, follow the same approach to reduce the variance of the GAM developed in B.3.9, and compute the RMSE on test data.\n",
    "\n",
    "*Note: You should get a lesser RMSE as compared to what you got in B.3.10.*\n",
    "\n",
    "*(8 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84050044",
   "metadata": {},
   "source": [
    "### Natural cubic splines\n",
    "\n",
    "Even though MARS is efficient and highly flexible, natural cubic splines work very well too, if tuned properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29c8ff",
   "metadata": {},
   "source": [
    "Consider the predictors identified in the model summary of the MARS model printed in B.3.3. For each predictor, create natural cubic splines basis functions with $d$ degrees of freedom. Include all-order interactions *(i.e., 2-factor, 3-factor, 4-factor interactions, and so on)* of all the basis functions. Use the `sklearn` function `cross_val_score()` to find and report the optimal degrees of freedom for the natural cubic spline of each predictor. \n",
    "\n",
    "Consider degrees of freedom from 3 to 6 for the natural cubic spline transformation of each predictor.\n",
    "\n",
    "*(8 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ffd04",
   "metadata": {},
   "source": [
    "### Fitting the natural cubic splines model\n",
    "\n",
    "With the optimal degrees of freedom identified in the previous question, fit a model to predict `money_made_inv`, where the basis functions correspond to the natural cubic splines of each predictor, and all-factor interactions of the basis functions. Compute the RMSE on test data.\n",
    "\n",
    "*Note: Predictions will be made using the model on the non-trivial test data, and without the model on the trivial test data*\n",
    "\n",
    "*(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43286849",
   "metadata": {},
   "source": [
    "## GAM for classification\n",
    "The data for this question is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls, where bank clients were called to subscribe for a term deposit. \n",
    "\n",
    "There is one train data - *train.csv*, which you will use to develop a model. There are two test datasets - *test1.csv* and *test2.csv*, which you will use to test your model. Each dataset has the following attributes about the clients called in the marketing campaign:\n",
    "\n",
    "1. `age`: Age of the client\n",
    "\n",
    "2. `education`: Education level of the client \n",
    "\n",
    "3. `day`: Day of the month the call is made\n",
    "\n",
    "4. `month`: Month of the call \n",
    "\n",
    "5. `y`: did the client subscribe to a term deposit? \n",
    "\n",
    "6. `duration`: Call duration, in seconds. This attribute highly affects the output target (e.g., if `duration`=0 then `y`='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call `y` is obviously known. Thus, this input should only be included for inference purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "(Raw data source: [Source](https://archive.ics.uci.edu/ml/datasets/bank+marketing). Do not use the raw data source for this assignment. It is just for reference.)\n",
    "\n",
    "Develop a **generalized additive model (GAM)** to predict the probability of a client subscribing to a term deposit based on *age, education, day* and *month*. The model must have: \\\n",
    "(a)  **Minimum overall classification accuracy of 75%** among the classification accuracies on *train.csv*, *test1.csv* and *test2.csv*. \\\n",
    "(b) **Minimum recall of 55%** among the recall on *train.csv*, *test1.csv* and *test2.csv*. \n",
    "\n",
    "Print the accuracy and recall for all the three datasets - *train.csv*, *test1.csv* and *test2.csv*.\n",
    "\n",
    "Note that: \n",
    "\n",
    "i. You cannot use `duration` as a predictor. The predictor is not useful for prediction because its value is determined after the marketing call ends. However, after the call ends, we already know whether the client responded positively or negatively. \n",
    "\n",
    "ii. One way to develop the model satisfying constrains (a) and (b) is to use **spline transformations for *age* and *day*, and interacting *month* with all the predictors (including the spline transformations)**\n",
    "\n",
    "iii. You may assume that the distribution of the predictors is the same in all the three datasets. Thus, you may create B-spline basis functions independently for the train and test datasets.\n",
    "\n",
    "iv. Use cross-validation on train data to optimize the model hyperparameters, and the decision threshold probability. Then, use the optimal hyperparameters to fit the model on train data. Then, evaluate its accuracy and recall on all the three datasets. Note that the test datasets must only be used to evaluate performance metrics, and not optimize any hyperparameters or decision threshold probability.\n",
    "\n",
    "*(20 points: 10 points for cross validation, 5 points for obtaining and showing the optimal values of the hyperparameters and decision threshold probability, 2 points for fitting the model with the optimal hyperparameters, and 3 points for printing the accuracy & recall on each of the three datasets)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
