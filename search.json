[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science II with python (Class notes)",
    "section": "",
    "text": "These are class notes for the course STAT303-2. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThe course notes are currently being written, and will continue to being developed as the course progresses (just like the course textbook last quarter). Please report any typos / mistakes / inconsistencies / issues with the class notes / class presentations in your comments here. Thank you!"
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html",
    "href": "Lec1_SimpleLinearRegression.html",
    "title": "1  Simple Linear Regression",
    "section": "",
    "text": "Read section 3.1 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately."
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html#simple-linear-regression",
    "href": "Lec1_SimpleLinearRegression.html#simple-linear-regression",
    "title": "1  Simple Linear Regression",
    "section": "1.1 Simple Linear Regression",
    "text": "1.1 Simple Linear Regression\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nDevelop a simple linear regression model that predicts car price based on engine size. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~engineSize', data = train)\n\n\n#Using the fit() function of the 'ols' class to fit the model\nmodel = ols_object.fit()\n\n\n#Printing model summary which contains among other things, the model coefficients\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.390 \n\n\n  Model:                   OLS         Adj. R-squared:        0.390 \n\n\n  Method:             Least Squares    F-statistic:           3177. \n\n\n  Date:             Thu, 19 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 16:44:04       Log-Likelihood:      -53949. \n\n\n  No. Observations:        4960        AIC:                1.079e+05\n\n\n  Df Residuals:            4958        BIC:                1.079e+05\n\n\n  Df Model:                   1                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -4122.0357   522.260    -7.893  0.000 -5145.896 -3098.176\n\n\n  engineSize  1.299e+04   230.450    56.361  0.000  1.25e+04  1.34e+04\n\n\n\n\n  Omnibus:       1271.986   Durbin-Watson:         0.517\n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   6490.719\n\n\n  Skew:            1.137    Prob(JB):               0.00\n\n\n  Kurtosis:        8.122    Cond. No.               7.64\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe model equation is: car price = -4122.0357 + 12990 * engineSize\nVisualize the regression line\n\nsns.regplot(x = 'engineSize', y = 'price', data = train, color = 'orange',line_kws={\"color\": \"red\"})\nplt.xlim(-1,7)\n#Note that some of the engineSize values are 0. They are incorrect, and should ideally be imputed before developing the model.\n\n(-1.0, 7.0)\n\n\n\n\n\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\n\n#Using the predict() function associated with the 'model' object to make predictions of car price on test (unknown) data\npred_price = model.predict(testf)#Note that the predict() function finds the predictor 'engineSize' in the testf dataframe, and plugs its values in the regression equation for prediction.\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.scatterplot(x = testp.price, y = pred_price)\n#In case of a perfect prediction, all the points must lie on the line x = y.\nsns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='orange') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\n\nText(0, 0.5, 'Predicted price')\n\n\n\n\n\nThe prediction doesn’t look too good. This is because we are just using one predictor - engine size. We can probably improve the model by adding more predictors when we learn multiple linear regression.\nWhat is the RMSE of the predicted car price?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n12995.1064515487\n\n\nThe root mean squared error in predicting car price is around $13k.\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n12810.109175214136\n\n\nThe residual standard error on the training data is close to the RMSE on the test data. This shows that the performance of the model on unknown data is comparable to its performance on known data. This implies that the model is not overfitting, which is good! In case we overfit a model on the training data, its performance on unknown data is likely to be worse than that on the training data.\nFind the confidence and prediction intervals of the predicted car price\n\n#Using the get_prediction() function associated with the 'model' object to get the intervals\nintervals = model.get_prediction(testf)\n\n\n#The function requires specifying alpha (probability of Type 1 error) instead of the confidence level to get the intervals\nintervals.summary_frame(alpha=0.05)\n\n\n\n\n\n  \n    \n      \n      mean\n      mean_se\n      mean_ci_lower\n      mean_ci_upper\n      obs_ci_lower\n      obs_ci_upper\n    \n  \n  \n    \n      0\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      1\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      2\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      3\n      8866.245277\n      316.580850\n      8245.606701\n      9486.883853\n      -16254.905974\n      33987.396528\n    \n    \n      4\n      47831.088340\n      468.949360\n      46911.740050\n      48750.436631\n      22700.782946\n      72961.393735\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2667\n      47831.088340\n      468.949360\n      46911.740050\n      48750.436631\n      22700.782946\n      72961.393735\n    \n    \n      2668\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      2669\n      8866.245277\n      316.580850\n      8245.606701\n      9486.883853\n      -16254.905974\n      33987.396528\n    \n    \n      2670\n      21854.526298\n      184.135754\n      21493.538727\n      22215.513869\n      -3261.551421\n      46970.604017\n    \n    \n      2671\n      21854.526298\n      184.135754\n      21493.538727\n      22215.513869\n      -3261.551421\n      46970.604017\n    \n  \n\n2672 rows × 6 columns\n\n\n\nShow the regression line predicting car price based on engine size for test data. Also show the confidence and prediction intervals for the car price.\n\ninterval_table = intervals.summary_frame(alpha=0.05)\n\n\nsns.scatterplot(x = testf.engineSize, y = pred_price,color = 'orange', s = 10)\nsns.lineplot(x = testf.engineSize, y = pred_price, color = 'red')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_lower, color = 'blue')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_upper, color = 'blue',label='_nolegend_')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_lower, color = 'green')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_upper, color = 'green')\nplt.legend(labels=[\"Regression line\",\"Confidence interval\", \"Prediction interval\"])\n\n<matplotlib.legend.Legend at 0x26a3a32c550>"
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html",
    "href": "Lec2_MultipleLinearRegression.html",
    "title": "2  Multiple Linear Regression",
    "section": "",
    "text": "Read section 3.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately."
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html#multiple-linear-regression",
    "href": "Lec2_MultipleLinearRegression.html#multiple-linear-regression",
    "title": "2  Multiple Linear Regression",
    "section": "2.1 Multiple Linear Regression",
    "text": "2.1 Multiple Linear Regression\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nDevelop a multiple linear regression model that predicts car price based on engine size, year, mileage, and mpg. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.660 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           2410. \n\n\n  Date:             Tue, 27 Dec 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:07:25       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4955        BIC:                1.050e+05\n\n\n  Df Model:                   4                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.661e+06  1.49e+05   -24.593  0.000 -3.95e+06 -3.37e+06\n\n\n  year        1817.7366    73.751    24.647  0.000  1673.151  1962.322\n\n\n  mileage       -0.1474     0.009   -16.817  0.000    -0.165    -0.130\n\n\n  mpg          -79.3126     9.338    -8.493  0.000   -97.620   -61.006\n\n\n  engineSize  1.218e+04   189.969    64.107  0.000  1.18e+04  1.26e+04\n\n\n\n\n  Omnibus:       2450.973   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31060.548\n\n\n  Skew:            2.045    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.557    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe model equation is: estimated car price = -3.661e6 + 1818 * year -0.15 * mileage - 79.31 * mpg + 12180 * engineSize\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\n\n#Using the predict() function associated with the 'model' object to make predictions of car price on test (unknown) data\npred_price = model.predict(testf)#Note that the predict() function finds the predictor 'engineSize' in the testf dataframe, and plugs its values in the regression equation for prediction.\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.scatterplot(x = testp.price, y = pred_price)\n#In case of a perfect prediction, all the points must lie on the line x = y.\nsns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='orange') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\n\nText(0, 0.5, 'Predicted price')\n\n\n\n\n\nThe prediction looks better as compared to the one with simple linear regression. This is because we have four predictors to help explain the variation in car price, instead of just one in the case of simple linear regression. Also, all the predictors have a significant relationship with price as evident from their p-values. Thus, all four of them are contributing in explaining the variation. Note the higher values of R2 as compared to the one in the case of simple linear regression.\nWhat is the RMSE of the predicted car price?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9956.82497993548\n\n\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n9563.74782917604\n\n\n\nsns.scatterplot(x = model.fittedvalues, y=model.resid,color = 'orange')\nsns.lineplot(x = [pred_price.min(),pred_price.max()],y = [0,0],color = 'blue')\nplt.xlabel('Predicted price')\nplt.ylabel('Residual')\n\nText(0, 0.5, 'Residual')\n\n\n\n\n\nWill the explained variation (R-squared) in car price always increase if we add a variable?\nShould we keep on adding variables as long as the explained variation (R-squared) is increasing?\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nnp.random.seed(1)\ntrain['rand_col'] = np.random.rand(train.shape[0])\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize+rand_col', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.661 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           1928. \n\n\n  Date:             Tue, 27 Dec 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:07:38       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4954        BIC:                1.050e+05\n\n\n  Df Model:                   5                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.662e+06  1.49e+05   -24.600  0.000 -3.95e+06 -3.37e+06\n\n\n  year        1818.1672    73.753    24.652  0.000  1673.578  1962.756\n\n\n  mileage       -0.1474     0.009   -16.809  0.000    -0.165    -0.130\n\n\n  mpg          -79.2837     9.338    -8.490  0.000   -97.591   -60.976\n\n\n  engineSize  1.218e+04   189.972    64.109  0.000  1.18e+04  1.26e+04\n\n\n  rand_col     451.1226   471.897     0.956  0.339  -474.004  1376.249\n\n\n\n\n  Omnibus:       2451.728   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31040.331\n\n\n  Skew:            2.046    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.552    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nAdding a variable with random values to the model (rand_col) increased the explained variation (R-squared). This is because the model has one more parameter to tune to reduce the residual squared error (RSS). However, the p-value of rand_col suggests that its coefficient is zero. Thus, using the model with rand_col may give poorer performance on unknown data, as compared to the model without rand_col. This implies that it is not a good idea to blindly add variables in the model to increase R-squared."
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html",
    "href": "Lec3_VariableTransformations_and_Interactions.html",
    "title": "3  Variable interactions and transformations",
    "section": "",
    "text": "Read sections 3.3.1 and 3.3.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately."
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#variable-interactions",
    "href": "Lec3_VariableTransformations_and_Interactions.html#variable-interactions",
    "title": "3  Variable interactions and transformations",
    "section": "3.1 Variable interactions",
    "text": "3.1 Variable interactions\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\nUntil now, we have assumed that the association between a predictor \\(X_j\\) and response \\(Y\\) does not depend on the value of other predictors. For example, the multiple linear regression model that we developed in Chapter 2 assumes that the average increase in price associated with a unit increase in engineSize is always $12,180, regardless of the value of other predictors. However, this assumption may be incorrect.\n\n3.1.1 Variable interaction between continuous predictors\nWe can relax this assumption by considering another predictor, called an interaction term. Let us assume that the average increase in price associated with a one-unit increase in engineSize depends on the model year of the car. In other words, there is an interaction between engineSize and year. This interaction can be included as a predictor, which is the product of engineSize and year. Note that there are several possible interactions that we can consider. Here the interaction between engineSize and year is just an example.\n\n#Considering interaction between engineSize and year\nols_object = smf.ols(formula = 'price~year*engineSize+mileage+mpg', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.682 \n\n\n  Model:                   OLS         Adj. R-squared:        0.681 \n\n\n  Method:             Least Squares    F-statistic:           2121. \n\n\n  Date:             Tue, 24 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 15:28:11       Log-Likelihood:      -52338. \n\n\n  No. Observations:        4960        AIC:                1.047e+05\n\n\n  Df Residuals:            4954        BIC:                1.047e+05\n\n\n  Df Model:                   5                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                     coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept        5.606e+05  2.74e+05     2.048  0.041   2.4e+04   1.1e+06\n\n\n  year             -275.3833   135.695    -2.029  0.042  -541.405    -9.361\n\n\n  engineSize      -1.796e+06  9.97e+04   -18.019  0.000 -1.99e+06  -1.6e+06\n\n\n  year:engineSize   896.7687    49.431    18.142  0.000   799.861   993.676\n\n\n  mileage            -0.1525     0.008   -17.954  0.000    -0.169    -0.136\n\n\n  mpg               -84.3417     9.048    -9.322  0.000  -102.079   -66.604\n\n\n\n\n  Omnibus:       2330.413   Durbin-Watson:         0.524 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   29977.437\n\n\n  Skew:            1.908    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.423    Cond. No.           7.66e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 7.66e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that the R-squared has increased as compared to the model in Chapter 2 since we added a predictor.\nThe model equation is:\n\\[\\begin{equation}\nprice = \\beta_0 + \\beta_1*year + \\beta_2*engineSize + \\beta_3*(year * engineSize) + \\beta4*mileage + \\beta_5*mpg,\n\\end{equation}\\]or\n\\[\\begin{equation}\nprice = \\beta_0 + \\beta_1*year + (\\beta_2+\\beta_3*year)*engineSize + \\beta4*mileage + \\beta_5*mpg,\n\\end{equation}\\]or\n\\[\\begin{equation}\nprice = \\beta_0 + \\beta_1*year + \\tilde \\beta*engineSize + \\beta4*mileage + \\beta_5*mpg,\n\\end{equation}\\]\nSince \\(\\tilde \\beta\\) is a function of year, the association between engineSize and price is no longer a constant. A change in the value of year will change the association between price and engineSize.\nSubstituting the values of the coefficients: \\[\\begin{equation}\nprice = 5.606e5 - 275.3833*year + (-1.796e6+896.7687*year)*engineSize -0.1525*mileage -84.3417*mpg\n\\end{equation}\\]\nThus, for cars launched in the year 2010, the average increase in price for one liter increase in engine size is -1.796e6 + 896.7687 * 2010 \\(\\approx\\) \\$6,500, assuming all the other predictors are constant. However, for cars launched in the year 2020, the average increase in price for one liter increase in engine size is -1.796e6 + 896.7687*2020 \\(\\approx\\) \\$15,500 , assuming all the other predictors are constant.\nSimilarly, the equation can be re-arranged as: \\[\\begin{equation}\nprice = 5.606e5 +(-275.3833+896.7687*engineSize)*year -1.796e6*engineSize -0.1525*mileage -84.3417*mpg\n\\end{equation}\\]\nThus, for cars with an engine size of 2 litres, the average increase in price for a one year newer model is -275.3833+896.7687 * 2 \\(\\approx\\) \\$1500, assuming all the other predictors are constant. However, for cars with an engine size of 3 litres, the average increase in price for a one year newer model is -275.3833+896.7687 * 3 \\(\\approx\\) \\$2400, assuming all the other predictors are constant.\n\n#Computing the RMSE of the model with the interaction term\npred_price = model.predict(testf)\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9423.598872501092\n\n\nNote that the RMSE is lower than that of the model in Chapter 2. This is because the interaction term between engineSize and year is significant and relaxes the assumption of constant association between price and engine size, and between price and year. This added flexibility makes the model better fit the data. Caution: Too much flexibility may lead to overfitting!\nNote that interaction terms corresponding to other variable pairs, and higher order interaction terms (such as those containing 3 or 4 variables) may also be significant and improve the model fit & thereby the prediction accuracy of the model.\n\n\n3.1.2 Including qualitative predictors in the model\nLet us develop a model for predicting price based on engineSize and the qualitative predictor transmission.\n\n#checking the distribution of values of transmission\ntrain.transmission.value_counts()\n\nManual       1948\nAutomatic    1660\nSemi-Auto    1351\nOther           1\nName: transmission, dtype: int64\n\n\nNote that the Other category of the variable transmission contains only a single observation, which is likely to be insufficient to train the model. We’ll remove that observation from the training data. Another option may be to combine the observation in the Other category with the nearest category, and keep it in the data.\n\ntrain_updated = train[train.transmission!='Other']\n\n\nols_object = smf.ols(formula = 'price~engineSize+transmission', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.459 \n\n\n  Model:                   OLS         Adj. R-squared:        0.458 \n\n\n  Method:             Least Squares    F-statistic:           1400. \n\n\n  Date:             Tue, 24 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 15:28:21       Log-Likelihood:      -53644. \n\n\n  No. Observations:        4959        AIC:                1.073e+05\n\n\n  Df Residuals:            4955        BIC:                1.073e+05\n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                  3042.6765   661.190     4.602  0.000  1746.451  4338.902\n\n\n  transmission[T.Manual]    -6770.6165   442.116   -15.314  0.000 -7637.360 -5903.873\n\n\n  transmission[T.Semi-Auto]  4994.3112   442.989    11.274  0.000  4125.857  5862.765\n\n\n  engineSize                 1.023e+04   247.485    41.323  0.000  9741.581  1.07e+04\n\n\n\n\n  Omnibus:       1575.518   Durbin-Watson:         0.579 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   11006.609\n\n\n  Skew:            1.334    Prob(JB):               0.00 \n\n\n  Kurtosis:        9.793    Cond. No.               11.4 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNote that there is no coefficient for the Automatic level of the variable Transmission. If a car doesn’t have Manual or Semi-Automatic transmission, then it has an Automatic transmission. Thus, the coefficient of Automatic will be redundant, and the dummy variable corresponding to Automatic transmission is dropped from the model.\nThe level of the categorical variable that is dropped from the model is called the baseline level. Here Automatic transmission is the baseline level. The coefficients of other levels of transmission should be interpreted with respect to the baseline level.\nQ: Interpret the intercept term\nAns: For the hypothetical scenario of a car with zero engine size and Automatic transmission, the estimated mean car price is \\(\\approx\\) \\$3042.\nQ: Interpret the coefficient of transmission[T.Manual]\nAns: The estimated mean price of a car with manual transmission is \\(\\approx\\) \\$6770 less than that of a car with Automatic transmission.\nLet us visualize the developed model.\n\n#Visualizing the developed model\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.set(font_scale = 1.3)\nx = np.linspace(train_updated.engineSize.min(),train_updated.engineSize.max(),100)\nax = sns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept'], color = 'red')\nsns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept']+model.params['transmission[T.Semi-Auto]'], color = 'blue')\nsns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept']+model.params['transmission[T.Manual]'], color = 'green')\nplt.legend(labels=[\"Automatic\",\"Semi-Automatic\", \"Manual\"])\nplt.xlabel('Engine size (in litre)')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nBased on the developed model, for a given engine size, the car with a semi-automatic transmission is estimated to be the most expensive on average, while the car with a manual transmission is estimated to be the least expensive on average.\nChanging the baseline level: By default, the baseline level is chosen as the one that comes first if the levels are arranged in alphabetical order. However, you can change the baseline level by specifying one explicitly.\nInternally, statsmodels uses the patsy package to convert formulas and data to the matrices that are used in model fitting. You may refer to this section in the patsy documentation to specify a particular level of the categorical variable as the baseline.\nFor example, suppose we wish to change the baseline level to Manual transmission. We can specify this in the formula as follows:\n\nols_object = smf.ols(formula = 'price~engineSize+C(transmission, Treatment(\"Manual\"))', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.459 \n\n\n  Model:                   OLS         Adj. R-squared:        0.458 \n\n\n  Method:             Least Squares    F-statistic:           1400. \n\n\n  Date:             Tue, 24 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 15:28:39       Log-Likelihood:      -53644. \n\n\n  No. Observations:        4959        AIC:                1.073e+05\n\n\n  Df Residuals:            4955        BIC:                1.073e+05\n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                                                       coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                                         -3727.9400   492.917    -7.563  0.000 -4694.275 -2761.605\n\n\n  C(transmission, Treatment(\"Manual\"))[T.Automatic]  6770.6165   442.116    15.314  0.000  5903.873  7637.360\n\n\n  C(transmission, Treatment(\"Manual\"))[T.Semi-Auto]  1.176e+04   473.110    24.867  0.000  1.08e+04  1.27e+04\n\n\n  engineSize                                         1.023e+04   247.485    41.323  0.000  9741.581  1.07e+04\n\n\n\n\n  Omnibus:       1575.518   Durbin-Watson:         0.579 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   11006.609\n\n\n  Skew:            1.334    Prob(JB):               0.00 \n\n\n  Kurtosis:        9.793    Cond. No.               8.62 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n3.1.3 Including qualitative predictors and their interaction with continuous predictors in the model\nNote that the qualitative predictor leads to fitting 3 parallel lines to the data, as there are 3 categories.\nHowever, note that we have made the constant association assumption. The fact that the lines are parallel means that the average increase in car price for one litre increase in engine size does not depend on the type of transmission. This represents a potentially serious limitation of the model, since in fact a change in engine size may have a very different association on the price of an automatic car versus a semi-automatic or manual car.\nThis limitation can be addressed by adding an interaction variable, which is the product of engineSize and the dummy variables for semi-automatic and manual transmissions.\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~engineSize*transmission', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.479 \n\n\n  Model:                   OLS         Adj. R-squared:        0.478 \n\n\n  Method:             Least Squares    F-statistic:           909.9 \n\n\n  Date:             Sun, 22 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 22:55:55       Log-Likelihood:      -53550. \n\n\n  No. Observations:        4959        AIC:                1.071e+05\n\n\n  Df Residuals:            4953        BIC:                1.072e+05\n\n\n  Df Model:                   5                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                                          coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                             3754.7238   895.221     4.194  0.000  1999.695  5509.753\n\n\n  transmission[T.Manual]                1768.5856  1294.071     1.367  0.172  -768.366  4305.538\n\n\n  transmission[T.Semi-Auto]            -5282.7164  1416.472    -3.729  0.000 -8059.628 -2505.805\n\n\n  engineSize                            9928.6082   354.511    28.006  0.000  9233.610  1.06e+04\n\n\n  engineSize:transmission[T.Manual]    -5285.9059   646.175    -8.180  0.000 -6552.695 -4019.117\n\n\n  engineSize:transmission[T.Semi-Auto]  4162.2428   552.597     7.532  0.000  3078.908  5245.578\n\n\n\n\n  Omnibus:       1379.846   Durbin-Watson:         0.622\n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   9799.471\n\n\n  Skew:            1.139    Prob(JB):               0.00\n\n\n  Kurtosis:        9.499    Cond. No.               30.8\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe model equation for the model with interactions is:\nAutomatic transmission: \\(price = 3754.7238 + 9928.6082 * engineSize\\),\nSemi-Automatic transmission: \\(price = 3754.7238 + 9928.6082 * engineSize + (-5282.7164+4162.2428*engineSize)\\),\nManual transmission: \\(price = 3754.7238 + 9928.6082 * engineSize +(1768.5856-5285.9059*engineSize)\\), or\nAutomatic transmission: \\(price = 3754.7238 + 9928.6082 * engineSize\\),\nSemi-Automatic transmission: \\(price = -1527 + 7046 * engineSize\\),\nManual transmission: \\(price = 5523 + 4642 * engineSize\\),\nQ: Interpret the coefficient of manual tranmission, i.e., the coefficient of transmission[T.Manual].\nA: For a given engine size, the estimated mean price of a car with Manual transmission is \\(\\approx\\) \\$1768 more than the estimated mean price of a car with Automatic transmission.\nQ: Interpret the coefficient of the interaction between engine size and manual transmission, i.e., the coefficient of engineSize:transmission[T.Manual].\nA: For a unit (or a litre) increase in engineSize , the increase in estimated mean price of a car with Manual transmission is \\(\\approx\\) \\$5285 less than the increase in estimated mean price of a car with Automatic transmission.\n\n#Visualizing the developed model with interaction terms\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.set(font_scale = 1.3)\nx = np.linspace(train_updated.engineSize.min(),train_updated.engineSize.max(),100)\nax = sns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept'], label='Automatic', color = 'red')\nplt.plot(x, (model.params['engineSize']+model.params['engineSize:transmission[T.Semi-Auto]'])*x+model.params['Intercept']+model.params['transmission[T.Semi-Auto]'], '-b', label='Semi-Automatic')\nplt.plot(x, (model.params['engineSize']+model.params['engineSize:transmission[T.Manual]'])*x+model.params['Intercept']+model.params['transmission[T.Manual]'], '-g', label='Manual')\nplt.legend(loc='upper left')\nplt.xlabel('Engine size (in litre)')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nNote the interaction term adds flexibility to the model.\nThe slope of the regression line for semi-automatic cars is the largest. This suggests that increase in engine size is associated with a higher increase in car price for semi-automatic cars, as compared to other cars."
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#variable-transformations",
    "href": "Lec3_VariableTransformations_and_Interactions.html#variable-transformations",
    "title": "3  Variable interactions and transformations",
    "section": "3.2 Variable transformations",
    "text": "3.2 Variable transformations\nSo far we have considered only a linear relationship between the predictors and the response. However, the relationship may be non-linear.\nConsider the regression plot of price on mileage.\n\nax = sns.regplot(x = train_updated.mileage, y =train_updated.price,color = 'orange', line_kws = {'color':'blue'})\nplt.xlabel('Mileage')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\n#R-squared of the model with just mileage\nmodel = smf.ols('price~mileage', data = train_updated).fit()\nmodel.rsquared\n\n0.22928048993376182\n\n\nFrom the first scatterplot, we see that the relationship between price and mileage doesn’t seem to be linear, as the points do not lie on a straight line. Also, we see the regression line (or the curve), which is the best fit line doesn’t seem to fit the points well. However, price on average seems to decrease with mileage, albeit in a non-linear manner.\n\n3.2.1 Quadratic transformation\nSo, we guess that if we model price as a quadratic function of mileage, the model may better fit the points (or the curve may better fit the points). Let us transform the predictor mileage to include \\(mileage^2\\) (i.e., perform a quadratic transformation on the predictor).\n\n#Including mileage squared as a predictor and developing the model\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.271 \n\n\n  Model:                   OLS         Adj. R-squared:        0.271 \n\n\n  Method:             Least Squares    F-statistic:           920.6 \n\n\n  Date:             Sun, 22 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 23:26:05       Log-Likelihood:      -54382. \n\n\n  No. Observations:        4959        AIC:                1.088e+05\n\n\n  Df Residuals:            4956        BIC:                1.088e+05\n\n\n  Df Model:                   2                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                     coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept         3.44e+04   332.710   103.382  0.000  3.37e+04   3.5e+04\n\n\n  mileage            -0.5662     0.017   -33.940  0.000    -0.599    -0.534\n\n\n  I(mileage ** 2)  2.629e-06  1.56e-07    16.813  0.000  2.32e-06  2.94e-06\n\n\n\n\n  Omnibus:       2362.973   Durbin-Watson:         0.325 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   22427.952\n\n\n  Skew:            2.052    Prob(JB):               0.00 \n\n\n  Kurtosis:       12.576    Cond. No.           4.81e+09 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 4.81e+09. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that in the formula specified within the ols() function, the I() operator isolates or insulates the contents within I(…) from the regular formula operators. Without the I() operator, mileage**2 will be treated as the interaction of mileage with itself, which is mileage. Thus, to add the square of mileage as a separate predictor, we need to use the I() operator.\nLet us visualize the model fit with the quadratic transformation of the predictor - mileage.\n\n#Visualizing the regression line with the model consisting of the quadratic transformation of the predictor - mileage\npred_price = model.predict(train_updated)\nax = sns.scatterplot(x = 'mileage', y = 'price', data = train_updated, color = 'orange')\nsns.lineplot(x = train_updated.mileage, y = pred_price, color = 'blue')\nplt.xlabel('Mileage')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nThe above model seems to better fit the data (as compared to the model without transformation) at least upto mileage around 125,000. The \\(R^2\\) of the model with the quadratic transformation of mileage is also higher than that of the model without transformation indicating a better fit.\n\n\n3.2.2 Cubic transformation\nLet us see if a cubic transformation of mileage can further improve the model fit.\n\n#Including mileage squared and mileage cube as predictors and developing the model\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)+I(mileage**3)', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.283 \n\n\n  Model:                   OLS         Adj. R-squared:        0.283 \n\n\n  Method:             Least Squares    F-statistic:           652.3 \n\n\n  Date:             Sun, 22 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 23:33:27       Log-Likelihood:      -54340. \n\n\n  No. Observations:        4959        AIC:                1.087e+05\n\n\n  Df Residuals:            4955        BIC:                1.087e+05\n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                     coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept        3.598e+04   371.926    96.727  0.000  3.52e+04  3.67e+04\n\n\n  mileage            -0.7742     0.028   -27.634  0.000    -0.829    -0.719\n\n\n  I(mileage ** 2)  6.875e-06  4.87e-07    14.119  0.000  5.92e-06  7.83e-06\n\n\n  I(mileage ** 3) -1.823e-11  1.98e-12    -9.199  0.000 -2.21e-11 -1.43e-11\n\n\n\n\n  Omnibus:       2380.788   Durbin-Watson:         0.321 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   23039.307\n\n\n  Skew:            2.065    Prob(JB):               0.00 \n\n\n  Kurtosis:       12.719    Cond. No.           7.73e+14 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 7.73e+14. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Visualizing the model with the cubic transformation of mileage\npred_price = model.predict(train_updated)\nax = sns.scatterplot(x = 'mileage', y = 'price', data = train_updated, color = 'orange')\nsns.lineplot(x = train_updated.mileage, y = pred_price, color = 'blue')\nplt.xlabel('Mileage')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\nNote that the model fit with the cubic transformation of mileage seems slightly better as compared to the models with the quadratic transformation, and no transformation of mileage, for mileage up to 180k. However, the model should not be used to predict car prices of cars with a mileage higher than 180k.\nLet’s update the model created earlier (in the beginning of this chapter) to include the transformed predictor.\n\n#Model with an interaction term and a variable transformation term\nols_object = smf.ols(formula = 'price~year*engineSize+mileage+mpg+I(mileage**2)', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.702 \n\n\n  Model:                   OLS         Adj. R-squared:        0.702 \n\n\n  Method:             Least Squares    F-statistic:           1947. \n\n\n  Date:             Sun, 22 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 23:42:13       Log-Likelihood:      -52162. \n\n\n  No. Observations:        4959        AIC:                1.043e+05\n\n\n  Df Residuals:            4952        BIC:                1.044e+05\n\n\n  Df Model:                   6                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                     coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept         1.53e+06   2.7e+05     5.671  0.000     1e+06  2.06e+06\n\n\n  year             -755.7419   133.791    -5.649  0.000 -1018.031  -493.453\n\n\n  engineSize      -2.022e+06  9.72e+04   -20.803  0.000 -2.21e+06 -1.83e+06\n\n\n  year:engineSize  1008.6993    48.196    20.929  0.000   914.215  1103.184\n\n\n  mileage            -0.3548     0.014   -25.973  0.000    -0.382    -0.328\n\n\n  mpg               -54.7450     8.896    -6.154  0.000   -72.185   -37.305\n\n\n  I(mileage ** 2)  1.926e-06  1.04e-07    18.536  0.000  1.72e-06  2.13e-06\n\n\n\n\n  Omnibus:       2355.448   Durbin-Watson:         0.562 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   38317.404\n\n\n  Skew:            1.857    Prob(JB):               0.00 \n\n\n  Kurtosis:       16.101    Cond. No.           6.40e+12 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.4e+12. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that the R-squared has increased as compared to the model with just the interaction term.\n\n#Computing RMSE on test data\npred_price = model.predict(testf)\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9074.494088619422\n\n\nNote that the prediction accuracy of the model has further increased, as the RMSE has reduced. The transformed predictor is statistically significant and provides additional flexibility to better capture the trend in the data, leading to an increase in prediction accuracy."
  },
  {
    "objectID": "Lec4_ModelAssumptions.html",
    "href": "Lec4_ModelAssumptions.html",
    "title": "4  Model assumptions",
    "section": "",
    "text": "Read section 3.3.3 (1 & 3) of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nConsider the model with interactions and transformation developed previously.\nLet us check if this model satisfies the assumptions of the linear regression model"
  },
  {
    "objectID": "Lec4_ModelAssumptions.html#non-linearity-of-data",
    "href": "Lec4_ModelAssumptions.html#non-linearity-of-data",
    "title": "4  Model assumptions",
    "section": "4.1 Non-linearity of data",
    "text": "4.1 Non-linearity of data\nWe have assumed that there is a linear relationship between the predictors and the response. Residual plots, which are scatter plots of residuals vs fitted values, can be used to identify non-linearity. Fitted values are the values estimated by the model on training data, denoted by \\(\\hat y_i\\), and residuals are given by \\(e_i = y_i - \\hat y_i\\).\n\n#Plotting residuals vs fitted values\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.set(font_scale=1.25)\nax = sns.scatterplot(x = model.fittedvalues, y=model.resid,color = 'orange')\nsns.lineplot(x = [pred_price.min(),pred_price.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nThe model seems to satisfy this assumption, as we do not observe a strong pattern in the residuals around the line Residuals = 0. Residuals are distributed more or less in a similar manner on both sides of the blue line for all fitted values.\nFor the model to satisfy the linearity assumption perfectly, the points above the line (Residuals = 0), should be mirror image of the points below the line, i.e., the blue line in the above plot should act as a mirror.\nWhat to do if there is non-linear association (page 94 of book): If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as \\(log X, \\sqrt X\\), and \\(X^2\\), in the regression model."
  },
  {
    "objectID": "Lec4_ModelAssumptions.html#non-constant-variance-of-error-terms",
    "href": "Lec4_ModelAssumptions.html#non-constant-variance-of-error-terms",
    "title": "4  Model assumptions",
    "section": "4.2 Non-constant variance of error terms",
    "text": "4.2 Non-constant variance of error terms\nThe variance of the error terms is assumed to be constant, i.e., \\(Var(\\epsilon_i) = \\sigma^2\\), and this assumption is used while deriving the standard errors of the regression coefficients. The standard errors in turn are used to test the significant of the predictors, and obtain their confidence interval. Thus, violation of this assumption may lead to incorrect inference. Non-constant variance of error terms, or violation of the constant variance assumption, is called heteroscedasticity.\nThis assumption can be checked by plotting the residuals against fitted values.\n\n#Plotting residuals vs fitted values\nax = sns.scatterplot(x = model.fittedvalues, y=model.resid,color = 'orange')\nsns.lineplot(x = [pred_price.min(),pred_price.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\nWe see that the variance of errors seems to increase with increase in the fitted values. In such a case a log transformation of the response can resolve the issue to some extent. This is because a log transformation will result in a higher shrinkage of larger values.\n\n#Model with an interaction term and a variable transformation term\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:      np.log(price)    R-squared:             0.803\n\n\n  Model:                   OLS         Adj. R-squared:        0.803\n\n\n  Method:             Least Squares    F-statistic:           1834.\n\n\n  Date:             Wed, 25 Jan 2023   Prob (F-statistic):    0.00 \n\n\n  Time:                 11:37:55       Log-Likelihood:      -1173.8\n\n\n  No. Observations:        4960        AIC:                   2372.\n\n\n  Df Residuals:            4948        BIC:                   2450.\n\n\n  Df Model:                  11                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                        coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept           -238.2125    25.790    -9.237  0.000  -288.773  -187.652\n\n\n  year                   0.1227     0.013     9.608  0.000     0.098     0.148\n\n\n  engineSize            13.8349     5.795     2.387  0.017     2.475    25.195\n\n\n  mileage                0.0005     0.000     3.837  0.000     0.000     0.001\n\n\n  mpg                   -1.2446     0.345    -3.610  0.000    -1.921    -0.569\n\n\n  year:engineSize       -0.0067     0.003    -2.324  0.020    -0.012    -0.001\n\n\n  year:mileage        -2.67e-07   6.8e-08    -3.923  0.000    -4e-07 -1.34e-07\n\n\n  year:mpg               0.0006     0.000     3.591  0.000     0.000     0.001\n\n\n  engineSize:mileage -2.668e-07  4.08e-07    -0.654  0.513 -1.07e-06  5.33e-07\n\n\n  engineSize:mpg         0.0028     0.000     6.842  0.000     0.002     0.004\n\n\n  mileage:mpg         7.235e-08  1.79e-08     4.036  0.000  3.72e-08  1.08e-07\n\n\n  I(mileage ** 2)     1.828e-11  5.64e-12     3.240  0.001  7.22e-12  2.93e-11\n\n\n\n\n  Omnibus:       711.515   Durbin-Watson:         0.498\n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):   2545.807\n\n\n  Skew:           0.699    Prob(JB):               0.00\n\n\n  Kurtosis:       6.220    Cond. No.           1.73e+13\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.73e+13. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that the coefficient of year turns out to be significant (at 5% significance level), unlike in the previous model. Intuitively, the coefficient of year should have been significant, as year has the highest linear correlation of 50% with car price.\nAlthough the R-squared has increased as compared to the previous model, violation of this assumption does not cause bias in the regression coefficients. Thus, there may not be a large improvement in the model fit, unless we add predictor(s) to address heteroscedasticity.\nLet us check the constant variance assumption again.\n\n#Plotting residuals vs fitted values\nsns.scatterplot(x = (model_log.fittedvalues), y=(model_log.resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\n\nText(0, 0.5, 'Residuals')\n\n\n\n\n\nNow we observe that the constant variance assumption is satisfied. Let us see the RMSE of this model on test data.\n\n#Computing RMSE on test data\npred_price_log = model_log.predict(testf)\nnp.sqrt(((testp.price - np.exp(pred_price_log))**2).mean())\n\n9094.209503063496\n\n\nNote that the RMSE of the log-transformed model has increased as compared to the model without transformation. Does it mean the log-transformed model is less accurate?\n\n#Computing MAE on test data\npred_price_log = model_log.predict(testf)\n((np.abs(testp.price - np.exp(pred_price_log))).mean())\n\n5268.398904745121\n\n\nAlthough the RMSE has increased a bit for the log-transformed model, the MAE has reduced. This means the log-transformed model does a bit worse on reducing relatively large errors, but does better in reducing the absolute errors on an average.\n\n#Comparing errors of the log-transformed model with the previous model\nerr = np.abs(testp.price - pred_price)\nerr_log = np.abs(testp.price - np.exp(pred_price_log))\nsns.scatterplot(x = err,y = err_log, color = 'orange')\nsns.lineplot(x = [0,100000], y = [0,100000], color = 'blue')\nnp.sum(err_log<err)/len(err)\n\n0.5572604790419161\n\n\n\n\n\nFor 56% of the cars, the log transformed makes a more accurate prediction than the previous model, which is another criterion based on which the log-transformed model is more accurate. However, the conclusion based on RMSE is different. This is because RMSE can be influenced by a few large errors. Thus, RMSE, though sometimes appropriate than other criteria, should not be used as the sole measure to compare the accuracy of models.\n\n#Visualizing the distribution of price and log(price)\nfig = plt.figure()\nfig.subplots_adjust(hspace=0.4, wspace=0.2)\nsns.set(rc = {'figure.figsize':(20,12)})\nsns.set(font_scale = 2)\nax = fig.add_subplot(2, 2, 1)\nsns.histplot(train.price,kde=True)\nax.set(xlabel='price', ylabel='Count')\nax = fig.add_subplot(2, 2, 2)\nsns.histplot(np.log(train.price),kde=True)\nax.set(xlabel='log price', ylabel='Count')\n\n[Text(0.5, 0, 'log price'), Text(0, 0.5, 'Count')]\n\n\n\n\n\nWe can see that the log transformation shrinked the higher values of price, making its distribution closer to normal.\nNote that heteroscedasticity can also occur due to model misspecification, i.e., in case of missing predictor(s). Some of the cars are too expensive, which makes the price distribution skewed. Perhaps, the price of expensive cars could be better explained by the car model, a predictor that is missing in the current model."
  },
  {
    "objectID": "Lec5_Potential_issues.html",
    "href": "Lec5_Potential_issues.html",
    "title": "5  Potential issues",
    "section": "",
    "text": "Read section 3.3.3 (4, 5, & 6) of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nLet us continue with the car price prediction example from the previous chapter."
  },
  {
    "objectID": "Lec5_Potential_issues.html#outliers",
    "href": "Lec5_Potential_issues.html#outliers",
    "title": "5  Potential issues",
    "section": "5.1 Outliers",
    "text": "5.1 Outliers\nAn outlier is a point for which the true response (\\(y_i\\)) is far from the value predicted by the model. Residual plots can be used to identify outliers.\nIf the the response at the \\(i^{th}\\) observation is \\(y_i\\), the prediction is \\(\\hat{y}_i\\), then the residual \\(e_i\\) is:\n\\[e_i = y_i - \\hat{y_i}\\]\n\n#Plotting residuals vs fitted values\nsns.set(rc={'figure.figsize':(10,6)})\nsns.scatterplot(x = (model_log.fittedvalues), y=(model_log.resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\n\nText(0, 0.5, 'Residuals')\n\n\n\n\n\nSome of the errors may be high. However, it is difficult to decide how large a residual needs to be before we can consider a point to be an outlier. To address this problem, we have standardized residuals, which are defined as:\n\\[r_i = \\frac{e_i}{RSE(\\sqrt{1-h_{ii}})},\\]\nwhere \\(r_i\\) is the standardized residual, \\(RSE\\) is the residual standard error, and \\(h_{ii}\\) is the leverage (introduced in the next section) of the \\(i^{th}\\) observation.\nStandardized residuals, allow the residuals to be compared on a standard scale.\nIssue with standardized residuals:, If the observation corresponding to the standardized residual has a high leverage, then it will drag the regression line / plane / hyperplane towards it, thereby influencing the estimate of the residual itself.\nStudentized residuals: To address the issue with standardized residuals, studentized residual for the \\(i^{th}\\) observation is computed as the standardized residual, but with the \\(RSE\\) (residual standard error) computed after removing the \\(i^{th}\\) observation from the data. Studentized residual, \\(t_i\\) for the \\(i^{th}\\) observation is given as:\n\\[t_i = \\frac{e_i}{RSE_{i}(\\sqrt{1-h_{ii}})},\\]\nwhere \\(RSE_{i}\\) is the residual standard error of the model developed on the data without the \\(i^{th}\\) observation.\nStudentized residuals follow a \\(t\\) distribution with \\((n–p–2)\\) degrees of freedom. Thus, in general, observations whose studentized residuals have a magnitude higher than 3 are potential outliers.\nLet us find the studentized residuals in our car price prediction model.\n\n#Studentized residuals\nout = model_log.outlier_test()\nout\n\n\n\n\n\n  \n    \n      \n      student_resid\n      unadj_p\n      bonf(p)\n    \n  \n  \n    \n      0\n      -1.164204\n      0.244398\n      1.0\n    \n    \n      1\n      -0.801879\n      0.422661\n      1.0\n    \n    \n      2\n      -1.263820\n      0.206354\n      1.0\n    \n    \n      3\n      -0.614171\n      0.539130\n      1.0\n    \n    \n      4\n      0.027930\n      0.977719\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      4955\n      -0.523361\n      0.600747\n      1.0\n    \n    \n      4956\n      -0.509539\n      0.610397\n      1.0\n    \n    \n      4957\n      -1.718802\n      0.085713\n      1.0\n    \n    \n      4958\n      -0.077595\n      0.938153\n      1.0\n    \n    \n      4959\n      -0.482388\n      0.629551\n      1.0\n    \n  \n\n4960 rows × 3 columns\n\n\n\nStudentized residuals are in the first column of the above table.\n\n#Plotting studentized residuals vs fitted values\nsns.scatterplot(x = (model_log.fittedvalues), y=(out.student_resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Studentized Residuals')\n\nText(0, 0.5, 'Studentized Residuals')\n\n\n\n\n\nPotential outliers: Observations whose studentized residuals have a magnitude greater than 3.\nImpact of outliers: Outliers do not have a large impact on the OLS line / plane / hyperplane. However, outliers do inflate the residual standard error (RSE). RSE in turn is used to compute the standard errors of regression coefficients. As a result, statistically significant variables may appear to be insignificant, and \\(R^2\\) may appear to be lower.\n\n#Number of points with absolute studentized residuals greater than 3\nnp.sum((np.abs(out.student_resid)>3))\n\n86\n\n\nAre there outliers in our example?: In the above plot, there are 86 points with absolute studentized residuals larger than 3. However, most of the predictors are significant and R-squared has a relatively high value of 80%. Thus, even if there are outliers, there is no need to remove them as it is unlikely to change the significance of individual variables. Furthermore, looking into the data, we find that the price of some of the luxury cars such as Mercedez G-class is actually much higher than average. So, the potential outliers in the data do not seem to be due to incorrect data. The high studentized residuals may be due to some deficiency in the model, such as missing predictor(s) (like car model), rather than incorrect data. Thus, we should not remove any data that has an outlying value of log(price).\nSince model seems to be a variable that can explain the price of overly expensive cars, let us include it in the regression model.\n\n#Model with an interaction term and a variable transformation term\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)+model', data = train)\nmodel_log = ols_object.fit()\n#Model summary not printed to save space\n#model_log.summary()\n\n\n#Computing RMSE on test data with car 'model' as one of the predictors\npred_price_log2 = model_log.predict(testf)\nnp.sqrt(((testp.price - np.exp(pred_price_log2))**2).mean())\n\n4252.20045604376\n\n\n\n#Plotting studentized residuals vs fitted values for the model with car 'model' as one of the predictors\nout = model_log.outlier_test()\nsns.scatterplot(x = (model_log.fittedvalues), y=(out.student_resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\n\nText(0, 0.5, 'Residuals')\n\n\n\n\n\n\n#Number of points with absolute studentized residuals greater than 3\nnp.sum((np.abs(out.student_resid)>3))\n\n69\n\n\nNote the RMSE has reduced to almost half of its value as compared to the regression model without the predictor - model. Car model does help better explain the variation in price of cars! The number of points with absolute studentized residuals greater than 3 has also reduced to 69 from 86."
  },
  {
    "objectID": "Lec5_Potential_issues.html#high-leverage-points",
    "href": "Lec5_Potential_issues.html#high-leverage-points",
    "title": "5  Potential issues",
    "section": "5.2 High leverage points",
    "text": "5.2 High leverage points\nHigh leverage points are those with an unsual value of the predictor(s). They have a relatively higher impact on the OLS line / plane / hyperplane, as compared to the outliers.\nLeverage statistic (page 99 of the book): In order to quantify an observation’s leverage, we compute the leverage statistic. A large value of this statistic indicates an observation with high leverage. For simple linear regression, \\[\\begin{equation}\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar x)^2}{\\sum_{i'=1}^{n}(x_{i'} - \\bar x)^2}.\n\\end{equation}\\]\nIt is clear from this equation that \\(h_i\\) increases with the distance of \\(x_i\\) from \\(\\bar x\\).The leverage statistic \\(h_i\\) is always between \\(1/n\\) and \\(1\\), and the average leverage for all the observations is always equal to \\((p+1)/n\\). So if a given observation has a leverage statistic that greatly exceeds \\((p+1)/n\\), then we may suspect that the corresponding point has high leverage.\nInfluential points: Note that if a high leverage point falls in line with the regression line, then it will not affect the regression line. However, it may inflate R-squared and increase the significance of predictors. If a high leverage point falls away from the regression line, then it is also an outlier, and will affect the regression line. The points whose presence significantly affects the regression line are called influential points. A point that is both a high leverage point and an outlier is likely to be an influential point. However, a high leverage point is not necessarily an influential point.\nSource for influential points: https://online.stat.psu.edu/stat501/book/export/html/973\nLet us see if there are any high leverage points in our regression model without the predictor - model.\n\n#Model with an interaction term and a variable transformation term\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:      np.log(price)    R-squared:             0.803\n\n\n  Model:                   OLS         Adj. R-squared:        0.803\n\n\n  Method:             Least Squares    F-statistic:           1834.\n\n\n  Date:             Sun, 05 Feb 2023   Prob (F-statistic):    0.00 \n\n\n  Time:                 19:31:59       Log-Likelihood:      -1173.8\n\n\n  No. Observations:        4960        AIC:                   2372.\n\n\n  Df Residuals:            4948        BIC:                   2450.\n\n\n  Df Model:                  11                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                        coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept           -238.2125    25.790    -9.237  0.000  -288.773  -187.652\n\n\n  year                   0.1227     0.013     9.608  0.000     0.098     0.148\n\n\n  engineSize            13.8349     5.795     2.387  0.017     2.475    25.195\n\n\n  mileage                0.0005     0.000     3.837  0.000     0.000     0.001\n\n\n  mpg                   -1.2446     0.345    -3.610  0.000    -1.921    -0.569\n\n\n  year:engineSize       -0.0067     0.003    -2.324  0.020    -0.012    -0.001\n\n\n  year:mileage        -2.67e-07   6.8e-08    -3.923  0.000    -4e-07 -1.34e-07\n\n\n  year:mpg               0.0006     0.000     3.591  0.000     0.000     0.001\n\n\n  engineSize:mileage -2.668e-07  4.08e-07    -0.654  0.513 -1.07e-06  5.33e-07\n\n\n  engineSize:mpg         0.0028     0.000     6.842  0.000     0.002     0.004\n\n\n  mileage:mpg         7.235e-08  1.79e-08     4.036  0.000  3.72e-08  1.08e-07\n\n\n  I(mileage ** 2)     1.828e-11  5.64e-12     3.240  0.001  7.22e-12  2.93e-11\n\n\n\n\n  Omnibus:       711.515   Durbin-Watson:         0.498\n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):   2545.807\n\n\n  Skew:           0.699    Prob(JB):               0.00\n\n\n  Kurtosis:       6.220    Cond. No.           1.73e+13\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.73e+13. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Computing the leverage statistic for each observation\ninfluence = model_log.get_influence()\nleverage = influence.hat_matrix_diag\n\n\n#Visualizng leverage against studentized residuals\nsns.set(rc={'figure.figsize':(15,8)})\nsm.graphics.influence_plot(model_log);\n\n\n\n\nLet us identify the high leverage points in the data, as they may be affecting the regression line if they are outliers as well, i.e., if they are influential points. Note that there is no defined threshold for a point to be classified as a high leverage point. Some statisticians consider points having twice the average leverage as high leverage points, some consider points having thrice the average leverage as high leverage points, and so on.\n\nout = model_log.outlier_test()\n\n\n#Average leverage of points\naverage_leverage = (model_log.df_model+1)/model_log.nobs\naverage_leverage\n\n0.0024193548387096775\n\n\nLet us consider points having four times the average leverage as high leverage points.\n\n#We will remove all observations that have leverage higher than the threshold value.\nhigh_leverage_threshold = 4*average_leverage\n\n\n#Number of high leverage points in the dataset\nnp.sum(leverage>high_leverage_threshold)\n\n197"
  },
  {
    "objectID": "Lec5_Potential_issues.html#influential-points",
    "href": "Lec5_Potential_issues.html#influential-points",
    "title": "5  Potential issues",
    "section": "5.3 Influential points",
    "text": "5.3 Influential points\nObservations that are both high leverage points and outliers are influential points that may affect the regression line. Let’s remove these influential points from the data and see if it improves the model prediction accuracy on test data.\n\n#Dropping influential points from data\ntrain_filtered = train.drop(np.intersect1d(np.where(np.abs(out.student_resid)>3)[0],\n                                           (np.where(leverage>high_leverage_threshold)[0])))\n\n\ntrain_filtered.shape\n\n(4921, 11)\n\n\n\n#Number of points removed as they were influential\ntrain.shape[0]-train_filtered.shape[0]\n\n39\n\n\nWe removed 39 influential data points from the training data.\n\n#Model after removing the influential observations\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train_filtered)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:      np.log(price)    R-squared:             0.830\n\n\n  Model:                   OLS         Adj. R-squared:        0.829\n\n\n  Method:             Least Squares    F-statistic:           2173.\n\n\n  Date:             Sun, 29 Jan 2023   Prob (F-statistic):    0.00 \n\n\n  Time:                 01:26:25       Log-Likelihood:      -775.51\n\n\n  No. Observations:        4921        AIC:                   1575.\n\n\n  Df Residuals:            4909        BIC:                   1653.\n\n\n  Df Model:                  11                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                        coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept           -262.7743    24.455   -10.745  0.000  -310.717  -214.832\n\n\n  year                   0.1350     0.012    11.148  0.000     0.111     0.159\n\n\n  engineSize            16.6645     5.482     3.040  0.002     5.917    27.412\n\n\n  mileage                0.0008     0.000     5.945  0.000     0.001     0.001\n\n\n  mpg                   -1.1217     0.324    -3.458  0.001    -1.758    -0.486\n\n\n  year:engineSize       -0.0081     0.003    -2.997  0.003    -0.013    -0.003\n\n\n  year:mileage       -3.927e-07   6.5e-08    -6.037  0.000  -5.2e-07 -2.65e-07\n\n\n  year:mpg               0.0005     0.000     3.411  0.001     0.000     0.001\n\n\n  engineSize:mileage -4.566e-07  3.86e-07    -1.183  0.237 -1.21e-06     3e-07\n\n\n  engineSize:mpg         0.0071     0.000    16.202  0.000     0.006     0.008\n\n\n  mileage:mpg          7.29e-08  1.68e-08     4.349  0.000     4e-08  1.06e-07\n\n\n  I(mileage ** 2)     1.418e-11  5.29e-12     2.683  0.007  3.82e-12  2.46e-11\n\n\n\n\n  Omnibus:       631.414   Durbin-Watson:         0.553\n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):   1851.015\n\n\n  Skew:           0.682    Prob(JB):               0.00\n\n\n  Kurtosis:       5.677    Cond. No.           1.73e+13\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.73e+13. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that we obtain a higher R-sqauared value of 83% as compared to 80% with the complete data. Removing the influential points helped obtain a better model fit. However, that may also happen just by reducing observations.\n\n#Computing RMSE on test data\npred_price_log = model_log.predict(testf)\nnp.sqrt(((testp.price - np.exp(pred_price_log))**2).mean())\n\n8820.685844070766\n\n\nThe RMSE on test data has also reduced. This shows that some of the influential points were impacting the regression line. With those points removed, the model better captures the general trend in the data."
  },
  {
    "objectID": "Lec5_Potential_issues.html#collinearity",
    "href": "Lec5_Potential_issues.html#collinearity",
    "title": "5  Potential issues",
    "section": "5.4 Collinearity",
    "text": "5.4 Collinearity\nCollinearity refers to the situation when two or more predictor variables have a high linear association. Linear association between a pair of variables can be measured by the correlation coefficient. Thus the correlation matrix can indicate some potential collinearity problems.\n\n5.4.1 Why and how is collinearity a problem\n(Source: page 100-101 of book)\nThe presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response.\nSince collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for \\(\\hat \\beta_j\\) to grow. Recall that the t-statistic for each predictor is calculated by dividing \\(\\hat \\beta_j\\) by its standard error. Consequently, collinearity results in a decline in the \\(t\\)-statistic. As a result, in the presence of collinearity, we may fail to reject \\(H_0: \\beta_j = 0\\). This means that the power of the hypothesis test—the probability of correctly detecting a non-zero coefficient—is reduced by collinearity.\n\n\n5.4.2 How to measure collinearity/multicollinearity\n(Source: page 102 of book)\nUnfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity. Instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF is variance inflation factor the ratio of the variance of \\(\\hat \\beta_j\\) when fitting the full model divided by the variance of \\(\\hat \\beta_j\\) if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. Typically in practice there is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.\nThe estimated variance of the coefficient \\(\\beta_j\\), of the \\(j^{th}\\) predictor \\(X_j\\), can be expressed as:\n\\[\\hat{var}(\\hat{\\beta_j}) = \\frac{(\\hat{\\sigma})^2}{(n-1)\\hat{var}({X_j})}.\\frac{1}{1-R^2_{X_j|X_{-j}}},\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the \\(R\\)-squared for the regression of \\(X_j\\) on the other covariates (a regression that does not involve the response variable \\(Y\\)).\nIn case of simple linear regression, the variance expression in the equation above does not contain the term \\(\\frac{1}{1-R^2_{X_j|X_{-j}}}\\), as there is only one predictor. However, in case of multiple linear regression, the variance of the estimate of the \\(j^{th}\\) coefficient (\\(\\hat{\\beta_j}\\)) gets inflated by a factor of \\(\\frac{1}{1-R^2_{X_j|X_{-j}}}\\) (Note that in the complete absence of collinearity, \\(R^2_{X_j|X_{-j}}=0\\), and the value of this factor will be 1).\nThus, the Variance inflation factor, or the VIF for the estimated coefficient of the \\(j^{th}\\) predictor \\(X_j\\) is:\n\\[\\begin{equation}\nVIF(\\hat \\beta_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\n\\end{equation}\\]\n\n#Correlation matrix\ntrain.corr()\n\n\n\n\n\n  \n    \n      \n      carID\n      year\n      mileage\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      carID\n      1.000000\n      0.006251\n      -0.001320\n      0.023806\n      -0.010774\n      0.011365\n      0.012129\n    \n    \n      year\n      0.006251\n      1.000000\n      -0.768058\n      -0.205902\n      -0.057093\n      0.014623\n      0.501296\n    \n    \n      mileage\n      -0.001320\n      -0.768058\n      1.000000\n      0.133744\n      0.125376\n      -0.006459\n      -0.478705\n    \n    \n      tax\n      0.023806\n      -0.205902\n      0.133744\n      1.000000\n      -0.488002\n      0.465282\n      0.144652\n    \n    \n      mpg\n      -0.010774\n      -0.057093\n      0.125376\n      -0.488002\n      1.000000\n      -0.419417\n      -0.369919\n    \n    \n      engineSize\n      0.011365\n      0.014623\n      -0.006459\n      0.465282\n      -0.419417\n      1.000000\n      0.624899\n    \n    \n      price\n      0.012129\n      0.501296\n      -0.478705\n      0.144652\n      -0.369919\n      0.624899\n      1.000000\n    \n  \n\n\n\n\nLet us compute the Variance Inflation Factor (VIF) for the four predictors.\n\nX = train[['mpg','year','mileage','engineSize']]\n\n\nX.columns[1:]\n\nIndex(['year', 'mileage', 'engineSize'], dtype='object')\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(X)\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n\nfor i in range(len(X.columns)):\n    vif_data.loc[i,'VIF'] = variance_inflation_factor(X.values, i)\n\nprint(vif_data)\n\n      feature           VIF\n0       const  1.201579e+06\n1         mpg  1.243040e+00\n2        year  2.452891e+00\n3     mileage  2.490210e+00\n4  engineSize  1.219170e+00\n\n\nAs all the values of VIF are close to one, we do not have the problem of multicollinearity in the model. Note that the VIF of year and mileage is relatively high as they are the most correlated.\nQ1: Why is the VIF of the constant so high?\nQ2: Why do we need to include the constant while finding the VIF?\n\n\n5.4.3 Manual computation of VIF\n\n#Manually computing the VIF for year\nols_object = smf.ols(formula = 'year~mpg+engineSize+mileage', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          year         R-squared:             0.592 \n\n\n  Model:                   OLS         Adj. R-squared:        0.592 \n\n\n  Method:             Least Squares    F-statistic:           2400. \n\n\n  Date:             Mon, 30 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 02:49:19       Log-Likelihood:      -10066. \n\n\n  No. Observations:        4960        AIC:                2.014e+04\n\n\n  Df Residuals:            4956        BIC:                2.017e+04\n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept   2018.3135     0.140  1.44e+04  0.000  2018.039  2018.588\n\n\n  mpg            0.0095     0.002     5.301  0.000     0.006     0.013\n\n\n  engineSize     0.1171     0.037     3.203  0.001     0.045     0.189\n\n\n  mileage    -9.139e-05  1.08e-06   -84.615  0.000 -9.35e-05 -8.93e-05\n\n\n\n\n  Omnibus:       2949.664   Durbin-Watson:         1.161 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   63773.271\n\n\n  Skew:           -2.426    Prob(JB):               0.00 \n\n\n  Kurtosis:       19.883    Cond. No.           1.91e+05 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.91e+05. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#VIF for year\n1/(1-0.592)\n\n2.4509803921568625\n\n\nNote that year and mileage have a high linear correlation. Removing one of them should decrease the standard error of the coefficient of the other, without significanty decrease R-squared.\n\nols_object = smf.ols(formula = 'price~mpg+engineSize+mileage+year', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.660 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           2410. \n\n\n  Date:             Tue, 07 Feb 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 21:39:45       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4955        BIC:                1.050e+05\n\n\n  Df Model:                   4                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.661e+06  1.49e+05   -24.593  0.000 -3.95e+06 -3.37e+06\n\n\n  mpg          -79.3126     9.338    -8.493  0.000   -97.620   -61.006\n\n\n  engineSize  1.218e+04   189.969    64.107  0.000  1.18e+04  1.26e+04\n\n\n  mileage       -0.1474     0.009   -16.817  0.000    -0.165    -0.130\n\n\n  year        1817.7366    73.751    24.647  0.000  1673.151  1962.322\n\n\n\n\n  Omnibus:       2450.973   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31060.548\n\n\n  Skew:            2.045    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.557    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nRemoving mileage from the above regression.\n\nols_object = smf.ols(formula = 'price~mpg+engineSize+year', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.641 \n\n\n  Model:                   OLS         Adj. R-squared:        0.641 \n\n\n  Method:             Least Squares    F-statistic:           2951. \n\n\n  Date:             Tue, 07 Feb 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 21:40:00       Log-Likelihood:      -52635. \n\n\n  No. Observations:        4960        AIC:                1.053e+05\n\n\n  Df Residuals:            4956        BIC:                1.053e+05\n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -5.586e+06  9.78e+04   -57.098  0.000 -5.78e+06 -5.39e+06\n\n\n  mpg         -101.9120     9.500   -10.727  0.000  -120.536   -83.288\n\n\n  engineSize  1.196e+04   194.848    61.392  0.000  1.16e+04  1.23e+04\n\n\n  year        2771.1844    48.492    57.147  0.000  2676.118  2866.251\n\n\n\n\n  Omnibus:       2389.075   Durbin-Watson:         0.528 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   26920.051\n\n\n  Skew:            2.018    Prob(JB):               0.00 \n\n\n  Kurtosis:       13.675    Cond. No.           1.41e+06 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.41e+06. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that the standard error of the coefficient of year has reduced from 73 to 48, without any large reduction in R-squared.\n\n\n5.4.4 When can we overlook multicollinearity?\n\nThe severity of the problems increases with the degree of the multicollinearity. Therefore, if there is only moderate multicollinearity (5 < VIF < 10), we may overlook it.\nMulticollinearity affects only the standard errors of the coefficients of collinear predictors. Therefore, if multicollinearity is not present for the predictors that we are particularly interested in, we may not need to resolve it.\nMulticollinearity affects the standard error of the coefficients and thereby their \\(p\\)-values, but in general, it does not influence the prediction accuracy, except in the case that the coefficients are so unstable that the predictions are outside of the domain space of the response. If our sole aim is prediction, and we don’t wish to infer the statistical significance of predictors, then we may avoid addressing multicollinearity. “The fact that some or all predictor variables are correlated among themselves does not, in general, inhibit our ability to obtain a good fit nor does it tend to affect inferences about mean responses or predictions of new observations, provided these inferences are made within the region of observations” - Neter, John, Michael H. Kutner, Christopher J. Nachtsheim, and William Wasserman. “Applied linear statistical models.” (1996): 318."
  },
  {
    "objectID": "Lec6_Autocorrelation.html",
    "href": "Lec6_Autocorrelation.html",
    "title": "6  Autocorrelation",
    "section": "",
    "text": "Read section 3.3.3 (2) of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nBelow is an example showing violation of the autocorrelation assumption (refer to the book to understand autocorrelation) in linear regression. Subsequently, it is shown that addressing the assumption violation leads to a much better model fit."
  },
  {
    "objectID": "Lec6_Autocorrelation.html#introduction",
    "href": "Lec6_Autocorrelation.html#introduction",
    "title": "6  Autocorrelation",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nExample: Using linear regression models to predict electricity demand in Toronto, CA.\nWe have hourly power demand and temperature (in Celsius) data from 2017 to 2020.\nWe are going to build a linear model to predict the hourly power demand for the next day (for example, when it is 1/1/2021, we predict hourly demand on 1/2/2021 using historical data and the weather forecasts).\nWhen we are building a model, it is important to keep in mind what data we can use as features. For this model:\n\nWe cannot use previous hourly data as features. (Although in a high frequency setting, it is possible)\nThe temperature in our raw data can not be used directly, since it is the actual, not the forecasted temperature. We are going to use the previous day temperature as the forecast.\n\nSource: Keep it simple, keep it linear: A linear regression model for time series\n\n%pylab inline\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.api as sm\nplt.rcParams['figure.figsize'] = [9, 5]\n\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\n# A few helper functions\nimport numpy.ma as ma\nfrom scipy.stats.stats import pearsonr, normaltest\nfrom scipy.spatial.distance import correlation\ndef build_model(features):\n  X=sm.add_constant(df[features])\n  y=df['power']\n  model = sm.OLS(y,X, missing='drop').fit()\n  predictions = model.predict(X) \n  display(model.summary()) \n  res=y-predictions\n  return res \n\n\ndef plt_residual(res):\n  plt.plot(range(len(res)), res) \n  plt.ylabel('Residual')\n  plt.xlabel(\"Hour\")\n\ndef plt_residual_lag(res, nlag):\n  x=res.values\n  y=res.shift(nlag).values\n  sns.kdeplot(x,y=y,color='blue',shade=True )\n  plt.xlabel('res')\n  plt.ylabel(\"res-lag-{}\".format(nlag))\n  rho,p=corrcoef(x,y)\n  plt.title(\"n_lag={} hours, correlation={:f}\".format(nlag, rho))\n  \ndef plt_acf(res):\n  plt.rcParams['figure.figsize'] = [18, 5]\n  acorr = sm.tsa.acf(res.dropna(), nlags = len(res.dropna())-1)\n  fig, (ax1, ax2) = plt.subplots(1, 2)\n  ax1.plot(acorr)\n  ax1.set_ylabel('corr')\n  ax1.set_xlabel('n_lag')\n  ax1.set_title('Auto Correlation')\n  ax2.plot(acorr[:4*7*24])\n  ax2.set_ylabel('corr')\n  ax2.set_xlabel('n_lag')\n  ax2.set_title('Auto Correlation (4-week zoomed in) ')\n  plt.show()\n  pd.set_option('display.max_columns', None)\n  adf=pd.DataFrame(np.round(acorr[:30*24],2).reshape([30, 24] ))\n  adf.index.name='day'\n  display(adf)\n  plt.rcParams['figure.figsize'] = [9, 5]\n\ndef corrcoef(x,y):\n    a,b=ma.masked_invalid(x),ma.masked_invalid(y)\n    msk = (~a.mask & ~b.mask)\n    return pearsonr(x[msk],y[msk])[0], normaltest(res, nan_policy='omit')[1]"
  },
  {
    "objectID": "Lec6_Autocorrelation.html#the-data",
    "href": "Lec6_Autocorrelation.html#the-data",
    "title": "6  Autocorrelation",
    "section": "6.2 The data",
    "text": "6.2 The data\n\ndf=pd.read_csv(\"./Datasets/Toronto_power_demand.csv\", parse_dates=['Date'], index_col=0)\ndf['temperature']=df['temperature'].shift(24*1)\ndf.tail()\n\n\n\n\n\n  \n    \n      \n      Date\n      Hour\n      power\n      temperature\n    \n    \n      key\n      \n      \n      \n      \n    \n  \n  \n    \n      20201231:19\n      2020-12-31\n      19\n      5948\n      4.9\n    \n    \n      20201231:20\n      2020-12-31\n      20\n      5741\n      4.5\n    \n    \n      20201231:21\n      2020-12-31\n      21\n      5527\n      3.7\n    \n    \n      20201231:22\n      2020-12-31\n      22\n      5301\n      2.9\n    \n    \n      20201231:23\n      2020-12-31\n      23\n      5094\n      2.1\n    \n  \n\n\n\n\n\nndays=len(set(df['Date']))\nprint(\"There are {} rows, which is {}*24={}, for {} days. And The data is already in sorted order\" .format(df.shape[0], ndays, ndays*24, ndays))\n\nThere are 35064 rows, which is 1461*24=35064, for 1461 days. And The data is already in sorted order\n\n\n\nprint(\"It is natural to think that there is a relationship between power demand and temperature.\")\nsns.kdeplot(df['temperature'].values, y=df['power'].values,color='blue',shade=True )\nplt.title(\"Power Demand vs Temperature\")\n\nIt is natural to think that there is a relationship between power demand and temperature.\n\n\nText(0.5, 1.0, 'Power Demand vs Temperature')\n\n\n\n\n\n\nprint(\"\"\"\nIt is not a linear relationship. We create two features corresponding to hot and cold weather, which makes \\\nit possible to develop a linear model. \n\"\"\")\nis_hot=(df['temperature']>15).astype(int)\nprint(\"{:f}% of data points are hot\".format(is_hot.mean()*100))\ndf['temp_hot']=df['temperature']*is_hot\ndf['temp_cold']=df['temperature']*(1-is_hot)\ndf.tail()\n\n\nIt is not a linear relationship. We create two features corresponding to hot and cold weather, which makes it possible to develop a linear model. \n\n34.813484% of data points are hot\n\n\n\n\n\n\n  \n    \n      \n      Date\n      Hour\n      power\n      temperature\n      temp_hot\n      temp_cold\n    \n    \n      key\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      20201231:19\n      2020-12-31\n      19\n      5948\n      4.9\n      0.0\n      4.9\n    \n    \n      20201231:20\n      2020-12-31\n      20\n      5741\n      4.5\n      0.0\n      4.5\n    \n    \n      20201231:21\n      2020-12-31\n      21\n      5527\n      3.7\n      0.0\n      3.7\n    \n    \n      20201231:22\n      2020-12-31\n      22\n      5301\n      2.9\n      0.0\n      2.9\n    \n    \n      20201231:23\n      2020-12-31\n      23\n      5094\n      2.1\n      0.0\n      2.1"
  },
  {
    "objectID": "Lec6_Autocorrelation.html#predictor-temperature",
    "href": "Lec6_Autocorrelation.html#predictor-temperature",
    "title": "6  Autocorrelation",
    "section": "6.3 Predictor: temperature",
    "text": "6.3 Predictor: temperature\n\nres=build_model(['temp_hot', 'temp_cold'])\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          power        R-squared:              0.195  \n\n\n  Model:                   OLS         Adj. R-squared:         0.195  \n\n\n  Method:             Least Squares    F-statistic:            4251.  \n\n\n  Date:             Sun, 05 Feb 2023   Prob (F-statistic):     0.00   \n\n\n  Time:                 23:15:53       Log-Likelihood:     -2.8766e+05\n\n\n  No. Observations:       35040        AIC:                 5.753e+05 \n\n\n  Df Residuals:           35037        BIC:                 5.753e+05 \n\n\n  Df Model:                   2                                       \n\n\n  Covariance Type:      nonrobust                                     \n\n\n\n\n               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const      5501.3027     6.222   884.115  0.000  5489.107  5513.499\n\n\n  temp_hot     31.8488     0.462    68.911  0.000    30.943    32.755\n\n\n  temp_cold   -37.5088     0.827   -45.364  0.000   -39.129   -35.888\n\n\n\n\n  Omnibus:       945.032   Durbin-Watson:         0.093 \n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):    469.200 \n\n\n  Skew:           0.034    Prob(JB):           1.30e-102\n\n\n  Kurtosis:       2.437    Cond. No.               17.0 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nplt_residual(res)  \n\n\n\n\n\nprint(\"acf shows that there is a strong correlation for 24 lags, which is one day.\")\nplt_acf(res)\n\nacf shows that there is a strong correlation for 24 lags, which is one day.\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\stattools.py:667: FutureWarning: fft=True will become the default after the release of the 0.12 release of statsmodels. To suppress this warning, explicitly set fft=False.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n    \n    \n      day\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      1.00\n      0.95\n      0.85\n      0.72\n      0.56\n      0.40\n      0.24\n      0.09\n      -0.02\n      -0.11\n      -0.16\n      -0.20\n      -0.22\n      -0.21\n      -0.19\n      -0.14\n      -0.07\n      0.03\n      0.15\n      0.30\n      0.45\n      0.58\n      0.70\n      0.78\n    \n    \n      1\n      0.81\n      0.77\n      0.68\n      0.55\n      0.40\n      0.25\n      0.09\n      -0.04\n      -0.15\n      -0.23\n      -0.29\n      -0.32\n      -0.34\n      -0.33\n      -0.31\n      -0.26\n      -0.19\n      -0.09\n      0.04\n      0.18\n      0.33\n      0.47\n      0.58\n      0.66\n    \n    \n      2\n      0.69\n      0.65\n      0.57\n      0.45\n      0.31\n      0.16\n      0.01\n      -0.12\n      -0.22\n      -0.30\n      -0.35\n      -0.38\n      -0.39\n      -0.38\n      -0.36\n      -0.31\n      -0.23\n      -0.13\n      -0.00\n      0.14\n      0.29\n      0.42\n      0.54\n      0.62\n    \n    \n      3\n      0.64\n      0.61\n      0.53\n      0.42\n      0.28\n      0.13\n      -0.01\n      -0.14\n      -0.24\n      -0.32\n      -0.37\n      -0.40\n      -0.41\n      -0.40\n      -0.37\n      -0.32\n      -0.25\n      -0.15\n      -0.02\n      0.12\n      0.27\n      0.41\n      0.52\n      0.60\n    \n    \n      4\n      0.63\n      0.60\n      0.52\n      0.41\n      0.27\n      0.12\n      -0.02\n      -0.14\n      -0.24\n      -0.32\n      -0.37\n      -0.40\n      -0.40\n      -0.39\n      -0.36\n      -0.31\n      -0.24\n      -0.13\n      -0.01\n      0.14\n      0.28\n      0.42\n      0.54\n      0.62\n    \n    \n      5\n      0.65\n      0.62\n      0.54\n      0.43\n      0.30\n      0.15\n      0.01\n      -0.11\n      -0.21\n      -0.28\n      -0.33\n      -0.36\n      -0.36\n      -0.35\n      -0.32\n      -0.27\n      -0.19\n      -0.08\n      0.04\n      0.19\n      0.34\n      0.48\n      0.60\n      0.69\n    \n    \n      6\n      0.72\n      0.69\n      0.61\n      0.50\n      0.36\n      0.21\n      0.07\n      -0.05\n      -0.15\n      -0.22\n      -0.27\n      -0.29\n      -0.30\n      -0.29\n      -0.26\n      -0.21\n      -0.13\n      -0.02\n      0.11\n      0.25\n      0.40\n      0.55\n      0.67\n      0.75\n    \n    \n      7\n      0.78\n      0.75\n      0.67\n      0.54\n      0.40\n      0.25\n      0.10\n      -0.03\n      -0.13\n      -0.21\n      -0.26\n      -0.29\n      -0.30\n      -0.30\n      -0.27\n      -0.22\n      -0.15\n      -0.05\n      0.07\n      0.21\n      0.36\n      0.49\n      0.61\n      0.69\n    \n    \n      8\n      0.71\n      0.68\n      0.60\n      0.48\n      0.34\n      0.19\n      0.04\n      -0.09\n      -0.19\n      -0.27\n      -0.32\n      -0.35\n      -0.36\n      -0.36\n      -0.33\n      -0.28\n      -0.21\n      -0.12\n      0.01\n      0.14\n      0.29\n      0.42\n      0.53\n      0.61\n    \n    \n      9\n      0.64\n      0.61\n      0.53\n      0.41\n      0.27\n      0.13\n      -0.02\n      -0.14\n      -0.24\n      -0.32\n      -0.37\n      -0.40\n      -0.41\n      -0.40\n      -0.37\n      -0.32\n      -0.25\n      -0.15\n      -0.03\n      0.11\n      0.26\n      0.39\n      0.50\n      0.58\n    \n    \n      10\n      0.61\n      0.58\n      0.50\n      0.39\n      0.25\n      0.11\n      -0.03\n      -0.16\n      -0.26\n      -0.33\n      -0.38\n      -0.40\n      -0.41\n      -0.40\n      -0.38\n      -0.33\n      -0.25\n      -0.15\n      -0.03\n      0.11\n      0.26\n      0.39\n      0.50\n      0.58\n    \n    \n      11\n      0.61\n      0.58\n      0.51\n      0.39\n      0.26\n      0.12\n      -0.02\n      -0.14\n      -0.24\n      -0.32\n      -0.36\n      -0.39\n      -0.40\n      -0.39\n      -0.36\n      -0.31\n      -0.24\n      -0.14\n      -0.01\n      0.13\n      0.28\n      0.41\n      0.53\n      0.61\n    \n    \n      12\n      0.63\n      0.61\n      0.53\n      0.42\n      0.28\n      0.14\n      0.00\n      -0.12\n      -0.22\n      -0.29\n      -0.33\n      -0.36\n      -0.36\n      -0.35\n      -0.32\n      -0.27\n      -0.19\n      -0.09\n      0.04\n      0.18\n      0.33\n      0.47\n      0.59\n      0.67\n    \n    \n      13\n      0.70\n      0.67\n      0.60\n      0.48\n      0.35\n      0.20\n      0.06\n      -0.06\n      -0.16\n      -0.23\n      -0.27\n      -0.30\n      -0.30\n      -0.29\n      -0.26\n      -0.21\n      -0.14\n      -0.03\n      0.09\n      0.24\n      0.39\n      0.53\n      0.65\n      0.73\n    \n    \n      14\n      0.76\n      0.73\n      0.64\n      0.52\n      0.38\n      0.23\n      0.09\n      -0.04\n      -0.14\n      -0.22\n      -0.27\n      -0.30\n      -0.31\n      -0.30\n      -0.27\n      -0.23\n      -0.16\n      -0.06\n      0.06\n      0.20\n      0.34\n      0.48\n      0.59\n      0.66\n    \n    \n      15\n      0.69\n      0.66\n      0.58\n      0.46\n      0.32\n      0.17\n      0.03\n      -0.10\n      -0.20\n      -0.28\n      -0.33\n      -0.36\n      -0.38\n      -0.37\n      -0.35\n      -0.30\n      -0.23\n      -0.14\n      -0.02\n      0.12\n      0.26\n      0.39\n      0.50\n      0.58\n    \n    \n      16\n      0.60\n      0.57\n      0.50\n      0.38\n      0.25\n      0.10\n      -0.04\n      -0.16\n      -0.26\n      -0.34\n      -0.38\n      -0.41\n      -0.42\n      -0.41\n      -0.39\n      -0.34\n      -0.27\n      -0.17\n      -0.05\n      0.09\n      0.23\n      0.36\n      0.48\n      0.55\n    \n    \n      17\n      0.58\n      0.55\n      0.47\n      0.36\n      0.23\n      0.09\n      -0.05\n      -0.17\n      -0.27\n      -0.34\n      -0.39\n      -0.42\n      -0.43\n      -0.42\n      -0.39\n      -0.35\n      -0.27\n      -0.18\n      -0.05\n      0.08\n      0.23\n      0.36\n      0.47\n      0.55\n    \n    \n      18\n      0.57\n      0.55\n      0.47\n      0.36\n      0.23\n      0.09\n      -0.05\n      -0.17\n      -0.27\n      -0.34\n      -0.39\n      -0.41\n      -0.42\n      -0.41\n      -0.38\n      -0.34\n      -0.26\n      -0.17\n      -0.04\n      0.10\n      0.24\n      0.37\n      0.48\n      0.56\n    \n    \n      19\n      0.59\n      0.57\n      0.49\n      0.38\n      0.25\n      0.11\n      -0.03\n      -0.14\n      -0.24\n      -0.31\n      -0.35\n      -0.38\n      -0.38\n      -0.37\n      -0.34\n      -0.29\n      -0.22\n      -0.11\n      0.01\n      0.15\n      0.30\n      0.44\n      0.55\n      0.64\n    \n    \n      20\n      0.67\n      0.64\n      0.56\n      0.45\n      0.32\n      0.18\n      0.04\n      -0.08\n      -0.17\n      -0.24\n      -0.29\n      -0.31\n      -0.32\n      -0.31\n      -0.28\n      -0.23\n      -0.16\n      -0.06\n      0.07\n      0.21\n      0.36\n      0.49\n      0.61\n      0.69\n    \n    \n      21\n      0.72\n      0.69\n      0.61\n      0.49\n      0.36\n      0.21\n      0.07\n      -0.06\n      -0.16\n      -0.23\n      -0.28\n      -0.31\n      -0.32\n      -0.32\n      -0.29\n      -0.25\n      -0.18\n      -0.08\n      0.03\n      0.17\n      0.31\n      0.44\n      0.56\n      0.63\n    \n    \n      22\n      0.66\n      0.63\n      0.55\n      0.43\n      0.29\n      0.15\n      0.01\n      -0.12\n      -0.22\n      -0.29\n      -0.34\n      -0.37\n      -0.38\n      -0.38\n      -0.35\n      -0.31\n      -0.24\n      -0.15\n      -0.03\n      0.10\n      0.24\n      0.37\n      0.48\n      0.55\n    \n    \n      23\n      0.58\n      0.55\n      0.47\n      0.36\n      0.23\n      0.09\n      -0.05\n      -0.17\n      -0.27\n      -0.34\n      -0.39\n      -0.42\n      -0.43\n      -0.42\n      -0.39\n      -0.35\n      -0.28\n      -0.18\n      -0.06\n      0.07\n      0.21\n      0.34\n      0.45\n      0.53\n    \n    \n      24\n      0.55\n      0.52\n      0.45\n      0.34\n      0.21\n      0.07\n      -0.07\n      -0.19\n      -0.29\n      -0.36\n      -0.40\n      -0.43\n      -0.44\n      -0.43\n      -0.40\n      -0.36\n      -0.29\n      -0.19\n      -0.07\n      0.06\n      0.20\n      0.33\n      0.44\n      0.52\n    \n    \n      25\n      0.55\n      0.52\n      0.45\n      0.34\n      0.21\n      0.07\n      -0.07\n      -0.19\n      -0.28\n      -0.35\n      -0.40\n      -0.42\n      -0.43\n      -0.42\n      -0.39\n      -0.35\n      -0.28\n      -0.18\n      -0.06\n      0.08\n      0.22\n      0.35\n      0.46\n      0.54\n    \n    \n      26\n      0.57\n      0.54\n      0.47\n      0.36\n      0.23\n      0.09\n      -0.04\n      -0.16\n      -0.25\n      -0.32\n      -0.36\n      -0.39\n      -0.39\n      -0.38\n      -0.35\n      -0.30\n      -0.23\n      -0.13\n      -0.00\n      0.13\n      0.28\n      0.42\n      0.53\n      0.61\n    \n    \n      27\n      0.64\n      0.61\n      0.54\n      0.43\n      0.30\n      0.16\n      0.03\n      -0.09\n      -0.19\n      -0.25\n      -0.30\n      -0.32\n      -0.33\n      -0.32\n      -0.29\n      -0.24\n      -0.17\n      -0.07\n      0.06\n      0.19\n      0.34\n      0.48\n      0.59\n      0.67\n    \n    \n      28\n      0.70\n      0.67\n      0.59\n      0.47\n      0.34\n      0.19\n      0.05\n      -0.07\n      -0.17\n      -0.24\n      -0.29\n      -0.32\n      -0.33\n      -0.33\n      -0.30\n      -0.26\n      -0.19\n      -0.10\n      0.02\n      0.15\n      0.29\n      0.42\n      0.53\n      0.61\n    \n    \n      29\n      0.63\n      0.60\n      0.53\n      0.41\n      0.28\n      0.13\n      -0.01\n      -0.13\n      -0.23\n      -0.30\n      -0.35\n      -0.38\n      -0.39\n      -0.39\n      -0.37\n      -0.32\n      -0.26\n      -0.16\n      -0.05\n      0.08\n      0.22\n      0.35\n      0.46\n      0.53\n    \n  \n\n\n\n\n\nprint(\"Although 1 hour lag correlation is more strong, but we cannot use it, as we intend to predict \\\nthe power consumption for the next day.\")\nplt_residual_lag(res,1)\nplt.show()\nplt_residual_lag(res,24)\n\nAlthough 1 hour lag correlation is more strong, but we cannot use it, as we intend to predict the power consumption for the next day."
  },
  {
    "objectID": "Lec6_Autocorrelation.html#predictors-temperature-one-day-lag-of-power.",
    "href": "Lec6_Autocorrelation.html#predictors-temperature-one-day-lag-of-power.",
    "title": "6  Autocorrelation",
    "section": "6.4 Predictors: Temperature + one day lag of power.",
    "text": "6.4 Predictors: Temperature + one day lag of power.\n\ndf['power_lag_1_day']=df['power'].shift(24)\ndf.tail()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      key\n      Date\n      Hour\n      power\n      temperature\n      temp_hot\n      temp_cold\n      power_lag_1_day\n    \n  \n  \n    \n      35059\n      20201231:19\n      2020-12-31\n      19\n      5948\n      4.9\n      0.0\n      4.9\n      6163.0\n    \n    \n      35060\n      20201231:20\n      2020-12-31\n      20\n      5741\n      4.5\n      0.0\n      4.5\n      5983.0\n    \n    \n      35061\n      20201231:21\n      2020-12-31\n      21\n      5527\n      3.7\n      0.0\n      3.7\n      5727.0\n    \n    \n      35062\n      20201231:22\n      2020-12-31\n      22\n      5301\n      2.9\n      0.0\n      2.9\n      5428.0\n    \n    \n      35063\n      20201231:23\n      2020-12-31\n      23\n      5094\n      2.1\n      0.0\n      2.1\n      5104.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nres=build_model(['temp_hot', 'temp_cold', 'power_lag_1_day' ])\n\n/usr/local/lib/python3.8/dist-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  x = pd.concat(x[::order], 1)\n\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          power        R-squared:              0.794  \n\n\n  Model:                   OLS         Adj. R-squared:         0.794  \n\n\n  Method:             Least Squares    F-statistic:         4.513e+04 \n\n\n  Date:             Sun, 22 Jan 2023   Prob (F-statistic):     0.00   \n\n\n  Time:                 19:21:14       Log-Likelihood:     -2.6375e+05\n\n\n  No. Observations:       35040        AIC:                 5.275e+05 \n\n\n  Df Residuals:           35036        BIC:                 5.275e+05 \n\n\n  Df Model:                   3                                       \n\n\n  Covariance Type:      nonrobust                                     \n\n\n\n\n                     coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const             689.2701    15.384    44.806  0.000   659.118   719.422\n\n\n  temp_hot            3.2158     0.250    12.853  0.000     2.725     3.706\n\n\n  temp_cold          -1.3464     0.433    -3.110  0.002    -2.195    -0.498\n\n\n  power_lag_1_day     0.8747     0.003   319.552  0.000     0.869     0.880\n\n\n\n\n  Omnibus:       2035.537   Durbin-Watson:         0.041\n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   5794.290\n\n\n  Skew:            0.301    Prob(JB):               0.00\n\n\n  Kurtosis:        4.899    Cond. No.           3.69e+04\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.69e+04. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nplt_residual(res)\n\n\n\n\n\nplt_acf(res)\n\n/usr/local/lib/python3.8/dist-packages/statsmodels/tsa/stattools.py:667: FutureWarning: fft=True will become the default after the release of the 0.12 release of statsmodels. To suppress this warning, explicitly set fft=False.\n  warnings.warn(\n\n\n\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n    \n    \n      day\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      1.00\n      0.98\n      0.93\n      0.87\n      0.81\n      0.75\n      0.70\n      0.64\n      0.59\n      0.54\n      0.50\n      0.46\n      0.42\n      0.39\n      0.35\n      0.31\n      0.28\n      0.25\n      0.22\n      0.20\n      0.17\n      0.15\n      0.12\n      0.09\n    \n    \n      1\n      0.07\n      0.05\n      0.03\n      0.00\n      -0.02\n      -0.04\n      -0.06\n      -0.08\n      -0.10\n      -0.12\n      -0.14\n      -0.15\n      -0.16\n      -0.17\n      -0.18\n      -0.19\n      -0.19\n      -0.20\n      -0.20\n      -0.20\n      -0.21\n      -0.21\n      -0.22\n      -0.22\n    \n    \n      2\n      -0.23\n      -0.23\n      -0.22\n      -0.22\n      -0.21\n      -0.21\n      -0.21\n      -0.21\n      -0.21\n      -0.20\n      -0.20\n      -0.19\n      -0.18\n      -0.18\n      -0.17\n      -0.16\n      -0.15\n      -0.14\n      -0.12\n      -0.11\n      -0.10\n      -0.09\n      -0.08\n      -0.08\n    \n    \n      3\n      -0.07\n      -0.07\n      -0.07\n      -0.07\n      -0.08\n      -0.09\n      -0.09\n      -0.10\n      -0.11\n      -0.11\n      -0.11\n      -0.12\n      -0.12\n      -0.12\n      -0.11\n      -0.11\n      -0.11\n      -0.10\n      -0.09\n      -0.09\n      -0.08\n      -0.07\n      -0.07\n      -0.07\n    \n    \n      4\n      -0.07\n      -0.07\n      -0.07\n      -0.08\n      -0.09\n      -0.10\n      -0.11\n      -0.12\n      -0.13\n      -0.14\n      -0.14\n      -0.15\n      -0.16\n      -0.16\n      -0.16\n      -0.17\n      -0.17\n      -0.17\n      -0.17\n      -0.17\n      -0.17\n      -0.17\n      -0.18\n      -0.18\n    \n    \n      5\n      -0.18\n      -0.18\n      -0.17\n      -0.17\n      -0.17\n      -0.16\n      -0.16\n      -0.16\n      -0.16\n      -0.15\n      -0.14\n      -0.14\n      -0.13\n      -0.12\n      -0.10\n      -0.09\n      -0.07\n      -0.05\n      -0.04\n      -0.02\n      0.00\n      0.02\n      0.04\n      0.06\n    \n    \n      6\n      0.07\n      0.08\n      0.09\n      0.09\n      0.10\n      0.10\n      0.11\n      0.12\n      0.13\n      0.14\n      0.16\n      0.18\n      0.19\n      0.21\n      0.23\n      0.25\n      0.27\n      0.30\n      0.33\n      0.36\n      0.39\n      0.43\n      0.46\n      0.48\n    \n    \n      7\n      0.50\n      0.49\n      0.46\n      0.43\n      0.40\n      0.37\n      0.34\n      0.31\n      0.28\n      0.26\n      0.24\n      0.22\n      0.21\n      0.19\n      0.18\n      0.16\n      0.15\n      0.14\n      0.13\n      0.13\n      0.12\n      0.12\n      0.12\n      0.11\n    \n    \n      8\n      0.10\n      0.09\n      0.07\n      0.06\n      0.04\n      0.02\n      -0.00\n      -0.02\n      -0.04\n      -0.05\n      -0.07\n      -0.08\n      -0.09\n      -0.10\n      -0.11\n      -0.11\n      -0.12\n      -0.12\n      -0.13\n      -0.13\n      -0.13\n      -0.14\n      -0.15\n      -0.15\n    \n    \n      9\n      -0.16\n      -0.16\n      -0.16\n      -0.15\n      -0.16\n      -0.16\n      -0.16\n      -0.16\n      -0.16\n      -0.16\n      -0.15\n      -0.15\n      -0.14\n      -0.14\n      -0.13\n      -0.12\n      -0.11\n      -0.10\n      -0.09\n      -0.07\n      -0.06\n      -0.05\n      -0.04\n      -0.04\n    \n    \n      10\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.04\n      -0.04\n      -0.05\n      -0.06\n      -0.06\n      -0.07\n      -0.07\n      -0.07\n      -0.07\n      -0.07\n      -0.07\n      -0.07\n      -0.06\n      -0.05\n      -0.05\n      -0.04\n      -0.03\n      -0.02\n      -0.02\n      -0.01\n    \n    \n      11\n      -0.01\n      -0.01\n      -0.02\n      -0.03\n      -0.03\n      -0.04\n      -0.05\n      -0.06\n      -0.07\n      -0.08\n      -0.09\n      -0.10\n      -0.11\n      -0.11\n      -0.11\n      -0.12\n      -0.12\n      -0.12\n      -0.12\n      -0.12\n      -0.12\n      -0.12\n      -0.13\n      -0.13\n    \n    \n      12\n      -0.14\n      -0.14\n      -0.13\n      -0.13\n      -0.13\n      -0.13\n      -0.14\n      -0.14\n      -0.13\n      -0.13\n      -0.13\n      -0.12\n      -0.11\n      -0.10\n      -0.09\n      -0.08\n      -0.06\n      -0.05\n      -0.03\n      -0.01\n      0.01\n      0.03\n      0.05\n      0.07\n    \n    \n      13\n      0.08\n      0.09\n      0.10\n      0.10\n      0.11\n      0.11\n      0.12\n      0.13\n      0.14\n      0.15\n      0.17\n      0.18\n      0.20\n      0.22\n      0.23\n      0.26\n      0.28\n      0.31\n      0.33\n      0.36\n      0.40\n      0.43\n      0.46\n      0.48\n    \n    \n      14\n      0.49\n      0.48\n      0.46\n      0.43\n      0.40\n      0.37\n      0.34\n      0.31\n      0.28\n      0.26\n      0.24\n      0.23\n      0.21\n      0.20\n      0.18\n      0.17\n      0.15\n      0.14\n      0.13\n      0.13\n      0.12\n      0.12\n      0.12\n      0.11\n    \n    \n      15\n      0.10\n      0.09\n      0.07\n      0.05\n      0.03\n      0.01\n      -0.01\n      -0.03\n      -0.05\n      -0.07\n      -0.08\n      -0.10\n      -0.11\n      -0.12\n      -0.13\n      -0.14\n      -0.14\n      -0.15\n      -0.15\n      -0.15\n      -0.15\n      -0.16\n      -0.16\n      -0.17\n    \n    \n      16\n      -0.17\n      -0.17\n      -0.17\n      -0.17\n      -0.16\n      -0.16\n      -0.17\n      -0.17\n      -0.16\n      -0.16\n      -0.16\n      -0.16\n      -0.15\n      -0.14\n      -0.13\n      -0.12\n      -0.11\n      -0.10\n      -0.09\n      -0.07\n      -0.06\n      -0.05\n      -0.04\n      -0.03\n    \n    \n      17\n      -0.03\n      -0.02\n      -0.02\n      -0.03\n      -0.03\n      -0.04\n      -0.04\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n    \n    \n      18\n      -0.03\n      -0.04\n      -0.04\n      -0.05\n      -0.06\n      -0.08\n      -0.09\n      -0.10\n      -0.11\n      -0.12\n      -0.13\n      -0.14\n      -0.15\n      -0.15\n      -0.16\n      -0.16\n      -0.16\n      -0.17\n      -0.17\n      -0.17\n      -0.17\n      -0.17\n      -0.17\n      -0.17\n    \n    \n      19\n      -0.17\n      -0.17\n      -0.16\n      -0.16\n      -0.15\n      -0.15\n      -0.15\n      -0.15\n      -0.14\n      -0.13\n      -0.13\n      -0.12\n      -0.11\n      -0.10\n      -0.09\n      -0.07\n      -0.05\n      -0.03\n      -0.02\n      0.00\n      0.03\n      0.05\n      0.07\n      0.08\n    \n    \n      20\n      0.10\n      0.10\n      0.11\n      0.11\n      0.12\n      0.12\n      0.12\n      0.13\n      0.14\n      0.15\n      0.17\n      0.18\n      0.20\n      0.21\n      0.23\n      0.25\n      0.27\n      0.29\n      0.32\n      0.35\n      0.38\n      0.41\n      0.44\n      0.47\n    \n    \n      21\n      0.47\n      0.46\n      0.44\n      0.41\n      0.38\n      0.35\n      0.32\n      0.29\n      0.26\n      0.24\n      0.22\n      0.21\n      0.19\n      0.18\n      0.16\n      0.15\n      0.13\n      0.13\n      0.12\n      0.12\n      0.11\n      0.11\n      0.11\n      0.10\n    \n    \n      22\n      0.10\n      0.09\n      0.07\n      0.05\n      0.03\n      0.01\n      -0.00\n      -0.02\n      -0.04\n      -0.05\n      -0.07\n      -0.08\n      -0.09\n      -0.10\n      -0.10\n      -0.11\n      -0.12\n      -0.12\n      -0.12\n      -0.13\n      -0.13\n      -0.13\n      -0.14\n      -0.14\n    \n    \n      23\n      -0.14\n      -0.14\n      -0.14\n      -0.14\n      -0.14\n      -0.14\n      -0.14\n      -0.14\n      -0.14\n      -0.14\n      -0.13\n      -0.13\n      -0.13\n      -0.12\n      -0.11\n      -0.11\n      -0.10\n      -0.09\n      -0.08\n      -0.07\n      -0.06\n      -0.05\n      -0.04\n      -0.03\n    \n    \n      24\n      -0.03\n      -0.03\n      -0.03\n      -0.04\n      -0.05\n      -0.05\n      -0.06\n      -0.07\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n      -0.09\n      -0.09\n      -0.08\n      -0.08\n      -0.08\n      -0.07\n      -0.06\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.03\n    \n    \n      25\n      -0.03\n      -0.04\n      -0.04\n      -0.05\n      -0.06\n      -0.07\n      -0.08\n      -0.09\n      -0.10\n      -0.11\n      -0.12\n      -0.13\n      -0.14\n      -0.14\n      -0.15\n      -0.15\n      -0.15\n      -0.15\n      -0.16\n      -0.16\n      -0.16\n      -0.16\n      -0.16\n      -0.16\n    \n    \n      26\n      -0.17\n      -0.17\n      -0.16\n      -0.16\n      -0.16\n      -0.15\n      -0.15\n      -0.15\n      -0.15\n      -0.14\n      -0.13\n      -0.13\n      -0.12\n      -0.11\n      -0.10\n      -0.08\n      -0.07\n      -0.05\n      -0.03\n      -0.01\n      0.01\n      0.03\n      0.05\n      0.07\n    \n    \n      27\n      0.08\n      0.09\n      0.10\n      0.11\n      0.11\n      0.12\n      0.12\n      0.13\n      0.14\n      0.16\n      0.18\n      0.19\n      0.21\n      0.22\n      0.24\n      0.26\n      0.28\n      0.31\n      0.34\n      0.37\n      0.40\n      0.43\n      0.46\n      0.48\n    \n    \n      28\n      0.49\n      0.48\n      0.45\n      0.42\n      0.39\n      0.36\n      0.33\n      0.30\n      0.27\n      0.25\n      0.23\n      0.22\n      0.20\n      0.19\n      0.17\n      0.16\n      0.14\n      0.13\n      0.12\n      0.12\n      0.11\n      0.11\n      0.11\n      0.10\n    \n    \n      29\n      0.09\n      0.08\n      0.06\n      0.04\n      0.02\n      -0.00\n      -0.02\n      -0.04\n      -0.06\n      -0.08\n      -0.09\n      -0.10\n      -0.12\n      -0.13\n      -0.14\n      -0.14\n      -0.15\n      -0.15\n      -0.16\n      -0.16\n      -0.16\n      -0.17\n      -0.17\n      -0.18\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt_residual_lag(res, 1)\n\n\n\n\n\nplt_residual_lag(res, 24)\n\n\n\n\n\nplt_residual_lag(res, 24*7)"
  },
  {
    "objectID": "Lec6_Autocorrelation.html#predictors-temperature-1-day-lag-of-power-1-week-lag-of-power",
    "href": "Lec6_Autocorrelation.html#predictors-temperature-1-day-lag-of-power-1-week-lag-of-power",
    "title": "6  Autocorrelation",
    "section": "6.5 Predictors: Temperature + 1 day lag of power + 1 week lag of power",
    "text": "6.5 Predictors: Temperature + 1 day lag of power + 1 week lag of power\n\ndf['power_lag_1_week']=df['power'].shift(24*7)\ndf.tail()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      key\n      Date\n      Hour\n      power\n      temperature\n      temp_hot\n      temp_cold\n      power_lag_1_day\n      power_lag_1_week\n    \n  \n  \n    \n      35059\n      20201231:19\n      2020-12-31\n      19\n      5948\n      4.9\n      0.0\n      4.9\n      6163.0\n      5833.0\n    \n    \n      35060\n      20201231:20\n      2020-12-31\n      20\n      5741\n      4.5\n      0.0\n      4.5\n      5983.0\n      5665.0\n    \n    \n      35061\n      20201231:21\n      2020-12-31\n      21\n      5527\n      3.7\n      0.0\n      3.7\n      5727.0\n      5474.0\n    \n    \n      35062\n      20201231:22\n      2020-12-31\n      22\n      5301\n      2.9\n      0.0\n      2.9\n      5428.0\n      5273.0\n    \n    \n      35063\n      20201231:23\n      2020-12-31\n      23\n      5094\n      2.1\n      0.0\n      2.1\n      5104.0\n      5010.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nres=build_model(['temp_hot', 'temp_cold', 'power_lag_1_day', 'power_lag_1_week' ])\n\n/usr/local/lib/python3.8/dist-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  x = pd.concat(x[::order], 1)\n\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          power        R-squared:              0.840  \n\n\n  Model:                   OLS         Adj. R-squared:         0.840  \n\n\n  Method:             Least Squares    F-statistic:         4.585e+04 \n\n\n  Date:             Sun, 22 Jan 2023   Prob (F-statistic):     0.00   \n\n\n  Time:                 19:22:49       Log-Likelihood:     -2.5830e+05\n\n\n  No. Observations:       34896        AIC:                 5.166e+05 \n\n\n  Df Residuals:           34891        BIC:                 5.167e+05 \n\n\n  Df Model:                   4                                       \n\n\n  Covariance Type:      nonrobust                                     \n\n\n\n\n                      coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const              290.4344    14.166    20.502  0.000   262.668   318.201\n\n\n  temp_hot             3.2967     0.221    14.896  0.000     2.863     3.730\n\n\n  temp_cold           -4.5938     0.385   -11.943  0.000    -5.348    -3.840\n\n\n  power_lag_1_day      0.6114     0.004   170.709  0.000     0.604     0.618\n\n\n  power_lag_1_week     0.3342     0.003    99.595  0.000     0.328     0.341\n\n\n\n\n  Omnibus:       2729.372   Durbin-Watson:         0.037 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   11234.560\n\n\n  Skew:            0.299    Prob(JB):               0.00 \n\n\n  Kurtosis:        5.715    Cond. No.           5.43e+04 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 5.43e+04. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nplt_residual(res)\n\n\n\n\n\nplt_acf(res)\n\n/usr/local/lib/python3.8/dist-packages/statsmodels/tsa/stattools.py:667: FutureWarning: fft=True will become the default after the release of the 0.12 release of statsmodels. To suppress this warning, explicitly set fft=False.\n  warnings.warn(\n\n\n\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n    \n    \n      day\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      1.00\n      0.98\n      0.94\n      0.89\n      0.84\n      0.79\n      0.74\n      0.70\n      0.65\n      0.61\n      0.58\n      0.54\n      0.51\n      0.48\n      0.45\n      0.42\n      0.39\n      0.37\n      0.34\n      0.32\n      0.30\n      0.27\n      0.25\n      0.22\n    \n    \n      1\n      0.20\n      0.18\n      0.16\n      0.14\n      0.12\n      0.10\n      0.09\n      0.07\n      0.06\n      0.04\n      0.03\n      0.02\n      0.01\n      0.00\n      -0.00\n      -0.01\n      -0.02\n      -0.02\n      -0.02\n      -0.03\n      -0.03\n      -0.04\n      -0.04\n      -0.05\n    \n    \n      2\n      -0.05\n      -0.06\n      -0.06\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.04\n      -0.03\n      -0.03\n      -0.02\n      -0.02\n      -0.01\n      -0.01\n      -0.00\n      -0.00\n      0.00\n      0.00\n    \n    \n      3\n      0.00\n      0.00\n      0.00\n      -0.00\n      -0.00\n      -0.01\n      -0.01\n      -0.01\n      -0.02\n      -0.02\n      -0.02\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.02\n      -0.02\n      -0.02\n      -0.03\n    \n    \n      4\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.04\n      -0.04\n      -0.04\n      -0.05\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.06\n      -0.07\n      -0.07\n      -0.07\n      -0.08\n      -0.08\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n      -0.10\n    \n    \n      5\n      -0.10\n      -0.09\n      -0.09\n      -0.08\n      -0.08\n      -0.07\n      -0.07\n      -0.07\n      -0.06\n      -0.06\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.03\n      -0.02\n      -0.02\n      -0.01\n      -0.00\n      0.00\n      0.01\n      0.02\n      0.03\n      0.03\n    \n    \n      6\n      0.03\n      0.03\n      0.03\n      0.03\n      0.03\n      0.03\n      0.03\n      0.03\n      0.03\n      0.04\n      0.04\n      0.05\n      0.05\n      0.05\n      0.06\n      0.06\n      0.06\n      0.07\n      0.07\n      0.08\n      0.08\n      0.09\n      0.09\n      0.10\n    \n    \n      7\n      0.10\n      0.10\n      0.09\n      0.08\n      0.06\n      0.05\n      0.05\n      0.04\n      0.03\n      0.02\n      0.01\n      0.01\n      0.00\n      -0.01\n      -0.02\n      -0.03\n      -0.03\n      -0.04\n      -0.05\n      -0.06\n      -0.06\n      -0.07\n      -0.07\n      -0.07\n    \n    \n      8\n      -0.08\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n      -0.09\n      -0.09\n      -0.10\n      -0.10\n      -0.10\n      -0.10\n      -0.11\n      -0.11\n      -0.11\n      -0.12\n      -0.12\n      -0.13\n      -0.13\n      -0.13\n      -0.14\n      -0.15\n      -0.15\n      -0.16\n      -0.16\n    \n    \n      9\n      -0.17\n      -0.16\n      -0.16\n      -0.15\n      -0.15\n      -0.14\n      -0.14\n      -0.13\n      -0.13\n      -0.12\n      -0.12\n      -0.11\n      -0.11\n      -0.10\n      -0.10\n      -0.09\n      -0.09\n      -0.08\n      -0.08\n      -0.08\n      -0.07\n      -0.07\n      -0.06\n      -0.06\n    \n    \n      10\n      -0.06\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.02\n      -0.02\n      -0.02\n      -0.01\n      -0.01\n    \n    \n      11\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.02\n      -0.02\n      -0.02\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.04\n      -0.04\n      -0.04\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n    \n    \n      12\n      -0.07\n      -0.07\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.03\n      -0.03\n      -0.02\n      -0.01\n      -0.01\n      0.00\n      0.01\n      0.02\n      0.03\n      0.04\n      0.04\n    \n    \n      13\n      0.05\n      0.06\n      0.06\n      0.07\n      0.08\n      0.08\n      0.09\n      0.10\n      0.11\n      0.12\n      0.13\n      0.14\n      0.15\n      0.16\n      0.17\n      0.18\n      0.19\n      0.20\n      0.22\n      0.23\n      0.25\n      0.26\n      0.28\n      0.29\n    \n    \n      14\n      0.29\n      0.29\n      0.27\n      0.26\n      0.24\n      0.23\n      0.21\n      0.20\n      0.19\n      0.18\n      0.17\n      0.16\n      0.15\n      0.14\n      0.13\n      0.12\n      0.11\n      0.10\n      0.09\n      0.08\n      0.07\n      0.07\n      0.06\n      0.06\n    \n    \n      15\n      0.05\n      0.04\n      0.03\n      0.02\n      0.01\n      -0.00\n      -0.01\n      -0.02\n      -0.03\n      -0.04\n      -0.05\n      -0.06\n      -0.06\n      -0.07\n      -0.08\n      -0.08\n      -0.09\n      -0.10\n      -0.10\n      -0.11\n      -0.11\n      -0.11\n      -0.12\n      -0.12\n    \n    \n      16\n      -0.13\n      -0.13\n      -0.12\n      -0.12\n      -0.11\n      -0.11\n      -0.11\n      -0.10\n      -0.10\n      -0.10\n      -0.10\n      -0.09\n      -0.09\n      -0.08\n      -0.08\n      -0.07\n      -0.07\n      -0.06\n      -0.06\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.04\n    \n    \n      17\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.04\n      -0.04\n      -0.04\n    \n    \n      18\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.06\n      -0.07\n      -0.07\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n      -0.10\n      -0.10\n      -0.10\n      -0.11\n      -0.11\n      -0.12\n      -0.12\n      -0.12\n      -0.13\n      -0.13\n      -0.14\n      -0.14\n      -0.14\n    \n    \n      19\n      -0.14\n      -0.14\n      -0.13\n      -0.13\n      -0.12\n      -0.11\n      -0.11\n      -0.10\n      -0.10\n      -0.09\n      -0.08\n      -0.08\n      -0.07\n      -0.06\n      -0.06\n      -0.05\n      -0.04\n      -0.03\n      -0.02\n      -0.01\n      -0.00\n      0.01\n      0.01\n      0.02\n    \n    \n      20\n      0.03\n      0.03\n      0.04\n      0.04\n      0.05\n      0.05\n      0.06\n      0.07\n      0.07\n      0.08\n      0.09\n      0.10\n      0.11\n      0.12\n      0.13\n      0.14\n      0.15\n      0.16\n      0.17\n      0.19\n      0.20\n      0.22\n      0.23\n      0.24\n    \n    \n      21\n      0.25\n      0.24\n      0.23\n      0.22\n      0.20\n      0.19\n      0.17\n      0.16\n      0.14\n      0.13\n      0.12\n      0.11\n      0.11\n      0.10\n      0.09\n      0.08\n      0.07\n      0.06\n      0.06\n      0.05\n      0.05\n      0.04\n      0.04\n      0.04\n    \n    \n      22\n      0.04\n      0.03\n      0.02\n      0.02\n      0.01\n      0.00\n      -0.01\n      -0.01\n      -0.02\n      -0.03\n      -0.03\n      -0.04\n      -0.04\n      -0.05\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.07\n      -0.07\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n    \n    \n      23\n      -0.10\n      -0.10\n      -0.09\n      -0.09\n      -0.09\n      -0.08\n      -0.08\n      -0.08\n      -0.07\n      -0.07\n      -0.07\n      -0.07\n      -0.07\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n    \n    \n      24\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n    \n    \n      25\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.06\n      -0.07\n      -0.07\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n      -0.09\n      -0.09\n      -0.10\n      -0.10\n      -0.10\n      -0.11\n      -0.11\n      -0.12\n      -0.12\n      -0.13\n    \n    \n      26\n      -0.13\n      -0.13\n      -0.12\n      -0.12\n      -0.11\n      -0.11\n      -0.10\n      -0.10\n      -0.09\n      -0.09\n      -0.08\n      -0.08\n      -0.07\n      -0.07\n      -0.06\n      -0.06\n      -0.05\n      -0.04\n      -0.03\n      -0.02\n      -0.02\n      -0.01\n      0.00\n      0.01\n    \n    \n      27\n      0.02\n      0.03\n      0.03\n      0.04\n      0.05\n      0.06\n      0.06\n      0.07\n      0.08\n      0.10\n      0.11\n      0.12\n      0.13\n      0.14\n      0.15\n      0.16\n      0.17\n      0.18\n      0.20\n      0.21\n      0.23\n      0.24\n      0.25\n      0.27\n    \n    \n      28\n      0.27\n      0.27\n      0.25\n      0.24\n      0.22\n      0.21\n      0.19\n      0.18\n      0.17\n      0.15\n      0.14\n      0.14\n      0.13\n      0.12\n      0.11\n      0.10\n      0.09\n      0.08\n      0.08\n      0.07\n      0.06\n      0.06\n      0.05\n      0.05\n    \n    \n      29\n      0.04\n      0.03\n      0.02\n      0.02\n      0.01\n      -0.00\n      -0.01\n      -0.02\n      -0.03\n      -0.04\n      -0.04\n      -0.05\n      -0.06\n      -0.07\n      -0.07\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n      -0.10\n      -0.10\n      -0.11\n      -0.12\n      -0.12\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt_residual_lag(res, 1)\n\n\n\n\n\nplt_residual_lag(res, 24)\n\n\n\n\n\nplt_residual_lag(res, 24*7)\n\n\n\n\n\nplt_residual_lag(res, 24*7*2)"
  },
  {
    "objectID": "Lec6_Autocorrelation.html#predictors-temperature-1-day-lag-of-power-1-week-lag-of-power-2-weeks-lag-of-power",
    "href": "Lec6_Autocorrelation.html#predictors-temperature-1-day-lag-of-power-1-week-lag-of-power-2-weeks-lag-of-power",
    "title": "6  Autocorrelation",
    "section": "6.6 Predictors: Temperature + 1 day lag of power + 1 week lag of power + 2 weeks lag of power",
    "text": "6.6 Predictors: Temperature + 1 day lag of power + 1 week lag of power + 2 weeks lag of power\nAlthough the data shows there is a significant (but not strong) correlation, we need to be cautious to use this feature because there are no simple reasons for this relationship.\nFor 1-day-lag feature, the correlation is easily understood.\nFor 1-week-lag feature, we could argue that the behaviour is different between weekday and weekend.\nBut for 2-week-lag feature, it is hard to understand especially when we have included 1-day-lag and 1-week-lag features. The relation is spurious.\n\ndf['power_lag_2_week']=df['power'].shift(24*7*2)\ndf.tail()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      key\n      Date\n      Hour\n      power\n      temperature\n      temp_hot\n      temp_cold\n      power_lag_1_day\n      power_lag_1_week\n      power_lag_2_week\n    \n  \n  \n    \n      35059\n      20201231:19\n      2020-12-31\n      19\n      5948\n      4.9\n      0.0\n      4.9\n      6163.0\n      5833.0\n      6826.0\n    \n    \n      35060\n      20201231:20\n      2020-12-31\n      20\n      5741\n      4.5\n      0.0\n      4.5\n      5983.0\n      5665.0\n      6663.0\n    \n    \n      35061\n      20201231:21\n      2020-12-31\n      21\n      5527\n      3.7\n      0.0\n      3.7\n      5727.0\n      5474.0\n      6407.0\n    \n    \n      35062\n      20201231:22\n      2020-12-31\n      22\n      5301\n      2.9\n      0.0\n      2.9\n      5428.0\n      5273.0\n      6068.0\n    \n    \n      35063\n      20201231:23\n      2020-12-31\n      23\n      5094\n      2.1\n      0.0\n      2.1\n      5104.0\n      5010.0\n      5709.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nres=build_model(['temp_hot', 'temp_cold', 'power_lag_1_day','power_lag_1_week', 'power_lag_2_week' ])\n\n/usr/local/lib/python3.8/dist-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  x = pd.concat(x[::order], 1)\n\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          power        R-squared:              0.848  \n\n\n  Model:                   OLS         Adj. R-squared:         0.847  \n\n\n  Method:             Least Squares    F-statistic:         3.860e+04 \n\n\n  Date:             Sun, 22 Jan 2023   Prob (F-statistic):     0.00   \n\n\n  Time:                 19:25:04       Log-Likelihood:     -2.5626e+05\n\n\n  No. Observations:       34728        AIC:                 5.125e+05 \n\n\n  Df Residuals:           34722        BIC:                 5.126e+05 \n\n\n  Df Model:                   5                                       \n\n\n  Covariance Type:      nonrobust                                     \n\n\n\n\n                      coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const              200.8402    14.046    14.298  0.000   173.309   228.371\n\n\n  temp_hot             3.2508     0.217    14.983  0.000     2.826     3.676\n\n\n  temp_cold           -5.6865     0.379   -15.005  0.000    -6.429    -4.944\n\n\n  power_lag_1_day      0.5637     0.004   152.597  0.000     0.556     0.571\n\n\n  power_lag_1_week     0.2415     0.004    60.139  0.000     0.234     0.249\n\n\n  power_lag_2_week     0.1565     0.004    40.465  0.000     0.149     0.164\n\n\n\n\n  Omnibus:       2229.659   Durbin-Watson:         0.036\n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   7850.238\n\n\n  Skew:            0.262    Prob(JB):               0.00\n\n\n  Kurtosis:        5.270    Cond. No.           6.72e+04\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.72e+04. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nplt_acf(res)\n\n/usr/local/lib/python3.8/dist-packages/statsmodels/tsa/stattools.py:667: FutureWarning: fft=True will become the default after the release of the 0.12 release of statsmodels. To suppress this warning, explicitly set fft=False.\n  warnings.warn(\n\n\n\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n    \n    \n      day\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      1.00\n      0.98\n      0.94\n      0.90\n      0.85\n      0.80\n      0.75\n      0.71\n      0.67\n      0.63\n      0.59\n      0.56\n      0.53\n      0.50\n      0.47\n      0.44\n      0.41\n      0.39\n      0.37\n      0.35\n      0.33\n      0.30\n      0.28\n      0.25\n    \n    \n      1\n      0.23\n      0.21\n      0.20\n      0.18\n      0.16\n      0.14\n      0.13\n      0.11\n      0.10\n      0.08\n      0.07\n      0.06\n      0.05\n      0.05\n      0.04\n      0.04\n      0.03\n      0.03\n      0.02\n      0.02\n      0.01\n      0.01\n      0.00\n      -0.00\n    \n    \n      2\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.00\n      -0.00\n      0.00\n      0.01\n      0.01\n      0.02\n      0.02\n      0.02\n      0.03\n      0.03\n      0.03\n      0.03\n    \n    \n      3\n      0.03\n      0.03\n      0.03\n      0.03\n      0.02\n      0.02\n      0.02\n      0.01\n      0.01\n      0.01\n      0.00\n      0.00\n      0.00\n      -0.00\n      -0.00\n      -0.00\n      0.00\n      0.00\n      0.00\n      0.00\n      0.01\n      0.01\n      0.01\n      0.01\n    \n    \n      4\n      0.01\n      0.01\n      0.00\n      0.00\n      0.00\n      -0.00\n      -0.00\n      -0.01\n      -0.01\n      -0.01\n      -0.02\n      -0.02\n      -0.02\n      -0.02\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.04\n      -0.04\n      -0.04\n      -0.05\n      -0.05\n    \n    \n      5\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.04\n      -0.03\n      -0.03\n      -0.03\n      -0.02\n      -0.02\n      -0.02\n      -0.01\n      -0.01\n      -0.00\n      0.01\n      0.01\n      0.02\n      0.03\n      0.03\n      0.04\n      0.05\n      0.05\n      0.06\n      0.07\n    \n    \n      6\n      0.07\n      0.07\n      0.07\n      0.07\n      0.07\n      0.07\n      0.07\n      0.07\n      0.08\n      0.08\n      0.09\n      0.09\n      0.10\n      0.10\n      0.11\n      0.11\n      0.12\n      0.12\n      0.13\n      0.14\n      0.14\n      0.15\n      0.16\n      0.16\n    \n    \n      7\n      0.17\n      0.16\n      0.15\n      0.14\n      0.13\n      0.12\n      0.11\n      0.10\n      0.09\n      0.09\n      0.08\n      0.07\n      0.07\n      0.06\n      0.05\n      0.04\n      0.03\n      0.03\n      0.02\n      0.02\n      0.01\n      0.01\n      0.00\n      -0.00\n    \n    \n      8\n      -0.00\n      -0.01\n      -0.01\n      -0.02\n      -0.02\n      -0.02\n      -0.03\n      -0.03\n      -0.03\n      -0.04\n      -0.04\n      -0.04\n      -0.05\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.06\n      -0.07\n      -0.07\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n    \n    \n      9\n      -0.10\n      -0.10\n      -0.09\n      -0.09\n      -0.09\n      -0.08\n      -0.08\n      -0.08\n      -0.07\n      -0.07\n      -0.06\n      -0.06\n      -0.06\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.03\n      -0.03\n      -0.03\n      -0.02\n      -0.02\n      -0.02\n      -0.01\n    \n    \n      10\n      -0.01\n      -0.01\n      -0.01\n      -0.01\n      -0.00\n      -0.00\n      -0.00\n      -0.00\n      -0.00\n      -0.00\n      0.00\n      0.00\n      0.00\n      0.00\n      0.00\n      0.00\n      0.01\n      0.01\n      0.01\n      0.01\n      0.02\n      0.02\n      0.02\n      0.02\n    \n    \n      11\n      0.02\n      0.02\n      0.02\n      0.02\n      0.02\n      0.02\n      0.02\n      0.02\n      0.02\n      0.02\n      0.01\n      0.01\n      0.01\n      0.01\n      0.01\n      0.00\n      0.00\n      -0.00\n      -0.00\n      -0.01\n      -0.01\n      -0.02\n      -0.02\n      -0.03\n    \n    \n      12\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.02\n      -0.02\n      -0.02\n      -0.02\n      -0.01\n      -0.01\n      -0.01\n      -0.00\n      0.00\n      0.01\n      0.01\n      0.02\n      0.02\n      0.03\n      0.04\n      0.04\n    \n    \n      13\n      0.05\n      0.05\n      0.05\n      0.05\n      0.06\n      0.06\n      0.06\n      0.07\n      0.07\n      0.08\n      0.08\n      0.09\n      0.09\n      0.10\n      0.10\n      0.11\n      0.11\n      0.12\n      0.13\n      0.13\n      0.14\n      0.15\n      0.16\n      0.16\n    \n    \n      14\n      0.16\n      0.16\n      0.15\n      0.14\n      0.13\n      0.12\n      0.11\n      0.10\n      0.09\n      0.08\n      0.08\n      0.07\n      0.06\n      0.06\n      0.05\n      0.04\n      0.03\n      0.02\n      0.01\n      0.01\n      0.00\n      -0.01\n      -0.01\n      -0.02\n    \n    \n      15\n      -0.02\n      -0.03\n      -0.03\n      -0.04\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.07\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n      -0.10\n      -0.10\n      -0.11\n      -0.11\n      -0.12\n      -0.12\n      -0.13\n      -0.13\n      -0.14\n      -0.14\n      -0.15\n    \n    \n      16\n      -0.15\n      -0.15\n      -0.14\n      -0.14\n      -0.13\n      -0.13\n      -0.12\n      -0.12\n      -0.11\n      -0.11\n      -0.11\n      -0.10\n      -0.10\n      -0.10\n      -0.09\n      -0.09\n      -0.08\n      -0.08\n      -0.07\n      -0.07\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n    \n    \n      17\n      -0.05\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.03\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.05\n    \n    \n      18\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.06\n      -0.07\n      -0.07\n      -0.07\n      -0.07\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n      -0.09\n      -0.10\n      -0.10\n      -0.10\n      -0.11\n      -0.11\n      -0.11\n      -0.12\n      -0.12\n      -0.13\n      -0.13\n    \n    \n      19\n      -0.13\n      -0.13\n      -0.12\n      -0.11\n      -0.11\n      -0.10\n      -0.09\n      -0.09\n      -0.08\n      -0.08\n      -0.07\n      -0.07\n      -0.06\n      -0.05\n      -0.05\n      -0.04\n      -0.03\n      -0.03\n      -0.02\n      -0.01\n      -0.00\n      0.00\n      0.01\n      0.02\n    \n    \n      20\n      0.02\n      0.03\n      0.03\n      0.04\n      0.04\n      0.05\n      0.05\n      0.06\n      0.07\n      0.07\n      0.08\n      0.09\n      0.10\n      0.10\n      0.11\n      0.12\n      0.13\n      0.14\n      0.15\n      0.16\n      0.17\n      0.18\n      0.20\n      0.21\n    \n    \n      21\n      0.21\n      0.20\n      0.19\n      0.18\n      0.17\n      0.16\n      0.15\n      0.13\n      0.12\n      0.11\n      0.11\n      0.10\n      0.09\n      0.09\n      0.08\n      0.07\n      0.06\n      0.06\n      0.05\n      0.05\n      0.04\n      0.04\n      0.04\n      0.03\n    \n    \n      22\n      0.03\n      0.03\n      0.02\n      0.02\n      0.01\n      0.00\n      -0.00\n      -0.01\n      -0.01\n      -0.02\n      -0.02\n      -0.03\n      -0.03\n      -0.04\n      -0.04\n      -0.04\n      -0.05\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.07\n      -0.07\n      -0.08\n    \n    \n      23\n      -0.08\n      -0.08\n      -0.08\n      -0.08\n      -0.07\n      -0.07\n      -0.07\n      -0.06\n      -0.06\n      -0.06\n      -0.06\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n    \n    \n      24\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n      -0.04\n    \n    \n      25\n      -0.04\n      -0.04\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.06\n      -0.07\n      -0.07\n      -0.07\n      -0.08\n      -0.08\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n      -0.10\n      -0.10\n      -0.10\n      -0.11\n      -0.11\n    \n    \n      26\n      -0.11\n      -0.11\n      -0.11\n      -0.10\n      -0.10\n      -0.09\n      -0.09\n      -0.08\n      -0.08\n      -0.08\n      -0.07\n      -0.07\n      -0.06\n      -0.06\n      -0.05\n      -0.05\n      -0.04\n      -0.03\n      -0.03\n      -0.02\n      -0.01\n      -0.01\n      0.00\n      0.01\n    \n    \n      27\n      0.02\n      0.02\n      0.03\n      0.04\n      0.04\n      0.05\n      0.06\n      0.07\n      0.07\n      0.09\n      0.10\n      0.11\n      0.11\n      0.12\n      0.13\n      0.14\n      0.15\n      0.16\n      0.17\n      0.18\n      0.20\n      0.21\n      0.22\n      0.23\n    \n    \n      28\n      0.23\n      0.23\n      0.22\n      0.20\n      0.19\n      0.18\n      0.17\n      0.16\n      0.15\n      0.14\n      0.13\n      0.12\n      0.11\n      0.11\n      0.10\n      0.09\n      0.08\n      0.08\n      0.07\n      0.06\n      0.06\n      0.05\n      0.04\n      0.04\n    \n    \n      29\n      0.03\n      0.03\n      0.02\n      0.01\n      0.01\n      -0.00\n      -0.01\n      -0.01\n      -0.02\n      -0.03\n      -0.03\n      -0.04\n      -0.05\n      -0.05\n      -0.06\n      -0.06\n      -0.07\n      -0.07\n      -0.08\n      -0.08\n      -0.09\n      -0.09\n      -0.10\n      -0.10\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt_residual_lag(res, 1)\n\n\n\n\n\nplt_residual_lag(res, 24)\n\n\n\n\n\nplt_residual_lag(res, 24*7)\n\n\n\n\n\nplt_residual_lag(res, 24*7*2)"
  },
  {
    "objectID": "Lec7_logistic_regression.html",
    "href": "Lec7_logistic_regression.html",
    "title": "7  Logistic regression",
    "section": "",
    "text": "Read sections 4.1 - 4.3 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately."
  },
  {
    "objectID": "Lec7_logistic_regression.html#theory-behind-logistic-regression",
    "href": "Lec7_logistic_regression.html#theory-behind-logistic-regression",
    "title": "7  Logistic regression",
    "section": "7.1 Theory Behind Logistic Regression",
    "text": "7.1 Theory Behind Logistic Regression\nLogistic regression is the go-to linear classification algorithm for two-class problems. It is easy to implement, easy to understand and gets great results on a wide variety of problems, even when the expectations the method has for your data are violated.\n\n7.1.1 Description\nLogistic regression is named for the function used at the core of the method, the logistic function.\nThe logistic function, also called the Sigmoid function was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It’s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n\\[\\frac{1}{1 + e^{-x}}\\]\n\\(e\\) is the base of the natural logarithms and \\(x\\) is value that you want to transform via the logistic function.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as sm\n\n\n%matplotlib inline\nsns.set_style('whitegrid')\nplt.style.use(\"fivethirtyeight\")\nx = np.linspace(-6, 6, num=1000)\nplt.figure(figsize=(10, 6))\nplt.plot(x, (1 / (1 + np.exp(-x))))\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Sigmoid Function\")\n\nText(0.5, 1.0, 'Sigmoid Function')\n\n\n\n\n\nThe logistic regression equation has a very similar representation like linear regression. The difference is that the output value being modelled is binary in nature.\n\\[\\hat{p}=\\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1}x_1}}{1+e^{\\hat{\\beta_0}+\\hat{\\beta_1}x_1}}\\]\nor\n\\[\\hat{p}=\\frac{1.0}{1.0+e^{-(\\hat{\\beta_0}+\\hat{\\beta_1}x_1)}}\\]\n\\(\\hat{\\beta_0}\\) is the estimated intercept term\n\\(\\hat{\\beta_1}\\) is the estimated coefficient for \\(x_1\\)\n\\(\\hat{p}\\) is the predicted output with real value between 0 and 1. To convert this to binary output of 0 or 1, this would either need to be rounded to an integer value or a cutoff point be provided to specify the class segregation point.\n\n\n7.1.2 Learning the Logistic Regression Model\nThe coefficients (Beta values b) of the logistic regression algorithm must be estimated from your training data. This is done using maximum-likelihood estimation.\nMaximum-likelihood estimation is a common learning algorithm used by a variety of machine learning algorithms, although it does make assumptions about the distribution of your data (more on this when we talk about preparing your data).\nThe best coefficients should result in a model that would predict a value very close to 1 (e.g. male) for the default class and a value very close to 0 (e.g. female) for the other class. The intuition for maximum-likelihood for logistic regression is that a search procedure seeks values for the coefficients (Beta values) that maximize the likelihood of the observed data. In other words, in MLE, we estimate the parameter values (Beta values) which are the most likely to produce that data at hand.\nHere is an analogy to understand the idea behind Maximum Likelihood Estimation (MLE). Let us say, you are listening to a song (data). You are not aware of the singer (parameter) of the song. With just the musical piece at hand, you try to guess the singer (parameter) who you feel is the most likely (MLE) to have sung that song. Your are making a maximum likelihood estimate! Out of all the singers (parameter space) you have chosen them as the one who is the most likely to have sung that song (data).\nWe are not going to go into the math of maximum likelihood. It is enough to say that a minimization algorithm is used to optimize the best values for the coefficients for your training data. This is often implemented in practice using efficient numerical optimization algorithm (like the Quasi-newton method).\nWhen you are learning logistic, you can implement it yourself from scratch using the much simpler gradient descent algorithm.\n\n\n7.1.3 Preparing Data for Logistic Regression\nThe assumptions made by logistic regression about the distribution and relationships in your data are much the same as the assumptions made in linear regression.\nMuch study has gone into defining these assumptions and precise probabilistic and statistical language is used. My advice is to use these as guidelines or rules of thumb and experiment with different data preparation schemes.\nUltimately in predictive modeling machine learning projects you are laser focused on making accurate predictions rather than interpreting the results. As such, you can break some assumptions as long as the model is robust and performs well.\n\nBinary Output Variable: This might be obvious as we have already mentioned it, but logistic regression is intended for binary (two-class) classification problems. It will predict the probability of an instance belonging to the default class, which can be snapped into a 0 or 1 classification.\nRemove Noise: Logistic regression assumes no error in the output variable (y), consider removing outliers and possibly misclassified instances from your training data.\nGaussian Distribution: Logistic regression is a linear algorithm (with a non-linear transform on output). It does assume a linear relationship between the input variables with the output. Data transforms of your input variables that better expose this linear relationship can result in a more accurate model. For example, you can use log, root, Box-Cox and other univariate transforms to better expose this relationship.\nRemove Correlated Inputs: Like linear regression, the model can overfit if you have multiple highly-correlated inputs. Consider calculating the pairwise correlations between all inputs and removing highly correlated inputs.\nFail to Converge: It is possible for the expected likelihood estimation process that learns the coefficients to fail to converge. This can happen if there are many highly correlated inputs in your data or the data is very sparse (e.g. lots of zeros in your input data)."
  },
  {
    "objectID": "Lec7_logistic_regression.html#logistic-regression-scikit-learn-vs-statsmodels",
    "href": "Lec7_logistic_regression.html#logistic-regression-scikit-learn-vs-statsmodels",
    "title": "7  Logistic regression",
    "section": "7.2 Logistic Regression: Scikit-learn vs Statsmodels",
    "text": "7.2 Logistic Regression: Scikit-learn vs Statsmodels\nPython gives us two ways to do logistic regression. Statsmodels offers modeling from the perspective of statistics. Scikit-learn offers some of the same models from the perspective of machine learning.\nSo we need to understand the difference between statistics and machine learning! Statistics makes mathematically valid inferences about a population based on sample data. Statistics answers the question, “What is the evidence that X is related to Y?” Machine learning has the goal of optimizing predictive accuracy rather than inference. Machine learning answers the question, “Given X, what prediction should we make for Y?”\nLet us see the use of statsmodels for logistic regression. We’ll see scikit-learn later in the course, when we learn methods that focus on prediction."
  },
  {
    "objectID": "Lec7_logistic_regression.html#training-a-logistic-regression-model",
    "href": "Lec7_logistic_regression.html#training-a-logistic-regression-model",
    "title": "7  Logistic regression",
    "section": "7.3 Training a logistic regression model",
    "text": "7.3 Training a logistic regression model\nRead the data on social network ads. The data shows if the person purchased a product when targeted with an ad on social media. Fit a logistic regression model to predict if a user will purchase the product based on their characteristics such as age, gender and estimated salary.\n\ntrain = pd.read_csv('./Datasets/Social_Network_Ads_train.csv') #Develop the model on train data\ntest = pd.read_csv('./Datasets/Social_Network_Ads_test.csv') #Test the model on test data\n\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      User ID\n      Gender\n      Age\n      EstimatedSalary\n      Purchased\n    \n  \n  \n    \n      0\n      15755018\n      Male\n      36\n      33000\n      0\n    \n    \n      1\n      15697020\n      Female\n      39\n      61000\n      0\n    \n    \n      2\n      15796351\n      Male\n      36\n      118000\n      1\n    \n    \n      3\n      15665760\n      Male\n      39\n      122000\n      1\n    \n    \n      4\n      15794661\n      Female\n      26\n      118000\n      0\n    \n  \n\n\n\n\n\n7.3.1 Examining the Distribution of the Target Column\nMake sure our target is not severely imbalanced.\n\ntrain.Purchased.value_counts()\n\n0    194\n1    106\nName: Purchased, dtype: int64\n\n\n\nsns.countplot(x = 'Purchased',data = train);\n\n\n\n\nLet us try to fit a linear regression model, instead of logistic regression. We fit a linear regression model to predict probability of purchase based on age.\n\nsns.scatterplot(x = 'Age', y = 'Purchased', data = train, color = 'orange') #Visualizing data\nlm = sm.ols(formula = 'Purchased~Age', data = train).fit() #Developing linear regression model\nsns.lineplot(x = 'Age', y= lm.predict(train), data = train, color = 'blue') #Visualizing model\n\n<AxesSubplot:xlabel='Age', ylabel='Purchased'>\n\n\n\n\n\nNote the issues with the linear regression model:\n\nThe regression line goes below 0 and over 1. However, probability of purchase must be in [0,1].\nThe linear regression model does not seem to fit the data well.\n\n\n\n7.3.2 Fitting the logistic regression model\nNow, let us fit a logistic regression model to predict probability of purchase based on Age.\n\nsns.scatterplot(x = 'Age', y = 'Purchased', data = train, color = 'orange') #Visualizing data\nlogit_model = sm.logit(formula = 'Purchased~Age', data = train).fit() #Developing logistic regression model\nsns.lineplot(x = 'Age', y= logit_model.predict(train), data = train, color = 'blue') #Visualizing model\n\nOptimization terminated successfully.\n         Current function value: 0.430107\n         Iterations 7\n\n\n<AxesSubplot:xlabel='Age', ylabel='Purchased'>\n\n\n\n\n\nAs logistic regression uses the sigmoid function, the probability stays in [0,1]. Also, it seems to better fit the points as compared to linear regression.\n\nlogit_model.summary()\n\n\n\nLogit Regression Results\n\n  Dep. Variable:       Purchased      No. Observations:       300  \n\n\n  Model:                 Logit        Df Residuals:           298  \n\n\n  Method:                 MLE         Df Model:                 1  \n\n\n  Date:            Tue, 19 Apr 2022   Pseudo R-squ.:       0.3378  \n\n\n  Time:                16:46:02       Log-Likelihood:      -129.03 \n\n\n  converged:             True         LL-Null:             -194.85 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        1.805e-30\n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept    -7.8102     0.885    -8.825  0.000    -9.545    -6.076\n\n\n  Age           0.1842     0.022     8.449  0.000     0.141     0.227\n\n\n\n\nInterpret the coefficient of age\nFor a unit increase in age, the log odds of purchase increase by 0.18, or the odds of purchase get multiplied by exp(0.18) = 1.2\nIs the increase in probability of purchase constant with a unit increase in age?\nNo, it depends on age.\nIs gender associated with probability of purchase?\n\nlogit_model_gender = sm.logit(formula = 'Purchased~Gender', data = train).fit()\nlogit_model_gender.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.648804\n         Iterations 4\n\n\n\n\nLogit Regression Results\n\n  Dep. Variable:       Purchased      No. Observations:       300 \n\n\n  Model:                 Logit        Df Residuals:           298 \n\n\n  Method:                 MLE         Df Model:                 1 \n\n\n  Date:            Tue, 19 Apr 2022   Pseudo R-squ.:      0.001049\n\n\n  Time:                16:46:04       Log-Likelihood:      -194.64\n\n\n  converged:             True         LL-Null:             -194.85\n\n\n  Covariance Type:     nonrobust      LLR p-value:         0.5225 \n\n\n\n\n                    coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept         -0.5285     0.168    -3.137  0.002    -0.859    -0.198\n\n\n  Gender[T.Male]    -0.1546     0.242    -0.639  0.523    -0.629     0.319\n\n\n\n\nNo, assuming a significance level of \\(\\alpha = 5\\%\\), Gender is not associated with probability of default, as the \\(p\\)-value for Male is greater than 0.05."
  },
  {
    "objectID": "Lec7_logistic_regression.html#confusion-matrix-and-classification-accuracy",
    "href": "Lec7_logistic_regression.html#confusion-matrix-and-classification-accuracy",
    "title": "7  Logistic regression",
    "section": "7.4 Confusion matrix and classification accuracy",
    "text": "7.4 Confusion matrix and classification accuracy\nA confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class.\n\n#Function to compute confusion matrix and prediction accuracy on training data\ndef confusion_matrix_train(model,cutoff=0.5):\n    # Confusion matrix\n    cm_df = pd.DataFrame(model.pred_table(threshold = cutoff))\n    #Formatting the confusion matrix\n    cm_df.columns = ['Predicted 0', 'Predicted 1'] \n    cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n    cm = np.array(cm_df)\n    # Calculate the accuracy\n    accuracy = (cm[0,0]+cm[1,1])/cm.sum()\n    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')\n    plt.ylabel(\"Actual Values\")\n    plt.xlabel(\"Predicted Values\")\n    print(\"Classification accuracy = {:.1%}\".format(accuracy))\n\nFind the confusion matrix and classification accuracy of the model with Age as the predictor on training data.\n\ncm = confusion_matrix_train(logit_model)\n\nClassification accuracy = 83.3%\n\n\n\n\n\nConfusion matrix:\n\nEach row: actual class\nEach column: predicted class\n\nFirst row: Non-purchasers, the negative class:\n\n181 were correctly classified as Non-purchasers. True negatives.\nRemaining 13 were wrongly classified as Non-purchasers. False positive\n\nSecond row: Purchasers, the positive class:\n\n37 were incorrectly classified as Non-purchasers. False negatives\n69 were correctly classified Purchasers. True positives\n\n\n#Function to compute confusion matrix and prediction accuracy on test data\ndef confusion_matrix_test(data,actual_values,model,cutoff=0.5):\n#Predict the values using the Logit model\n    pred_values = model.predict(data)\n# Specify the bins\n    bins=np.array([0,cutoff,1])\n#Confusion matrix\n    cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n    cm_df = pd.DataFrame(cm)\n    cm_df.columns = ['Predicted 0','Predicted 1']\n    cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n    accuracy = (cm[0,0]+cm[1,1])/cm.sum()\n    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')\n    plt.ylabel(\"Actual Values\")\n    plt.xlabel(\"Predicted Values\")\n    print(\"Classification accuracy = {:.1%}\".format(accuracy))\n\nFind the confusion matrix and classification accuracy of the model with Age as the predictor on test data.\n\nconfusion_matrix_test(test,test.Purchased,logit_model)\n\nClassification accuracy = 86.0%\n\n\n\n\n\nThe model classifies a bit more accurately on test data as compared to the training data, which is a bit unusual. However, it shows that the model did not overfit on training data.\nInclude EstimatedSalary as a predictor in the above model\n\nlogit_model2 = sm.logit(formula = 'Purchased~Age+EstimatedSalary', data = train).fit()\nlogit_model2.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.358910\n         Iterations 7\n\n\n\n\nLogit Regression Results\n\n  Dep. Variable:       Purchased      No. Observations:       300  \n\n\n  Model:                 Logit        Df Residuals:           297  \n\n\n  Method:                 MLE         Df Model:                 2  \n\n\n  Date:            Tue, 14 Feb 2023   Pseudo R-squ.:       0.4474  \n\n\n  Time:                12:03:29       Log-Likelihood:      -107.67 \n\n\n  converged:             True         LL-Null:             -194.85 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        1.385e-38\n\n\n\n\n                     coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept         -11.9432     1.424    -8.386  0.000   -14.735    -9.152\n\n\n  Age                 0.2242     0.028     7.890  0.000     0.168     0.280\n\n\n  EstimatedSalary   3.48e-05  6.15e-06     5.660  0.000  2.27e-05  4.68e-05\n\n\n\n\n\nconfusion_matrix_train(logit_model2)\n\nClassification accuracy = 83.3%\n\n\n\n\n\n\nconfusion_matrix_test(test,test.Purchased,logit_model2)\n\nClassification accuracy = 89.0%\n\n\n\n\n\nThe log likelihood of the model has increased, while also increasing the prediction accuracy on test data, which shows that the additional predictor is helping explain the response better, without overfitting the data.\nInclude Gender as a predictor in the above model\n\nlogit_model = sm.logit(formula = 'Purchased~Age+EstimatedSalary+Gender', data = train).fit()\nlogit_model.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.357327\n         Iterations 7\n\n\n\n\nLogit Regression Results\n\n  Dep. Variable:       Purchased      No. Observations:       300  \n\n\n  Model:                 Logit        Df Residuals:           296  \n\n\n  Method:                 MLE         Df Model:                 3  \n\n\n  Date:            Tue, 14 Feb 2023   Pseudo R-squ.:       0.4498  \n\n\n  Time:                12:17:28       Log-Likelihood:      -107.20 \n\n\n  converged:             True         LL-Null:             -194.85 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        9.150e-38\n\n\n\n\n                     coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept         -12.2531     1.478    -8.293  0.000   -15.149    -9.357\n\n\n  Gender[T.Male]      0.3356     0.346     0.970  0.332    -0.342     1.013\n\n\n  Age                 0.2275     0.029     7.888  0.000     0.171     0.284\n\n\n  EstimatedSalary  3.494e-05  6.17e-06     5.666  0.000  2.29e-05   4.7e-05\n\n\n\n\n\nconfusion_matrix_train(logit_model)\n\nClassification accuracy = 84.3%\n\n\n\n\n\n\nconfusion_matrix_test(test,test.Purchased,logit_model)\n\nClassification accuracy = 88.0%\n\n\n\n\n\nGender is a statistically insignificant predictor, and including it slightly lowers the classification accuracy on test data. Note that the classification accuracy on training data will continue to increase on adding more predictors, irrespective of their relevance (similar to the idea of RSS on training data in linear regression).\nIs there a residual in logistic regression?\nNo, since the response is assumed to have a Bernoulli distribution, instead of a normal distribution.\nIs the odds ratio for a unit increase in a predictor \\(X_j\\), a constant (assuming that the rest of the predictors are held constant)?\nYes, the odds ratio in this case will \\(e^{\\beta_j}\\)"
  },
  {
    "objectID": "Lec7_logistic_regression.html#variable-transformations-in-logistic-regression",
    "href": "Lec7_logistic_regression.html#variable-transformations-in-logistic-regression",
    "title": "7  Logistic regression",
    "section": "7.5 Variable transformations in logistic regression",
    "text": "7.5 Variable transformations in logistic regression\nRead the dataset diabetes.csv that contains if a person has diabetes (Outcome = 1) based on health parameters such as BMI, blood pressure, age etc. Develop a model to predict the probability of a person having diabetes based on their age.\n\ndata = pd.read_csv('./Datasets/diabetes.csv')\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      Pregnancies\n      Glucose\n      BloodPressure\n      SkinThickness\n      Insulin\n      BMI\n      DiabetesPedigreeFunction\n      Age\n      Outcome\n    \n  \n  \n    \n      0\n      6\n      148\n      72\n      35\n      0\n      33.6\n      0.627\n      50\n      1\n    \n    \n      1\n      1\n      85\n      66\n      29\n      0\n      26.6\n      0.351\n      31\n      0\n    \n    \n      2\n      8\n      183\n      64\n      0\n      0\n      23.3\n      0.672\n      32\n      1\n    \n    \n      3\n      1\n      89\n      66\n      23\n      94\n      28.1\n      0.167\n      21\n      0\n    \n    \n      4\n      0\n      137\n      40\n      35\n      168\n      43.1\n      2.288\n      33\n      1\n    \n  \n\n\n\n\nRandomly select 80% of the observations to create a training dataset. Create a test dataset with the remaining 20% observations.\n\n#Creating training and test datasets\nnp.random.seed(2)\ntrain = data.sample(round(data.shape[0]*0.8))\ntest = data.drop(train.index)\n\nDoes Age seem to distinguish Outcome levels?\n\nsns.boxplot(x = 'Outcome', y = 'Age', data = train)\n\n<AxesSubplot:xlabel='Outcome', ylabel='Age'>\n\n\n\n\n\nYes it does!\nDevelop and visualize a logistic regression model to predict Outcome using Age.\n\n#Jittering points to better see the density of points in any given region of the plot\ndef jitter(values,j):\n    return values + np.random.normal(j,0.02,values.shape)\nsns.scatterplot(x = jitter(train.Age,0), y = jitter(train.Outcome,0), data = train, color = 'orange')\nlogit_model = sm.logit(formula = 'Outcome~Age', data = train).fit()\nsns.lineplot(x = 'Age', y= logit_model.predict(train), data = train, color = 'blue') \nprint(logit_model.llf) #Printing the log likelihood to compare it with the next model we build\n\nOptimization terminated successfully.\n         Current function value: 0.612356\n         Iterations 5\n-375.9863802089716\n\n\n\n\n\n\nconfusion_matrix_train(logit_model)\n\nClassification accuracy = 65.6%\n\n\n\n\n\nClassification accuracy on train data = 66%\n\nconfusion_matrix_test(test,test.Outcome,logit_model)\n\nClassification accuracy = 59.7%\n\n\n\n\n\nClassification accuracy on test data = 60%\nCan a tranformation of Age provide a more accurate model?\nLet us visualize how the probability of people having diabetes varies with Age. We will bin Age to get the percentage of people having diabetes within different Age bins.\n\n#Binning Age\nbinned_age = pd.qcut(train['Age'],11,retbins=True)\ntrain['age_binned'] = binned_age[0]\n\n\n#Finding percentage of people having diabetes in each Age bin\nage_data = train.groupby('age_binned')['Outcome'].agg([('diabetes_percent','mean'),('nobs','count')]).reset_index(drop=False)\nage_data\n\n\n\n\n\n  \n    \n      \n      age_binned\n      diabetes_percent\n      nobs\n    \n  \n  \n    \n      0\n      (20.999, 22.0]\n      0.110092\n      109\n    \n    \n      1\n      (22.0, 23.0]\n      0.206897\n      29\n    \n    \n      2\n      (23.0, 25.0]\n      0.243243\n      74\n    \n    \n      3\n      (25.0, 26.0]\n      0.259259\n      27\n    \n    \n      4\n      (26.0, 28.0]\n      0.271186\n      59\n    \n    \n      5\n      (28.0, 31.0]\n      0.415094\n      53\n    \n    \n      6\n      (31.0, 35.0]\n      0.434783\n      46\n    \n    \n      7\n      (35.0, 39.0]\n      0.450980\n      51\n    \n    \n      8\n      (39.0, 43.545]\n      0.500000\n      54\n    \n    \n      9\n      (43.545, 52.0]\n      0.576271\n      59\n    \n    \n      10\n      (52.0, 81.0]\n      0.415094\n      53\n    \n  \n\n\n\n\n\n#Visualizing percentage of people having diabetes with increasing Age (or Age bins)\nsns.lineplot(x = age_data.index, y= age_data['diabetes_percent'])\nplt.xlabel('Age_bin')\n\nText(0.5, 0, 'Age_bin')\n\n\n\n\n\nWe observe that the probability of people having diabetes does not keep increasing monotonically with age. People with ages 52 and more have a lower probability of having diabetes than people in the immediately younger Age bin.\nA quadratic transformation of Age may better fit the above trend\n\n#Model with the quadratic transformation of Age\ndef jitter(values,j):\n    return values + np.random.normal(j,0.02,values.shape)\nsns.scatterplot(x = jitter(train.Age,0), y = jitter(train.Outcome,0), data = train, color = 'orange')\nlogit_model = sm.logit(formula = 'Outcome~Age+I(Age**2)', data = train).fit()\nsns.lineplot(x = 'Age', y= logit_model.predict(train), data = train, color = 'blue') \nlogit_model.llf\n\nOptimization terminated successfully.\n         Current function value: 0.586025\n         Iterations 6\n\n\n-359.81925590230185\n\n\n\n\n\n\nlogit_model.summary()\n\n\n\nLogit Regression Results\n\n  Dep. Variable:        Outcome       No. Observations:       614  \n\n\n  Model:                 Logit        Df Residuals:           611  \n\n\n  Method:                 MLE         Df Model:                 2  \n\n\n  Date:            Tue, 14 Feb 2023   Pseudo R-squ.:       0.08307 \n\n\n  Time:                12:25:54       Log-Likelihood:      -359.82 \n\n\n  converged:             True         LL-Null:             -392.42 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        6.965e-15\n\n\n\n\n                 coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept      -6.6485     0.908    -7.320  0.000    -8.429    -4.868\n\n\n  Age             0.2936     0.048     6.101  0.000     0.199     0.388\n\n\n  I(Age ** 2)    -0.0031     0.001    -5.280  0.000    -0.004    -0.002\n\n\n\n\nThe log likelihood of the model is higher and both the predictors are statistically significant indicating a better model fit. However, the model may also be overfitting. Let us check the model accuracy on test data.\n\nconfusion_matrix_train(logit_model)\n\nClassification accuracy = 68.1%\n\n\n\n\n\n\nconfusion_matrix_test(test,test.Outcome,logit_model)\n\nClassification accuracy = 68.8%\n\n\n\n\n\nThe classification accuracy on test data has increased to 69%. However, the number of false positives have increased. But in case of diabetes, false negatives are more concerning than false positives. This is because if a person has diabetes, and is told that they do not have diabetes, their condition may deteriorate. If a person does not have diabetes, and is told that they have diabetes, they may take unnecessary precautions or tests, but it will not be as harmful to the person as in the previous case. So, in this problem, we will be more focused on reducing the number of false negatives, instead of reducing the false positives or increasing the overall classification accuracy.\nWe can decrease the cutoff for classifying a person as having diabetes to reduce the number of false negatives.\n\n#Reducing the cutoff for classifying a person as diabetic to 0.3 (instead of 0.5)\nconfusion_matrix_test(test,test.Outcome,logit_model,0.3)\n\nClassification accuracy = 69.5%\n\n\n\n\n\nNote that the changed cut-off reduced the number of false negatives, but at the cost of increasing the false positives. However, the stakeholders may prefer the reduced cut-off to be safer.\nIs there another way to transform Age?\nYes, binning age into bins that have similar proportion of people with diabetes may provide a better model fit.\n\n#Creating a function to bin age so that it can be applied to both the test and train datasets\ndef var_transform(data):\n    binned_age = pd.qcut(train['Age'],10,retbins=True)\n    bins = binned_age[1]\n    data['age_binned'] = pd.cut(data['Age'],bins = bins)\n    dum = pd.get_dummies(data.age_binned,drop_first = True)\n    dum.columns = ['age'+str(x) for x in range(1,len(bins)-1)]\n    data = pd.concat([data,dum], axis = 1)\n    return data\n\n\n#Binning age using the function var_transform()\ntrain = var_transform(train)\ntest = var_transform(test)\n\n\n#Re-creating the plot of diabetes_percent vs age created earlier, just to check if the function binned age correctly. Yes, it did.\nage_data = train.groupby('age_binned')['Outcome'].agg([('diabetes_percent','mean'),('nobs','count')]).reset_index(drop=False)\nsns.lineplot(x = age_data.index, y= age_data['diabetes_percent'])\nplt.xlabel('Age_bin')\n\nText(0.5, 0, 'Age_bin')\n\n\n\n\n\n\n#Model with binned Age\ndef jitter(values,j):\n    return values + np.random.normal(j,0.02,values.shape)\nsns.scatterplot(x = jitter(train.Age,0), y = jitter(train.Outcome,0), data = train, color = 'orange')\nlogit_model = sm.logit(formula = 'Outcome~' + '+'.join(['age'+str(x) for x in range(1,10)]), data = train).fit()\nsns.lineplot(x = 'Age', y= logit_model.predict(train), data = train, color = 'blue') \n\nOptimization terminated successfully.\n         Current function value: 0.585956\n         Iterations 6\n\n\n<AxesSubplot:xlabel='Age', ylabel='Outcome'>\n\n\n\n\n\n\nlogit_model.summary()\n\n\n\nLogit Regression Results\n\n  Dep. Variable:        Outcome       No. Observations:       614  \n\n\n  Model:                 Logit        Df Residuals:           604  \n\n\n  Method:                 MLE         Df Model:                 9  \n\n\n  Date:            Sun, 19 Feb 2023   Pseudo R-squ.:       0.08318 \n\n\n  Time:                14:19:51       Log-Likelihood:      -359.78 \n\n\n  converged:             True         LL-Null:             -392.42 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        1.273e-10\n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept    -2.0898     0.306    -6.829  0.000    -2.690    -1.490\n\n\n  age1          0.7461     0.551     1.354  0.176    -0.334     1.826\n\n\n  age2          0.9548     0.409     2.336  0.019     0.154     1.756\n\n\n  age3          1.0602     0.429     2.471  0.013     0.219     1.901\n\n\n  age4          1.3321     0.438     3.044  0.002     0.474     2.190\n\n\n  age5          1.9606     0.398     4.926  0.000     1.180     2.741\n\n\n  age6          1.8303     0.399     4.586  0.000     1.048     2.612\n\n\n  age7          1.7596     0.410     4.288  0.000     0.955     2.564\n\n\n  age8          2.4544     0.402     6.109  0.000     1.667     3.242\n\n\n  age9          1.8822     0.404     4.657  0.000     1.090     2.674\n\n\n\n\nNote that the probability of having diabetes for each age bin is a constant, as per the above plot.\n\nconfusion_matrix_test(test,test.Outcome,logit_model,0.3)\n\nClassification accuracy = 67.5%\n\n\n\n\n\nBinning Age provides a similar result as compared to the model with the quadratic transformation of Age.\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      Pregnancies\n      Glucose\n      BloodPressure\n      SkinThickness\n      Insulin\n      BMI\n      DiabetesPedigreeFunction\n      Age\n      Outcome\n      age_binned\n      age1\n      age2\n      age3\n      age4\n      age5\n      age6\n      age7\n      age8\n      age9\n    \n  \n  \n    \n      158\n      2\n      88\n      74\n      19\n      53\n      29.0\n      0.229\n      22\n      0\n      (21.0, 22.0]\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      251\n      2\n      129\n      84\n      0\n      0\n      28.0\n      0.284\n      27\n      0\n      (25.0, 27.0]\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      631\n      0\n      102\n      78\n      40\n      90\n      34.5\n      0.238\n      24\n      0\n      (23.0, 25.0]\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      757\n      0\n      123\n      72\n      0\n      0\n      36.3\n      0.258\n      52\n      1\n      (51.0, 81.0]\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      689\n      1\n      144\n      82\n      46\n      180\n      46.1\n      0.335\n      46\n      1\n      (42.0, 51.0]\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n  \n\n\n\n\n\n#Model with the quadratic transformation of Age and more predictors\nlogit_model_diabetes = sm.logit(formula = 'Outcome~Age+I(Age**2)+Glucose+BloodPressure+BMI+DiabetesPedigreeFunction', data = train).fit()\nlogit_model_diabetes.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.470478\n         Iterations 6\n\n\n\n\nLogit Regression Results\n\n  Dep. Variable:        Outcome       No. Observations:       614  \n\n\n  Model:                 Logit        Df Residuals:           607  \n\n\n  Method:                 MLE         Df Model:                 6  \n\n\n  Date:            Thu, 23 Feb 2023   Pseudo R-squ.:       0.2639  \n\n\n  Time:                10:26:00       Log-Likelihood:      -288.87 \n\n\n  converged:             True         LL-Null:             -392.42 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        5.878e-42\n\n\n\n\n                              coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept                  -12.3347     1.282    -9.621  0.000   -14.847    -9.822\n\n\n  Age                          0.2852     0.056     5.121  0.000     0.176     0.394\n\n\n  I(Age ** 2)                 -0.0030     0.001    -4.453  0.000    -0.004    -0.002\n\n\n  Glucose                      0.0309     0.004     8.199  0.000     0.024     0.038\n\n\n  BloodPressure               -0.0141     0.006    -2.426  0.015    -0.025    -0.003\n\n\n  BMI                          0.0800     0.016     4.978  0.000     0.049     0.112\n\n\n  DiabetesPedigreeFunction     0.7138     0.322     2.213  0.027     0.082     1.346\n\n\n\n\nAdding more predictors has increased the log likelihood of the model as expected.\n\nconfusion_matrix_train(logit_model_diabetes,cutoff=0.3)\n\nClassification accuracy = 74.3%\n\n\n\n\n\n\nconfusion_matrix_test(test,test.Outcome,logit_model_diabetes,0.3)\n\nClassification accuracy = 80.5%\n\n\n\n\n\nThe model with more predictors also has lesser number of false negatives, and higher overall classification accuracy.\nHow many bins must you make for Age to get the most accurate model?\nIf the number of bins are too less, the trend may not be captured accurately. If the number of bins are too many, it may lead to overfitting of the model. There is an optimal value of the number of bins that captures the trend, but does not overfit. A couple of ways of estimating the optimal number of bins can be:\n\nThe number of bins for which the trend continues to be “almost” the same for several samples of the data.\nTesting the model on multiple test datasets.\n\nOptimizing the number of bins for each predictor may be a time-consuming exercises. You may do it for your course project. However, we will not do it here in the class notes."
  },
  {
    "objectID": "Lec7_logistic_regression.html#performance-measurement",
    "href": "Lec7_logistic_regression.html#performance-measurement",
    "title": "7  Logistic regression",
    "section": "7.6 Performance Measurement",
    "text": "7.6 Performance Measurement\nWe have already seen the confusion matrix, and classification accuracy. Now, let us see some other useful performance metrics that can be computed from the confusion matrix. The metrics below are computed for the confusion matrix immediately above this section (or the confusion matrix on test data corresponding to the model logit_model_diabetes).\n\n7.6.1 Precision-recall\nPrecision measures the accuracy of positive predictions. Also called the precision of the classifier\n\\[\\textrm{precision} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Positives}}\\]\n==> 70.13%\nPrecision is typically used with recall (Sensitivity or True Positive Rate). The ratio of positive instances that are correctly detected by the classifier.\n\\[\\textrm{recall} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Negatives}}\\] ==> 88.52%\nPrecision / Recall Tradeoff: Increasing precision reduces recall and vice versa.\nVisualize the precision-recall curve for the model logit_model_diabetes.\n\nfrom sklearn.metrics import precision_recall_curve\ny=train.Outcome\nypred = logit_model_diabetes.predict(train)\np, r, thresholds = precision_recall_curve(y, ypred)\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\nAs the decision threshold probability increases, the precision increases, while the recall decreases.\nQ: How are the values of the thresholds chosen to make the precision-recall curve?\nHint: Look at the documentation for precision_recall_curve.\n\n\n7.6.2 The Receiver Operating Characteristics (ROC) Curve\nA ROC(Receiver Operator Characteristic Curve) is a plot of sensitivity (True Positive Rate) on the y axis against (1−specificity) (False Positive Rate) on the x axis for varying values of the threshold t. The 45° diagonal line connecting (0,0) to (1,1) is the ROC curve corresponding to random chance. The ROC curve for the gold standard is the line connecting (0,0) to (0,1) and (0,1) to (1,1).\n\n\n\n\n\n\n\n\n\n\nAn animation to demonstrate how an ROC curve relates to sensitivity and specificity for all possible cutoffs (Source)\nHigh Threshold:\n\nHigh specificity\nLow sensitivity\n\nLow Threshold\n\nLow specificity\nHigh sensitivity\n\nThe area under ROC is called Area Under the Curve(AUC). AUC gives the rate of successful classification by the logistic model. To get a more in-depth idea of what a ROC-AUC curve is and how is it calculated, here is a good blog link.\nHere is good post by google developers on interpreting ROC-AUC, and its advantages / disadvantages.\nVisualize the ROC curve and compute the ROC-AUC for the model logit_model_diabetes.\n\nfrom sklearn.metrics import roc_curve, auc\ny=train.Outcome\nypred = logit_model_diabetes.predict(train)\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nprint(auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nplot_roc_curve(fpr, tpr)\n\n0.8325914847653979\n\n\n\n\n\nQ: How are the values of the auc_thresholds chosen to make the ROC curve? Why does it look like a step function?\nBelow is a function that prints the confusion matrix along with all the performance metrics we discussed above for a given decision threshold probability, on train / test data. Note that ROC-AUC does not depend on a decision threshold probability.\n\n#Function to compute confusion matrix and prediction accuracy on test/train data\ndef confusion_matrix_data(data,actual_values,model,cutoff=0.5):\n#Predict the values using the Logit model\n    pred_values = model.predict(data)\n# Specify the bins\n    bins=np.array([0,cutoff,1])\n#Confusion matrix\n    cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n    cm_df = pd.DataFrame(cm)\n    cm_df.columns = ['Predicted 0','Predicted 1']\n    cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n# Calculate the accuracy\n    accuracy = (cm[0,0]+cm[1,1])/cm.sum()\n    fnr = (cm[1,0])/(cm[1,0]+cm[1,1])\n    precision = (cm[1,1])/(cm[0,1]+cm[1,1])\n    fpr = (cm[0,1])/(cm[0,0]+cm[0,1])\n    tpr = (cm[1,1])/(cm[1,0]+cm[1,1])\n    fpr_roc, tpr_roc, auc_thresholds = roc_curve(actual_values, pred_values)\n    auc_value = (auc(fpr_roc, tpr_roc))# AUC of ROC\n    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')\n    plt.ylabel(\"Actual Values\")\n    plt.xlabel(\"Predicted Values\")\n    print(\"Classification accuracy = {:.1%}\".format(accuracy))\n    print(\"Precision = {:.1%}\".format(precision))\n    print(\"TPR or Recall = {:.1%}\".format(tpr))\n    print(\"FNR = {:.1%}\".format(fnr))\n    print(\"FPR = {:.1%}\".format(fpr))\n    print(\"ROC-AUC = {:.1%}\".format(auc_value))\n\n\nconfusion_matrix_data(test,test.Outcome,logit_model_diabetes,0.3)\n\nClassification accuracy = 80.5%\nPrecision = 70.1%\nTPR or Recall = 88.5%\nFNR = 11.5%\nFPR = 24.7%\nROC-AUC = 90.1%"
  },
  {
    "objectID": "Lec8_ModelSelection_BestSubset_FwdBwd_stepwise.html",
    "href": "Lec8_ModelSelection_BestSubset_FwdBwd_stepwise.html",
    "title": "8  Best subset and Stepwise selection",
    "section": "",
    "text": "Read section 6.1 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately."
  },
  {
    "objectID": "Lec8_ModelSelection_BestSubset_FwdBwd_stepwise.html#best-subsets-selection",
    "href": "Lec8_ModelSelection_BestSubset_FwdBwd_stepwise.html#best-subsets-selection",
    "title": "8  Best subset and Stepwise selection",
    "section": "8.1 Best subsets selection",
    "text": "8.1 Best subsets selection\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\nimport time\n\n\ntrainf = pd.read_csv('./Datasets/house_feature_train.csv')\ntrainp = pd.read_csv('./Datasets/house_price_train.csv')\ntestf = pd.read_csv('./Datasets/house_feature_test.csv')\ntestp = pd.read_csv('./Datasets/house_price_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      house_id\n      house_age\n      distance_MRT\n      number_convenience_stores\n      latitude\n      longitude\n      house_price\n    \n  \n  \n    \n      0\n      210\n      5.2\n      390.5684\n      5\n      24.97937\n      121.54245\n      2724.84\n    \n    \n      1\n      190\n      35.3\n      616.5735\n      8\n      24.97945\n      121.53642\n      1789.29\n    \n    \n      2\n      328\n      15.9\n      1497.7130\n      3\n      24.97003\n      121.51696\n      556.96\n    \n    \n      3\n      5\n      7.1\n      2175.0300\n      3\n      24.96305\n      121.51254\n      1030.41\n    \n    \n      4\n      412\n      8.1\n      104.8101\n      5\n      24.96674\n      121.54067\n      2756.25\n    \n  \n\n\n\n\nDevelop a model to predict house price using the rest of the columns as predictors (except house_id).\n\n#Model with log house price as the response and the remaining variables as predictors\nmodel = sm.ols('np.log(house_price)~house_age+distance_MRT+number_convenience_stores+latitude+\\\nlongitude', data = train).fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:    np.log(house_price)   R-squared:             0.772\n\n\n  Model:                    OLS           Adj. R-squared:        0.767\n\n\n  Method:              Least Squares      F-statistic:           181.8\n\n\n  Date:              Thu, 16 Feb 2023     Prob (F-statistic): 4.47e-84\n\n\n  Time:                  18:31:07         Log-Likelihood:      -118.47\n\n\n  No. Observations:          275          AIC:                   248.9\n\n\n  Df Residuals:              269          BIC:                   270.6\n\n\n  Df Model:                    5                                      \n\n\n  Covariance Type:       nonrobust                                    \n\n\n\n\n                               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                  -482.9401   312.000    -1.548  0.123 -1097.212   131.332\n\n\n  house_age                    -0.0131     0.002    -6.437  0.000    -0.017    -0.009\n\n\n  distance_MRT                 -0.0003  3.69e-05    -8.318  0.000    -0.000    -0.000\n\n\n  number_convenience_stores     0.0598     0.010     6.247  0.000     0.041     0.079\n\n\n  latitude                     18.7044     2.353     7.951  0.000    14.073    23.336\n\n\n  longitude                     0.1923     2.465     0.078  0.938    -4.660     5.045\n\n\n\n\n  Omnibus:        4.413   Durbin-Watson:         2.260\n\n\n  Prob(Omnibus):  0.110   Jarque-Bera (JB):      5.515\n\n\n  Skew:           0.077   Prob(JB):             0.0634\n\n\n  Kurtosis:       3.677   Cond. No.           2.28e+07\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.28e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nFind the best subset of predictors that can predict house price in a linear regression model.\n\n#Creating a set of predictors from which we need to find the best subset of predictors\nX = train[['house_age','number_convenience_stores','latitude', 'longitude','distance_MRT']]\n\n\n8.1.1 Best subset selection algorithm\nNow, we will implement the algorithm of finding the best subset of predictors from amongst all sets of predictors.\n\n#Function to develop a model based on all predictors in predictor_subset\ndef processSubset(predictor_subset):\n    # Fit model on feature_set and calculate R-squared\n    model = sm.ols('np.log(house_price)~' + '+'.join(predictor_subset),data = train).fit()\n    Rsquared = model.rsquared\n    return {\"model\":model, \"Rsquared\":Rsquared}\n\n\n#Function to select the best model amongst all models with 'k' predictors\ndef getBest_model(k):\n    tic = time.time()\n    results = []\n    for combo in itertools.combinations(X.columns, k):\n        results.append(processSubset((list(combo))))\n\n    # Wrap everything up in a dataframe\n    models = pd.DataFrame(results)\n\n    # Choose the model with the highest RSS\n    best_model = models.loc[models['Rsquared'].argmax()]\n    \n    toc = time.time()\n    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n    return best_model\n\n\n#Function to select the best model amongst the best models for 'k' predictors, where k = 1,2,3,..\nmodels_best = pd.DataFrame(columns=[\"Rsquared\", \"model\"])\n\ntic = time.time()\nfor i in range(1,1+X.shape[1]):\n    models_best.loc[i] = getBest_model(i)\n\ntoc = time.time()\nprint(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n\nProcessed 5 models on 1 predictors in 0.02393651008605957 seconds.\nProcessed 10 models on 2 predictors in 0.04688239097595215 seconds.\nProcessed 10 models on 3 predictors in 0.04986691474914551 seconds.\nProcessed 5 models on 4 predictors in 0.029920578002929688 seconds.\nProcessed 1 models on 5 predictors in 0.008975982666015625 seconds.\nTotal elapsed time: 0.17253828048706055 seconds.\n\n\n\ndef best_sub_plots():\n    plt.figure(figsize=(20,10))\n    plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n\n    # Set up a 2x2 grid so we can look at 4 plots at once\n    plt.subplot(2, 2, 1)\n\n    # We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n    # The argmax() function can be used to identify the location of the maximum point of a vector\n    plt.plot(models_best[\"Rsquared\"])\n    plt.xlabel('# Predictors')\n    plt.ylabel('Rsquared')\n\n    # We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n    # The argmax() function can be used to identify the location of the maximum point of a vector\n\n    rsquared_adj = models_best.apply(lambda row: row[1].rsquared_adj, axis=1)\n\n    plt.subplot(2, 2, 2)\n    plt.plot(rsquared_adj)\n    plt.plot(1+rsquared_adj.argmax(), rsquared_adj.max(), \"or\")\n    plt.xlabel('# Predictors')\n    plt.ylabel('adjusted rsquared')\n\n    # We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n    aic = models_best.apply(lambda row: row[1].aic, axis=1)\n\n    plt.subplot(2, 2, 3)\n    plt.plot(aic)\n    plt.plot(1+aic.argmin(), aic.min(), \"or\")\n    plt.xlabel('# Predictors')\n    plt.ylabel('AIC')\n\n    bic = models_best.apply(lambda row: row[1].bic, axis=1)\n\n    plt.subplot(2, 2, 4)\n    plt.plot(bic)\n    plt.plot(1+bic.argmin(), bic.min(), \"or\")\n    plt.xlabel('# Predictors')\n    plt.ylabel('BIC')\nbest_sub_plots()\n\n\n\n\nThe model with 4 predictors is the best model, according to all 3 criteria - Adjusted R-squared, AIC and BIC.\nNote that we have not considered the null model (i.e., the model with only the intercept and no predictors) explicitly in the best subsets algorithm. However, the null model is considered when selecting the best model. The R-squared and the adjusted R-squared for the null model is 0. So, if the adjusted R-squared of all the models with at least one predictor is negative, then the null model will be the best model.\n\nbest_subset_model = models_best.loc[4,'model']\nmodels_best.loc[4,'model'].summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:    np.log(house_price)   R-squared:             0.772\n\n\n  Model:                    OLS           Adj. R-squared:        0.768\n\n\n  Method:              Least Squares      F-statistic:           228.0\n\n\n  Date:              Thu, 16 Feb 2023     Prob (F-statistic): 2.79e-85\n\n\n  Time:                  19:51:50         Log-Likelihood:      -118.47\n\n\n  No. Observations:          275          AIC:                   246.9\n\n\n  Df Residuals:              270          BIC:                   265.0\n\n\n  Df Model:                    4                                      \n\n\n  Covariance Type:       nonrobust                                    \n\n\n\n\n                               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                  -459.0262    58.231    -7.883  0.000  -573.671  -344.381\n\n\n  house_age                    -0.0131     0.002    -6.451  0.000    -0.017    -0.009\n\n\n  number_convenience_stores     0.0597     0.010     6.271  0.000     0.041     0.078\n\n\n  latitude                     18.6828     2.332     8.012  0.000    14.092    23.274\n\n\n  distance_MRT                 -0.0003  2.53e-05   -12.221  0.000    -0.000    -0.000\n\n\n\n\n  Omnibus:        4.422   Durbin-Watson:         2.261\n\n\n  Prob(Omnibus):  0.110   Jarque-Bera (JB):      5.555\n\n\n  Skew:           0.073   Prob(JB):             0.0622\n\n\n  Kurtosis:       3.681   Cond. No.           4.25e+06\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 4.25e+06. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Finding the RMSE of the model selected using the best subset selection procedure\npred_price = np.exp(best_subset_model.predict(test))\nnp.sqrt(((pred_price - test.house_price)**2).mean())\n\n403.4635674362065\n\n\n\n#RMSE of the model using all the predictors\nmodel = sm.ols('np.log(house_price)~' + '+'.join(X.columns),data = train).fit()\npred_price = np.exp(model.predict(test))\nnp.sqrt(((pred_price - test.house_price)**2).mean())\n\n403.8409399214197\n\n\nThe RMSE of the best subset model is similar to the RMSE of the model with all the predictors. This is because longitude varies only in [121.47, 121.57]. The coefficient of longitude is 0.1923 in the model with all the predictors. So, the change in the response due to longitude is in [23.36, 23.38 ]. This change in the response due to longitude is almost a constant, and hence is adjusted in the intercept of the model without longitude. Note the intercept of the model without longitude is 23.91 more than the intercept of the model with longitude.\n\n[0.1923*train.longitude.min(),0.1923*train.longitude.max()]\n\n[23.359359818999998, 23.377193721]\n\n\n\n\n8.1.2 Including interactions for best subset selection\nLet’s perform best subset selection including all the predictors and their 2-factor interactions\n\n#Creating a dataframe with all the predictors\nX = train[['house_age', 'distance_MRT', 'number_convenience_stores','latitude','longitude']]\n#Since 'X' will change when we include interactions, we need a backup containing all individual predictors\nX_backup = train[['house_age', 'distance_MRT', 'number_convenience_stores','latitude','longitude']]\n\n\n#Including 2-factor interactions of predictors in train and 'X'. Note that we need train to develop the model, and X to \n#find 'k' variable subsets from amongst all the predictors under consideration\nfor combo in itertools.combinations(X_backup.columns, 2):    \n    train['_'.join(combo)] = train[combo[0]]*train[combo[1]]\n    test['_'.join(combo)] = test[combo[0]]*test[combo[1]]\n    X.loc[:,'_'.join(combo)] = train.loc[:,'_'.join(combo)] \n\n\nmodels_best = pd.DataFrame(columns=[\"Rsquared\", \"model\"])\n\ntic = time.time()\nfor i in range(1,1+X.shape[1]):\n    models_best.loc[i] = getBest_model(i)\n\ntoc = time.time()\nprint(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n\nProcessed 15 models on 1 predictors in 0.07200050354003906 seconds.\nProcessed 105 models on 2 predictors in 0.536522388458252 seconds.\nProcessed 455 models on 3 predictors in 2.6639997959136963 seconds.\nProcessed 1365 models on 4 predictors in 9.176022052764893 seconds.\nProcessed 3003 models on 5 predictors in 24.184194803237915 seconds.\nProcessed 5005 models on 6 predictors in 43.54697918891907 seconds.\nProcessed 6435 models on 7 predictors in 65.83688187599182 seconds.\nProcessed 6435 models on 8 predictors in 78.97277760505676 seconds.\nProcessed 5005 models on 9 predictors in 64.53991365432739 seconds.\nProcessed 3003 models on 10 predictors in 38.39328980445862 seconds.\nProcessed 1365 models on 11 predictors in 18.715795755386353 seconds.\nProcessed 455 models on 12 predictors in 6.93279504776001 seconds.\nProcessed 105 models on 13 predictors in 1.6240253448486328 seconds.\nProcessed 15 models on 14 predictors in 0.256000280380249 seconds.\nProcessed 1 models on 15 predictors in 0.024001121520996094 seconds.\nTotal elapsed time: 356.2638840675354 seconds.\n\n\n\nbest_sub_plots()\n\n\n\n\nThe model with 7 predictors is the best model based on the BIC criterion, and very close to the best model based on the AIC and Adjusted R-squared criteria. Let us select the model with 7 predictors.\n\nbest_interaction_model = models_best['model'][7]\nbest_interaction_model.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:    np.log(house_price)   R-squared:             0.818\n\n\n  Model:                    OLS           Adj. R-squared:        0.814\n\n\n  Method:              Least Squares      F-statistic:           171.7\n\n\n  Date:              Thu, 16 Feb 2023     Prob (F-statistic): 5.29e-95\n\n\n  Time:                  20:17:02         Log-Likelihood:      -87.046\n\n\n  No. Observations:          275          AIC:                   190.1\n\n\n  Df Residuals:              267          BIC:                   219.0\n\n\n  Df Model:                    7                                      \n\n\n  Covariance Type:       nonrobust                                    \n\n\n\n\n                                            coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                              -1316.6156   135.152    -9.742  0.000 -1582.716 -1050.515\n\n\n  distance_MRT                               0.2424     0.044     5.539  0.000     0.156     0.329\n\n\n  number_convenience_stores                152.0179    23.356     6.509  0.000   106.033   198.003\n\n\n  latitude                                  53.0284     5.413     9.797  0.000    42.371    63.686\n\n\n  house_age_longitude                       -0.0001  1.51e-05    -6.842  0.000    -0.000 -7.36e-05\n\n\n  distance_MRT_number_convenience_stores -5.691e-05  1.19e-05    -4.763  0.000 -8.04e-05 -3.34e-05\n\n\n  distance_MRT_latitude                     -0.0097     0.002    -5.544  0.000    -0.013    -0.006\n\n\n  number_convenience_stores_latitude        -6.0847     0.935    -6.506  0.000    -7.926    -4.243\n\n\n\n\n  Omnibus:        5.350   Durbin-Watson:         2.136\n\n\n  Prob(Omnibus):  0.069   Jarque-Bera (JB):      7.524\n\n\n  Skew:           0.045   Prob(JB):             0.0232\n\n\n  Kurtosis:       3.805   Cond. No.           2.78e+08\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.78e+08. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that only 3 of the 10 two factor interactions are included in the best subset model, and the predictor longitude has been dropped.\n\n#Finding the RMSE of the model selected using the best subset selection procedure, where the predictors\n#include 2-factor interactions\npred_price = np.exp(best_interaction_model.predict(test))\nnp.sqrt(((pred_price - test.house_price)**2).mean())\n\n346.4100962681362\n\n\n\n#Model with the predictors and all their 2-factor interactions\nmodel = sm.ols('np.log(house_price)~' + '+'.join(X.columns),data = train).fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:    np.log(house_price)   R-squared:             0.825\n\n\n  Model:                    OLS           Adj. R-squared:        0.814\n\n\n  Method:              Least Squares      F-statistic:           81.14\n\n\n  Date:              Thu, 16 Feb 2023     Prob (F-statistic): 1.33e-88\n\n\n  Time:                  20:13:01         Log-Likelihood:      -82.228\n\n\n  No. Observations:          275          AIC:                   196.5\n\n\n  Df Residuals:              259          BIC:                   254.3\n\n\n  Df Model:                   15                                      \n\n\n  Covariance Type:       nonrobust                                    \n\n\n\n\n                                            coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                               7.455e+05  1.03e+06     0.725  0.469 -1.28e+06  2.77e+06\n\n\n  house_age                                 83.1021    40.562     2.049  0.041     3.228   162.976\n\n\n  distance_MRT                               0.1391     0.174     0.798  0.425    -0.204     0.482\n\n\n  number_convenience_stores                252.5261   212.276     1.190  0.235  -165.481   670.533\n\n\n  latitude                               -2.992e+04  4.12e+04    -0.727  0.468 -1.11e+05  5.12e+04\n\n\n  longitude                              -6144.1732  8454.331    -0.727  0.468 -2.28e+04  1.05e+04\n\n\n  house_age_distance_MRT                 -2.904e-06  4.44e-06    -0.654  0.514 -1.16e-05  5.84e-06\n\n\n  house_age_number_convenience_stores        0.0011     0.001     1.409  0.160    -0.000     0.003\n\n\n  house_age_latitude                         0.2119     0.261     0.811  0.418    -0.303     0.726\n\n\n  house_age_longitude                       -0.7274     0.330    -2.207  0.028    -1.376    -0.078\n\n\n  distance_MRT_number_convenience_stores -6.192e-05  1.99e-05    -3.115  0.002    -0.000 -2.28e-05\n\n\n  distance_MRT_latitude                     -0.0082     0.003    -2.387  0.018    -0.015    -0.001\n\n\n  distance_MRT_longitude                     0.0005     0.001     0.417  0.677    -0.002     0.003\n\n\n  number_convenience_stores_latitude        -6.4014     1.113    -5.753  0.000    -8.592    -4.210\n\n\n  number_convenience_stores_longitude       -0.7620     1.700    -0.448  0.654    -4.109     2.585\n\n\n  latitude_longitude                       246.5995   338.773     0.728  0.467  -420.500   913.699\n\n\n\n\n  Omnibus:        3.911   Durbin-Watson:         2.134\n\n\n  Prob(Omnibus):  0.142   Jarque-Bera (JB):      4.552\n\n\n  Skew:           0.090   Prob(JB):              0.103\n\n\n  Kurtosis:       3.604   Cond. No.           1.05e+13\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 1.07e-13. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\n\n\n\n# RMSE of the model using all the predictors and their 2-factor interactions\npred_price = np.exp(model.predict(test))\nnp.sqrt(((pred_price - test.house_price)**2).mean())\n\n360.40099598821615\n\n\nThe best subset model seems to be slightly better than the model with all the predictors, based on the RMSE on test data."
  },
  {
    "objectID": "Lec8_ModelSelection_BestSubset_FwdBwd_stepwise.html#stepwise-selection",
    "href": "Lec8_ModelSelection_BestSubset_FwdBwd_stepwise.html#stepwise-selection",
    "title": "8  Best subset and Stepwise selection",
    "section": "8.2 Stepwise selection",
    "text": "8.2 Stepwise selection\nBest subset selection cannot be used in case of even a slightly large number of predictors. In the previous example, we had 15 predictors. The number of models that we developed to find the best subset of predictors from the set of 15 predictors was \\(2^{15} \\approx 32,000\\). In case of 20 predictors, the number of models to use the best subset selection approach will be \\(2^{20} \\approx 1\\) million, which is computationally too expensive. Due to this limitation of the best subsets selection method, we will use stepwise regression, which explores a far more restricted set of models, and thus is an attractive alternative to the best subset selection method."
  },
  {
    "objectID": "Lec8_ModelSelection_BestSubset_FwdBwd_stepwise.html#forward-stepwise-selection",
    "href": "Lec8_ModelSelection_BestSubset_FwdBwd_stepwise.html#forward-stepwise-selection",
    "title": "8  Best subset and Stepwise selection",
    "section": "8.3 Forward stepwise selection",
    "text": "8.3 Forward stepwise selection\nSource - Page 229: “Forward stepwise selection is a computationally efficient alternative to best subset selection. While the best subset selection procedure considers all \\(2^p\\) possible models containing subsets of the \\(p\\) predictors, forward stepwise considers a much smaller set of models. Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.”\n\n#Function to find the best predictor out of p-k predictors and add it to the model containing the k predictors\ndef forward(predictors):\n\n    # Pull out predictors we still need to process\n    remaining_predictors = [p for p in X.columns if p not in predictors]\n    \n    tic = time.time()\n    \n    results = []\n    \n    for p in remaining_predictors:\n        results.append(processSubset(predictors+[p]))\n    \n    # Wrap everything up in a nice dataframe\n    models = pd.DataFrame(results)\n    \n    # Choose the model with the highest RSS\n    best_model = models.loc[models['Rsquared'].argmax()]\n    \n    toc = time.time()\n    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n    \n    # Return the best model, along with some other useful information about the model\n    return best_model\n\n\ndef forward_selection():\n    models_best = pd.DataFrame(columns=[\"Rsquared\", \"model\"])\n\n    tic = time.time()\n    predictors = []\n\n    for i in range(1,len(X.columns)+1):    \n        models_best.loc[i] = forward(predictors)\n        predictors = list(models_best.loc[i][\"model\"].params.index[1:])\n\n    toc = time.time()\n    print(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n    return models_best\n\n\nmodels_best = forward_selection()\n\nProcessed  15 models on 1 predictors in 0.06280803680419922 seconds.\nProcessed  14 models on 2 predictors in 0.054885149002075195 seconds.\nProcessed  13 models on 3 predictors in 0.05983686447143555 seconds.\nProcessed  12 models on 4 predictors in 0.06781768798828125 seconds.\nProcessed  11 models on 5 predictors in 0.07380270957946777 seconds.\nProcessed  10 models on 6 predictors in 0.07380390167236328 seconds.\nProcessed  9 models on 7 predictors in 0.06981182098388672 seconds.\nProcessed  8 models on 8 predictors in 0.07480072975158691 seconds.\nProcessed  7 models on 9 predictors in 0.0718071460723877 seconds.\nProcessed  6 models on 10 predictors in 0.06380081176757812 seconds.\nProcessed  5 models on 11 predictors in 0.054854631423950195 seconds.\nProcessed  4 models on 12 predictors in 0.05385565757751465 seconds.\nProcessed  3 models on 13 predictors in 0.04188799858093262 seconds.\nProcessed  2 models on 14 predictors in 0.027925491333007812 seconds.\nProcessed  1 models on 15 predictors in 0.016956090927124023 seconds.\nTotal elapsed time: 0.9055600166320801 seconds.\n\n\n\nbest_sub_plots()\n\n\n\n\nThe model with 8 predictors is the best model based on the BIC criterion, and very close to the best model based on the AIC and Adjusted R-squared criteria. Let us select the model with 8 predictors.\n\nbest_fwd_reg_model = models_best['model'][8]\nbest_fwd_reg_model.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:    np.log(house_price)   R-squared:             0.820\n\n\n  Model:                    OLS           Adj. R-squared:        0.815\n\n\n  Method:              Least Squares      F-statistic:           151.6\n\n\n  Date:              Thu, 16 Feb 2023     Prob (F-statistic): 1.91e-94\n\n\n  Time:                  20:35:14         Log-Likelihood:      -85.667\n\n\n  No. Observations:          275          AIC:                   189.3\n\n\n  Df Residuals:              266          BIC:                   221.9\n\n\n  Df Model:                    8                                      \n\n\n  Covariance Type:       nonrobust                                    \n\n\n\n\n                                            coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                              -1365.5045   154.113    -8.860  0.000 -1668.942 -1062.067\n\n\n  distance_MRT_longitude                     0.0021     0.000     5.062  0.000     0.001     0.003\n\n\n  latitude                                  54.9844     6.171     8.909  0.000    42.833    67.136\n\n\n  house_age_longitude                       -0.3240     0.119    -2.725  0.007    -0.558    -0.090\n\n\n  number_convenience_stores_longitude        1.3242     0.212     6.246  0.000     0.907     1.742\n\n\n  distance_MRT_number_convenience_stores -4.805e-05  1.21e-05    -3.973  0.000 -7.19e-05 -2.42e-05\n\n\n  number_convenience_stores_latitude        -6.4419     1.032    -6.243  0.000    -8.473    -4.410\n\n\n  distance_MRT_latitude                     -0.0101     0.002    -5.067  0.000    -0.014    -0.006\n\n\n  house_age                                 39.3625    14.450     2.724  0.007    10.911    67.814\n\n\n\n\n  Omnibus:        5.017   Durbin-Watson:         2.176\n\n\n  Prob(Omnibus):  0.081   Jarque-Bera (JB):      6.923\n\n\n  Skew:           0.022   Prob(JB):             0.0314\n\n\n  Kurtosis:       3.776   Cond. No.           1.56e+09\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.56e+09. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Finding the RMSE of the model selected using the forward selection procedure, where the predictors\n#include 2-factor interactions\npred_price = np.exp(best_fwd_reg_model.predict(test))\nnp.sqrt(((pred_price - test.house_price)**2).mean())\n\n364.2004089481364\n\n\nWe get a different model than what we got with the best subsets selection method. However, we got it in 0.9 seconds, instead of 6 minutes taken by the best subset selection algorithm. Note that this model has a higher RMSE as compared to the model obtained with the best subset selection procedure, which is expected. However, the RMSE is even slightly higher than the model that includes all the two factor interactions. This may be due to the following reasons:\n\nThis may be due to chance - the test data set may be biased.\nThe stepwise variable selection algorithms are greedy algorithms, and certainly don’t guarantee the best model, or even a model better than the one without variable selection. However, in general, they are likely to provide a better model than the base model that includes all the predictors, especially if there are several predictors that are not associated with the response.\nFor metrics such as adjusted R-squared, the adjustment is not directly tied to the model being more accurate on test data. The adjustment only ensures that the adjusted R-squared increases if the added predictor sufficiently reduces the RSS (Residual sum of squares) on training data.\nAIC is an unbiased estimate of test error. However, AIC will have some variance as we are using sample data for training the model."
  },
  {
    "objectID": "Lec8_ModelSelection_BestSubset_FwdBwd_stepwise.html#backward-stepwise-selection",
    "href": "Lec8_ModelSelection_BestSubset_FwdBwd_stepwise.html#backward-stepwise-selection",
    "title": "8  Best subset and Stepwise selection",
    "section": "8.4 Backward Stepwise Selection",
    "text": "8.4 Backward Stepwise Selection\nSource - Page 231: “Like forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection. However, unlike forward stepwise selection, it begins with the full least squares model containing all \\(p\\) predictors, and then iteratively removes the least useful predictor, one-at-a-time.”\nLet us try the backward selection procedure on the model with 15 predictors - house_age, distance_MRT, number_convenience_stores, latitude, longitude and their 2-factor interactions.\n\ndef backward(predictors):\n    \n    tic = time.time()\n    \n    results = []\n    \n    for combo in itertools.combinations(predictors, len(predictors)-1):\n        results.append(processSubset(combo))\n    \n    # Wrap everything up in a nice dataframe\n    models = pd.DataFrame(results)\n    \n    # Choose the model with the highest RSS\n    best_model = models.loc[models['Rsquared'].argmax()]\n    \n    toc = time.time()\n    print(\"Processed \", models.shape[0], \"models on\", len(predictors)-1, \"predictors in\", (toc-tic), \"seconds.\")\n    \n    # Return the best model, along with some other useful information about the model\n    return best_model\n\n\ndef backward_selection():\n    models_best = pd.DataFrame(columns=[\"Rsquared\", \"model\"], index = range(1,len(X.columns)))\n\n    tic = time.time()\n    predictors = X.columns\n    models_best.loc[len(predictors)] = processSubset(predictors)\n    \n    while(len(predictors) > 1):  \n        models_best.loc[len(predictors)-1] = backward(predictors)\n        predictors = models_best.loc[len(predictors)-1][\"model\"].params.index[1:]\n\n    toc = time.time()\n    print(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n    return models_best\n\n\nmodels_best = backward_selection()\n\nProcessed  15 models on 14 predictors in 0.24733757972717285 seconds.\nProcessed  14 models on 13 predictors in 0.1765275001525879 seconds.\nProcessed  13 models on 12 predictors in 0.16356277465820312 seconds.\nProcessed  12 models on 11 predictors in 0.13364267349243164 seconds.\nProcessed  11 models on 10 predictors in 0.11968183517456055 seconds.\nProcessed  10 models on 9 predictors in 0.09571337699890137 seconds.\nProcessed  9 models on 8 predictors in 0.08377647399902344 seconds.\nProcessed  8 models on 7 predictors in 0.06981253623962402 seconds.\nProcessed  7 models on 6 predictors in 0.048902273178100586 seconds.\nProcessed  6 models on 5 predictors in 0.04088902473449707 seconds.\nProcessed  5 models on 4 predictors in 0.029920101165771484 seconds.\nProcessed  4 models on 3 predictors in 0.020944595336914062 seconds.\nProcessed  3 models on 2 predictors in 0.013962507247924805 seconds.\nProcessed  2 models on 1 predictors in 0.007978677749633789 seconds.\nTotal elapsed time: 1.286529779434204 seconds.\n\n\n\nbest_sub_plots()\n\n\n\n\n\nbest_bwd_reg_model = models_best['model'][8]\nbest_bwd_reg_model.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:    np.log(house_price)   R-squared:             0.820\n\n\n  Model:                    OLS           Adj. R-squared:        0.815\n\n\n  Method:              Least Squares      F-statistic:           151.5\n\n\n  Date:              Thu, 16 Feb 2023     Prob (F-statistic): 2.00e-94\n\n\n  Time:                  20:40:43         Log-Likelihood:      -85.714\n\n\n  No. Observations:          275          AIC:                   189.4\n\n\n  Df Residuals:              266          BIC:                   222.0\n\n\n  Df Model:                    8                                      \n\n\n  Covariance Type:       nonrobust                                    \n\n\n\n\n                                            coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                              -1317.5329   145.605    -9.049  0.000 -1604.218 -1030.847\n\n\n  house_age                                 57.3124    14.583     3.930  0.000    28.600    86.025\n\n\n  distance_MRT                               0.2365     0.047     5.044  0.000     0.144     0.329\n\n\n  number_convenience_stores                154.8362    24.984     6.197  0.000   105.644   204.029\n\n\n  house_age_longitude                       -0.4717     0.120    -3.931  0.000    -0.708    -0.235\n\n\n  distance_MRT_number_convenience_stores -4.789e-05  1.24e-05    -3.869  0.000 -7.23e-05 -2.35e-05\n\n\n  distance_MRT_latitude                     -0.0095     0.002    -5.050  0.000    -0.013    -0.006\n\n\n  number_convenience_stores_latitude        -6.1977     1.001    -6.194  0.000    -8.168    -4.228\n\n\n  latitude_longitude                         0.4366     0.048     9.100  0.000     0.342     0.531\n\n\n\n\n  Omnibus:        4.945   Durbin-Watson:         2.137\n\n\n  Prob(Omnibus):  0.084   Jarque-Bera (JB):      6.228\n\n\n  Skew:           0.110   Prob(JB):             0.0444\n\n\n  Kurtosis:       3.703   Cond. No.           3.01e+08\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.01e+08. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nWe get a slightly different model than what we got with the best subsets selection method and the forward selection method. As in forward selection, we got it relatively very quickly (in 1.28 seconds), instead of 6 minutes taken by the best subset selection algorithm.\n\n#Finding the RMSE of the model selected using the backward selection procedure, where the predictors\n#include 2-factor interactions\npred_price = np.exp(best_bwd_reg_model.predict(test))\nnp.sqrt(((pred_price - test.house_price)**2).mean())\n\n363.63365786020694\n\n\nNote that we have not considered the null model (i.e., the model with only the intercept and no predictors) explicitly in the forward and backward stepwise algorithms. However, the null model is considered when selecting the best model. The R-squared and the adjusted R-squared for the null model is 0. So, if the adjusted R-squared of all the models with at least one predictor is negative, then the null model will be the best model."
  },
  {
    "objectID": "Lec9_RidgeRegression_Lasso.html",
    "href": "Lec9_RidgeRegression_Lasso.html",
    "title": "9  Ridge regression and Lasso",
    "section": "",
    "text": "Read section 6.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately."
  },
  {
    "objectID": "Lec9_RidgeRegression_Lasso.html#ridge-regression",
    "href": "Lec9_RidgeRegression_Lasso.html#ridge-regression",
    "title": "9  Ridge regression and Lasso",
    "section": "9.1 Ridge regression",
    "text": "9.1 Ridge regression\nLet us develop a ridge regression model to predict house price based on the five house features.\n\n#Taking the log transform of house_price as house prices have a right-skewed distribution\ny = np.log(train.house_price)\n\n\n9.1.1 Standardizing the predictors\n\n#Standardizing predictors so that each of them have zero mean and unit variance\n\n#Filtering all predictors\nX = train.iloc[:,1:6]; \n\n#Defining a scaler object\nscaler = StandardScaler()\n\n#The scaler object will contain the mean and variance of each column (predictor) of X. \n#These values will be useful to scale test data based on the same mean and variance as obtained on train data\nscaler.fit(X)\n\n#Using the scaler object (or the values of mean and variance stored in it) to standardize X (or train data)\nXstd = scaler.transform(X)\n\n\n\n9.1.2 Optimizing the tuning parameter λ\n\n#The tuning parameter lambda is referred as alpha in sklearn\n\n#Creating a range of values of the tuning parameter to visualize the ridge regression coefficients\n#for different values of the tuning parameter\nalphas = 10**np.linspace(10,-2,200)*0.5\n\n\n#Finding the ridge regression coefficients for increasing values of the tuning parameter\ncoefs = []\nfor a in alphas:        \n    ridge = Ridge(alpha = a)\n    ridge.fit(Xstd, y)\n    coefs.append(ridge.coef_)\n\n\n#Visualizing the shrinkage in ridge regression coefficients with increasing values of the tuning parameter lambda\nplt.xlabel('xlabel', fontsize=18)\nplt.ylabel('ylabel', fontsize=18)\nplt.plot(alphas, coefs)\nplt.xscale('log')\nplt.xlabel('$\\lambda$')\nplt.ylabel('Standardized coefficient')\nplt.legend(train.columns[1:6]);\n\n\n\n\n\n#Let us use cross validation to find the optimal value of the tuning parameter - lambda\n#For the optimal lambda, the cross validation error will be the least\n\n#Note that we are reducing the range of alpha so as to better visualize the minimum\nalphas = 10**np.linspace(1.5,-3,200)*0.5\nridgecv = RidgeCV(alphas = alphas,store_cv_values=True)\nridgecv.fit(Xstd, y)\n\n#Optimal value of the tuning parameter - lambda\nridgecv.alpha_\n\n3.87629874431473\n\n\n\n#Visualizing the LOOCV (leave one out cross validatation error vs lambda)\nplt.xlabel('xlabel', fontsize=18)\nplt.ylabel('ylabel', fontsize=18)\nplt.plot(ridgecv.alphas,ridgecv.cv_values_.sum(axis=0))\nplt.plot([ridgecv.alpha_,ridgecv.alpha_],[40,40.2],':')\nplt.xlabel('$\\lambda$')\nplt.ylabel('Cross-validation error')\n\nText(0, 0.5, 'Cross-validation error')\n\n\n\n\n\nNote that the cross validation error is minimum at the optimal value of the tuning parameter.\n\n#Visualizing the shrinkage in ridge regression coefficients with increasing values of the tuning parameter lambda\nalphas = 10**np.linspace(10,-2,200)*0.5\nplt.xlabel('xlabel', fontsize=18)\nplt.ylabel('ylabel', fontsize=18)\nplt.plot(alphas, coefs)\nplt.plot([ridgecv.alpha_,ridgecv.alpha_],[-0.4,0.2],':')\nplt.xscale('log')\nplt.xlabel('$\\lambda$')\nplt.ylabel('Standardized coefficient')\nplt.legend(train.columns[1:6]);\n\n\n\n\n\n\n9.1.3 RMSE on test data\n\n#Test dataset\nXtest = test.iloc[:,1:6]\n\n#Standardizing test data\nXtest_std = scaler.transform(Xtest)\n\n\n#Using the developed ridge regression model to predict on test data\nridge = Ridge(alpha = ridgecv.alpha_)\nridge.fit(Xstd, y)\npred=ridge.predict(Xtest_std)\n\n\n#RMSE on test data\nnp.sqrt(((np.exp(pred)-test.house_price)**2).mean())\n\n405.6227485138042\n\n\nNote that the RMSE is similar to the one obtained using least squares regression on all the five predictors. This is because the coefficients were required to shrink very slightly for the best ridge regression fit. This may happen when we have a low number of predictors, where most of them are significant. Ridge regression is likely to perform better than least squares in case of a large number of predictors, where an OLS model will be prone to overfitting.\n\n\n9.1.4 Model coefficients & \\(R\\)-squared\n\n#Checking the coefficients of the ridge regression model\nridge.coef_\n\narray([-0.1444778 , -0.36856553,  0.17986479,  0.22566444,  0.01413125])\n\n\nNote that none of the coefficients are shrunk to zero. The coefficient of longitude is smaller than the rest, but not zero.\n\n#R-squared on train data for the ridge regression model\nr2_score(ridge.predict(Xstd),y)\n\n0.6994484432136066\n\n\n\n#R-squared on test data for the ridge regression model\nr2_score(pred,np.log(test.house_price))\n\n0.7573027646359806"
  },
  {
    "objectID": "Lec9_RidgeRegression_Lasso.html#lasso",
    "href": "Lec9_RidgeRegression_Lasso.html#lasso",
    "title": "9  Ridge regression and Lasso",
    "section": "9.2 Lasso",
    "text": "9.2 Lasso\nLet us develop a lasso model to predict house price based on the five house features.\n\n9.2.1 Standardizing the predictors\nWe have already standardized the predictors in the previous section. The standardized predictors are the NumPy array object Xstd.\n\n\n9.2.2 Optimizing the tuning parameter λ\n\n#Creating a range of values of the tuning parameter to visualize the lasso coefficients\n#for different values of the tuning parameter\nalphas = 10**np.linspace(10,-2,100)*0.1\n\n\n#Finding the lasso coefficients for increasing values of the tuning parameter\nlasso = Lasso(max_iter = 10000)\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(Xstd, y)\n    coefs.append(lasso.coef_)\n\n\n#Visualizing the shrinkage in lasso coefficients with increasing values of the tuning parameter lambda\nplt.xlabel('xlabel', fontsize=18)\nplt.ylabel('ylabel', fontsize=18)\nplt.plot(alphas, coefs)\nplt.xscale('log')\nplt.xlabel('$\\lambda$')\nplt.ylabel('Standardized coefficient')\nplt.legend(train.columns[1:6]);\n\n\n\n\nNote that lasso performs variable selection. For certain values of lambda, some of the predictor coefficients are zero, while others are non-zero. This is different than ridge regression, which only shrinks the coefficients, but doesn’t do variable selection.\n\n#Let us use cross validation to find the optimal value of the tuning parameter - lambda\n#For the optimal lambda, the cross validation error will be the least\n\n#Note that we are reducing the range of alpha so as to better visualize the minimum\nalphas = 10**np.linspace(-1,-5,200)*0.5\nlassocv = LassoCV(alphas = alphas, cv = 10, max_iter = 100000)\nlassocv.fit(Xstd, y)\n\n#Optimal value of the tuning parameter - lamda\nlassocv.alpha_\n\n0.009020932046960358\n\n\n\n#Visualizing the LOOCV (leave one out cross validatation error vs lambda)\nplt.xlabel('xlabel', fontsize=18)\nplt.ylabel('ylabel', fontsize=18)\nplt.plot(lassocv.alphas_,lassocv.mse_path_.mean(axis=1))\nplt.plot([lassocv.alpha_,lassocv.alpha_],[0.145,0.151],':')\nplt.xlabel('$\\lambda$')\nplt.ylabel('Cross-validation error')\n\nText(0, 0.5, 'Cross-validation error')\n\n\n\n\n\nThe 10-fold cross validation error minimizes at lambda = 0.009.\n\n#Visualizing the shrinkage in lasso coefficients with increasing values of the tuning parameter lambda\nalphas = 10**np.linspace(10,-2,100)*0.1\nplt.xlabel('xlabel', fontsize=18)\nplt.ylabel('ylabel', fontsize=18)\nplt.plot(alphas, coefs)\nplt.xscale('log')\nplt.xlabel('$\\lambda$')\nplt.ylabel('Standardized coefficient')\nplt.legend(train.columns[1:6]);\n\n\n\n\n\n\n9.2.3 RMSE on test data\n\n#Using the developed lasso model to predict on test data\nlasso = Lasso(alpha = lassocv.alpha_)\nlasso.fit(Xstd, y)\npred=lasso.predict(Xtest_std)\n\n\n#RMSE on test data\nnp.sqrt(((np.exp(pred)-test.house_price)**2).mean())\n\n400.77289943396534\n\n\n\n\n9.2.4 Model coefficients & \\(R\\)-squared\n\n#Checking the coefficients of the lasso model\nlasso.coef_\n\narray([-0.13720237, -0.38405197,  0.17252859,  0.21949239,  0.        ])\n\n\nNote that the coefficient of longitude is shrunk to zero. Lasso performs variable selection.\n\n#R-squared on train data for the lasso model\nr2_score(lasso.predict(Xstd),y)\n\n0.692606850601813\n\n\n\n#R-squared on test data for the lasso model\nr2_score(pred,np.log(test.house_price))\n\n0.7524177148260849"
  },
  {
    "objectID": "Assignment 1.html",
    "href": "Assignment 1.html",
    "title": "Appendix A — Assignment A",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Tuesday, 17th January 2023 at 11:59 pm.\nThere is a bonus question worth 5 points.\nFive points are for properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (1 pt); If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g., printouts of the working directory should not be included in the final submission (1 pt).\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt).\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt).\n\n\nThe maximum possible score in the assigment is 95 + 5 (formatting) + 5 (bonus question) = 105 out of 100. There is no partial credit for the bonus question."
  },
  {
    "objectID": "Assignment 1.html#regression-vs-classification-prediction-vs-inference",
    "href": "Assignment 1.html#regression-vs-classification-prediction-vs-inference",
    "title": "Appendix A — Assignment A",
    "section": "A.1 Regression vs Classification; Prediction vs Inference",
    "text": "A.1 Regression vs Classification; Prediction vs Inference\nExplain (1) whether each scenario is a classification or regression problem, and (2) whether we are most interested in inference or prediction. Answers to both parts must be supported by a justification.\n\nA.1.1 \nConsider a company that is interested in conducting a marketing campaign. The goal is to identify individuals who are likely to respond positively to a marketing campaign, based on observations of demographic variables (such as age, gender, income, etc.) measured on each individual.\n(2+2 points)\n\n\nA.1.2 \nConsider that the company mentioned in the previous question is interested in understanding the impact of advertising promotions in different media types on the company sales. For example, the company is interested in the question, ‘how large of an increase in sales is associated with a given increase in radio vis-a-vis TV advertising?’\n(2+2 points)\n\n\nA.1.3 \nConsider a company selling furniture is interested in the finding the association between demographic characterisitcs of customers (such as age, gender, income, etc.) and their probability of purchase of a particular company product.\n(2+2 points)\n\n\nA.1.4 \nWe are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2022. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\n(2+2 points)"
  },
  {
    "objectID": "Assignment 1.html#rmse-vs-mae",
    "href": "Assignment 1.html#rmse-vs-mae",
    "title": "Appendix A — Assignment A",
    "section": "A.2 RMSE vs MAE",
    "text": "A.2 RMSE vs MAE\n\nA.2.1 \nDescribe a regression problem, where it will be more appropriate to assess the model accuracy using the root mean squared error (RMSE) metric as compared to the mean absolute error (MAE) metric.\nNote: Don’t use the examples presented in class\n(4 points)\n\n\nA.2.2 \nDescribe a regression problem, where it will be more appropriate to assess the model accuracy using the mean absolute error (MAE) metric as compared to the root mean squared error (RMSE) metric.\nNote: Don’t use the examples presented in class\n(4 points)"
  },
  {
    "objectID": "Assignment 1.html#fnr-vs-fpr",
    "href": "Assignment 1.html#fnr-vs-fpr",
    "title": "Appendix A — Assignment A",
    "section": "A.3 FNR vs FPR",
    "text": "A.3 FNR vs FPR\n\nA.3.1 \nA classification model is developed to predict those customers who will respond positively to a company’s tele-marketing campaign. All those customers that are predicted to respond positively to the campaign will be called by phone to buy the product being marketed. If the customer being called purchases the product (\\(y = 1\\)), the company will get a profit of $100. On the other hand, if they are called and they don’t purchase (\\(y = 0\\)), the company will have a loss of $1. Among FPR (False positive rate) and FNR (False negative rate), which metric is more important to be minimized to reduce the loss associated with misclassification? Justify your answer.\nIn your justification, you must clearly interpret False Negatives (FN) and False Postives (FP) first.\nAssumption: Assume that based on the past marketing campaigns, around 50% of the customers will actually respond positively to the campaign.\n(4 points)\n\n\nA.3.2 \nCan the answer to the previous question change if the assumption stated in the question is false? Justify your answer.\n(6 points)"
  },
  {
    "objectID": "Assignment 1.html#petrol-consumption",
    "href": "Assignment 1.html#petrol-consumption",
    "title": "Appendix A — Assignment A",
    "section": "A.4 Petrol consumption",
    "text": "A.4 Petrol consumption\nRead the dataset petrol_consumption_train.csv. It contains the following five columns:\nPetrol_tax: Petrol tax (cents per gallon)\nPer_capita_income: Average income (dollars)\nPaved_highways: Paved Highways (miles)\nProp_license: Proportion of population with driver’s licenses\nPetrol_consumption: Consumption of petrol (millions of gallons)\n\nA.4.1 \nMake a pairwise plot of all the variables in the dataset. Which variable seems to have the highest linear correlation with Petrol_consumption? Let this variable be predictor P. Note: If you cannot figure out P by looking at the visualization, you may find the pairwise linear correlation coefficient to identify P.\n(4 points)\n\n\nA.4.2 \nFit a simple linear regression model to predict Petrol_consumption based on predictor P (identified in the previous part). Print the model summary.\n(4 points)\n\n\nA.4.3 \nInterpret the coefficient of P. What is the increase in petrol consumption for an increase of 0.05 in P?\n(2+2 points)\n\n\nA.4.4 \nDoes petrol consumption have a statistically significant relationship with the predictor P? Justify your answer.\n(4 points)\n\n\nA.4.5 \nWhat is the R-squared? Interpret its value.\n(4 points)\n\n\nA.4.6 \nUse the model developed above to estimate the petrol consumption for a state in which 50% of the population has a driver’s license. What are the confidence and prediction intervals for your estimate? Which interval includes the irreducible error?\n(4+3+3+2 = 12 points)\n\n\nA.4.7 \nUse the model developed above to estimate the petrol consumption for a state in which 10% of the population has a driver’s license. Are you getting a reasonable estimate? Why or why not?\n(5 points)\n\n\nA.4.8 \nWhat is the residual standard error of the model?\n(4 points)\n\n\nA.4.9 \nUsing the model developed above, predict the petrol consumption for the observations in petrol_consumption_test.csv. Find the RMSE (Root mean squared error). Include the units of RMSE in your answer.\n(5 points)\n\n\nA.4.10 \nBased on the answers to the previous two questions, do you think the model is overfitting? Justify your answer.\n(4 points)\nMake a scatterplot of Petrol_consumption vs Prop_license using petrol_consumption_test.csv. Over the scatterplot, plot the regression line, the prediction interval, and the confidence interval. Distinguish the regression line, prediction interval lines, and confidence interval lines with the following colors. Include the legend as well.\n\nRegression line: red\nConfidence interval lines: blue\nPrediction interval lines: green\n\n(4 points)\nAmong the confidence and prediction intervals, which interval is wider, and why?\n(1+2 points)\n\n\nA.4.11 \nFind the correlation between Petrol_consumption and the rest of the variables in petrol_consumption_train.csv. Based on the correlations, a simple linear regression model with which predictor will have the least R-squared value for predicting Petrol_consumption. Don’t develop any linear regression models.\n(4 points)\nBonus point question\n(5 points - no partial credit)\n\n\nA.4.12 \nFit a simple linear regression model to predict Petrol_consumption based on predictor P, but without an intercept term.\n(you must answer this correctly to qualify for earning bonus points)\n\n\nA.4.13 \nEstimate the petrol consumption for the observations in petrol_consumption_test.csv using the model in developed in the previous question. Find the RMSE.\n(you must answer this correctly to qualify for earning bonus points)\n\n\nA.4.14 \nThe RMSE for the models with and without the intercept are similar, which indicates that both models are almost equally good. However, the R-squared for the model without intercept is much higher than the R-squared for the model with the intercept. Why? Justify your answer.\n(5 points)"
  },
  {
    "objectID": "Assignment B.html",
    "href": "Assignment B.html",
    "title": "Appendix B — Assignment B",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Thursday, 26th January 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (1 pt). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission (1 pt)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)"
  },
  {
    "objectID": "Assignment B.html#multiple-linear-regression",
    "href": "Assignment B.html#multiple-linear-regression",
    "title": "Appendix B — Assignment B",
    "section": "B.1 Multiple linear regression",
    "text": "B.1 Multiple linear regression\nA study was conducted on 97 men with prostate cancer who were due to receive a radical prostatectomy. The dataset prostate.csv contains data on 9 measurements made on these 97 men. The description of variables can be found here:\n\nB.1.1 Training MLR\nFit a linear regression model with lpsa as the response and all the other variables as predictors. Write down the equation to predict lpsa based on the other eight variables.\n(2+2 points)\n\n\nB.1.2 Model significance\nIs the overall regression significant at 5% level? Justify your answer.\n(2 points)\n\n\nB.1.3 Coefficient interpretation\nInterpret the coefficient of svi.\n(2 points)\n\n\nB.1.4 Variable significance\nReport the \\(p\\)-values for gleason and age. What do you conclude about the significance of these variables?\n(2+2 points)\n\n\nB.1.5 Variable significance from confidence interval\nWhat is the 95% confidence interval for the coefficient of age? Can you conclude anything about its significance based on the confidence interval?\n(2+2 points)\n\n\nB.1.6 \\(p\\)-value\nFit a simple linear regression on lpsa against gleason. What is the \\(p\\)-value for gleason?\n(1+1 points)\n\n\nB.1.7 Predictor significance in presence / absence of other predictors\nIs the predictor gleason statistically significant in the model developed in the previous question (B.1.6)?\nWas gleason statistically significant in the model developed in the first question (B.1.1) with multiple predictors?\nDid the statistical significance of gleason change in the absence of other predictors? Why or why not?\n(1+1+4 points)\n\n\nB.1.8 Prediction\nPredict lpsa of a 65-year old man with lcavol = 1.35, lweight = 3.65, lbph = 0.1, svi = 0.22, lcp = -0.18, gleason = 6.75, and pgg45 = 25 and find 95% prediction intervals.\n(2 points)\n\n\nB.1.9 Variable selection\nFind the largest subset of predictors in the model developed in the first question (B.1.1), such that their coefficients are zero, i.e., none of the predictors in the subset are statistically significant.\nDoes the model \\(R\\)-squared change a lot if you remove the set of predictors identifed above from the model in the first question (B.1.1)?\nHint: You may use the f_test() method to test hypotheses.\n(4+1 points)"
  },
  {
    "objectID": "Assignment B.html#using-mlr-coefficients-and-variable-transformation",
    "href": "Assignment B.html#using-mlr-coefficients-and-variable-transformation",
    "title": "Appendix B — Assignment B",
    "section": "B.2 Using MLR coefficients and variable transformation",
    "text": "B.2 Using MLR coefficients and variable transformation\nThe dataset infmort.csv gives the infant mortality of different countries in the world. The column mortality contains the infant mortality in deaths per 1000 births.\n\nB.2.1 Data visualisation\nMake the following plots:\n\na boxplot of log(mortality) against region (note that a plot of log(mortality) against region better distinguishes the mortality among regions as compared to a plot of mortality against region,\na boxplot of income against region, and\na scatter plot of mortality against income.\n\nWhat trends do you see in these plots? Mention the trend separately for each plot.\n(3+2 points)\n\n\nB.2.2 Removing effect of predictor from response\nEurope seems to have the lowest infant mortality, but it also has the highest per capita annual income. We want to see if Europe still has the lowest mortality if we remove the effect of income from the mortality. We will answer this question with the following steps.\n\nB.2.2.1 Variable transformation\nPlot:\n\nmortality against income,\nlog(mortality) against income,\nmortality against log(income), and\nlog(mortality) against log(income).\n\nBased on the plots, postulate an appropriate model to predict mortality as a function of income. Print the model summary.\n(2+4 points)\n\n\nB.2.2.2 Model update\nUpdate the model developed in the previous question by adding region as a predictor. Print the model summary.\n(2 points)\nUse the model developed in the previous question to compute adjusted_mortality for each observation in the data, where adjusted mortality is the mortality after removing the estimated effect of income. Make a boxplot of log(adjusted_mortality) against region.\n(4+2 points)\n\n\n\nB.2.3 Data visualisation after removing effect of predictor from response\nFrom the plot in the previous question:\n\nDoes Europe still seem to have the lowest mortality as compared to other regions after removing the effect of income from mortality?\nAfter adjusting for income, is there any change in the mortality comparison among different regions. Compare the plot developed in the previous question to the plot of log(mortality) against region developed earlier (B.2.1) to answer this question.\n\nHint: Do any African / Asian / American countries seem to do better than all the European countries with regard to mortality after adjusting for income?\n(1+3 points)"
  },
  {
    "objectID": "Assignment B.html#variable-transformations-and-interactions",
    "href": "Assignment B.html#variable-transformations-and-interactions",
    "title": "Appendix B — Assignment B",
    "section": "B.3 Variable transformations and interactions",
    "text": "B.3 Variable transformations and interactions\nThe dataset soc_ind.csv contains the GDP per capita of some countries along with several social indicators.\n\nB.3.1 Training SLR\nFor a simple linear regression model predicting gdpPerCapita. Which predictor will provide the best model fit (ignore categorical predictors)? Let that predictor be \\(P\\).\n(2 points)\n\n\nB.3.2 Linearity in relationship\nMake a scatterplot of gdpPerCapita vs \\(P\\). Does the relationship between gdpPerCapita and \\(P\\) seem linear or non-linear?\n(1 + 2 points)\n\n\nB.3.3 Variable transformation\nIf the relationship identified in the previous question is non-linear, identify and include transformation(s) of the predictor \\(P\\) in the model to improve the model fit.\nMention the predictors of the transformed model, and report the change in the \\(R\\)-squared value of the transformed model as compared to the simple linear regression model with only \\(P\\).\n(4+4 points)\n\n\nB.3.4 Model visualisation with transformed predictor\nPlot the regression curve of the transformed model (developed in the previous question) over the scatterplot in (b) to visualize model fit. Also make the regression line of the simple linear regression model with only \\(P\\) on the same plot.\n(3 + 1 points)\n\n\nB.3.5 Training MLR with qualitative predictor\nDevelop a model to predict gdpPerCapita with \\(P\\) and continent as predictors.\n\nInterpert the intercept term.\nFor a given value of \\(P\\), are there any continents that do not have a signficant difference between their mean gdpPerCapita and that of Africa? If yes, then which ones, and why? If no, then why not? Consider a significance level of 5%.\n\n(4 + 4 points)\n\n\nB.3.6 Variable interaction\nThe model developed in the previous question has a limitation. It assumes that the increase in mean gdpPerCapita with a unit increase in \\(P\\) does not depend on the continent.\n\nEliminate this limitation by including interaction of continent with \\(P\\) in the model developed in the previous question. Print the model summary of the model with interactions.\nInterpret the coefficient of any one of the interaction terms.\n\n(4 + 4 points)\n\n\nB.3.7 Model visualisation with qualitative predictor\nUse the model developed in the previous question to plot the regression lines for Africa, Asia, and Europe. Put gdpPerCapita on the vertical axis and \\(P\\) on the horizontal axis. Use a legend to distinguish among the regression lines of the three continents.\n(4 points)\n\n\nB.3.8 Model interpretation\nBased on the plot develop in the previous question, which continent has the highest increase in mean gdpPerCapita for a unit increase in \\(P\\), and which one has the least? Justify your answer.\n(2+2 points)"
  },
  {
    "objectID": "Assignment C.html",
    "href": "Assignment C.html",
    "title": "Appendix C — Assignment C",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Thursday, 9th February 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (1 pt). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission (1 pt)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)"
  },
  {
    "objectID": "Assignment C.html#model-assumptions",
    "href": "Assignment C.html#model-assumptions",
    "title": "Appendix C — Assignment C",
    "section": "C.1 Model assumptions",
    "text": "C.1 Model assumptions\n\nC.1.1 \nUsing house_feature_train.csv and house_price_train.csv, fit a multiple linear regression model without transformation to predict house_price based on distance_MRT, latitude, and longitude, house_age, and number_convenience_stores. Print the model summary. What is the model R2R^2?\n(1 + 1 points)\n\n\nC.1.2 \nObtain the residuals and plot them separately against fitted values and each of the five feature variables. Make one plot including the 6 subplots.\n(4 points)\n\n\nC.1.3 \nComment on the plot of residuals against fitted values. Does the model violate the assumption of linearity? Does the model violate the constant variance assumption?\n(2 + 2 points)\n\n\nC.1.4 \nComment on the plot of residuals against the predictor variables. On the basis of these plots, should any further modifications of the regression model be attempted?\n(5 points)\n\n\nC.1.5 \nCalculate the RMSE using the test datasets for the model constructed in the first question. The test datasets are house_feature_test.csv and house_price_test.csv.\n(2 points)\n\n\nC.1.6 \nUsing appropriate transformation(s) and/or variable interaction(s), update the model to obtain a model that has a R2R^{2} of at least 80%, and a RMSE (Root mean squared error) of at max $350k on test data.\nPrint the model summary and report the R2R^2, and RMSE on test data. Note:\n\nHouse prices are provided in thousands of dollars. A value of 556 in the house_price column indicates a house price of $556k.\nThe test datasets are house_feature_test.csv and house_price_test.csv.\nR2R^2 is computed on training data, and RMSE is computed on test data.\nYou must proceed logically, i.e., justify every transformation that you introduce into the model to improve it. If you are introducing interactions, there should be some rationale behind including only certain interactions in the model, unless you are including all possible interactions.\n\n(12 points for achieving the objectives + 8 points for justifications)\n\n\nC.1.7 \nAre the assumptions of linearity and constant variance of errors satisfied in the model developed in the previous question? Make a scatterplot between the residuals and fitted values and use it to answer the question.\n(4 points)"
  },
  {
    "objectID": "Assignment C.html#multicollinearity-and-outliers",
    "href": "Assignment C.html#multicollinearity-and-outliers",
    "title": "Appendix C — Assignment C",
    "section": "C.2 Multicollinearity and Outliers",
    "text": "C.2 Multicollinearity and Outliers\nThe datasets Austin_Affordable_Housing_Train.csv and Austin_Affordable_Housing_Test.csv provide data on housing development projects that have received funding from the Affordable Housing Development Fund in Austin, Texas. The city provides property developers with tax credits and other forms of funding in exchange for agreements to set housing prices (e.g. rent) below market rate.\nEach row represents a housing development in Austin. Variables include the amount (USD) provided by the city, the status of the housing project, the number of housing units, the period of affordability, and more.\nLet’s say that you’re hired by the city as a consultant to work with subject matter experts in their Housing and Planning Department.\nGeneral Hint: For written sections, writing “it depends” (along with an explanation) often characterizes a good answer.\nNote for Grading Team: Written answers should be given full credit as long as they’re thoughtful answers that address the question fully, base findings on relevant data/results, and align with the relevant regression theory/thinking. Many questions don’t have a single right answer and/or depend on context that isn’t provided here.\n\nC.2.1 \nSuppose you run the code status_vars = pd.get_dummies(housing_dataframe[\"Status\"]), append the columns of status_vars to your original data frame, and use the columns as predictors in a linear regression model. What potential problem would you likely be introducing into the model? How could it affect your results?\n(4 points)\n\n\nC.2.2 \nSuppose that a subject matter expert recommends using the variables Total_Units, Total_Affordable_Units, Total_Accessible_Units, and Market_Rate_Units as predictors in your model. From a regression modeling standpoint, does this sound advisable? Produce metrics to quantify the potential impact of including the four predictors in a model. Interpret at least one of the metrics you provide, both statistically and in the context of the problem.\n(4 points)\n\n\nC.2.3 \nSay that the subject matter expert agrees to use Total_Affordable_Units, Affordability_Expiration_Year, and Units_Under_50_Percent_MFI as predictors for City_Amount. Fit the appropriate model (without transformations). Then interpret the results associated with Total_Affordable_Units, and comment on the overall model fit.\n(4 points)\n\n\nC.2.4 \nUsing visualizations, investigate whether the model you fit in in the previous question yields outlying observations. What count and proportion of observations would you classify as outliers?\nNote: Show separate plots for both - residuals and studentized residuals. However, consider studentized residuals when identifying outliers.\n(4 points)\n\n\nC.2.5 \nBased on your results in the previous question, would you choose to remove outlying observations? Why or why not?\n(4 points)\n\n\nC.2.6 \nConsider a scenario in which the model will be used by property owners seeking to predict the amount of money they may receive from the city of Austin. How would this change, support, or complicate your answer in the previous question, if at all?\n(3 points)\n\n\nC.2.7 \nSay that the model will be used by a team of sociologists seeking statistical evidence at the α=0.01\\alpha = 0.01 significance level that a property’s affordability expiration year has an effect on the amount of money issued by the city of Austin? How would this change, support, or complicate your answer in C.2.5, if at all?\n(3 points)\n\n\nC.2.8 \nDetermine whether the model you fit in C.2.3 contains any high-leverage points. Produce a visualization, then report the count and proportions of observations that are high-leverage (define an observation as “high-leverage” if its leverage is greater than four times the average leverage of all observations).\n(4 points)\n\n\nC.2.9 \nBased on your results in the previous question, would you choose to remove high-leverage observations? Why or why not?\n(3 points)\n\n\nC.2.10 \nIdentify and remove any influential points from the training data and refit the model. How does removing the influential points affect the model, if at all?\nThink about using the model summary, and use the test data provided.\n(6 points)"
  },
  {
    "objectID": "Assignment C.html#autocorrelation",
    "href": "Assignment C.html#autocorrelation",
    "title": "Appendix C — Assignment C",
    "section": "C.3 Autocorrelation",
    "text": "C.3 Autocorrelation\nRefer to the autocorrelation example in the class notes. Predict the power consumption for each hour of each day of the year 2020. For predicting a power consumption on a particular hour of a day, use all the data you have until the previous day. However, don’t use any data of the day on which you are making the predictions. For example, for making 24 predictions for each hour of 4th April, 2020, use all the data upto 3rd April 2020. Make the predictions using four different models:\n\nModel with only temp_hot and temp_cold as the predictors\nModel including one day lag of power as a predictor in addition to the predictors in model (1)\nModel including one week lag of power as a predictor in addition to the predictors in model (2)\nModel including two weeks lag of power as a predictor in addition to the predictors in model (3)\n\nFor each model:\n\nReport the RMSE for the predicted power in 2020. You should have 366 x 24 = 8784 predicted values of power for each model.\nMake a scatterplot of predicted power vs actual power (use color = ‘orange’). Plot the line x = y over the scatterplot (use color = ‘blue’).\n\nWhich model makes the most accurate predictions?\n(4 points for developing the models + 4 points for computing the predictions + 4 points for computing the RMSEs + 2 points for the visualizations + 1 point for identifying the most accurate model)"
  },
  {
    "objectID": "Assignment D.html",
    "href": "Assignment D.html",
    "title": "Appendix D — Assignment D",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Tuesday, 21st February 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (1 pt). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission (1 pt)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)"
  },
  {
    "objectID": "Assignment D.html#data-description",
    "href": "Assignment D.html#data-description",
    "title": "Appendix D — Assignment D",
    "section": "Data description",
    "text": "Data description\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls, where bank clients were called to subscribe for a term deposit.\nThere is one train data - train.csv, which you will use to develop a model. There are two test datasets - test1.csv and test2.csv, which you will use to test your model. Each dataset has the following attributes about the clients called in the marketing campaign:\n\nage: Age of the client\neducation: Education level of the client\nday: Day of the month the call is made\nmonth: Month of the call\ny: did the client subscribe to a term deposit?\nduration: Call duration, in seconds. This attribute highly affects the output target (e.g., if duration=0 then y=‘no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for inference purposes and should be discarded if the intention is to have a realistic predictive model.\n\n(Raw data source: Source. Do not use the raw data source for this assingment. It is just for reference.)"
  },
  {
    "objectID": "Assignment D.html#instructions-suggestions-for-answering-questions",
    "href": "Assignment D.html#instructions-suggestions-for-answering-questions",
    "title": "Appendix D — Assignment D",
    "section": "Instructions / suggestions for answering questions",
    "text": "Instructions / suggestions for answering questions\n\nInstruction: Use train.csv for all questions, unless otherwise stated.\nSuggestion 1: You may use the functions in the class notes for printing the confusion matrix and the overall classification accuracy based on test / train data.\nSuggestion 2:: If you make variable transformations, you will need to do it for all the three datasets. Your code will be a bit concise if you make a function containing all the transformations, and then call it for the training and the two test datasets. You can put this function in the beginning of the code and keep adding transformations to it as you proceed with the assignment. You may need transformations in questions (1) and (13)."
  },
  {
    "objectID": "Assignment D.html#probability-of-response-vs-call-duration",
    "href": "Assignment D.html#probability-of-response-vs-call-duration",
    "title": "Appendix D — Assignment D",
    "section": "D.1 Probability of response vs call duration",
    "text": "D.1 Probability of response vs call duration\nRead the datasets. Make an appropriate visualization to visualize how the proportion of clients subscribing to a term deposit change with increasing call duration.\n(4 points)\nHints:\n\nBin duration to create duration_binned. Group the data to find the fraction of clients responding positively to the marketing campaign for each bin in duration_binned. Make a lineplot of percentage of clients subscribing to a term deposit vs duration_binned, where the bins in duration_binned are arranged in increasing order of duration.\nYou may choose an appropriate number of bins & type of binning that helps you visualize well.\nYou may also think of other ways of visualization. You don’t need to stick with this one."
  },
  {
    "objectID": "Assignment D.html#predictor-duration",
    "href": "Assignment D.html#predictor-duration",
    "title": "Appendix D — Assignment D",
    "section": "D.2 Predictor duration",
    "text": "D.2 Predictor duration\nBased on the plot in D.1, comment whether duration seems to be a useful variable to predict if the client will subscribe to a term deposit.\n(1 point)"
  },
  {
    "objectID": "Assignment D.html#model-based-on-duration",
    "href": "Assignment D.html#model-based-on-duration",
    "title": "Appendix D — Assignment D",
    "section": "D.3 Model based on duration",
    "text": "D.3 Model based on duration\nDevelop a logistic regression model to predict if the client subscribed to a term deposit based on call duration. Use the model to make a lineplot showing the probability of the client subscribing to a term deposit based on call duration.\n(3 points)"
  },
  {
    "objectID": "Assignment D.html#note",
    "href": "Assignment D.html#note",
    "title": "Appendix D — Assignment D",
    "section": "Note",
    "text": "Note\nAnswer questions D.4 to D.11 based on the regression model developed in D.3."
  },
  {
    "objectID": "Assignment D.html#model-significance",
    "href": "Assignment D.html#model-significance",
    "title": "Appendix D — Assignment D",
    "section": "D.4 Model significance",
    "text": "D.4 Model significance\nIs the regression model in statistically significant? Justify your answer.\n(1 point for code, 1 point for answer)"
  },
  {
    "objectID": "Assignment D.html#subscription-probability-in-5-minutes",
    "href": "Assignment D.html#subscription-probability-in-5-minutes",
    "title": "Appendix D — Assignment D",
    "section": "D.5 Subscription probability in 5 minutes",
    "text": "D.5 Subscription probability in 5 minutes\nWhat is the probability that the client subscribes to a term deposit with a 5-minute marketing call? Note that the call duration in data is given in seconds.\n(2 points)"
  },
  {
    "objectID": "Assignment D.html#call-duration-for-subscription",
    "href": "Assignment D.html#call-duration-for-subscription",
    "title": "Appendix D — Assignment D",
    "section": "D.6 Call duration for subscription",
    "text": "D.6 Call duration for subscription\nWhat is the minimum call duration (in minutes) for which a client has a 95% or higher chance of subscribing to a term deposit?\n(4 points)"
  },
  {
    "objectID": "Assignment D.html#maximum-call-duration",
    "href": "Assignment D.html#maximum-call-duration",
    "title": "Appendix D — Assignment D",
    "section": "D.7 Maximum call duration",
    "text": "D.7 Maximum call duration\nWhat is the maximum call duration (in minutes) in which a client refused to subscribe to a term deposit? What was the probability of the client subscribing to the term deposit in that call?\n(4 points)"
  },
  {
    "objectID": "Assignment D.html#percent-increase-in-odds",
    "href": "Assignment D.html#percent-increase-in-odds",
    "title": "Appendix D — Assignment D",
    "section": "D.8 Percent increase in odds",
    "text": "D.8 Percent increase in odds\nWhat is the percentage increase in the odds of a client subscribing to a term deposit when the call duration increases by a minute?\n(4 points)"
  },
  {
    "objectID": "Assignment D.html#doubling-the-subscription-odds",
    "href": "Assignment D.html#doubling-the-subscription-odds",
    "title": "Appendix D — Assignment D",
    "section": "D.9 Doubling the subscription odds",
    "text": "D.9 Doubling the subscription odds\nHow much must the call duration increase (in minutes) so that it doubles the odds of the client subscribing to a term deposit.\n(3 points)"
  },
  {
    "objectID": "Assignment D.html#classification-accuracy",
    "href": "Assignment D.html#classification-accuracy",
    "title": "Appendix D — Assignment D",
    "section": "D.10 Classification accuracy",
    "text": "D.10 Classification accuracy\nWhat is minimum overall classification accuracy of the model among the classification accuracies on train.csv, test1.csv and test2.csv? Consider a threshold of 30% when classifying observations.\n(2 + 1 + 1 points)"
  },
  {
    "objectID": "Assignment D.html#recall",
    "href": "Assignment D.html#recall",
    "title": "Appendix D — Assignment D",
    "section": "D.11 Recall",
    "text": "D.11 Recall\nWhat is the minimum Recall of the model among the Recall performance on train.csv, test1.csv and test2.csv? Consider a decision threshold probability of 30% when classifying observations.\nHere, Recall is the proportion of clients predicted to subscribe to a term deposit among those who actually subscribed.\n(3 points)"
  },
  {
    "objectID": "Assignment D.html#subscription-probability-based-on-age-and-education",
    "href": "Assignment D.html#subscription-probability-based-on-age-and-education",
    "title": "Appendix D — Assignment D",
    "section": "D.12 Subscription probability based on age and education",
    "text": "D.12 Subscription probability based on age and education\nDevelop a logistic regression model to predict the probability of a client subscribing to a term deposit based on age, education and the two-factor interaction between age and education. Based on the model, answer:\n\nPeople with which type of education (primary / secondary / tertiary / unknown) have the highest percentage increase in odds of subscribing to a term deposit with a unit increase in age? Justify your answer.\nWhat is the percentage increase in odds of a person subscribing to a term deposit for a unit increase in age, if the person has tertiary education.\nWhat is the percentage increase in odds of a person subscribing to a term deposit for a unit increase in age, if the person has primary education.\n\n(1 point for developing the model, 3 points for (a), 3 points for (b), 3 points for (c))"
  },
  {
    "objectID": "Assignment D.html#model-development",
    "href": "Assignment D.html#model-development",
    "title": "Appendix D — Assignment D",
    "section": "D.13 Model development",
    "text": "D.13 Model development\nDevelop a logistic regression model (using train.csv) to predict the probability of a client subscribing to a term deposit based on age, education, day and month. The model must have:\n\nMinimum overall classification accuracy of 75% among the classification accuracies on train.csv, test1.csv and test2.csv.\nMinimum recall of 50% among the recall performance on train.csv, test1.csv and test2.csv.\n\nFor all the three datasets - train.csv, test1.csv and test2.csv, print the:\n\nModel summary (only for train.csv),\nConfusion matrices,\nOverall classification accuracies, and\nRecall\n\nNote that:\n\nYou cannot use duration as a predictor because its value is determined after the marketing call ends. However, after the call ends, we already know whether the client responded positively or negatively. That is why we have used duration only for inference in the previous questions. It helped us understand the effect of the length of the call on marketing success.\nIt is possible to develop the model satisfying constrains (a) and (b) with just appropriate transformation(s) of the predictor(s). However, you may consider interactions if you wish. Justify the transformations, if any, with visualizations.\nYou are free to choose any value of the decision threshold probability for classifying observations. However, you must use the same threshold on all the three datasets.\n\n(15 points)"
  },
  {
    "objectID": "Assignment D.html#roc-auc",
    "href": "Assignment D.html#roc-auc",
    "title": "Appendix D — Assignment D",
    "section": "D.14 ROC-AUC",
    "text": "D.14 ROC-AUC\nReport the probability that the model will predict a higher probability of response for a customer who signs up for the term deposit as compared to the customer who does not sign up, i.e., the ROC-AUC of the developed model in D.13.\nHint: Use the functions roc_curve, and auc from the sklearn.metrics module\n(3 points)"
  },
  {
    "objectID": "Assignment D.html#net-profit",
    "href": "Assignment D.html#net-profit",
    "title": "Appendix D — Assignment D",
    "section": "D.15 Net-profit",
    "text": "D.15 Net-profit\nSuppose that the model developed in D.13 is used to predict the clients in test1.csv and test2.csv who will respond positively to the campaign. Only those clients who are predicted to respond positively are called during the marketing campaign. Assume that:\n\nA profit of \\$100 is associated with a client who responds positively to the campaign,\nA loss of \\$10 is associated with a client who responds negatively to the campaign\n\nWhat is the net profit from the campaign? Use the confusion matrices printed in D.13.\n(4 points)"
  },
  {
    "objectID": "Assignment D.html#decision-threshold-probability",
    "href": "Assignment D.html#decision-threshold-probability",
    "title": "Appendix D — Assignment D",
    "section": "D.16 Decision threshold probability",
    "text": "D.16 Decision threshold probability\nBased on the profit and loss associated with client responses specified in D.15, and the model developed in D.13, find the decision threshold probability of classification, such that the net profit is maximized. Use train.csv\nProceed as follows:\n\nYou would have obtained FPR and TPR for all potential decision threshold probabilities in D.14.\nFormulate an expression quantifying the net profit per client, in terms of FPR, TPR, and the overall response rate, i.e., proportion of people actually subscribing to the term deposit.\nFind the decision threshold probability that maximizes the expression in (2).\n\n(5 points)"
  },
  {
    "objectID": "Assignment D.html#net-profit-based-on-new-decision-threshold-probability",
    "href": "Assignment D.html#net-profit-based-on-new-decision-threshold-probability",
    "title": "Appendix D — Assignment D",
    "section": "D.17 Net profit based on new decision threshold probability",
    "text": "D.17 Net profit based on new decision threshold probability\nUsing the new decision threshold probability obtained in D.16, answer D.15, i.e., what is the net-profit associated with the clients in test1.csv and test2.csv if a marketing campaign is performed. Again, only those clients who are predicted to respond positively, based on the new decision threshold probability, are called during the marketing campaign\nAlso, print the confusion matrices for predictions on test1.csv and test2.csv with the new threshold probability.\n(4 points)"
  },
  {
    "objectID": "Assignment D.html#model-preference",
    "href": "Assignment D.html#model-preference",
    "title": "Appendix D — Assignment D",
    "section": "D.18 Model preference",
    "text": "D.18 Model preference\nWas the classification accuracy of the model in D.13 higher than that of the model in D.17? If yes, then should you prefer the model in D.13 for the marketing campaign? Why or why not?\nNote: The model in D.17 is the same as in D.13, except with a different decision threshold probability\n(3 points)"
  },
  {
    "objectID": "Assignment D.html#roc-curve",
    "href": "Assignment D.html#roc-curve",
    "title": "Appendix D — Assignment D",
    "section": "D.19 ROC curve",
    "text": "D.19 ROC curve\nPlot the ROC curve for the model developed in D.13. Mark the point on the curve corresponding to the decision threshold probability identified in D.16.\nNote that the ROC curve is independent of the decision threshold probability used by the model for prediction\n(3 points)"
  },
  {
    "objectID": "Assignment D.html#profit-with-tpr-fpr",
    "href": "Assignment D.html#profit-with-tpr-fpr",
    "title": "Appendix D — Assignment D",
    "section": "D.20 Profit with TPR / FPR",
    "text": "D.20 Profit with TPR / FPR\nMake a scatterplot of TPR vs FPR, and color the points based on net profit per client.\nYou can use the following code to make the plot if you have the relevant metrics in tpr, fpr, and net_profit\n(1 point)\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nplt.rcParams[\"figure.autolayout\"] = True\nf, ax = plt.subplots()\npoints = ax.scatter(fpr, tpr, c = net_profit, s=50, cmap=\"Blues\")\nf.colorbar(points, label = \"Net profit ($) \\n(per client)\")\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.show()"
  },
  {
    "objectID": "Assignment D.html#precision-recall",
    "href": "Assignment D.html#precision-recall",
    "title": "Appendix D — Assignment D",
    "section": "D.21 Precision-recall",
    "text": "D.21 Precision-recall\nCompare the precision and recall of the models in D.13 and D.17 on train.csv.\nNote: The model in D.17 is the same as in D.13, except with a different decision threshold probability\n(4 points)"
  },
  {
    "objectID": "Assignment D.html#precision-recall-important-metric",
    "href": "Assignment D.html#precision-recall-important-metric",
    "title": "Appendix D — Assignment D",
    "section": "D.22 Precision-recall: important metric",
    "text": "D.22 Precision-recall: important metric\nBased on the above comparison, which metric among precision and recall turns out to be more important for maximizing the net profit in the marketing campaign?\n(1 point)"
  },
  {
    "objectID": "Assignment D.html#precision-recall-curve",
    "href": "Assignment D.html#precision-recall-curve",
    "title": "Appendix D — Assignment D",
    "section": "D.23 Precision-recall curve",
    "text": "D.23 Precision-recall curve\nPlot the precision-recall curve vs decision threshold probability for the model developed in D.13. Mark the points on the curve corresponding to the decision threshold probability identified in D.16.\n(3 points)"
  },
  {
    "objectID": "Assignment D.html#precision-recall-vs-fpr-tpr",
    "href": "Assignment D.html#precision-recall-vs-fpr-tpr",
    "title": "Appendix D — Assignment D",
    "section": "D.24 Precision-recall vs FPR-TPR",
    "text": "D.24 Precision-recall vs FPR-TPR\nInstead of using the FPR and TPR metrics to find the optimum decision threshold probability in D.16, use the precision-recall metrics to find the same.\n(5 points)"
  },
  {
    "objectID": "Assignment E.html",
    "href": "Assignment E.html",
    "title": "Appendix E — Assignment E",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Tuesday, 7th March 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (1 pt). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission (1 pt)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)"
  },
  {
    "objectID": "Assignment E.html#calculating-root-mean-square-error-rmse-in-sklearn",
    "href": "Assignment E.html#calculating-root-mean-square-error-rmse-in-sklearn",
    "title": "Appendix E — Assignment E",
    "section": "Calculating Root Mean Square Error (RMSE) in Sklearn",
    "text": "Calculating Root Mean Square Error (RMSE) in Sklearn\nYou may use sklearn to compute the RMSE.\nsklearn.metrics has mean_squared_error function with a squared kwarg (default to True). Setting squared to False will return the RMSE\nfrom sklearn.metrics import mean_squared_error\nrmse = mean_squared_error(y_actual, y_predicted, squared=False)"
  },
  {
    "objectID": "Assignment E.html#energy-model",
    "href": "Assignment E.html#energy-model",
    "title": "Appendix E — Assignment E",
    "section": "E.1 Energy model",
    "text": "E.1 Energy model\nThe datasets ENB2012_Train.csv and ENB2012_Test.csv provide data on energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. Below is the description of the data columns:\n\nX1: Relative Compactness\nX2: Surface Area\nX3: Wall Area\nX4: Roof Area\nX5: Overall Height\nX6: Orientation\nX7: Glazing Area\nX8: Glazing Area Distribution\nY1: Heating Load\n\n\nE.1.1 \nSuppose that we want to implement the best subset selection algorithm to find the first order predictors (X1-X8) that can predict heating load (Y1). How many models for EE(Y1) are possible, if the model includes (i) one variable, (ii) three variables, and (iii) eight variables? Show your steps without running any code.\nNote: The notation EE(Y1) means the expected value of Y1 or the mean of Y1.\n(3 points)\n\n\nE.1.2 \nImplement the best subset selection algorithm to find the best first-order predictors of heating load Y1. Print out the model summary.\nNote:\n\nUse ENB2012_Train.csv and consider only the first-order terms.\nUse the BIC criterion for model selection.\n\n(4 points)\n\n\nE.1.3 \nShould RR-squared be used to select from among a set of models with different numbers of predictors? Justify your answer.\n(1 point for answer, 2 points for justification)\n\n\nE.1.4 \nCalculate the RMSE of the model found in E.2. Compare it with the RMSE of the model using all first-order predictors. You will find that the two RMSEs are similar. Seems like the best subset model didn’t help improve prediction.\n\nWhy did the best subset model not help improve prediction accuracy as compared to the model with all the predictors?\nDoes the best subset model provide a more accurate inference as compared to the model with all the predictors? Why or why not?\n\nHint: Very Important Fact!\n(2 points for computing the RMSEs, 3 + 3 points for justifications)\n\n\nE.1.5 \nLet us consider adding all the 2-factor interactions of the predictors in the model. Answer the following questions without running code.\n\nHow many predictors do we have in total?\nAssume best subset selection is used. How many models are fitted in total?\nAssume forward selection is used. How many models are fitted in total?\nAssume backward selection is used. How many models are fitted in total?\nHow many models will be developed in the iteration that contains exactly 10 predictors in each model – for best subsets, fwd/bwd regression?\nWhat approach would you choose for variable selection (amongst best subset, forward selection, backward selection)?\n\n(6 x 2 = 12 points)\n\n\nE.1.6 \nUse forward selection to find the best first-order predictors and 2-factor interactions of the predictors of Y1 (Heating Load). Print out the model summary.\n(5 points)\n\n\nE.1.7 \nCalculate the RMSE of the model found in E.1.6. Compare it with:\n\nthe RMSE of model you found in E.1.2 and,\nthe RMSE of the model using all the predictors and all their 2-factor interaction terms.\n\nAmong the three models (the model developed in E.1.2, the model developed in E.1.6, the model that has all the predictors and all their 2-factor interactions), discuss which model will you prefer for prediction, and which model will you prefer for inference, and why?\n(2 points for computing the RMSEs, 3 + 3 points for discussion)\n\n\nE.1.8 \nAssume that we found another dataset of 32 variables on the same set of 768 buildings (542 for training) that we would want to add into our model. We want find the “best” model of all 40 predictors and their 2-factor interaction terms. Would you choose forward or backward selection? Justify your answer.\n(1 point for answer, 4 points for justification)"
  },
  {
    "objectID": "Assignment E.html#planetary-radius-model",
    "href": "Assignment E.html#planetary-radius-model",
    "title": "Appendix E — Assignment E",
    "section": "E.2 Planetary radius model",
    "text": "E.2 Planetary radius model\nSee https://exoplanetarchive.ipac.caltech.edu (for context/source). We are using the Composite Planetary Systems dataset\n\nE.2.1 \nSay we’re interested in modeling the radius of exoplanets in kilometers, which is named as pl_rade in the data. Note that the variable pl_rade captures the radius of each planet as a proportion of Earth’s radius, which is approximately 6,378.1370 km.\nDevelop a linear regression model to predict pl_rade using all the variables in train_CompositePlanetarySystems.csv except pl_name, disc_facility and disc_locale. Find the RMSE (Root mean squared error) of the model on test1_CompositePlanetarySystems.csv and test2_CompositePlanetarySystems.csv.\n(4 points)\n\n\nE.2.2 \nDevelop a ridge regression model to predict pl_rade using all the variables in train_CompositePlanetarySystems.csv except pl_name, disc_facility and disc_locale. What is the optimal value of the tuning parameter λ\\lambda?\nHint: You may use the following grid of lambda values to find the optimal λ\\lambda: alphas = 10**np.linspace(2,0.5,200)*0.5\nRemember to standardize data before fitting the ridge regression model\n(5 points)\n\n\nE.2.3 \nUse the optimal value of λ\\lambda found in the previous question to develop a ridge regression model. What is the RMSE of the model on test1_CompositePlanetarySystems.csv and test2_CompositePlanetarySystems.csv?\n(5 points)\n\n\nE.2.4 \nNote that ridge regression has a much lower RMSE on test datasets as compared to Ordinary least squares (OLS) regression. Shrinking the coefficients has reduced the variance of the estimated coefficients with a little increase in bias, thereby improving the model fit. Appreciate it. Which are the top two predictors for which the coefficients have shrunk the most?\nTo answer this question, find the ridge regression estimates for λ=10−10\\lambda = 10^{-10} (almost zero regularization). Treat these estimates as OLS estimates and find the predictors for which these estimates have shrunk the most as compared to the model developed in E.2.3.\n(4 points for code, 1 point for answer)\n\n\nE.2.5 \nWhy do you think the coefficients of the two variables identified in the previous question shrunk the most?\n(4 points for justification - including code used)\n\n\nE.2.6 \nDevelop a lasso model to predict pl_rade using all the variables in train_CompositePlanetarySystems.csv except pl_name, disc_facility and disc_locale. What is the optimal value of the tuning parameter λ\\lambda?\nHint: You may use the following grid of lambda values to find the optimal λ\\lambda: alphas = 10**np.linspace(0,-2.5,200)*0.5\n(4 points)\n\n\nE.2.7 \nUse the optimal value of λ\\lambda found in the previous question to develop a lasso model. What is the RMSE of the model on test1_CompositePlanetarySystems.csv and test2_CompositePlanetarySystems.csv?\n(5 points)\n\n\nE.2.8 \nNote that lasso has a much lower RMSE on test datasets as compared to Ordinary least squares (OLS) regression. Shrinking the coefficients has improved the model fit. Appreciate it. Which variables have been eliminated by lasso?\nTo answer this question, find the predictors whose coefficients are 0 in the lasso model.\n(2 points for code, 1 point for answer)"
  },
  {
    "objectID": "Assignment E.html#k-fold-cross-validation",
    "href": "Assignment E.html#k-fold-cross-validation",
    "title": "Appendix E — Assignment E",
    "section": "E.3 KK-fold cross validation",
    "text": "E.3 KK-fold cross validation\nWe haves used car_features_train.csv and car_prices_train.csv in class notes to predict car price. Based on correlation with price, the four most relevant continuous predictors to predict car price are year, mpg, mileage, and engineSize. In this question, you will use KK-fold cross validation to find out the relevant interactions of these predictors and the relevant interactions of the polynomial transformations of these predictors for predicting car price. We’ll consider quadratic and cubic transformations of each predictor, and the interactions of these transformed predictors. For example, some of the interaction terms that you will consider are (year2^2), (year)(mpg), (year2^2)(mpg), (year)(mpg)(mileage), (year)(mpg2^2)(mileage), (year)(mpg2^2)(mileage)(engineSize3^3), etc. The highest degree interaction term will be of degree 12 - (year3^3)(mpg3^3)(mileage3^3)(engineSize3^3), and the lowest degree interaction terms will be of degree two, such as (engineSize2^2), (engineSize)(mpg), etc.\nThe algorithm to find out the relevant interactions using KK-fold cross validation is as follows. Most of the algorithm is already coded for you. You need to code only step 2 as indicated below.\n\nStart with considering interactions of degree d = 2:\nFind out the 5-fold cross validation RMSE if an interaction of degree d is added to the model (You need to code only this step).\nRepeat step (2) for all possible interactions of degree d.\nInclude the interaction of degree d in the model that leads to the highest reduction in the 5-fold cross validation error as compared to the previous model (forward stepwise selection based on K-fold cross validation)\nRepeat steps 2-4 until no more reduction is possible in the 5-fold cross validation RMSE.\nIf d = 12, then stop, otherwise increment d by one, and repeat steps 2-5.\n\nThe above algorithm is coded below. The algorithm calls a function KFoldCV to compute the 5-fold cross validation RMSE given the interaction terms already selected in the model are selected_interactions, and the interaction term to be tested is interaction_being_tested. The function must return the 5-fold cross validation RMSE if the interaction_being_tested is included to the model consisting of year, mpg, mileage, and engineSize as predictors in addition to the already added interactions in selected_interactions. The model equation for which you need to find the 55-fold cross validation RMSE will be'price~year+mpg+mileage+engineSize' + selected_interactions + interaction_being_tested\nYou need to do the following:\n\nFill out the function KFoldCV.\nExecute the code to obtain the relevant interactions in selected_interactions. Print out the object selected_interactions.\nFit the model with the four predictors, the selected_interactions and compute the RMSE on test data.\n\nRelevance of this question to the linear regression prediction problem: Once you figure out the four most useful predictors to predict money_made_inv, use this algorithm to find out their useful interactions. Combine the model with the EDA-based insight of developing the model only where out_prncp_inv>0, and you should get a RMSE of less than 400 on the public leaderboard! (This algorithm is inspired by Victoria Shi’s solution to the linear regression prediction problem).\nNote that this brute-force approach of finding relevant interactions may work sometimes, especially when n>>pn>>p (the number of observations are much higher than the number of predictors), and/or if the relationship can approximated as a linear combination of polynomial interactions. However, it is unlikely to work in case of relatively less number of observations, and in case of highly non-linear relationships that are difficult to approximate with polynomial transformations. In such problems, a few minutes spent on EDA to figure out transformations, etc. may help develop a better model than this brute-force approach. For example, EDA-based transformations helped you get a RMSE of less than $350k in question C.1.6, while this approach gives a relatively much higher RMSE. Note that the data in C.1.6 has a relatively less number of observations.\n(10 (filling out the function) + 1 + 1 points)\n\n\nCode\nimport pandas as pd\nimport numpy as np\ntrainf = pd.read_csv('Car_features_train.csv')\ntrainp = pd.read_csv('Car_prices_train.csv')\ntestf = pd.read_csv('Car_features_test.csv')\ntestp = pd.read_csv('Car_prices_test.csv')\ntest = pd.merge(testf, testp)\ntrain = pd.merge(trainf, trainp)\n\n\n\n\nCode\n# Creating a dataframe that will consist of all combinations of polynomial transformations of the \n# predictors to be considered for interactions\n\npredictor_set = ['year','mpg','engineSize','mileage']\nfrom itertools import product\n\n#Considering quadratic and cubic transformations\nvalues = np.arange(0,4)\npolynomial_transformations = pd.DataFrame(product(values, repeat=4), columns=predictor_set).loc[1:,]\npolynomial_transformations.loc[:,'sum_degree'] = (polynomial_transformations).astype(int).sum(axis=1)\npolynomial_transformations.loc[:,'count_zeros'] = (polynomial_transformations == 0).astype(int).sum(axis=1)\npolynomial_transformations.sort_values(by = ['count_zeros', 'sum_degree'], ascending=[False, True], inplace=True)\npolynomial_transformations.drop(columns = ['count_zeros'], inplace=True)\npolynomial_transformations.reset_index(inplace = True, drop = True)\n\n\n\n\nCode\n#Setting the seed as we are shuffling the data before splitting it into K-folds\nnp.random.seed(123)\n# Shuffling the training set before creating K folds\ntrain = train.sample(frac=1)\nk = 5 #5-fold cross validation\nfold_size = np.round(train.shape[0]/k)\n\n\n\n\nCode\n# Fill out this function - that is all you need to do to make the code work!\n\n# The function must return the mean 5-fold cross validation RMSE for the model\n# that has the 4 individual predictors - 'year', 'mpg', 'mileage', and 'engineSize',\n# the 'selected_interactions', and the 'interaction_being_tested'\n\n# Uncomment the lines below and fill the function\n#def KFoldCV(selected_interactions, interaction_being_tested):\n    \n        # model = sm.ols('price~year+mpg+mileage+engineSize'+selected_interactions+\\\n        # interaction_being_tested, data = ...).fit()\n        \n    #return \n\n\n\n\nCode\n# This code implements the algorithm of systematically considering interactions of degree 2 and going upto \n# the interaction of degree 12. For a given degree 'd' the interactions are selected greedily based on \n# highest reduction in the 5-fold cross validation RMSE. Once no more reduction in the 5-fold cross validation\n# RMSE is possible using interactions of degree 'd', interaction terms of the next higher degree 'd+1' are considered.\n\n# 5-fold cross validation RMSE of the initial model with the 4 predictors of degree one\ncv_previous_model = KFoldCV(selected_interactions = '', interaction_being_tested = '')\ninteraction_being_tested = '+'\nselected_interactions = ''\n\n# Considering interactions of degree 'd' = 2 to 12\nfor d in np.arange(2,13):\n    \n    # Selecting interaction terms of degree = 'd'\n    degree_set = polynomial_transformations.loc[polynomial_transformations.sum_degree==d, :]\n    \n    # Initializing objects to store the interactions of degree 'd' that reduce the\n    # 5-fold cross validation RMSEs as compared to the previous model\n    interactions_that_reduce_KfoldCV = []; cv_degree = []; \n    \n    # Creating another DataFrame that will consist of the updated set of interactions of degree 'd' to be considered\n    # as interactions that do not reduce the 5-fold cross validation RMSE will be discarded\n    degree_set_updated = pd.DataFrame(columns = degree_set.columns)\n    \n    # Continue adding interactions of degree 'd' in the model until no interactions reduce \n    # the 5-fold cross-validation RMSE\n    while True:\n        \n        #Iterating over all possible interactions of degree 'd'\n        for index, row in degree_set.iterrows():\n            \n            # Creating the formula expression for the interaction term to be tested\n            for predictor in predictor_set:\n                interaction_being_tested = interaction_being_tested + ('I('+predictor +'**' +\\\n                                         str(row[predictor]) + ')*' if row[predictor]>1 else\\\n                                               predictor + '*' if row[predictor]==1 else '')\n            interaction_being_tested = interaction_being_tested[:-1]\n            \n            # Call the function 'KFoldCV' to find out the 5-fold cross validation error on adding the \n            # interaction term being tested to the model\n            cv = KFoldCV(selected_interactions, interaction_being_tested)\n            \n            # If the interaction term being tested reduces the 5-fold cross validation RMSE as compared to the\n            # previous model, then consider adding it to the model\n            if cv<cv_previous_model:\n                interactions_that_reduce_KfoldCV.append(interaction_being_tested)\n                cv_degree.append(cv)\n                degree_set_updated = pd.concat([degree_set_updated, row.to_frame().T])\n            interaction_being_tested = '+'\n        cv_data = pd.DataFrame({'interaction':interactions_that_reduce_KfoldCV, 'cv':cv_degree})\n        \n        # Sort the interaction terms that reduce the 5-fold cross valdiation RMSE based on their respective\n        # 5-fold cross validation RMSE\n        cv_data.sort_values(by = 'cv', inplace = True)\n        \n        # Break the loop if no interaction of degree 'd' reduces the 5-fold cross validation RMSE as\n        # compared to the previous model\n        if cv_data.shape[0]==0:\n            break\n            \n        # Select the interaction that corresponds to the least 5-fold cross validation RMSE\n        selected_interactions = selected_interactions + cv_data.iloc[0,0]\n        cv_previous_model = cv_data.iloc[0,1]\n        cv_degree = []; interactions_that_reduce_KfoldCV = []\n        degree_set = degree_set_updated.copy()\n        degree_set_updated = pd.DataFrame(columns = degree_set.columns)\n        \n        # Print the progress after each model update, i.e., after an interaction term is selected\n        print(\"Degree of interactions being considered:\",d, \", 5-fold CV RMSE:\", cv_previous_model)"
  },
  {
    "objectID": "Assignment E_updated.html",
    "href": "Assignment E_updated.html",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Tuesday, 7th March 2023 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (1 pt). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission (1 pt)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)"
  },
  {
    "objectID": "Assignment E_updated.html#calculating-root-mean-square-error-rmse-in-sklearn",
    "href": "Assignment E_updated.html#calculating-root-mean-square-error-rmse-in-sklearn",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "Calculating Root Mean Square Error (RMSE) in Sklearn",
    "text": "Calculating Root Mean Square Error (RMSE) in Sklearn\nsklearn.metrics has mean_squared_error function with a squared kwarg (default to True). Setting squared to False will return the RMSE\nfrom sklearn.metrics import mean_squared_error\nrmse = mean_squared_error(y_actual, y_predicted, squared=False)"
  },
  {
    "objectID": "Assignment E_updated.html#energy-model",
    "href": "Assignment E_updated.html#energy-model",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "Energy model",
    "text": "Energy model\nThe datasets ENB2012_Train.csv and ENB2012_Test.csv provide data on energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. Below is the description of the data columns:\n\nX1: Relative Compactness\nX2: Surface Area\nX3: Wall Area\nX4: Roof Area\nX5: Overall Height\nX6: Orientation\nX7: Glazing Area\nX8: Glazing Area Distribution\ny1: Heating Load"
  },
  {
    "objectID": "Assignment E_updated.html#e.1.1",
    "href": "Assignment E_updated.html#e.1.1",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.1 E.1.1",
    "text": "F.1 E.1.1\nSuppose that we want to implement the best subset selection algorithm to find the first order predictors (X1-X8) that can predict heating load (y1). How many models for EE(y1) are possible, if the model includes (i) one variable, (ii) three variables, and (iii) eight variables? Show your steps without running any code.\nNote: The notation EE(y1) means the expected value of y1 or the mean of y1.\n(3 points)"
  },
  {
    "objectID": "Assignment E_updated.html#e.1.2",
    "href": "Assignment E_updated.html#e.1.2",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.2 E.1.2",
    "text": "F.2 E.1.2\nImplement the best subset selection algorithm to find the best first-order predictors of heating load y1.\nNote:\n\nUse ENB2012_Train.csv and consider only the first-order terms.\nUse the BIC criterion for model selection.\n\n(4 points)"
  },
  {
    "objectID": "Assignment E_updated.html#e.1.3",
    "href": "Assignment E_updated.html#e.1.3",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.3 E.1.3",
    "text": "F.3 E.1.3\nShould RR-squared be used to select from among a set of models with different numbers of predictors? Justify your answer.\n(1 point for answer, 2 points for justification)"
  },
  {
    "objectID": "Assignment E_updated.html#e.1.4",
    "href": "Assignment E_updated.html#e.1.4",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.4 E.1.4",
    "text": "F.4 E.1.4\nCalculate the RMSE of the model found in E.2. Compare it with the RMSE of the model using all first-order predictors. You will find that the two RMSEs are similar. Seems like the best subset model didn’t help improve prediction.\n\nWhy did the best subset model not help improve prediction accuracy as compared to the model with all the predictors?\nDoes the best subset model provide a more accurate inference as compared to the model with all the predictors? Why or why npt?\n\nHint: Very Important Fact!\n(2 points for computing the RMSEs, 3 + 3 points for justifications)"
  },
  {
    "objectID": "Assignment E_updated.html#e.1.5",
    "href": "Assignment E_updated.html#e.1.5",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.5 E.1.5",
    "text": "F.5 E.1.5\nLet us consider adding all the 2-factor interactions of the predictors in the model. Answer the following questions without running code.\n\nHow many predictors do we have in total?\nAssume best subset selection is used. How many models are fitted in total?\nAssume forward selection is used. How many models are fitted in total?\nAssume backward selection is used. How many models are fitted in total?\nHow many models will be developed in the iteration that contains exactly 10 predictors in each model – for best subsets, fwd/bwd regression?\nWith sklearn.feature_selection.SequentialFeatureSelector, how many models will be deleloped when setting the n_features_to_select to 10 for forward selection and backward selection respectively?\n\n(62 = 12 points)*"
  },
  {
    "objectID": "Assignment E_updated.html#e.1.6",
    "href": "Assignment E_updated.html#e.1.6",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.6 E.1.6",
    "text": "F.6 E.1.6\nUse forward selection to find the best first-order predictors and 2-factor interactions of the predictors of y1 (Heating Load).\n(5 points)"
  },
  {
    "objectID": "Assignment E_updated.html#e.1.7",
    "href": "Assignment E_updated.html#e.1.7",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.7 E.1.7",
    "text": "F.7 E.1.7\nUse forward selection in sklearn.feature_selection.SequentialFeatureSelector to find the best first-order predictors and 2-factor interactions of the predictors of y1 (Heating Load), setting the n_features_to_select to the number of predictors of the best model found in E.1.6.\nIs the best subset found using sklearn the same as the one found in E.1.5. why or why not?\n(5 points)"
  },
  {
    "objectID": "Assignment E_updated.html#e.1.8",
    "href": "Assignment E_updated.html#e.1.8",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.8 E.1.8",
    "text": "F.8 E.1.8\nCalculate the RMSE of the model found in E.1.7. Compare it with:\n\nthe RMSE of model you found in E.1.2 and,\nthe RMSE of the model using all the predictors and all their 2-factor interaction terms.\n\nAmong the 4 models (the model developed in E.1.2, the model developed in E.1.7, the model that has all the predictors and all their 2-factor interactions), discuss which model will you prefer for prediction, and which model will you prefer for inference, and why?\n(2 points for computing the RMSEs, 3 + 3 points for discussion)"
  },
  {
    "objectID": "Assignment E_updated.html#e.1.9",
    "href": "Assignment E_updated.html#e.1.9",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.9 E.1.9",
    "text": "F.9 E.1.9\nAssume that we found another dataset of 32 variabels on the same set of 768 buildings (542 for training) that we would want to add into our model. We want find the “best” model of all 40 predictors and their 2-factor interaction terms. Would you choose forward or backward selection? Justify your answer.\n(1 point for answer, 4 points for justification)"
  },
  {
    "objectID": "Assignment E_updated.html#planetary-radius-model",
    "href": "Assignment E_updated.html#planetary-radius-model",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "Planetary radius model",
    "text": "Planetary radius model\nSee https://exoplanetarchive.ipac.caltech.edu (for context/source). We are using the Composite Planetary Systems dataset"
  },
  {
    "objectID": "Assignment E_updated.html#e.2.1",
    "href": "Assignment E_updated.html#e.2.1",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.10 E.2.1",
    "text": "F.10 E.2.1\nSay we’re interested in modeling the radius of exoplanets in kilometers, which is named as pl_rade in the data. Note that the variable pl_rade captures the radius of each planet as a proportion of Earth’s radius, which is approximately 6,378.1370 km.\nDevelop a linear regression model to predict pl_rade using all the variables in train_CompositePlanetarySystems.csv except pl_name, disc_facility and disc_locale. Find the RMSE (Root mean squared error) of the model on test1_CompositePlanetarySystems.csv and test2_CompositePlanetarySystems.csv.\n(4 points)"
  },
  {
    "objectID": "Assignment E_updated.html#e.2.2",
    "href": "Assignment E_updated.html#e.2.2",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.11 E.2.2",
    "text": "F.11 E.2.2\nDevelop a ridge regression model to predict pl_rade using all the variables in train_CompositePlanetarySystems.csv except pl_name, disc_facility and disc_locale. What is the optimal value of the tuning parameter λ\\lambda?\nHint: You may use the following grid of lambda values to find the optimal λ\\lambda: alphas = 10**np.linspace(2,0.5,200)*0.5\nRemember to standardize data before fitting the ridge regression model\n(5 points)"
  },
  {
    "objectID": "Assignment E_updated.html#e.2.3",
    "href": "Assignment E_updated.html#e.2.3",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.12 E.2.3",
    "text": "F.12 E.2.3\nUse the optimal value of λ\\lambda found in the previous question to develop a ridge regression model. What is the RMSE of the model on test1_CompositePlanetarySystems.csv and test2_CompositePlanetarySystems.csv?\n(5 points)"
  },
  {
    "objectID": "Assignment E_updated.html#e.2.4",
    "href": "Assignment E_updated.html#e.2.4",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.13 E.2.4",
    "text": "F.13 E.2.4\nNote that ridge regression has a much lower RMSE on test datasets as compared to Ordinary least squares (OLS) regression. Shrinking the coefficients has reduced the variance of the estimated coefficents with a little increase in bias, thereby improving the model fit. Appreciate it. Which are the top two predictors for which the coefficients have shrunk the most?\nTo answer this question, find the ridge regression estimates for λ=10−10\\lambda = 10^{-10} (almost zero regularization). Treat these estimates as OLS estimates and find the predictors for which these estimates have shrunk the most as compared to the model developed in E.2.3.\n(4 points for code, 1 point for answer)"
  },
  {
    "objectID": "Assignment E_updated.html#e.2.5",
    "href": "Assignment E_updated.html#e.2.5",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.14 E.2.5",
    "text": "F.14 E.2.5\nWhy do you think the coefficients of the two variables identified in the previous question shrunk the most?\n(4 points for justification - including code used)"
  },
  {
    "objectID": "Assignment E_updated.html#e.2.6",
    "href": "Assignment E_updated.html#e.2.6",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.15 E.2.6",
    "text": "F.15 E.2.6\nDevelop a lasso model to predict pl_rade using all the variables in train_CompositePlanetarySystems.csv except pl_name, disc_facility and disc_locale. What is the optimal value of the tuning parameter λ\\lambda?\nHint: You may use the following grid of lambda values to find the optimal λ\\lambda: alphas = 10**np.linspace(0,-2.5,200)*0.5\n(4 points)"
  },
  {
    "objectID": "Assignment E_updated.html#e.2.7",
    "href": "Assignment E_updated.html#e.2.7",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.16 E.2.7",
    "text": "F.16 E.2.7\nUse the optimal value of λ\\lambda found in the previous question to develop a lasso model. What is the RMSE of the model on test1_CompositePlanetarySystems.csv and test2_CompositePlanetarySystems.csv?\n(5 points)"
  },
  {
    "objectID": "Assignment E_updated.html#e.2.8",
    "href": "Assignment E_updated.html#e.2.8",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.17 E.2.8",
    "text": "F.17 E.2.8\nNote that lasso has a much lower RMSE on test datasets as compared to Ordinary least squares (OLS) regression. Shrinking the coefficients has improved the model fit. Appreciate it. Which variables have been eliminated by lasso?\nTo answer this question, find the predictors whose coefficients are 0 in the lasso model.\n(2 points for code, 1 point for answer)"
  },
  {
    "objectID": "Assignment E_updated.html#e.3",
    "href": "Assignment E_updated.html#e.3",
    "title": "Appendix F — Assignment E (Section 22)",
    "section": "F.18 E.3",
    "text": "F.18 E.3\nWe haves used car_features_train.csv and car_prices_train.csv in class notes to predict car price. Based on correlation with price, the four most relevant continuous predictors to predict car price are year, mpg, mileage, and engineSize. In this question, you will use KK-fold cross validation to find out the relevant interactions of these predictors and the relevant interactions of the polynomial transformations of these predictors for predicting car price. We’ll consider quadratic and cubic transfromations of each predictor, and the interactions of these transformed predictors. For example, some of the interaction terms that you will consider are (year2^2), (year)(mpg), (year2^2)(mpg), (year)(mpg)(mileage), (year)(mpg2^2)(mileage), (year)(mpg2^2)(mileage)(engineSize3^3), etc. The highest degree interaction term will be of degree 12 - (year3^3)(mpg3^3)(mileage3^3)(engineSize3^3), and the lowest degree interaction terms will be of degree two, such as (engineSize2^2) or (engineSize)(mpg), etc.\nThe algorithm to find out the relevant interactions using KK-fold cross validation is as follows. Most of the algorithm is already coded for you. You need to code only part 2 as indicated below.\n\nStart with considering interactions of degree d = 2:\nFind out the 5-fold cross validation RMSE if an interaction of degree d is added to the model (You need to code only this part).\nRepeat step (2) for all possible interactions of degree d.\nInclude the interaction of degree d in the model that leads to the highest reduction in the 5-fold cross validation error as compared to the previous model (forward stepwise selection based on K-fold cross validation)\nRepeat steps 2-4 until no more reduction is possible in the 5-fold cross validation RMSE.\nIf d = 12, then stop, otherwise increment d by one, and repeat steps 2-5.\n\nThe above algorithm is coded below. The algorithm calls a function KFoldCV to compute the 5-fold cross validation RMSE given the interaction terms already selected in the model as selected_interactions, and the interaction term to be tested as interaction_being_tested. The function must return the 5-fold cross validation RMSE if interaction_being_tested is included to the model consisting of year, mpg, mileage, and engineSize in addition to the already added interactions in selected_interactions. The features for which you need to find the 55-fold cross validation RMSE will be year+mpg+mileage+engineSize'+selected_interactions+interaction_being_tested\nYou need to do the following:\n\nFill out the function KFoldCV.\nExecute the code to obtain the relevant interactions in selected_interactions. Print out the object selected_interactions.\nFit the model with the four predictors, the selected_interactions and compute the RMSE on test data.\n\nRelevance of this question to the linear regression prediction problem: Once you figure out the four most useful predictors to predict money_made_inv, use this algorithm to find out their useful interactions. Combine the model with the EDA-based insight of developing the model only where out_prncp_inv>0, and you should get a RMSE of less than 400 on the public leaderboard! (This algorithm is inspired by Victoria Shi’s solution to the linear regression prediction problem).\nNote that this brute-force approach of finding relevant interactions may work sometimes, especially when n>>pn>>p (the number of observations are much higher than the number of predictors). However, in certain problems, a few minutes spent on EDA to figure out transformations, etc. will help develop a better model than this brute force approach. For example, EDA-based transformations helped you get a RMSE of less than $350k on question C.1.6, which is not possible with this approach.\n(10 (filling out the function) + 1 + 1 points)\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n\n\n\nCode\n#build the train dataset\npredictor_set = ['year','mpg','engineSize','mileage']\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv',index_col=0)\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv',index_col=0)\nfrom sklearn.preprocessing import PolynomialFeatures\ndef make_dataset(features_df, target_df, max_degree=3):\n  poly = PolynomialFeatures(max_degree, include_bias=False)\n  X=poly.fit_transform(features_df[predictor_set].values)\n  df = pd.DataFrame(X, columns=poly.get_feature_names_out(predictor_set), index=features_df.index)\n  df['price']=target_df\n  df=df.sample(frac=1)\n  return df\ntrain=make_dataset(trainf,trainp, 12)\n\n\n\n\nCode\n# Fill out this function - that is all you need to do to make the code work!\n\n# The function must return the mean 5-fold cross validation RMSE for the model\n# that has the 'selected_interactions', and the 'interaction_being_tested'\ndef KFoldCV(selected_interactions, interaction_being_tested=None):\n  \"\"\"\n  All the variable names can be found in `train` dataframe.\n  selected_interactions: List[String] ->  the list of variable names that current selected\n  interaction_being_tested: String    ->  a single variable name that are testing.\n  return: float -> mean of RSME of 5-folder validation.\n  \"\"\"\n  return 0\n\n\n\n\nCode\n# This code implements the algorithm of systematically considering interactions of degree 2 and going upto \n# the interaction of degree 12\nhistory=[]\n\nselected_interactions = list(predictor_set)\ncv_previous_model = KFoldCV(selected_interactions = selected_interactions, interaction_being_tested = None)\nhistory.append([cv_previous_model, \",\".join(selected_interactions)])\n\nprint(\"Initially, RMSE={}, features={}\".format(cv_previous_model, selected_interactions))\n\ncandidates = [col for col in train.columns if col not in predictor_set +['price']]\n\nfor interaction_being_tested in tqdm(candidates):\n  cv=KFoldCV(selected_interactions.copy(), interaction_being_tested)\n  if cv<cv_previous_model:\n    selected_interactions.append(interaction_being_tested)\n    cv_previous_model=cv \n    history.append([cv_previous_model, \",\".join(selected_interactions)])\n\n\nInitially, RMSE=9561.448654675505, features=['year', 'mpg', 'engineSize', 'mileage']\n\n\n100%|██████████████████████████████████████████████████████████████████████████████| 1815/1815 [00:40<00:00, 45.37it/s]"
  },
  {
    "objectID": "Practice_Final_Answer_Key.html",
    "href": "Practice_Final_Answer_Key.html",
    "title": "Appendix G — Practice Final Solutions",
    "section": "",
    "text": "Presence of which of the following potential problems in a linear regression model may lead to statistically significant variables appearing insignificant?\n\nMulticollinearity\nOutliers\nOverfitting\n\nAnswer: A and B\nExplanation:\nA) Multicollinearity:\nRecall, the estimated variance of the coefficient \\(\\beta_j\\), of the \\(j^{th}\\) predictor \\(X_j\\), can be expressed as:\n\\[\\hat{var}(\\hat{\\beta_j}) = \\frac{(\\hat{\\sigma})^2}{(n-1)\\hat{var}({X_j})}.\\frac{1}{1-R^2_{X_j|X_{-j}}}         \\hspace{5cm} (1)\\] \nIf the predictor \\(X_j\\) is collinear with other predictors, \\(R^2_{X_j|X_{-j}}\\) will be large, which in turn will inflate \\(\\hat{var}(\\hat{\\beta_j})\\). In other words, multicollinearity inflates the standard errors of the coefficients for which the variables are collinear. Since \\(t\\)-statistic is calculated by dividing the estimated coefficient by its standard error, the \\(t\\)-statistics shrinks, and the corresponding \\(p\\)-value increases. Therefore, the hypothesis test loses the power to reject the null hypotheses, and thus statistically significant variables appearing insignificant. \nAnother way to think about this can be that if some predictors are collinear, it can be difficult to separate out the individual effects of these variables in the response and significant variables may appear insignificant. \nB) Outliers\nRecall, the estimate of error variance is given by:\n\\[\\hat{\\sigma}^2 = {\\frac{RSS}{n-2}},\\] where RSS is the residual sum of squared errors. Outliers result in an increase in \\(RSS\\), leading to an increase in the estimated error variance \\(\\hat{\\sigma}^2\\), which in turn inflates \\(\\hat{var}(\\hat{\\beta_j})\\). The rest of the explanation follows from the previous explanation on multicollinearity.\n\nC) Overfitting\nOverfitting shrinks \\(RSS\\), which in turn shrinks \\(\\hat{\\sigma}^2\\), thereby shrinking \\(\\hat{var}(\\hat{\\beta_j})\\). Thus overfitting will act in way opposite to what we observe in (A) and (B). \n\n\n\nClassify a data point as influential / outlier / high leverage in a linear regression model, based on the description.\n\nThe data point is likely to have a large effect on the model in terms of prediction: Influential point\nThe data point has the potential to have a large effect on the model in terms of prediction: High leverage point\nThe data point is likely to inflate the model R-squared: High leverage point that is not influencial \nThe data point is unlikely to have a large effect on the model in terms of prediction: outlier\n\n\nExplanation:\nSee the graphics in class presentation on Chapter3_Outliers_high_leverage_influential_points. Think of influential points / high leverage points / outliers as a force (proportional to the residual corresponding to the point) pulling a canteliver beam. Depending on the position from where you pull the cantilever beam, you may move it too much or too little.\nA) Inluential point (high leverage & outlier): an outlier with the respect to both the predictor and the response. It has a large effect on the regression line. As shown in the graphics, influence is higher for more extreme outliers with same leverage and for points with higher leverage & similar outlying distance.\nB) High leverage point: Observations with high leverage have an unusual value for the predictor (ie. lie outside the domain of most points). High leverage point has the potential to have a large affect on the regression line. It is cause for concern if the least squares line is heavily affected by just a couple of observations, because any problems with these points may invalidate the entire fit.\nC) If you have a high leverage point that is not influencial: The variance of the response may increase in the presence of high leverage points, since an unusual set of predictor values may correspond to an unusual response, which may increase the total variation. However, as the point is not inluential, the increase in the unexplained variation (the squared residual) will not be proportionate to the increase in total variation. As \\(R^2\\) is one minus the ratio of unexplained variation to total variation, it is likely to increase.\nD) Outliers: As shown in the graphics, outliers very small effect on prediction.\n\n\n\nA linear regression model was developed to predict the number of passengers taking a flight per month. The data consists of number of passengers flying each month from January 1949 to December 1960. The autocorrelation plot below shows the correlation of the residuals with the lagged residuals of the model. Choose the most appropriate option.\n\n\n\n\n\n\nThe above plot shows the presence of autocorrelation. The 6-month lagged response is the most appropriate lag to be added as a predictor in the model to address autocorrelation\nThe above plot shows the presence of autocorrelation. The 12-month lagged response is the most appropriate lag to be added as a predictor in the model to address autocorrelation\nThe above plot shows the presence of autocorrelation. The 1-month lagged response is the most appropriate lag to be added as a predictor in the model to address autocorrelation\nThe above plot shows the absence of autocorrelation as the plot must have a cyclical pattern in the presence of autocorrelation\nThe above plot shows the absence of autocorrelation as the one month lagged residual must have the highest correlation with the residual in the presence of autocorrelation\n\nAnswer: B\nExplanation: As seen in the plot, the residuals are highly correlated (correlation of more than 60%) with lagged residuals of 12 months. This shows the presence of autocorrelation. To address autocorrelation, the 12-month laggged response will be the most appropriate as it has the highest correlation with the response. Thus, it will explain the variation in the respone the most. \nThere is no need for there to be a cyclical pattern for autocorrelation. Even if one of the lagged residuals are highly correlated with the residual, it shows the presence of autocorrelation.\n\n\n\nWhich of the following metrics can be used to assess the goodness-of-fit of a logistic regression model?\n\nAll of these\nLL-Null\nLog-Likelihood\nDf Model\nR-squared\n\nAnswer: Log-Likelihood\nExplanation In logistic regression, the response is assumed to follow a Bernoulli distribution, where the probability of success is a function of the predictors and its coefficients (the model parameters). With this assumption, one can compute the the joint probability density of the observed data as a function of the model parameters. This creates a set of probability distributions (based on different values of model parameters) that could have generated the data. The algorithm finds the values of the model parameters (the beta coefficients) such that the probability of observing the data maximizes. This probability is the likelihood, and its logarithm is the log-likelihood. The higher the log-likelihood, the more probable it is to observe the data. Thus, log-likelihood is a way to measure the goodness-of-fit of the model.\nLL-NULL is the log-likelihood of the model with no parameters. This is compared with the log-likelihood of the model with predictors to test if the regression is statistically significant. \nDf Model is the number of predictors in the model.\nR-squared cannot be used for logistic regression as there are no residuals.\n\n\n\nFor a logistic regression model, as we increase the decision threshold probability,\n\nNone of these\nthe recall will reduce or stay the same\nthe ROC-AUC will increase or stay the same\nthe precision will increase or stay the same\nthe classification accuracy will increase or stay the same\n\nAnswer: B\nExplanation: See class slide on the confusion matrix below. \n\n\n\n\n\nIncreasing threshold probability means that less observations are predicted to be positive. Hence, some TP could turn into FN, reducing the recall. (this might not happen if there is no observations of actual positives between the thresholds). ROC-AUC is independent of the threshold probability. Both precision and classification accuracy might decrease if the number of FP among actual negatives increase more than the increase of TP among actual positives by the shift in the threshold.\n\n\n\nWhich of the following metrics is independent of the decision threshold probability?\n\nNone of these\nROC-AUC\nAll of these (except the “None of these” option)\nPrecision\nRecall\n\nAnswer: ROC-AUC \nExplanation By changing the threshold, the number of points classified as negative and positive may change, and so TP, FP, TN and FN may change. Recall and precision may change as they are based on these metrics (TP ,FP, TN, and FN). However, the ROC-AUC specifically analyzes different thresholds. The ROC curve is a plot of TPR against FPR for all possible thresholds, and ROC-AUC is the area under the ROC curve, so the value itself is independent from the decision threshold probability. \n\n\n\nConsider the following logistic regression model:\n\\[p(x) =\\frac{1}{1+e^{-(\\beta_0+\\beta_1x)}}\\].\nWhich of the following metrics will depend on the value of x?\n\nOdds ratio when x increases by 2 units\nincrease in log odds when x increases by 10 units\nAll of these\nIncrease in predicted probability when x increases by 1 unit\n\nE)none of these\nAnswer: D\nExplanation: \n\\[p(x) =\\frac{1}{1+e^{-(\\beta_0+\\beta_1x)}}\\]\n\\[\\implies \\log\\bigg(\\frac{p(x)}{1-p(x)}\\bigg) = \\beta_0 + \\beta_1x\\]\n\\[\\implies \\log\\big(Odds(x)\\big) = \\beta_0 + \\beta_1x\\]\nWhen \\(x\\) increases by ‘c’ units,\n\\[p(x+c) - p(x) =\\frac{1}{1+e^{-(\\beta_0+\\beta_1(x+c))}}-\\frac{1}{1+e^{-(\\beta_0+\\beta_1(x))}}\\]\n\\[log({Odds(x+c)}) - log({Odds(x)}) = \\beta_1 c\\]\n\\[\\frac{Odds(x+c))}{Odds(x)} = e^{\\beta_1 c}\\]\nWe can see that only the increase in predicted probability when \\(x\\) increases by 1 unit is dependent on \\(x\\).\n\n\n\nWe develop a logistic regression model to predict whether someone will pay a loan back or not. Loans are “approved” by us only for those borrowers who are predicted to pay back. The positive class is the borrowers that pay back the loans. What would a recall of 81% mean?\n\n81% of the borrowers that would pay back the loan are approved by us: Recall = TP/(TP + FN). TP here are those who are [approved by us] who [pay back the loan], while FN are those who were [not approved by us] but actually [pay back the loans]. The denominator is [all who pay back the loan]. Thus, Recall here means: among [all who pay back the loan], 81% are [approved by us].\nOf all the loans we approve, 81% pay us back: This is Precision = TP/(TP + FP)\nOf all the loans we don’t approve, 81% would not have paid us back if they were given the loan: This is the proportion of negatives correctly predicted - like precision for the negative class\nOf all the loans we don’t approve, 19% would not have paid us back if they were given the loan: This is the proportion of negatives incorrectly predicted.\n\nAnswer: 81% of the borrowers that would pay back the loan are approved by us.\nExplanation: Recall = True Positives/(True Positives + False Negatives).\nIn this case, True positives are those who got approved and would pay back. False Negatives are those we didn’t approve, but would pay back. Therefore, 81% Recall means 81% of the borrowers that would pay back the loan are approved by us.\n\n\n\nWhich of the following algorithms can be used for variable selection?\n\nLasso\nRidge regression\nForward stepwise selection\nBest subset selection\n\nAnswer: A,C,D\nExplanation: Both lasso and ridge regression are regularized least squares model, where the a shrinkage penalty is added to the ordinary least squares cost function. The shrinkage penalty in ridge regression shrinks the regression coefficients estimate towards zero, but not exactly zero, while the shrinkage penalty in lasso tends to give a set of zero regression coefficients and leads to a sparse model. Therefore, lasso can be used for variable selection, but not ridge regression.\nForward stepwise and best subset selection are variable selection algorithms by fitting multiple models having different combinations/number of predictors and choosing the best model.\n\n\n\nYou are building a facial recognition model to allow people to unlock their phone. If the phone recognizes the person as the authorized user, it will unlock the phone. If it doesn’t recognize the user, it will prompt them to try again or try an alternative method (such as a passphrase). The facial recognition model is a classification model that identifies if the person unlocking the phone is the authorized user (positive response) or not (negative response).\nAssume that letting a stranger (unauthorized user) unlock the phone is more risky (or more expensive) than not letting the authorized user unlock the phone.\nWhich of the following metric is the most important to optimize in the model?\n\nPrecision\nClassification accuracy\nRecall\nROC-AUC\n\nAnswer: Precision\nExplanation:\nA) Precision: Precision = TP/(TP + FN). Here, FN are those who are falsely assigned as an unauthorized user when they are actually the authorized user. FP are those who are assigned as the authorized user and are actually an unauthorized user. In this case, it’s important to optimize precision because it is more important to reduce the number of FP (strangers being recognized as the authorized user) than to reduce the number of FN (authorized user not being recognized).\nB) Classification accuracy: This is incorrect because a model with high accuracy but a high FPR would be unacceptable since it would increase the risk of a stranger unlocking the phone. \nC) Recall: This is incorrect because a high recall indicates that many of the positive cases are being detected. However, it does not measure the fraction of unauthorized users that the model identifies as authorized. A high FPR could lead to an unauthorized user unlocking the phone, which is a more expensive mistake than an FN.\nD) ROC-AUC: This is incorrect because ROC-AUC does not take into account the cost of the positive and negative classes. It only measures how well the model can distinguish between authorized users and unauthorized users. \n\n\n\nConsider the following logistic regression model:\n\\[p(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1  x_1 + \\beta_2 x_2)}}\\]\nwhere assuming the threshold probability for classifying observations is 0.5. All observation with predicted probability greater than 0.5 are classified as belonging to class \\(y=1\\), while others are classified as belonging to class \\(y=0\\).\nWhich of the following plots correctly visualizes the predicted class based on \\(x_1\\) and \\(x_2\\)?\n\n\n\n\n\nAnswer: D\nExplanation:\n\\(x_1\\) will not have an impact on the outcome because its coefficient is 0. When \\(x_2>5, p(x)\\) will be less than \\(0.5\\) and \\(y\\) will equal 0, as the decisions threshold probability ois 0.5. When \\(x_2<5, p(x)\\) will be greater than \\(0.5\\) and \\(y\\) will equal \\(1\\). \n\n\n\nIn which of the following cases will ROC-AUC be the most appropriate metric to optimize among the all the performance metrics we have seen in this course.\n\nThere are wide disparities in the cost of false negatives vs. false positives, for example, predicting if the person has a serious disease.\nThe predicted probabilities will be used to rank observations, instead of classifying them, for example, the Google search engine using the predicted probabilities to rank pages in the decreasing order of relevance to the search query, instead of classifying the observations as ‘relevant’ and ‘not relevant’.\nWe wish to maximize the overall classification accuracy, for example, predicting if a person will vote for the Democrat or the Republican candidate in the US Presidential elections. Here, you may assume that the cost of false positives is similar to the cost of false negatives.\n\nAnswer: (B) only \nExplanation:\n(A) is incorrect because in cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize the performance metric associated with a higher loss. For example when predicting if the person has a serious disease, a false positive could lead to expensive and unnecessary medical treatment. Conversely, a false negative could result in a delay in diagnosis and treatment, potentially leading to a worse outcome. Thus, we want to prioritize minimizing false negatives. Since ROC-AUC is decision-threshold invariant, it’s not a useful metric for this type of optimization.\n(B) is correct. ROC-AUC is scale-invariant. It measures how well predictions are ranked, rather than the absolute values of the predicted probabilities. Check the link. \n(C) is incorrect because AUC is classification-threshold-invariant. It measures the quality of the model’s predictions irrespective of what classification threshold is chosen. However, the overall accuracy changes with change in decision threshold probability. To maximize overall accuracy, we need to find the optimal decision threshold probability.\n\n\n\nWhich of the following linear model selection methods can be used when number of predictors is greater than the number of observations in linear regression?\n\nLasso\nRidge regression\nForward stepwise selection\nBackward stepwise selection\nBest subset selection\n\nAnswer: A, B, C\nExpanation:\nWhen number of predictors is greater than the number of observations, then, in case of ordinary least squares regression, the number of parameters are greater than the number of equations available to estimate those parameters. Thus, there is no unique solution. Also, in equation (1), \\(R^2_{X_j|X_{-j}}\\) is 1, and so \\(\\hat{var}(\\hat{\\beta_j})\\) tends to infinity. Thus, it is not possible to fit an ordinary least squares model in this case. As backward stepwise selection begins with a model considering all predictors, it cannot be used as it is not possible to develop a model with all predictors in this case. \nThe best subset selection must consider models with all possible combination/number of predictors. Howvever, as number of predictors cannot be greater than the number of observations, we cannot develop all possible models, and thus we cannot use best subset selection.\nForward stepwise starts with no predictors and adds one predictor at a time. It is possible to keep adding predictors until the number of predictors (excluding the intercept) is one less than the number of observations. From this set of models, the best model can be chosen based on AIC, BIC, or any goodness-of-fit criteria that accounts for the number of predictors. Thus, it is possible to use forward stepwise selection.\nThe shrinkage penalty in lasso and ridge regression reduces the variance of the coefficents. The variance is infinity without any penalty when the number of predictors is greater than the number of observations. However, as lasso and ridge regression shrink the penalty, it is possible to obtain a unique solution.\nThe following is just for your information, but beyond the scope of this course: With lasso, there will be at most as many non-zero predictors as the number of observations. With ridge regression, there may be more non-zero predictors as compared to the number of observations.\n\n\n\nWhich of the following metrics can be used to compare the goodness-of-fit of models with different number of predictors?\n\nAIC (Akaike Information criterion)\nR-squared\nLog-Likelihood\nPseudo R-squared\nLLR p-value\n\nAnswer: AIC\nExplanation: AIC is correct because it takes into account both the goodness-of-fit of the model and the complexity of the model (number of predictors). \n\\[AIC = -2logL + 2d,\\]\nwhere \\(L\\) is the maximized value of the likelihood function for the estimated model, and \\(d\\) is the number of predictors. From the above equation, we can see that \\(AIC\\) penalizes models with more parameters. Therefore, AIC allows for comparison between models with different numbers of predictors and helps to determine which model is the best fit.\nAll the other metrics will increase, while the LLR p-value will decrease with increase in number of predictors, and thus cannot be used to compare models with different number of predictors.\n\n\n\nGiven a set of predictors, which of the following model selection methods guarantees to provide the best ‘least squares’ linear regression model, based on adjusted R-squared?\n\nBest subset selection\nForward stepwise selection\nBackward stepwise selection\nLinear regression with all the statistically insignificant predictors removed\n\nAnswer: A\nExplanation: Best subset selection considers every single model possible, while forward and backward selection don’t. Therefore, best subset selection necessarily gives the best model, while stepwise selection methods do not. For example, if a model consisting of predictors \\(x_2\\) and \\(x_3\\) is the best possible model, with regards to adjusted \\(R\\)-squared, while the model consisting of \\(x_1\\) is the best one predictor model, then forward stepwise selection will fail to identify the best possible model.\nAdjusted \\(R\\)-squared depends on the residuals, among other things. However, the residuals don’t relate directly to statistical significance of predictors. Statistical significance of a predictor implies that the predictor is significantly linearly associated with the response, but it does not determine the variation in response explained by the predictor.\n\n\n\nWhich of the following metrics gives the least biased estimate of MSE (mean squared error) on test data?\n\nLeave-one-out cross validation error\nMSE (mean squared error) on a test dataset (or validation set)\nK-fold cross validation error, where 1<k<n, where n is the number of observations\nAll of these\n\nAnswer: Leave-one-out cross validation error \nExplanation: Leave-one-out cross-validation offers two advantages:\n- It provides a much less biased measure of test MSE compared to using a single test set because we repeatedly fit a model to a dataset that contains n-1 observations. - It tends not to overestimate the test MSE compared to using a single test set.\nTextbook p200 Details: The test MSE gives us an idea of how well a model will perform on data it hasn’t previously seen, i.e. data that wasn’t used to “train” the model.\nHowever, the drawback of using only one testing set is that the test MSE can vary greatly depending on which observations were used in the training and testing sets.\nOne way to avoid this problem is to fit a model several times using a different training and testing set each time, then calculating the test MSE to be the average of all of the test MSE’s.\nLike the validation set approach, LOOCV involves splitting the set of observations into two parts (test & train). However, instead of creating two subsets of comparable size, LOOCV uses a single observation \\((x_1,y_1)\\) for the validation set, and the remaining observations \\({(x_2, y_2), . . . , (x_n, y_n)}\\) for the training set.\nThe statistical learning method is fit on the \\(n − 1\\) training observations, and a prediction \\(\\hat{y_1}\\) is made for the excluded observation using its value \\(x_1\\). Since \\((x_1,y_1)\\) was not used in the fitting process, \\(MSE_1 = (y_1 − \\hat{y_1})^2\\) provides an approximately unbiased estimate for the test error.\n\n\n\nSuppose you have a categorical predictor gender in the dataset with 3 distinct values - ‘male’, ‘female’, and ‘other’. Following are three ways to transform this predictor to make it suitable for forward stepwise selection. Which method is likely to provide the best model and which method is likely to provide the worst model, with regard to prediction accuracy on unknown (test) data?\n\nUse the predictor gender as it is for forward stepwise selection.\nConvert the predictor to 3 dummy variables - ‘male’, ‘female’, and ‘other’, where each dummy variable has 0s and 1s, depending on the ‘gender’, and use the dummy variables for forward stepwise selection, instead of gender\nReplace the values of ‘male’ to 0, ‘female’ to 1, and ‘other’ to 2 in ‘gender’, and then use gender in forward stepwise selection\nB is likely to provide the best model and, A or C are likely to provide the worst model\nC is likely to provide the best model and A is likely to provide the worst model\nC is likely to provide the best model and B is likely to provide the worst model\nB is likely to provide the best model and A is likely to provide the worst model\nNone of these\n\nAnswer:  B is likely to provide the best model, and A or C are likely to provide the worst model.\nExplanation:\nB is likely to provide the best model because breaking down a categorical variable into dummy variables allows us to perform stepwise selection for each class in gender. So it gives the algorithm the option to choose among more models than using the predictor gender as it is in the selection.\nC is likely to provide a worse model as compared to B since it introduces an unreasonable constraint in the model that holding all other predictors constant, the difference in response between female and male is the same as the difference in response between other and female. In other words, we are converting categorical variables into ordinals which is constraining the model to find the relationship of the predictor in the order of male-female-other even though that may not be the case.\nA is likely to provide a worse model as compared to B because it introuces a constraint in the model to either include all genders, or no gender. If only the female gender explains some variation in response, while there is no distinction between male and other, then the model should not be forced to keep the male and other categories. Suppose the other category happens to have a very few observations leading to an unstable coefficient (high standard error), then it may reduce the prediction accuracy.\n\n\n\nWhich of the following statements regarding ROC-AUC (area under the ROC curve) are true, with regard to a binary classification problem where the classes are named as ‘positive’ and ‘negative’?\n\nROC-AUC is zero if the model makes random predictions\nThe larger the ROC-AUC the better the performance of a logistic regression model\nROC-AUC is as the probability that the model ranks a random positive observation more highly than a random negative observation, where a higher rank corresponds to a higher predicted probability\nThe ROC-AUC can never be negative\n\nAnswer: B, C, D\nExplanation:\nA) is incorrect because ROC-AUC is 50% if the model makes random predictions.\nB) is correct because AUC ranges from 0-1. The larger the ROC-AUC, the better the model is distinguishing between the classes. ROC-AUC is 1.0 when there is perfect seperation of classes based on the predicted probability.\nC) is correct because AUC is the probability that a randomly selected positive observation has a higher predicted probability as compared to a randomly selected negative observation.\nD) is correct because ROC-AUC is a probability.\n\n\n\nThe lasso, relative to least squares, is:\n\nMore flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance\nLess flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance\nLess flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias\nMore flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias\n\nAnswer: B \nExplanation: \nThe cost function for lasso consists of the sum of absolute value of coefficients called the shrinkage penalty or the regularization term. This term helps to reduce the complexity of the model by shrinking the regression coefficients towards zero. Because of this penalty, lasso is less flexible (compared to the least squares), as it restricts the range of possible coefficient values, whereas least squares can assign any value to the coefficients. The decreased flexibility means that lasso has less variance in its predictions and more bias. \nThe penalty terms in the optimization will lead to bias in estimates, leading to less accuracy in prediction. At the same time, reducing the size of coefficients gives them less variance, increasing the accuracy of prediction. Thus, when the increase in bias is less than the decrease in variance, it can lead to improved prediction accuracy. This is a bias-variance trade-off.\n\n\n\nArrange the following model selection methods in increasing order of computational complexity:\nRidge regression, best subset selection, forward stepwise selection\nAnswer: Ridge regression, forward stepwise selection, best subset selection \nExplanation: Best subset selection requires the most computation complexity because it considers all \\(2^p\\) possible models containing subsets of the \\(p\\) predictors. It cannot be used in case of even a slightly large number of predictors. For example, in case of 30 predictors, more than a billion models will need to be developed to find the best subset model.\nIn forward stepwise selection, the total number of models with \\(p\\) predictors is \\(\\frac{p(p+1)}{2}\\). In case of 30 predictors, this will 435 models.\nIn ridge regression, we need to fit only one model.\n\n\n\nFor optimizing parameters of a model, \\(K\\)-fold cross validation is preferred over the validation set approach (computing error on a test dataset) because:\n\nValidation dataset may have observations overlapping with the training dataset\nError on validation dataset is likely to be similar to the error on training data, leading to less value addition\n\\(K\\)-fold cross validation is computationally less expensive\nError on the validation dataset can be highly variable\n\nAnswer: D\nExplanation: \nTesting on one validation set is unreliable because the result is highly dependent on the distribution of the data in the test set. By testing on several validation sets, this concern is alleviated to some extent. \n\\(K\\)-fold cross validation is computationally more expensive because \\(K\\)-fold requires multiple training/testing iterations in order to generate reliable results. This means that the entire dataset must be split into subsets, trained on each subset, and tested. This process must be repeated \\(K\\) times.\nError on validation dataset is not likely to be similar to the error on training data in scenarios such as overfitting.\nThe validation dataset approach assigns each observation to either train or test data, but not both.\n\n\n\nBinning of a continuous predictor, and then using the bins as predictors in logistic regression is likely to be useful when:\n\nThe proportion of observations belonging to a class have a non-monotonic relationship with the predictor\nThe proportion of observations belonging to a class have a monotonic relationship with the predictor\nThe proportion of observations belonging to a class are almost constant for each bin of the predictor\nThe predictor has low variance:\n\nAnswer: A\nExplanation: A single coefficient for a predictor will only provide predicted probabilities that are either continuously increasing or continuously decreasing with increasing predictor values. Thus, it will model only a monotonic (non-decreasing / non-increasing) relationship with the response. In case of a non-monotonic relationship, binning provides the required flexibility to have predicted probabilites that may increase or decrease with increase in predictor values.\nIf the proportion of observations belonging to a class are almost constant for each bin of the predictor, then the bins do not explain the variation in the response, and thus are not useful.\nIf the predictor itself is not varying, then it is unlikely to explain the variation in response, and binning cannot help increase the predictor variance."
  },
  {
    "objectID": "Practice_Final_Answer_Key.html#inference---logistic-regression",
    "href": "Practice_Final_Answer_Key.html#inference---logistic-regression",
    "title": "Appendix G — Practice Final Solutions",
    "section": "G.23 Inference - Logistic regression",
    "text": "G.23 Inference - Logistic regression\nDevelop a logistic regression model to predict if a patient has a risk of a 10 year coronary heart disease. TenYearCHD = 1 means ‘Yes’ and TenYearCHD = 0 means ‘No’. Use all the available predictors plus only one relevant interaction to answer the question below.\nAssuming all other predictors are constant, how much percent higher are the odds of male smokers getting diagnosed with heart disease as compared to female smokers. Round up the answer to the nearest integer greater than the answer. For example, if the odds of male smokers getting diagnosed with heart disease are 50.1% higher than that of female smokers, then enter 51 in the box.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.metrics import r2_score\nimport statsmodels.formula.api as sm\nimport itertools\nimport time\n\n\ntrain = pd.read_csv('./Datasets/train_heart.csv')\ntest = pd.read_csv('./Datasets/test_heart.csv')\n\n\npredictors = list(train.columns)[0:train.shape[1]-1]\n\n\n#logistic model with all predictors and interaction of \"male\"&\"currentSmoker\"\nmodel = sm.logit('TenYearCHD~male*currentSmoker+' + '+'.join(predictors), data = train).fit(disp=0)\nmodel.summary()\n\n\n\nLogit Regression Results\n\n  Dep. Variable:      TenYearCHD      No. Observations:      2742  \n\n\n  Model:                 Logit        Df Residuals:          2725  \n\n\n  Method:                 MLE         Df Model:                16  \n\n\n  Date:            Fri, 10 Mar 2023   Pseudo R-squ.:       0.1085  \n\n\n  Time:                02:35:34       Log-Likelihood:      -1031.3 \n\n\n  converged:             True         LL-Null:             -1156.8 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        3.307e-44\n\n\n\n\n                        coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept             -7.7337     0.835    -9.266  0.000    -9.369    -6.098\n\n\n  male                   0.4546     0.173     2.634  0.008     0.116     0.793\n\n\n  currentSmoker          0.0353     0.203     0.174  0.862    -0.362     0.433\n\n\n  male:currentSmoker     0.0149     0.247     0.060  0.952    -0.469     0.499\n\n\n  age                    0.0563     0.008     7.242  0.000     0.041     0.071\n\n\n  education             -0.1069     0.058    -1.835  0.067    -0.221     0.007\n\n\n  cigsPerDay             0.0189     0.007     2.586  0.010     0.005     0.033\n\n\n  BPMeds                 0.0595     0.284     0.209  0.834    -0.498     0.617\n\n\n  prevalentStroke        0.6269     0.552     1.135  0.256    -0.456     1.709\n\n\n  prevalentHyp           0.2067     0.161     1.281  0.200    -0.109     0.523\n\n\n  diabetes              -0.1072     0.386    -0.278  0.781    -0.863     0.649\n\n\n  totChol                0.0014     0.001     1.061  0.288    -0.001     0.004\n\n\n  sysBP                  0.0174     0.004     3.904  0.000     0.009     0.026\n\n\n  diaBP                 -0.0050     0.007    -0.678  0.498    -0.020     0.010\n\n\n  BMI                    0.0036     0.015     0.238  0.812    -0.026     0.033\n\n\n  heartRate             -0.0030     0.005    -0.608  0.543    -0.013     0.007\n\n\n  glucose                0.0076     0.003     2.974  0.003     0.003     0.013\n\n\n\n\n\n#Ratio of odds (of having a heart disease) of a male smoker to a female smoker\nnp.exp(model.params['male']+model.params['male:currentSmoker'])\n\n1.5992822263126012"
  },
  {
    "objectID": "Practice_Final_Answer_Key.html#odds-1",
    "href": "Practice_Final_Answer_Key.html#odds-1",
    "title": "Appendix G — Practice Final Solutions",
    "section": "G.24 Odds",
    "text": "G.24 Odds\nAre the odds of male non-smokers being diagnosed with heart disease even higher than female smokers, based on the model developed in the previous question?\n\n#Ratio of odds (of having a heart disease) of a male non-smoker to a female smoker\nnp.exp(model.params['male']-model.params['currentSmoker'])\n\n1.5209571795703307"
  },
  {
    "objectID": "Practice_Final_Answer_Key.html#tuning-threshold-probability",
    "href": "Practice_Final_Answer_Key.html#tuning-threshold-probability",
    "title": "Appendix G — Practice Final Solutions",
    "section": "G.25 Tuning threshold probability",
    "text": "G.25 Tuning threshold probability\nAmong the options below, which is the maximum threshold probability of classifying observations into classes (TenYearCHD = 1 and TenYearCHD = 0), such that the false negative rate less than 20% on both the test and train datasets?\n\n#Function to compute confusion matrix and prediction accuracy on training data\ndef confusion_matrix_train(model,cutoff=0.5):\n    # Confusion matrix\n    cm_df = pd.DataFrame(model.pred_table(threshold = cutoff))\n    #Formatting the confusion matrix\n    cm_df.columns = ['Predicted 0', 'Predicted 1'] \n    cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n    cm = np.array(cm_df)\n    # Calculate the accuracy\n    accuracy = 100*(cm[0,0]+cm[1,1])/cm.sum()\n    fnr = 100*cm[1,0]/(cm[1,0]+cm[1,1])\n    return cm_df, accuracy, fnr\n\n#Function to compute confusion matrix and prediction accuracy on test data\ndef confusion_matrix_test(data,actual_values,model,cutoff=0.5):\n#Predict the values using the Logit model\n    pred_values = model.predict(data)\n# Specify the bins\n    bins=np.array([0,cutoff,1])\n#Confusion matrix\n    cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n    cm_df = pd.DataFrame(cm)\n    cm_df.columns = ['Predicted 0','Predicted 1']\n    cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n# Calculate the accuracy\n    accuracy = 100*(cm[0,0]+cm[1,1])/cm.sum()\n    fnr = 100*cm[1,0]/(cm[1,0]+cm[1,1])\n# Return the confusion matrix and the accuracy\n    return cm_df, accuracy, fnr\n\n\nprint(confusion_matrix_test(test,test.TenYearCHD,model,0.1))\nconfusion_matrix_train(model,0.1)\n\n(          Predicted 0  Predicted 1\nActual 0        388.0        379.0\nActual 1         18.0        129.0, 56.564551422319475, 12.244897959183673)\n\n\n(          Predicted 0  Predicted 1\n Actual 0       1091.0       1241.0\n Actual 1         67.0        343.0,\n 52.29759299781182,\n 16.341463414634145)\n\n\n\nprint(confusion_matrix_test(test,test.TenYearCHD,model,0.1))\nconfusion_matrix_train(model,0.1)\n\n(          Predicted 0  Predicted 1\nActual 0        388.0        379.0\nActual 1         18.0        129.0, 56.564551422319475, 12.244897959183673)\n\n\n(          Predicted 0  Predicted 1\n Actual 0       1091.0       1241.0\n Actual 1         67.0        343.0,\n 52.29759299781182,\n 16.341463414634145)"
  },
  {
    "objectID": "Practice_Final_Answer_Key.html#forward-stepwise",
    "href": "Practice_Final_Answer_Key.html#forward-stepwise",
    "title": "Appendix G — Practice Final Solutions",
    "section": "G.26 Forward stepwise",
    "text": "G.26 Forward stepwise\nUse forward stepwise selection to select a logistic regression model for predicting if a patient has a risk of a 10 year coronary heart disease. How many predictors are there in the best model as per the BIC criterion?\n\ndef best_sub_plots():\n    plt.figure(figsize=(20,10))\n    plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n\n    # Set up a 2x2 grid so we can look at 4 plots at once\n    plt.subplot(1, 2, 1)\n\n    # We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n    # The argmax() function can be used to identify the location of the maximum point of a vector\n    plt.plot(models_best[\"Rsquared\"])\n    plt.xlabel('# Predictors')\n    plt.ylabel('Log likelihood')\n\n    bic = models_best.apply(lambda row: row[1].bic, axis=1)\n\n    plt.subplot(1, 2, 2)\n    plt.plot(bic)\n    plt.plot(1+bic.argmin(), bic.min(), \"or\")\n    plt.xlabel('# Predictors')\n    plt.ylabel('BIC')\n\n\n#Function to develop a model based on all predictors in predictor_subset\ndef processSubset(predictor_subset):\n    # Fit model on feature_set and calculate R-squared\n    model = sm.logit('TenYearCHD~' + '+'.join(predictor_subset),data = train).fit(disp=0)\n    Rsquared = model.llf\n    return {\"model\":model, \"Rsquared\":Rsquared}\n\n\n#Function to find the best predictor out of p-k predictors and add it to the model containing the k predictors\ndef forward(predictors):\n\n    # Pull out predictors we still need to process\n    remaining_predictors = [p for p in X.columns if p not in predictors]\n    \n    tic = time.time()\n    \n    results = []\n    \n    for p in remaining_predictors:        \n        results.append(processSubset(predictors+[p]))\n    \n    # Wrap everything up in a nice dataframe\n    models = pd.DataFrame(results)\n    \n    # Choose the model with the highest RSS\n    best_model = models.loc[models['Rsquared'].argmax()]\n    \n    toc = time.time()\n    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n    \n    # Return the best model, along with some other useful information about the model\n    return best_model\n\n\ndef forward_selection():\n    models_best = pd.DataFrame(columns=[\"Rsquared\", \"model\"])\n\n    tic = time.time()\n    predictors = []\n\n    for i in range(1,len(X.columns)+1):    \n        models_best.loc[i] = forward(predictors)\n        predictors = list(models_best.loc[i][\"model\"].params.index[1:])\n\n    toc = time.time()\n    print(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n    return models_best\n\n\nX=train.iloc[:,0:train.shape[1]-1]\n\n\nmodels_best = forward_selection()\n\nProcessed  15 models on 1 predictors in 0.10073018074035645 seconds.\nProcessed  14 models on 2 predictors in 0.08876228332519531 seconds.\nProcessed  13 models on 3 predictors in 0.09574484825134277 seconds.\nProcessed  12 models on 4 predictors in 0.10770988464355469 seconds.\nProcessed  11 models on 5 predictors in 0.1107032299041748 seconds.\nProcessed  10 models on 6 predictors in 0.10970640182495117 seconds.\nProcessed  9 models on 7 predictors in 0.10073137283325195 seconds.\nProcessed  8 models on 8 predictors in 0.10275673866271973 seconds.\nProcessed  7 models on 9 predictors in 0.09374809265136719 seconds.\nProcessed  6 models on 10 predictors in 0.14561104774475098 seconds.\nProcessed  5 models on 11 predictors in 0.08178091049194336 seconds.\nProcessed  4 models on 12 predictors in 0.06682133674621582 seconds.\nProcessed  3 models on 13 predictors in 0.07779192924499512 seconds.\nProcessed  2 models on 14 predictors in 0.04288506507873535 seconds.\nProcessed  1 models on 15 predictors in 0.020943880081176758 seconds.\nTotal elapsed time: 1.3882873058319092 seconds.\n\n\n\nbest_sub_plots()"
  },
  {
    "objectID": "Practice_Final_Answer_Key.html#multicollinearity",
    "href": "Practice_Final_Answer_Key.html#multicollinearity",
    "title": "Appendix G — Practice Final Solutions",
    "section": "G.27 Multicollinearity",
    "text": "G.27 Multicollinearity\nYou are developing a linear regression model to predict ‘SalePrice’. Assume all columns except ‘Id’ and ‘SalePrice’ to be predictors.\nWhat is the minimum number of predictors to be removed from the model so that there is no multicollinearity?\nAssume a VIF of less than 15 indicates absence of multicollinearity.\n\ntrain = pd.read_csv('./Datasets/housing_train.csv')\ntest = pd.read_csv('./Datasets/housing_test.csv')\n\n\npredictors = list(train.columns)[1:train.shape[1]-1]\nX = train[predictors]\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\ndef vif(X):\n    X = add_constant(X)\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = X.columns\n\n    for i in range(len(X.columns)):\n        vif_data.loc[i,'VIF'] = variance_inflation_factor(X.values, i)\n\n    print(vif_data)\nvif(X)\n\n          feature           VIF\n0           const  2.419858e+06\n1      MSSubClass  1.480034e+00\n2         LotArea  1.342832e+00\n3     OverallQual  3.438243e+00\n4     OverallCond  1.601301e+00\n5       YearBuilt  3.965491e+00\n6    YearRemodAdd  2.223979e+00\n7      BsmtFinSF1           inf\n8      BsmtFinSF2           inf\n9       BsmtUnfSF           inf\n10    TotalBsmtSF           inf\n11     FirstFlrSF           inf\n12    SecondFlrSF           inf\n13   LowQualFinSF           inf\n14      GrLivArea           inf\n15   BsmtFullBath  2.242659e+00\n16   BsmtHalfBath  1.140692e+00\n17       FullBath  2.911473e+00\n18       HalfBath  2.190673e+00\n19   BedroomAbvGr  2.199007e+00\n20   KitchenAbvGr  1.624357e+00\n21   TotRmsAbvGrd  4.917930e+00\n22     Fireplaces  1.556896e+00\n23     GarageCars  5.565460e+00\n24     GarageArea  5.397997e+00\n25     WoodDeckSF  1.253248e+00\n26    OpenPorchSF  1.219440e+00\n27  EnclosedPorch  1.242530e+00\n28       SsnPorch  1.032740e+00\n29    ScreenPorch  1.127507e+00\n30       PoolArea  1.095423e+00\n31        MiscVal  1.032655e+00\n32         MoSold  1.058330e+00\n33         YrSold  1.048841e+00\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:193: RuntimeWarning: divide by zero encountered in double_scalars\n  vif = 1. / (1. - r_squared_i)\n\n\n\n#Remove collinear predictors one at a time\nremove = ['BsmtFinSF1']\npred_filter = [x for x in predictors if x not in remove]\nX = train[pred_filter]\n\n#Recompute VIF\nvif(X)\n\n          feature           VIF\n0           const  2.419858e+06\n1      MSSubClass  1.480034e+00\n2         LotArea  1.342832e+00\n3     OverallQual  3.438243e+00\n4     OverallCond  1.601301e+00\n5       YearBuilt  3.965491e+00\n6    YearRemodAdd  2.223979e+00\n7      BsmtFinSF2  1.131439e+00\n8       BsmtUnfSF  2.577553e+00\n9     TotalBsmtSF  5.130743e+00\n10     FirstFlrSF           inf\n11    SecondFlrSF           inf\n12   LowQualFinSF           inf\n13      GrLivArea           inf\n14   BsmtFullBath  2.242659e+00\n15   BsmtHalfBath  1.140692e+00\n16       FullBath  2.911473e+00\n17       HalfBath  2.190673e+00\n18   BedroomAbvGr  2.199007e+00\n19   KitchenAbvGr  1.624357e+00\n20   TotRmsAbvGrd  4.917930e+00\n21     Fireplaces  1.556896e+00\n22     GarageCars  5.565460e+00\n23     GarageArea  5.397997e+00\n24     WoodDeckSF  1.253248e+00\n25    OpenPorchSF  1.219440e+00\n26  EnclosedPorch  1.242530e+00\n27       SsnPorch  1.032740e+00\n28    ScreenPorch  1.127507e+00\n29       PoolArea  1.095423e+00\n30        MiscVal  1.032655e+00\n31         MoSold  1.058330e+00\n32         YrSold  1.048841e+00\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:193: RuntimeWarning: divide by zero encountered in double_scalars\n  vif = 1. / (1. - r_squared_i)\n\n\n\n#Remove another collinear predictor\nremove = ['BsmtFinSF1','FirstFlrSF']\npred_filter = [x for x in predictors if x not in remove]\nX = train[pred_filter]\n\n#Recompute VIF\nvif(X)\n\n          feature           VIF\n0           const  2.419858e+06\n1      MSSubClass  1.480034e+00\n2         LotArea  1.342832e+00\n3     OverallQual  3.438243e+00\n4     OverallCond  1.601301e+00\n5       YearBuilt  3.965491e+00\n6    YearRemodAdd  2.223979e+00\n7      BsmtFinSF2  1.131439e+00\n8       BsmtUnfSF  2.577553e+00\n9     TotalBsmtSF  5.130743e+00\n10    SecondFlrSF  6.338090e+00\n11   LowQualFinSF  1.145700e+00\n12      GrLivArea  1.085217e+01\n13   BsmtFullBath  2.242659e+00\n14   BsmtHalfBath  1.140692e+00\n15       FullBath  2.911473e+00\n16       HalfBath  2.190673e+00\n17   BedroomAbvGr  2.199007e+00\n18   KitchenAbvGr  1.624357e+00\n19   TotRmsAbvGrd  4.917930e+00\n20     Fireplaces  1.556896e+00\n21     GarageCars  5.565460e+00\n22     GarageArea  5.397997e+00\n23     WoodDeckSF  1.253248e+00\n24    OpenPorchSF  1.219440e+00\n25  EnclosedPorch  1.242530e+00\n26       SsnPorch  1.032740e+00\n27    ScreenPorch  1.127507e+00\n28       PoolArea  1.095423e+00\n29        MiscVal  1.032655e+00\n30         MoSold  1.058330e+00\n31         YrSold  1.048841e+00\n\n\nThere is no more multicollinearity."
  },
  {
    "objectID": "Practice_Final_Answer_Key.html#lasso-1",
    "href": "Practice_Final_Answer_Key.html#lasso-1",
    "title": "Appendix G — Practice Final Solutions",
    "section": "G.28 Lasso",
    "text": "G.28 Lasso\nDevelop a lasso regression model to predict sale price based on all the predictors (except Id) in housing_train.csv. Find the RMSE (root mean squared error) of the developed model on housing_test.csv. Round up your answer to the nearest 100 greater than the answer. For example is the RMSE is 1001, enter 1100 in the box.\nNote: Use this range of tuning parameter to find its optimal value:\nalphas = 10**np.linspace(0,-4,200)*0.5\n\nX = train[predictors]\n#Test dataset\nXtest = test[predictors]\n\n\n#Standardizing test data\ny = np.log(train.SalePrice)\nscaler = StandardScaler()\nscaler.fit(X)\nXstd = scaler.transform(X)\nXtest_std = scaler.transform(Xtest)\n\n\nalphas = 10**np.linspace(0,-4,200)*0.5\n\nlassocv = LassoCV(alphas = alphas, cv = 10, max_iter = 100000)\nlassocv.fit(Xstd, y)\n\n#Optimal value of the tuning parameter - lamda\nlassocv.alpha_\n\n0.005359456596025638\n\n\n\n#Using the developed lasso model to predict on test data\nlasso = Lasso(alpha = lassocv.alpha_)\nlasso.fit(Xstd, y)\n\nLasso(alpha=0.005359456596025638)\n\n\n\npred=np.exp(lasso.predict(Xtest_std))\nnp.sqrt(((pred-test.SalePrice)**2).mean())\n\n25296.540649657465"
  },
  {
    "objectID": "Practice_Final_Answer_Key.html#predictor-importance",
    "href": "Practice_Final_Answer_Key.html#predictor-importance",
    "title": "Appendix G — Practice Final Solutions",
    "section": "G.29 Predictor importance",
    "text": "G.29 Predictor importance\nWhich predictor is the most important in predicting sale price based on the lasso regression model (developed in Q28)?\nHint: Find the predictor with the highest magnitude of coefficient.\n\nX.columns[np.argmax(np.abs(lasso.coef_))]\n\n'OverallQual'"
  },
  {
    "objectID": "Practice_Final_Answer_Key.html#improving-model-fit",
    "href": "Practice_Final_Answer_Key.html#improving-model-fit",
    "title": "Appendix G — Practice Final Solutions",
    "section": "G.30 Improving model fit",
    "text": "G.30 Improving model fit\nRemove the influential points from the train data housing_train.csv. Re-develop the lasso regression model, and compute the RMSE (root mean squared error) of the developed model on housing_test.csv. Round up your answer to the nearest 100 greater than the answer. For example is the RMSE is 1001, enter 1100 in the box.\nNote: Assume that a data point having a leverage more than 4 times the average leverage and a studentized residual with a magnitude of more than 3 is an influential point.\n\nmodel_log = sm.ols('np.log(SalePrice)~' + '+'.join(predictors), data = train).fit()\nout = model_log.outlier_test()\n\n#Average leverage of points\naverage_leverage = (model_log.df_model+1)/model_log.nobs\naverage_leverage\n\n#Computing the leverage statistic for each observation\ninfluence = model_log.get_influence()\nleverage = influence.hat_matrix_diag\n\n#We will remove all observations that have leverage higher than the threshold value.\nhigh_leverage_threshold = 4*average_leverage\n\n#Number of high leverage points in the dataset\nnp.sum(leverage>high_leverage_threshold)\n\n15\n\n\n\n#Dropping influential points from data\ntrain_filtered = train.drop(np.intersect1d(np.where(np.abs(out.student_resid)>3)[0],\n                                           (np.where(leverage>high_leverage_threshold)[0])))\n\n\nX = train_filtered[predictors]\n\n\n#Standardizing test data\ny = np.log(train_filtered.SalePrice)\nscaler = StandardScaler()\nscaler.fit(X)\nXstd = scaler.transform(X)\nXtest_std = scaler.transform(Xtest)\n\n\nalphas = 10**np.linspace(0,-4,200)*0.5\n\nlassocv = LassoCV(alphas = alphas, cv = 10, max_iter = 100000)\nlassocv.fit(Xstd, y)\n\n#Optimal value of the tuning parameter - lamda\nlassocv.alpha_\n\n0.005117057010527266\n\n\n\n#Using the developed lasso model to predict on test data\nlasso = Lasso(alpha = lassocv.alpha_)\nlasso.fit(Xstd, y)\n\nLasso(alpha=0.005117057010527266)\n\n\n\npred=np.exp(lasso.predict(Xtest_std))\nnp.sqrt(((pred-test.SalePrice)**2).mean())\n\n22684.858174164117"
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix H — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here"
  }
]