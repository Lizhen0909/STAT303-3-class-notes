{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6569ab",
   "metadata": {},
   "source": [
    "## Ensemble modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157b0a4",
   "metadata": {},
   "source": [
    "Ensembling models can help reduce error by leveraging the diversity and collective wisdom of multiple models. When ensembling, several individual models are trained independently and their predictions are combined to make the final prediction.\n",
    "\n",
    "We have already seen examples of ensemble models in chapters 5 - 13. The ensembled models may reduce error by reducing the bias *(boosting)* and / or reducing the variance *(bagging / random forests / boosting)*.\n",
    "\n",
    "However, in this chapter we'll ensemble different types of models, instead of the same type of model. We may ensemble a linear regression model, a random forest, a gradient boosting model, and as many different types of models as we wish. \n",
    "\n",
    "Below are a couple of reasons why ensembling models can be effective in reducing error:\n",
    "\n",
    "1. **Bias reduction:** Different models may have different biases and the ensemble can help mitigate the individual biases, leading to a more generalized and accurate prediction. For example, consider that one model has a positive bias, and another model has a negative bias for the same instance. By averaging or combining the predictions of the two models, the biases may cancel out.\n",
    "\n",
    "2. **Variance reduction:** As seen in the case of random forests and bagged trees, by averaging or combining the predictions of multiple models, the ensemble can reduce the overall variance and improve the accuracy of the final prediction. Note that for variance reduction, the models should have a low correlation *(recall the variance reduction formula of random forests)*.\n",
    "\n",
    "Mathematically also, we can show the effectiveness of an ensemble model. Let's consider the case of regression, and let the predictors be denoted as $X$, and the response as $Y$. Let $f_1, ..., f_m$ be the individual models. The expected MSE of an ensemble can be written as:\n",
    "\n",
    "$$ MSE_{Ensemble} = E\\bigg[\\bigg( \\frac{1}{m} \\sum_{i = 1}^{m} f_i(X) - Y \\bigg)^2 \\bigg] = \\frac{1}{m^2} \\sum_{i = 1}^{m} E \\bigg[\\big(f_i(X) - Y\\big)^2 \\bigg] + \\frac{1}{m^2} \\sum_{i \\ne j} E\\bigg[\\big(f_i(X) - Y\\big)\\big(f_j(X) - Y\\big) \\bigg]$$\n",
    "\n",
    "Assuming the **models are uncorrelated** *(i.e., they have a zero correlation)*, the second term *(covariance of $f_i(.)$ and $f_j(.)$)* reduces to zero, and the expected MSE of the ensemble reduces to:\n",
    "\n",
    "$$\n",
    "MSE_{Ensemble} = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m MSE_{f_i}\\bigg)\n",
    "$$ {#eq-ensemble}\n",
    "\n",
    "Thus, the expected MSE of an ensemble model with uncorrelated models is much smaller than the average MSE of all the models. Unless there is a model that is much better than the rest of the models, the MSE of the ensemble model is likely to be lower than the MSE of the individual models. However, there is no guarantee that the MSE of the ensemble model will be lower than the MSE of the individual models. Consider an extreme case where only one of the models have a zero MSE. The MSE of this model will be lower than the expected MSE of the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f819f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score,train_test_split, GridSearchCV, ParameterGrid, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingRegressor, VotingClassifier, StackingRegressor, StackingClassifier, GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import itertools as it\n",
    "import time as time\n",
    "import xgboost as xgb\n",
    "from pyearth import Earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9036ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carID</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>year</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>tax</th>\n",
       "      <th>mpg</th>\n",
       "      <th>engineSize</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18473</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2020</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>11</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.3282</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15064</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2019</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>10813</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.0430</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18268</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2020</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>6</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.4379</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18480</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2017</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>18895</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>51.5140</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18492</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2015</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>62953</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>160</td>\n",
       "      <td>51.4903</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carID brand      model  year transmission  mileage fuelType  tax      mpg  \\\n",
       "0  18473   bmw   6 Series  2020    Semi-Auto       11   Diesel  145  53.3282   \n",
       "1  15064   bmw   6 Series  2019    Semi-Auto    10813   Diesel  145  53.0430   \n",
       "2  18268   bmw   6 Series  2020    Semi-Auto        6   Diesel  145  53.4379   \n",
       "3  18480   bmw   6 Series  2017    Semi-Auto    18895   Diesel  145  51.5140   \n",
       "4  18492   bmw   6 Series  2015    Automatic    62953   Diesel  160  51.4903   \n",
       "\n",
       "   engineSize  price  \n",
       "0         3.0  37980  \n",
       "1         3.0  33980  \n",
       "2         3.0  36850  \n",
       "3         3.0  25998  \n",
       "4         3.0  18990  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the same datasets as used for linear regression in STAT303-2, \n",
    "#so that we can compare the non-linear models with linear regression\n",
    "trainf = pd.read_csv('./Datasets/Car_features_train.csv')\n",
    "trainp = pd.read_csv('./Datasets/Car_prices_train.csv')\n",
    "testf = pd.read_csv('./Datasets/Car_features_test.csv')\n",
    "testp = pd.read_csv('./Datasets/Car_prices_test.csv')\n",
    "train = pd.merge(trainf,trainp)\n",
    "test = pd.merge(testf,testp)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db6b5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['mileage','mpg','year','engineSize']]\n",
    "Xtest = test[['mileage','mpg','year','engineSize']]\n",
    "y = train['price']\n",
    "ytest = test['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c964f9",
   "metadata": {},
   "source": [
    "## Ensembling regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb4c98",
   "metadata": {},
   "source": [
    "### Voting Regressor\n",
    "Here, we will combine the predictions of different models. The function `VotingRegressor()` averages the predictions of all the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9730a85",
   "metadata": {},
   "source": [
    "Below are the individual models tuned in the previous chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834366ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for XGBoost =  5497.553788113875\n",
      "RMSE for AdaBoost =  5693.165811600585\n",
      "RMSE for Random forest =  5642.45839697972\n",
      "RMSE for Gradient Boosting =  5405.787029062213\n"
     ]
    }
   ],
   "source": [
    "# Tuned XGBoost model from Section 9.2.6\n",
    "model_xgb = xgb.XGBRegressor(random_state=1,max_depth=8,n_estimators=1000, subsample = 0.75, colsample_bytree = 1.0,\n",
    "                                         learning_rate = 0.01,reg_lambda=1, gamma = 100).fit(X, y)\n",
    "print(\"RMSE for XGBoost = \", np.sqrt(mean_squared_error(model_xgb.predict(Xtest), ytest)))\n",
    "\n",
    "#Tuned AdaBoost model from Section 7.2.4\n",
    "model_ada = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,learning_rate=1.0,\n",
    "                         random_state=1).fit(X, y)\n",
    "print(\"RMSE for AdaBoost = \", np.sqrt(mean_squared_error(model_ada.predict(Xtest), ytest)))\n",
    "\n",
    "#Tuned Random forest model from Section 6.1.2\n",
    "model_rf = RandomForestRegressor(n_estimators=300, random_state=1,\n",
    "                        n_jobs=-1, max_features=2).fit(X, y)\n",
    "print(\"RMSE for Random forest = \", np.sqrt(mean_squared_error(model_rf.predict(Xtest), ytest)))\n",
    "\n",
    "#Tuned gradient boosting model from Section 8.2.5\n",
    "model_gb = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n",
    "                         random_state=1,loss='huber').fit(X, y)\n",
    "print(\"RMSE for Gradient Boosting = \", np.sqrt(mean_squared_error(model_gb.predict(Xtest), ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fde72fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model RMSE =  5361.7260763197\n"
     ]
    }
   ],
   "source": [
    "#Voting ensemble: Averaging the predictions of all models\n",
    "en=VotingRegressor(estimators = [('xgb',model_xgb),('ada',model_ada),('rf',model_rf),('gb',model_gb)])\n",
    "en.fit(X,y)\n",
    "print(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22dde3e",
   "metadata": {},
   "source": [
    "RMSE of the ensembled model is less than that of each of the individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aeaea6",
   "metadata": {},
   "source": [
    "### Stacking Regressor\n",
    "Stacking is a more sophisticated method of ensembling models. The method is as follows:\n",
    "\n",
    "1. The training data is split into *K* folds. Each of the *K* folds serves as a test data in one of the *K* iterations, and the rest of the folds serve as train data. \n",
    "\n",
    "2. Each model is used to make predictions on each of the *K* folds, after being trained on the remaining *K-1* folds. In this manner, each model predicts the response on each train data point - when that train data point was not used to train the model.\n",
    "\n",
    "3. Predictions at each training data points are generated by each model in step 2 (the above step). These predictions are now used as predictors to train a meta-model (referred by the argument `final_estimator`), with the original response as the response. The meta-model (or `final_estimator`) learns to combine predictions of different models to make a better prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1551cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression metamodel RMSE =  5311.789386389769\n"
     ]
    }
   ],
   "source": [
    "#Stacking using LinearRegression as the metamodel\n",
    "en = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),('gb', model_gb)],\n",
    "                     final_estimator=LinearRegression(),                                          \n",
    "                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\n",
    "en.fit(X,y)\n",
    "print(\"Linear regression metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fdff36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29641759, 0.25626987, 0.051808  , 0.41978153])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Co-efficients of the meta-model\n",
    "en.final_estimator_.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc823e84",
   "metadata": {},
   "source": [
    "Note the above coefficients of the meta-model. The model gives the highest weight to the gradient boosting model, and the lowest weight to the random forest model. Also, note that the coefficients need not sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85ed301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso metamodel RMSE =  5311.185592456483\n"
     ]
    }
   ],
   "source": [
    "#Stacking using Lasso as the metamodel\n",
    "en = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),('gb', model_gb)],\n",
    "                     final_estimator=LassoCV(),                                          \n",
    "                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\n",
    "en.fit(X,y)\n",
    "print(\"Lasso metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "404a2480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17639973, 0.28186944, 0.1152561 , 0.45119952])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coefficients of the lasso metamodel\n",
    "en.final_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "228eac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model RMSE =  5303.308982301974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  pruning_passer.run()\n",
      "C:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n"
     ]
    }
   ],
   "source": [
    "#Stacking using MARS as the meta-model\n",
    "en = StackingRegressor(estimators = [('xgb',m1),('ada',m2),('rf',m3),('gb',m4)],\n",
    "                     final_estimator=Earth(max_degree=1),                                          \n",
    "                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\n",
    "en.fit(X,y)\n",
    "print(\"MARS metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c8d3c05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth Model\n",
      "-------------------------------------\n",
      "Basis Function  Pruned  Coefficient  \n",
      "-------------------------------------\n",
      "(Intercept)     No      59644        \n",
      "h(x3-75435)     No      0.402779     \n",
      "h(75435-x3)     No      -0.406517    \n",
      "h(x1-74988)     No      0.822699     \n",
      "h(74988-x1)     No      -0.119104    \n",
      "h(x2-72702.8)   No      -0.449716    \n",
      "h(72702.8-x2)   No      -0.280938    \n",
      "x0              No      0.211986     \n",
      "-------------------------------------\n",
      "MSE: 25038308.7322, GCV: 25226136.6357, RSQ: 0.9070, GRSQ: 0.9063\n"
     ]
    }
   ],
   "source": [
    "print(en.final_estimator_.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875d383",
   "metadata": {},
   "source": [
    "## Ensembling classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d96bc36",
   "metadata": {},
   "source": [
    "We'll ensemble models for predicting accuracy of identifying people having a heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8490f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Datasets/Heart.csv')\n",
    "data.dropna(inplace = True)\n",
    "#Response variable\n",
    "y = pd.get_dummies(data['AHD'])['Yes']\n",
    "\n",
    "#Creating a dataframe for predictors with dummy variables replacing the categorical variables\n",
    "X = data.drop(columns = ['AHD','ChestPain','Thal'])\n",
    "X = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\n",
    "\n",
    "#Creating train and test datasets\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f1ec4",
   "metadata": {},
   "source": [
    "Let us tune the individual models first.\n",
    "\n",
    "#### AdaBoost {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbf61bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.871494 using {'base_estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 0.01, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Tuning Adaboost for maximizing accuracy\n",
    "model = AdaBoostClassifier(random_state=1)\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100,200,500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\n",
    "grid['base_estimator'] = [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), \n",
    "                          DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=4)]\n",
    "# define the evaluation procedure\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',refit='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(Xtrain, ytrain)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbcde10",
   "metadata": {},
   "source": [
    "#### Gradient Boosting {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d979116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.871954 using {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 100, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Tuning gradient boosting for maximizing accuracy\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100,200,500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\n",
    "grid['max_depth'] = [1,2,3,4,5]\n",
    "grid['subsample'] = [0.5,1.0]\n",
    "# define the evaluation procedure\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',refit='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(Xtrain, ytrain)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345949c",
   "metadata": {},
   "source": [
    "#### XGBoost {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0d4f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n",
      "{'gamma': 0, 'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 25, 'reg_lambda': 0, 'scale_pos_weight': 1.25} 0.872183908045977\n",
      "Time taken =  0.9524135629336039  minutes\n"
     ]
    }
   ],
   "source": [
    "# Tuning XGBoost for maximizing accuracy\n",
    "start_time = time.time()\n",
    "param_grid = {'n_estimators':[25, 100,250,500],\n",
    "                'max_depth': [4, 6 ,8],\n",
    "              'learning_rate': [0.01,0.1,0.2],\n",
    "               'gamma': [0, 1, 10, 100],\n",
    "               'reg_lambda':[0, 10, 100],\n",
    "               'subsample': [0.5, 0.75, 1.0]\n",
    "                'scale_pos_weight':[1.25,1.5,1.75]#Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances).\n",
    "             }\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n",
    "optimal_params = GridSearchCV(estimator=xgb.XGBClassifier(random_state=1),\n",
    "                             param_grid = param_grid,\n",
    "                             scoring = 'accuracy',\n",
    "                             verbose = 1,\n",
    "                             n_jobs=-1,\n",
    "                             cv = cv)\n",
    "optimal_params.fit(Xtrain,ytrain)\n",
    "print(optimal_params.best_params_,optimal_params.best_score_)\n",
    "print(\"Time taken = \", (time.time()-start_time)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "54ccc812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost accuracy =  0.7986577181208053\n",
      "Random forest accuracy =  0.8120805369127517\n",
      "Gradient boost accuracy =  0.7986577181208053\n",
      "XGBoost model accuracy =  0.7785234899328859\n"
     ]
    }
   ],
   "source": [
    "#Tuned Adaboost model\n",
    "model_ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=200, \n",
    "                               random_state=1,learning_rate=0.01).fit(Xtrain, ytrain)    \n",
    "test_accuracy_ada = model_ada.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n",
    "    \n",
    "#Tuned Random forest model from Section 6.3\n",
    "model_rf = RandomForestClassifier(n_estimators=500, random_state=1,max_features=3,\n",
    "                        n_jobs=-1,oob_score=False).fit(Xtrain, ytrain)\n",
    "test_accuracy_rf = model_rf.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n",
    "    \n",
    "#Tuned gradient boosting model\n",
    "model_gb = GradientBoostingClassifier(n_estimators=100, random_state=1,max_depth=4,learning_rate=1.0,\n",
    "                                     subsample = 1.0).fit(Xtrain, ytrain)\n",
    "test_accuracy_gb = model_gb.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n",
    "\n",
    "#Tuned XGBoost model\n",
    "model_xgb = xgb.XGBClassifier(random_state=1,gamma=0,learning_rate = 0.2,max_depth=4,\n",
    "                              n_estimators = 25,reg_lambda = 0,scale_pos_weight=1.25).fit(Xtrain,ytrain)\n",
    "test_accuracy_xgb = model_xgb.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n",
    "\n",
    "print(\"Adaboost accuracy = \",test_accuracy_ada)\n",
    "print(\"Random forest accuracy = \",test_accuracy_rf)\n",
    "print(\"Gradient boost accuracy = \",test_accuracy_gb)\n",
    "print(\"XGBoost model accuracy = \",test_accuracy_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd653a7",
   "metadata": {},
   "source": [
    "### Voting classifier - hard voting\n",
    "In this type of ensembling, the predicted class is the one predicted by the majority of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d30cc7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.825503355704698"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)])\n",
    "ensemble_model.fit(Xtrain,ytrain)\n",
    "ensemble_model.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5915b57",
   "metadata": {},
   "source": [
    "Note that the prediction accuracy of the ensemble is higher than the prediction accuracy of each of the individual models on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a5d02",
   "metadata": {},
   "source": [
    "### Voting classifier - soft voting\n",
    "In this type of ensembling, the predicted class is the one based on the average predicted probabilities of all the classifiers. The threshold probability is 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e7c0f301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7919463087248322"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n",
    "                                 voting='soft')\n",
    "ensemble_model.fit(Xtrain,ytrain)\n",
    "ensemble_model.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c70bed",
   "metadata": {},
   "source": [
    "Note that soft voting will be good only for well calibrated classifiers, i.e., all the classifiers must have probabilities at the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a4c3f4",
   "metadata": {},
   "source": [
    "### Stacking classifier\n",
    "Conceptually, the idea is similar to that of Stacking regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "20cd5cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7986577181208053"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Logistic regression as the meta model (final_estimator)\n",
    "ensemble_model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n",
    "                                   final_estimator=LogisticRegression(random_state=1,max_iter=10000),n_jobs=-1,\n",
    "                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1))\n",
    "ensemble_model.fit(Xtrain,ytrain)\n",
    "ensemble_model.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b48ecdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81748051, 1.28663164, 1.64593342, 1.50947087]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coefficients of the logistic regression metamodel\n",
    "ensemble_model.final_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "57693cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8322147651006712"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using random forests as the meta model (final_estimator). Note that random forest will require tuning\n",
    "ensemble_model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n",
    "                                   final_estimator=RandomForestClassifier(n_estimators=500, max_features=1,\n",
    "                                                                          random_state=1,oob_score=True),n_jobs=-1,\n",
    "                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1))\n",
    "ensemble_model.fit(Xtrain,ytrain)\n",
    "ensemble_model.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd67695",
   "metadata": {},
   "source": [
    "Note that a complex `final_estimator` such as random forest will require tuning. In the above case, the `max_features` argument of random forests has been tuned to obtain the maximum OOB score. The tuning is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc5a6852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken =  0.33713538646698  minutes\n",
      "max accuracy =  0.8445945945945946\n",
      "Best value of max_features=  1\n"
     ]
    }
   ],
   "source": [
    "#Tuning the random forest parameters\n",
    "start_time = time.time()\n",
    "oob_score = {}\n",
    "\n",
    "i=0\n",
    "for pr in range(1,5):\n",
    "    model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n",
    "                                   final_estimator=RandomForestClassifier(n_estimators=500, max_features=pr,\n",
    "                                    random_state=1,oob_score=True),n_jobs=-1,\n",
    "                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)).fit(Xtrain, ytrain)\n",
    "    oob_score[pr] = model.final_estimator_.oob_score_\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"time taken = \", (end_time-start_time)/60, \" minutes\")\n",
    "print(\"max accuracy = \", np.max(list(oob_score.values())))\n",
    "print(\"Best value of max_features= \", np.argmax(list(oob_score.values()))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4af7c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.8445945945945946,\n",
       " 2: 0.831081081081081,\n",
       " 3: 0.8378378378378378,\n",
       " 4: 0.831081081081081}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The final predictor (metamodel) - random forest obtains the maximum oob_score for max_features = 1\n",
    "oob_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954fa55",
   "metadata": {},
   "source": [
    "### Tuning all models simultaneously\n",
    "\n",
    "Individual model hyperparameters can be tuned simultaneously while ensembling them with a `VotingClassifier()`. However, this approach can be too expensive for even moderately-sized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the param grid with the names of the models as prefixes\n",
    "\n",
    "model_ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier())\n",
    "model_rf = RandomForestClassifier()\n",
    "model_gb = GradientBoostingClassifier()\n",
    "model_xgb = xgb.XGBClassifier()\n",
    "\n",
    "ensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)])\n",
    "\n",
    "hp_grid = dict()\n",
    "\n",
    "# XGBoost\n",
    "hp_grid['xgb__n_estimators'] = [25, 100,250,50]\n",
    "hp_grid['xgb__max_depth'] = [4, 6 ,8]\n",
    "hp_grid['xgb__learning_rate'] = [0.01, 0.1, 1.0]\n",
    "hp_grid['xgb__gamma'] = [0, 1, 10, 100]\n",
    "hp_grid['xgb__reg_lambda'] = [0, 1, 10, 100]\n",
    "hp_grid['xgb__subsample'] = [0, 1, 10, 100]\n",
    "hp_grid['xgb__scale_pos_weight'] = [1.0, 1.25, 1.5]\n",
    "hp_grid['xgb__colsample_bytree'] = [0.5, 0.75, 1.0]\n",
    "\n",
    "# AdaBoost\n",
    "hp_grid['ada__n_estimators'] = [10, 50, 100,200,500]\n",
    "hp_grid['ada__base_estimator__max_depth'] = [1, 3, 5]\n",
    "hp_grid['ada__learning_rate'] = [0.01, 0.1, 0.2]\n",
    "\n",
    "# Random Forest\n",
    "hp_grid['rf__n_estimators'] = [100]\n",
    "hp_grid['rf__max_features'] = [3, 6, 9, 12, 15]\n",
    "\n",
    "# GradBoost\n",
    "hp_grid['gb__n_estimators'] = [10, 50, 100,200,500]\n",
    "hp_grid['gb__max_depth'] = [1, 3, 5]\n",
    "hp_grid['gb__learning_rate'] = [0.01, 0.1, 0.2, 1.0]\n",
    "hp_grid['gb__subsample'] = [0.5, 0.75, 1.0]\n",
    "\n",
    "start_time = time.time()\n",
    "grid = RandomizedSearchCV(ensemble_model, hp_grid, cv=5, scoring='accuracy', verbose = True,\n",
    "                         n_iter = 100, n_jobs=-1).fit(Xtrain, ytrain)\n",
    "print(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "97a2b9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8120805369127517"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(Xtest, ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
