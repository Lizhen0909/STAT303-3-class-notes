{
 "cells": [
  {
   "cell_type": "raw",
   "id": "09543cc9-56f8-4438-ab1b-aa31b8ca584f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Assignment 4\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9c350-0cec-463d-aebc-1ef2767c8414",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity. \n",
    "\n",
    "2. Write your code in the **Code cells** and your answers in the **Markdown cells** of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\n",
    "\n",
    "3. Use [Quarto](https://quarto.org/docs/output-formats/html-basics.html) to render the **.ipynb** file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: `quarto render filename.ipynb --to html`. Submit the HTML file.\n",
    "\n",
    "4. The assignment is worth 100 points, and is due on **Friday, 23th May 2025 at 11:59 pm**.\n",
    "\n",
    "5. **Five points are properly formatting the assignment**. The breakdown is as follows:\n",
    "    - Must be an HTML file rendered using Quarto **(1 point)**. *If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.* \n",
    "    - No name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission.  **(1 point)**\n",
    "    - There aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) **(1 point)**\n",
    "    - Final answers to each question are written in the Markdown cells. **(1 point)**\n",
    "    - There is no piece of unnecessary / redundant code, and no unnecessary / redundant text. **(1 point)**\n",
    "6. Please make sure your code results are clearly incorporated in your submitted HTML file.\n",
    "\n",
    "**Feel free to add data visualizations of your hyperparameter tuning process. Visualizing and analyzing tuning results is important—even if it's not explicitly required in the instructions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32fb2ff-3e82-4a17-9096-3bae056482ac",
   "metadata": {},
   "source": [
    "##  AdaBoost vs Bagging (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476667e-949d-4534-8056-1c3a061ee790",
   "metadata": {},
   "source": [
    "Which model among AdaBoost and Random Forest is more sensitive to outliers? **(1 point)** Explain your reasoning with the theory you learned on the training process of both models. **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f558c-ee6f-403c-869b-469f77b73aa0",
   "metadata": {},
   "source": [
    "##  Regression with Boosting (54 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c5af5f-510a-4475-924b-66d7f37762ee",
   "metadata": {},
   "source": [
    "For this question, you will use the **miami_housing.csv** file. You can find the description for the variables [here](https://www.kaggle.com/datasets/deepcontractor/miami-housing-dataset).\n",
    "\n",
    "The `SALE_PRC` variable is the regression response and the rest of the variables, except `PARCELNO`, are the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfd5da-f966-4fee-b23e-87455cf02ba6",
   "metadata": {},
   "source": [
    "### a): Preprocessing\n",
    "\n",
    "Read the dataset. Create the training and test sets with a 60%-40% split and `random_state = 1`. **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1882f-12ac-43de-818d-c7c6fd9b750d",
   "metadata": {},
   "source": [
    "### b) AdaBoost\n",
    "\n",
    "Tune an **AdaBoost Regressor** to achieve a **test MAE below \\$47,000**.\n",
    "\n",
    "- You **must set `random_state=1` for all components** (e.g., base estimator, AdaBoost model, etc.).\n",
    "- **Submissions that meet the MAE cutoff using any other `random_state` will receive zero credit.**\n",
    "\n",
    "**Scoring:**\n",
    "- 5 points for achieving test MAE < \\$47,000\n",
    "- 1 point for reporting the training MAE of your tuned model to evaluate generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf10e4-b9a0-4527-8f97-6004e3a9047e",
   "metadata": {},
   "source": [
    "### c) Loss Functions in Gradient Boosting\n",
    "\n",
    "Gradient Boosting supports multiple loss functions, including **`squared_error`**, **`absolute_error`**, and **`huber`**.  \n",
    "\n",
    "- **(1 point)** Which loss function performs best on this dataset?\n",
    "- **(3 points)** What are the advantages of this loss function compared to the other two?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72a607-5d01-4395-8f18-04943b552382",
   "metadata": {},
   "source": [
    "###  Task: Tune a Gradient Boosting Model\n",
    "\n",
    "Your goal is to tune a **Gradient Boosting Regressor** to achieve a **cross-validation MAE below \\$45,000**.\n",
    "\n",
    "- You **must keep all `random_state` values set to 1**.  \n",
    "- **Submissions using any other `random_state` will receive zero credit, even if the MAE cutoff is met.**\n",
    "\n",
    " **Scoring (10 points total):**  \n",
    "- 5 points for using a well-reasoned hyperparameter search strategy  \n",
    "- 5 points for achieving MAE < \\$45,000\n",
    "- 1 point for reporting the training MAE of your tuned model to evaluate generalization \n",
    "\n",
    "\n",
    "\n",
    "**Hints**\n",
    "\n",
    "- **Parallel processing is not supported** in the vanilla `GradientBoostingRegressor`.\n",
    "- **`BayesSearchCV`**, like gradient boosting itself, performs a sequential search—each trial depends on the result of the previous one—so it does **not support parallel exploration**.\n",
    "- **Optuna** is generally faster and more efficient than both `BayesSearchCV` and `GridSearchCV`. It supports **parallel execution** of trials and includes several built-in performance enhancements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0cbb8a-eb0d-4701-8f31-c03289f30cf1",
   "metadata": {},
   "source": [
    "### d) XGBoost vs. Gradient Boosting\n",
    "\n",
    "**XGBoost Enhancements:**\n",
    "\n",
    "- What improvements make **XGBoost superior to vanilla Gradient Boosting** in terms of performance and runtime?  \n",
    "  - Explain the **enhancements** (1 point)  \n",
    "  - Provide the **reasons** behind the improvements (1 point)  \n",
    "  - Identify relevant **hyperparameters** and describe how they influence model behavior (2 points)\n",
    "\n",
    "**XGBoost Limitations:**\n",
    "\n",
    "- What important feature or behavior is **missing in XGBoost** but well-implemented in **vanilla Gradient Boosting**? (1 point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d7733-90c9-40e5-a0d8-2467fe294589",
   "metadata": {},
   "source": [
    "### e) Tuning XGBoost with Different Search Strategies\n",
    "\n",
    "Tune an **XGBoost Regressor** to achieve a **cross-validation MAE below \\$42,500**.\n",
    "\n",
    "- You **must keep `random_state=1` in all components** (e.g., XGBoost model, CV splits, search objects).\n",
    "- **Submissions that meet the cutoff using any other `random_state` will receive zero credit.**\n",
    "\n",
    "**Scoring (10 points total):**\n",
    "\n",
    "- 5 points for a well-designed and appropriate hyperparameter search strategy\n",
    "- 5 using 3 different search strategies\n",
    "- 5 points for achieving MAE < \\$42,500\n",
    "\n",
    "\n",
    "**Search Strategies (Required Comparison)**\n",
    "\n",
    "You must tune the model using **three different search settings**:\n",
    "\n",
    "1. **BayesSearchCV**\n",
    "   - Unlike vanilla `GradientBoostingRegressor`, XGBoost supports parallel training and can benefit from multi-core processing (`n_jobs=-1`), so `BayesSearchCV` is practica with it.\n",
    "\n",
    "2. **Optuna (with `n_jobs=-1`)**\n",
    "\n",
    "3. **Optuna (default single-threaded)**\n",
    "\n",
    "\n",
    "**Execution Time**\n",
    "\n",
    "You must report the **execution time** for each tuning strategy.\n",
    "\n",
    "- You can measure this using:\n",
    "  - A **Jupyter magic command** like `%%time`, or\n",
    "  - Python’s `time.time()` (end - start)\n",
    "\n",
    "For a **fair comparison**, use the **same search space** across all methods.  \n",
    "Only one of the tuned models needs to meet the performance cutoff, but you should still report times for all three.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a8eb8",
   "metadata": {},
   "source": [
    "### f) Feature Importance\n",
    "\n",
    "Using the **best hyperparameter settings**, fit the final model and **output the feature importances**.\n",
    "\n",
    "- Use the `.feature_importances_` attribute or equivalent method from your model.\n",
    "- Visualize the importances if possible (e.g., with a bar plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d12a4d4-53b2-4aca-8bef-f6a61aa40792",
   "metadata": {},
   "source": [
    "## Imbalanced Classification with Regularized Gradient Boosting (42 points)\n",
    "\n",
    "In this question, you will use the **train.csv** and **test.csv** datasets. Each observation represents a marketing call made by a banking institution. The target variable `y` indicates whether the client subscribed to a term deposit (`1`) or not (`0`), making this a binary classification task.\n",
    "\n",
    "The predictors you should use are: `age`, `day`, `month`, and `education`.\n",
    "\n",
    "⚠️ **Note:** As discussed last quarter, the variable `duration` **must not be used as a predictor**.  \n",
    "**No credit will be given** for models that include it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84001e-b053-4d86-95a3-c2bc3bd559a5",
   "metadata": {},
   "source": [
    "### a) Data Preprocessing\n",
    "\n",
    "Perform the following preprocessing steps:\n",
    "\n",
    "- Read in the training and testing datasets.\n",
    "- Create a new `season` feature by mapping each `month` to its corresponding season.\n",
    "- Define the predictor and response variables.\n",
    "- Convert all categorical predictors to `pandas.Categorical` dtype before passing them to the models.\n",
    "- Convert the response variable `y` to binary values (`0` and `1`).\n",
    "\n",
    "\n",
    "**(5 points)**\n",
    "\n",
    "We will rely on the **native categorical feature support** provided by each library (XGBoost, LightGBM, and CatBoost), so explicit one-hot encoding is **not** required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcdfab1",
   "metadata": {},
   "source": [
    "### b) Target Exploration\n",
    "\n",
    "For classification tasks, it's important to examine the distribution of the target variable to determine whether the classes are imbalanced. This helps you avoid common pitfalls when dealing with imbalanced classification.\n",
    "\n",
    "- Explore the class distribution in both the **training** and **test** sets.\n",
    "\n",
    "**(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04632d-cf47-41a8-808a-919cc78bba95",
   "metadata": {},
   "source": [
    "### c) LightGBM and CatBoost\n",
    "\n",
    "LightGBM and CatBoost are gradient boosting frameworks, like XGBoost, but each introduces unique innovations.\n",
    "\n",
    "- What do LightGBM and CatBoost have in common with XGBoost? **(2 points)**  \n",
    "- What advantages do they offer over XGBoost? **(2 points)**  \n",
    "- How are these advantages implemented in each model? **(3 points)**  \n",
    "- All three libraries support native categorical feature handling.  \n",
    "  Do they use the same approach? If not, explain the differences. **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742f7e7-4d54-4b7d-a35e-d6cce1d05f90",
   "metadata": {},
   "source": [
    "### c) Handling Imbalanced Classification in Gradient Boosting Extensions\n",
    "\n",
    "For all extensions of Gradient Boosting (XGBoost, LightGBM, and CatBoost):\n",
    "\n",
    "- Are there additional inputs or hyperparameters available to handle imbalanced classification? **(1 point)**  \n",
    "- If yes, describe how the method works. **(1 point)**  \n",
    "- How should the value of this hyperparameter be set or tuned for best results? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48754bc6",
   "metadata": {},
   "source": [
    "### d) Model Evaluation: XGBoost, LightGBM, and CatBoost\n",
    "\n",
    "Evaluate the performance of the following models: **XGBoost**, **LightGBM**, and **CatBoost**, using the metrics on test set below:\n",
    "\n",
    "- **Recall**\n",
    "- **Precision**\n",
    "- **F1 Score**\n",
    "- **AUPRC** (Area Under the Precision-Recall Curve)\n",
    "- **ROC AUC**\n",
    "\n",
    "For each model, build and compare **two versions**:\n",
    "\n",
    "1. **Baseline model**: using default settings with `random_state=1`, without addressing class imbalance.\n",
    "2. **Imbalance-aware model**: with `scale_pos_weight` enabled to handle class imbalance.\n",
    "\n",
    "- Compare the performance of both versions for each model.\n",
    "- Summarize which model and approach performed best for imbalanced classification, and try to explain why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe7f7d-9c4a-4b7c-8d22-eeaf4e41fd71",
   "metadata": {},
   "source": [
    "### d) Tuning LightGBM for Classification\n",
    "\n",
    "Tune a **LightGBM classifier** to achieve:\n",
    "\n",
    "- **Cross-validation accuracy ≥ 70%**\n",
    "- **Cross-validation recall ≥ 65%**\n",
    "\n",
    "You **must set `random_state=1` in all components** (e.g., model, cross-validation, search objects).  \n",
    "**Submissions that exceed the cutoffs using any other `random_state` will receive zero credit.**\n",
    "\n",
    "**Scoring (15 points total):**  \n",
    "- 7.5 points for a well-designed and justified search strategy  \n",
    "- 7.5 points for meeting both performance thresholds\n",
    "\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- For classification, you may also tune the **decision threshold** (not just model hyperparameters).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42dca91-4f49-4f8d-99cf-bf28b9003cca",
   "metadata": {},
   "source": [
    "### e) Test Set Evaluation\n",
    "\n",
    "Evaluate the **tuned LightGBM model** on the **test set**:\n",
    "\n",
    "- Report the **test accuracy** and **test recall**.\n",
    "- Include the **threshold** used for classification.\n",
    "\n",
    "This will help assess how well the model generalizes beyond the training data.  \n",
    "\n",
    "**(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b470a4-9519-41ef-93a5-7478c529da43",
   "metadata": {},
   "source": [
    "### f) Tuning CatBoost for Classification\n",
    "\n",
    "Tune a **CatBoost classifier** to achieve:\n",
    "\n",
    "- **Cross-validation accuracy ≥ 70%**\n",
    "- **Cross-validation recall ≥ 65%**\n",
    "\n",
    "You **must set `random_state=1` in all components** (e.g., model, cross-validation, search objects).  \n",
    "**Submissions that exceed the cutoffs using any other `random_state` will receive zero credit.**\n",
    "\n",
    "**Scoring (15 points total):**  \n",
    "- 7.5 points for a well-structured and appropriate hyperparameter search  \n",
    "- 7.5 points for meeting both performance thresholds\n",
    "\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You are free to use **any tuning strategy** and define **any reasonable search space**.\n",
    "- In addition to tuning hyperparameters, you may also need to **tune the decision threshold** to meet the classification performance criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2808f697-3474-452f-be76-229d99ed3b3f",
   "metadata": {},
   "source": [
    "### g) Test Set Evaluation\n",
    "\n",
    "Evaluate the **tuned CatBoost model** on the **test set**:\n",
    "\n",
    "- Report the **test accuracy** and **test recall**.\n",
    "- Include the **classification threshold** used.\n",
    "\n",
    "This will help assess whether the model generalizes well beyond the training data.  \n",
    "**(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71048f2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77b5d40a",
   "metadata": {},
   "source": [
    "## 🎁 Bonus (Extra Credit) – 20 Points\n",
    "\n",
    "To help you prepare for your upcoming prediction project involving hyperparameter tuning, I’ve created the following optional tasks.  \n",
    "Feel free to skip them if time does not permit.\n",
    "\n",
    "\n",
    "### a) Comparing Tuning Strategies\n",
    "\n",
    "Compare the tuning time and results of **`GridSearchCV`** and **`RandomizedSearchCV`** using the same search space you used in Task 2e (`BayesSearchCV` and `Optuna`).\n",
    "\n",
    "- What are the trade-offs between **exhaustive search**, **random search**, and **smarter strategies** like **Bayesian optimization** and **Optuna**?\n",
    "- Are the differences in runtime justified by improvements in model performance?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2391f5",
   "metadata": {},
   "source": [
    "### b) Resumable Tuning Strategies\n",
    "\n",
    "Do your own research: Among all the tuning strategies you have used, which ones allow you to **continue tuning without starting from scratch** when increasing `n_trials` or `n_iter`?\n",
    "\n",
    "- Identify the methods that support **incremental or resumable search**.\n",
    "- Explain how they work and why they are efficient.\n",
    "- Provide code to demonstrate how these strategies **reuse previous results** rather\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8ec52",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
