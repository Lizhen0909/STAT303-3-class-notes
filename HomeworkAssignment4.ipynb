{
 "cells": [
  {
   "cell_type": "raw",
   "id": "09543cc9-56f8-4438-ab1b-aa31b8ca584f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Assignment 4\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "    number-sections: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9c350-0cec-463d-aebc-1ef2767c8414",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity. \n",
    "\n",
    "2. Write your code in the **Code cells** and your answers in the **Markdown cells** of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\n",
    "\n",
    "3. Use [Quarto](https://quarto.org/docs/output-formats/html-basics.html) to render the **.ipynb** file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: `quarto render filename.ipynb --to html`. Submit the HTML file.\n",
    "\n",
    "4. The assignment is worth 100 points, and is due on **Friday, 24th May 2024 at 11:59 pm**.\n",
    "\n",
    "5. **Five points are properly formatting the assignment**. The breakdown is as follows:\n",
    "    - Must be an HTML file rendered using Quarto **(1 point)**. *If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.* \n",
    "    - No name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission.  **(1 point)**\n",
    "    - There aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) **(1 point)**\n",
    "    - Final answers to each question are written in the Markdown cells. **(1 point)**\n",
    "    - There is no piece of unnecessary / redundant code, and no unnecessary / redundant text. **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32fb2ff-3e82-4a17-9096-3bae056482ac",
   "metadata": {},
   "source": [
    "## 1) AdaBoost vs Bagging (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476667e-949d-4534-8056-1c3a061ee790",
   "metadata": {},
   "source": [
    "Which model among AdaBoost and Random Forest is more sensitive to outliers? **(1 point)** Explain your reasoning with the theory you learned on the training process of both models. **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde25ebb-6c1e-4773-88ce-f7f439dcbec0",
   "metadata": {},
   "source": [
    "In general, AdaBoost is more sensitive to outliers than Random Forest, both in regression and classification problems. \n",
    "\n",
    "In AdaBoost classification problems, misclassified observations are given higher weights during each iteration of the algorithm. As outliers are more likely to be misclassified, they will tend to get higher weights, and the model may become biased towards these outliers. This can result in poor generalization performance on new, unseen data. In regression problems, the weak learners are fit on the residuals obtained from the previous weak learner. As outliers will correspond to larger residuals, the AdaBoost algorithm will make the splits to minimize the residuals correspond to outliers, which in turn may bias the model towards the outliers, and result in poor generalization performance on new, unseen data.\n",
    "\n",
    "On the other hand, all observations have the same weight in random forests. Thus, outliers will tend to be misclassified in classification problems, and have their own leaf in the regression problem, but will not influence the model too much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f558c-ee6f-403c-869b-469f77b73aa0",
   "metadata": {},
   "source": [
    "## 2) Regression with Boosting (55 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c5af5f-510a-4475-924b-66d7f37762ee",
   "metadata": {},
   "source": [
    "For this question, you will use the **miami_housing.csv** file. You can find the description for the variables [here](https://www.kaggle.com/datasets/deepcontractor/miami-housing-dataset).\n",
    "\n",
    "The `SALE_PRC` variable is the regression response and the rest of the variables, except `PARCELNO`, are the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfd5da-f966-4fee-b23e-87455cf02ba6",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Read the dataset. Create the training and test sets with a 60%-40% split and `random_state = 1`. **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1882f-12ac-43de-818d-c7c6fd9b750d",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "Tune an AdaBoost model to get below a cross-validation MAE of $48000. Keep **all** the random_states as 1. **Getting below the given cutoff with a different random_state in ANY object will not receive any credit.** \n",
    "**(5 points for a search that makes sense + 5 points for the cutoff = 10 points)**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Remember how you need to approach the tuning process with coarse and fine grids.\n",
    "- Remember that you have different cross-validation settings available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794728d-0cbd-4656-a0ed-94101a460e6d",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "Find the test MAE of the tuned AdaBoost model to see if it generalizes well. **(1 point)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fc590-7eb5-4dc8-af18-4e3b089d26e6",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Using the tuned AdaBoost model, print the predictor names with their importances in **decreasing order**. You need to print a DataFrame with the predictor names in the first column and the importances in the second. **(1 points)**\n",
    "\n",
    "**Note:** Features importances can be taken with pretty much the same line of code for all the models in this assignment. It is asked only for AdaBoost and omitted for the remaining models to avoid repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf10e4-b9a0-4527-8f97-6004e3a9047e",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "Moving on to Gradient Boosting, in general, which is the most preferred loss function? **(1 point)** What are its advantages over other loss functions? **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72a607-5d01-4395-8f18-04943b552382",
   "metadata": {},
   "source": [
    "### f) \n",
    "\n",
    "Tune a Gradient Boosting model to get below a cross-validation MAE of $45000. Keep **all** the random_states as 1. **Getting below the given cutoff with a different random_state in ANY object will not receive any credit.** \n",
    "**(5 points for a search that makes sense + 5 points for the cutoff = 10 points)**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Remember how you need to approach the grid of Gradient Boosting.\n",
    "- Remember that you have different cross-validation settings available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e510dd-62af-422c-b67d-73e41961feb3",
   "metadata": {},
   "source": [
    "### g)\n",
    "\n",
    "Find the test MAE of the tuned Gradient Boosting model to see if it generalizes well. **(1 point)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5b604-b1f3-465c-b2a5-2bb624ebd0d9",
   "metadata": {},
   "source": [
    "### h)\n",
    "\n",
    "Explain how the tuned hyperparameters of AdaBoost and Gradient Boosting affect the bias and the variance of their model. Note that most hyperparameters are the same between the models, so give only one explanation for those. (You need to include four hyperparameters in total.) **(1x4 = 4 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0cbb8a-eb0d-4701-8f31-c03289f30cf1",
   "metadata": {},
   "source": [
    "### i)\n",
    "\n",
    "Moving on to XGBoost: \n",
    "\n",
    "- What are the additions that makes XGBoost superior to Gradient Boosting? You need to explain this in terms of runtime **(1 point)** with its reason **(1 point)** and the hyperparameters **(1 point)** with their effect of model behavior. **(2 points)**.\n",
    "\n",
    "- What is missing in XGBoost that is well-implemented in Gradient Boosting? **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d7733-90c9-40e5-a0d8-2467fe294589",
   "metadata": {},
   "source": [
    "### j)\n",
    "\n",
    "Tune a XGBoost model to get below a cross-validation MAE of $43500. Keep **all** the random_states as 1. **Getting below the given cutoff with a different random_state in ANY object will not receive any credit.** \n",
    "**(5 points for a search that makes sense + 5 points for the cutoff = 10 points)**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Remember how you need to approach the grid of XGBoost.\n",
    "- Remember that you have different cross-validation settings available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb2408-deef-4ff6-8935-005fbdc69020",
   "metadata": {},
   "source": [
    "### k)\n",
    "\n",
    "Find the test MAE of the tuned XGBoost model to see if it generalizes well. **(1 point)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b9a28-db5a-4307-8723-b4c5f471f80d",
   "metadata": {},
   "source": [
    "## 2) Classification with Boosting (42 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d12a4d4-53b2-4aca-8bef-f6a61aa40792",
   "metadata": {},
   "source": [
    "For this question, you will use the **train.csv** and **test.csv** files. Each observation is a marketing call from a banking institution. `y` variable represents if the client subscribed for a term deposit (1) or not (0) and it is the classification response.\n",
    "\n",
    "The predictors are `age`, `day`, `month`, and `education`. (As mentioned last quarter, `duration` cannot be used as a predictor - no credit will be given to models that use it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84001e-b053-4d86-95a3-c2bc3bd559a5",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Preprocess the data:\n",
    "\n",
    "- Read the files.\n",
    "- Create the predictor and response variables. \n",
    "- Convert the response to 1s and 0s. \n",
    "- One-hot-encode the categorical predictors (**Do not use drop_first.**)\n",
    "\n",
    "**(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04632d-cf47-41a8-808a-919cc78bba95",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "Moving on to LightGBM and CatBoost, what are their advantages compared to Gradient Boosting and XGBoost? **(2 points)** How are these advantages implemented into the models? **(2 points)** Does any of them have any disadvantages? Describe if there is any. **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742f7e7-4d54-4b7d-a35e-d6cce1d05f90",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "For all extensions of Gradient Boosting, (XGBoost/LightGBM/CatBoost) is there an additional input/hyperparameter you can use to handle a certain issue that is specific to classification? **(1 point)** If yes, describe what it stands for **(1 point)** and how its value should be handled most efficiently. **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe7f7d-9c4a-4b7c-8d22-eeaf4e41fd71",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Tune a LightGBM model to get above a cross-validation accuracy of 70% and a cross-validation recall of 65%. Keep **all** the random_states as 1. **Getting above the given cutoffs with a different random_state in ANY object will not receive any credit. (7.5 points for a search that makes sense + 7.5 points for the cutoff = 15 points)**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Handling the grid efficiently can be useful again.\n",
    "- Remember that there are cross-validation settings that are specific to classification.\n",
    "- Remember that for classification, you need to tune the threshold as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42dca91-4f49-4f8d-99cf-bf28b9003cca",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "Find the test accuracy and the test recall of the tuned LightGBM model and threshold to see if they generalize well. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b470a4-9519-41ef-93a5-7478c529da43",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "Tune a CatBoost model to get above a cross-validation accuracy of 75% and a cross-validation recall of 65%. Keep **all** the random_states as 1. **Getting above the given cutoffs with a different random_state in ANY object will not receive any credit. (7.5 points for a search that makes sense + 7.5 points for the cutoff = 15 points)**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Handling the grid efficiently can be useful again.\n",
    "- Remember that there are cross-validation settings that are specific to classification.\n",
    "- Remember that for classification, you need to tune the threshold as well. (Use a stepsize of 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2808f697-3474-452f-be76-229d99ed3b3f",
   "metadata": {},
   "source": [
    "### g)\n",
    "\n",
    "Find the test accuracy and the test recall of the tuned CatBoost model and threshold to see if they generalize well. **(1 point)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
