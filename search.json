[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science III with python (Class notes)",
    "section": "",
    "text": "Preface\nThese are class notes for the course STAT303-2. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThe course notes are currently being written, and will continue to being developed as the course progresses (just like the course textbook last quarter). Please report any typos / mistakes / inconsistencies / issues with the class notes / class presentations in your comments here. Thank you!"
  },
  {
    "objectID": "L1_Scikit-learn.html#simple-linear-regression",
    "href": "L1_Scikit-learn.html#simple-linear-regression",
    "title": "1Â  Introduction to Scikit-learn",
    "section": "1.1 Simple Linear Regression",
    "text": "1.1 Simple Linear Regression\n\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# sklearn has 100s of models - grouped in sublibraries, such as linear_model\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\n# sklearn also has many tools for cleaning/processing data, also grouped in sublibraries\nfrom sklearn.model_selection import train_test_split # splitting one dataset into train and test\nfrom sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\n\n\ndata = pd.read_csv('./Datasets/diabetes.csv')\n\n\n# Separating the predictors and response - THIS IS HOW ALL SKLEARN OBJECTS ACCEPT DATA (different from statsmodels)\ny = data.Outcome\nX = data.drop(\"Outcome\", axis = 1)\n\n\n# Creating training and test data\n    # 80-20 split, which is usual - 70-30 split is also fine, 90-10 is fine if the dataset is large\n    # random_state to set a random seed for the splitting - reproducible results\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)\n# stratify\n\n\n# With linear/logistic regression in scikit-learn, especially when the predictors have different orders \n# of magn., scaling is necessary. This is to enable the training algo. which we did not cover. (Gradient Descent)\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test) # Do NOT refit the scaler with the test data, just transform it.\n\nX_train = X_train_scaled \nX_test = X_test_scaled\n\n\n# Create a model - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train, y_train)\n\n# Test the model - prediction - two ways to go\ny_pred = logreg.predict(X_test) # Get the predicted classes first\nprint(accuracy_score(y_pred, y_test)*100) # Use the predicted and true classes for accuracy\n\n73.37662337662337\n\n\n\nprint(logreg.score(X_test, y_test)*100) # Use .score with test predictors and response to get the accuracy\n                                            # Implements the same thing under the hood\n\n73.37662337662337\n\n\n\nprint(logreg.coef_) # Use coef_ to return the coefficients - only log reg inference you can do with sklearn\n\n[[ 0.32572891  1.20110566 -0.32046591  0.06849882 -0.21727131  0.72619528\n   0.40088897  0.29698818]]\n\n\n\n# all metrics exist in sklearn\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred))\nprint(precision_score(y_test, y_pred))\nprint(recall_score(y_test, y_pred))\n\n# What we covered today:\n    # A recap of Log. Reg. with sklearn\n        # Separate the predictors and response (if necessary)\n        # Split the data into train and test (if necessary)\n        \n        # Create a model\n        # Train with .fit\n        # Predict with .predict or get the accuracy with .score\n        # Use sklearn metrics with y_pred and y_test\n\n\n        # Same idea with LinearRegression()\n        # .score returns r-squared by default\n        # use the appropriate metrics!\n\n\n###################################################################################################\n\n[[87 17]\n [24 26]]\n0.6046511627906976\n0.52\n\n\n\n# More details on the LogisticRegression model:\n    # Inputs - for regularization and \n    # prediction prob.s instead of classes - so we can change the thresholds\n    \n# .predict_proba returns the prob.s for both classes\n    # Two cols for two classes\n    # Apply your threshold to y_pred_probs[1] (second col)\ny_pred_probs = logreg.predict_proba(X_test)    \n\ncutoff = 0.3\n\ny_pred2 = y_pred_probs[:,1] > cutoff\ny_pred2 = y_pred2.astype(int)\n\nprint(confusion_matrix(y_test, y_pred2))\nprint(precision_score(y_test, y_pred2))\nprint(recall_score(y_test, y_pred2))\n\n[[78 26]\n [15 35]]\n0.5737704918032787\n0.7\n\n\n\n\n\n# Test accuracy stayed the same - reg was not very necessary\n\n# Too much reg\nlogreg2 = LogisticRegression(C=1e-10)\nlogreg2.fit(X_train, y_train)\ny_pred = logreg2.predict(X_test) # Get the predicted classes first\nprint(accuracy_score(y_pred, y_test)*100)\n\n# Test accuracy is even lower - too much reg caused underfitting\n\n\n# The key to take full advantage of sklearn models is their inputs\n    # Always read the doc\n    # In Log Reg, you can switch to Lasso with penalty = 'l1' - for variable selection\n    # For no regression, besides what we did above, you can use penalty = None\n    \n# Recall that C, or lambda, is a hyperparameter, which is optimized with cross-validation\n    # There is LogisticRegressionCV, just like LassoCV and RidgeCV\n    # Works the exact same way - check LassoCV and RidgeCV notes\n    \n# For all the sklearn models we will create in this course, there will be hyperparameters.\n    # Mostly more than one for each model\n    # These hyperparameters will determine how much regularization the model will have\n    # These models will not have a CV version\n    # So, we need to use two sklearn tools that implement cross-validation\n        # cross_val_score - now\n        # GridSearchCV - later when we get to trees and tree-based models\n\nfrom sklearn.model_selection import cross_val_score\n\nval_scores = []\n\nhyperparam_vals = 10**np.linspace(-5, 10)\n\nfor c_val in hyperparam_vals: # For each possible C value in your grid\n    logreg_model = LogisticRegression(C=c_val) # Create a model with the C value\n    \n    val_scores.append(cross_val_score(logreg_model, X_train, y_train, scoring='accuracy', cv=5)) # Find the cv results\n    \n    \nimport matplotlib.pyplot as plt\n\nplt.plot(hyperparam_vals, np.mean(np.array(val_scores), axis=1))\nplt.xlabel('possible C value')\nplt.ylabel('avg 5-fold CV value')\nplt.xscale('log')\nplt.show()\n\n\n# Train the best model with the hyperparam val that returns the highest average accuracy\nlogreg_model_best = LogisticRegression(C=hyperparam_vals[np.argmax(np.mean(np.array(val_scores), axis=1))])\n\n# .fit\n# .predict & .predict_proba\n# .score\n# ...\n\n# Log. reg. has one hyperparameter - C.\n# More complex models will have more\n# If we have two hyperparams - we can use a nested loop and cross_val_score\n    # or we can use GridSearchCV - more on that when we get to trees and tree-based models. \n\nDevelop a simple linear regression model that predicts car price based on engine size. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~engineSize', data = train)\n\n\n#Using the fit() function of the 'ols' class to fit the model\nmodel = ols_object.fit()\n\n\n#Printing model summary which contains among other things, the model coefficients\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.390 \n\n\n  Model:                   OLS         Adj. R-squared:        0.390 \n\n\n  Method:             Least Squares    F-statistic:           3177. \n\n\n  Date:             Thu, 19 Jan 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 16:44:04       Log-Likelihood:      -53949. \n\n\n  No. Observations:        4960        AIC:                1.079e+05\n\n\n  Df Residuals:            4958        BIC:                1.079e+05\n\n\n  Df Model:                   1                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -4122.0357   522.260    -7.893  0.000 -5145.896 -3098.176\n\n\n  engineSize  1.299e+04   230.450    56.361  0.000  1.25e+04  1.34e+04\n\n\n\n\n  Omnibus:       1271.986   Durbin-Watson:         0.517\n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   6490.719\n\n\n  Skew:            1.137    Prob(JB):               0.00\n\n\n  Kurtosis:        8.122    Cond. No.               7.64\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe model equation is: car price = -4122.0357 + 12990 * engineSize\nVisualize the regression line\n\nsns.regplot(x = 'engineSize', y = 'price', data = train, color = 'orange',line_kws={\"color\": \"red\"})\nplt.xlim(-1,7)\n#Note that some of the engineSize values are 0. They are incorrect, and should ideally be imputed before developing the model.\n\n(-1.0, 7.0)\n\n\n\n\n\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\n\n#Using the predict() function associated with the 'model' object to make predictions of car price on test (unknown) data\npred_price = model.predict(testf)#Note that the predict() function finds the predictor 'engineSize' in the testf dataframe, and plugs its values in the regression equation for prediction.\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.scatterplot(x = testp.price, y = pred_price)\n#In case of a perfect prediction, all the points must lie on the line x = y.\nsns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='orange') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\n\nText(0, 0.5, 'Predicted price')\n\n\n\n\n\nThe prediction doesnât look too good. This is because we are just using one predictor - engine size. We can probably improve the model by adding more predictors when we learn multiple linear regression.\nWhat is the RMSE of the predicted car price?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n12995.1064515487\n\n\nThe root mean squared error in predicting car price is around $13k.\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n12810.109175214136\n\n\nThe residual standard error on the training data is close to the RMSE on the test data. This shows that the performance of the model on unknown data is comparable to its performance on known data. This implies that the model is not overfitting, which is good! In case we overfit a model on the training data, its performance on unknown data is likely to be worse than that on the training data.\nFind the confidence and prediction intervals of the predicted car price\n\n#Using the get_prediction() function associated with the 'model' object to get the intervals\nintervals = model.get_prediction(testf)\n\n\n#The function requires specifying alpha (probability of Type 1 error) instead of the confidence level to get the intervals\nintervals.summary_frame(alpha=0.05)\n\n\n\n\n\n  \n    \n      \n      mean\n      mean_se\n      mean_ci_lower\n      mean_ci_upper\n      obs_ci_lower\n      obs_ci_upper\n    \n  \n  \n    \n      0\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      1\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      2\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      3\n      8866.245277\n      316.580850\n      8245.606701\n      9486.883853\n      -16254.905974\n      33987.396528\n    \n    \n      4\n      47831.088340\n      468.949360\n      46911.740050\n      48750.436631\n      22700.782946\n      72961.393735\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2667\n      47831.088340\n      468.949360\n      46911.740050\n      48750.436631\n      22700.782946\n      72961.393735\n    \n    \n      2668\n      34842.807319\n      271.666459\n      34310.220826\n      35375.393812\n      9723.677232\n      59961.937406\n    \n    \n      2669\n      8866.245277\n      316.580850\n      8245.606701\n      9486.883853\n      -16254.905974\n      33987.396528\n    \n    \n      2670\n      21854.526298\n      184.135754\n      21493.538727\n      22215.513869\n      -3261.551421\n      46970.604017\n    \n    \n      2671\n      21854.526298\n      184.135754\n      21493.538727\n      22215.513869\n      -3261.551421\n      46970.604017\n    \n  \n\n2672 rows Ã 6 columns\n\n\n\nShow the regression line predicting car price based on engine size for test data. Also show the confidence and prediction intervals for the car price.\n\ninterval_table = intervals.summary_frame(alpha=0.05)\n\n\nsns.scatterplot(x = testf.engineSize, y = pred_price,color = 'orange', s = 10)\nsns.lineplot(x = testf.engineSize, y = pred_price, color = 'red')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_lower, color = 'blue')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_upper, color = 'blue',label='_nolegend_')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_lower, color = 'green')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_upper, color = 'green')\nplt.legend(labels=[\"Regression line\",\"Confidence interval\", \"Prediction interval\"])\n\n<matplotlib.legend.Legend at 0x26a3a32c550>"
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html#multiple-linear-regression",
    "href": "Lec2_MultipleLinearRegression.html#multiple-linear-regression",
    "title": "2Â  Multiple Linear Regression",
    "section": "2.1 Multiple Linear Regression",
    "text": "2.1 Multiple Linear Regression\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nDevelop a multiple linear regression model that predicts car price based on engine size, year, mileage, and mpg. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      carID\n      brand\n      model\n      year\n      transmission\n      mileage\n      fuelType\n      tax\n      mpg\n      engineSize\n      price\n    \n  \n  \n    \n      0\n      18473\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      11\n      Diesel\n      145\n      53.3282\n      3.0\n      37980\n    \n    \n      1\n      15064\n      bmw\n      6 Series\n      2019\n      Semi-Auto\n      10813\n      Diesel\n      145\n      53.0430\n      3.0\n      33980\n    \n    \n      2\n      18268\n      bmw\n      6 Series\n      2020\n      Semi-Auto\n      6\n      Diesel\n      145\n      53.4379\n      3.0\n      36850\n    \n    \n      3\n      18480\n      bmw\n      6 Series\n      2017\n      Semi-Auto\n      18895\n      Diesel\n      145\n      51.5140\n      3.0\n      25998\n    \n    \n      4\n      18492\n      bmw\n      6 Series\n      2015\n      Automatic\n      62953\n      Diesel\n      160\n      51.4903\n      3.0\n      18990\n    \n  \n\n\n\n\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.660 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           2410. \n\n\n  Date:             Tue, 27 Dec 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:07:25       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4955        BIC:                1.050e+05\n\n\n  Df Model:                   4                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.661e+06  1.49e+05   -24.593  0.000 -3.95e+06 -3.37e+06\n\n\n  year        1817.7366    73.751    24.647  0.000  1673.151  1962.322\n\n\n  mileage       -0.1474     0.009   -16.817  0.000    -0.165    -0.130\n\n\n  mpg          -79.3126     9.338    -8.493  0.000   -97.620   -61.006\n\n\n  engineSize  1.218e+04   189.969    64.107  0.000  1.18e+04  1.26e+04\n\n\n\n\n  Omnibus:       2450.973   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31060.548\n\n\n  Skew:            2.045    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.557    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe model equation is: estimated car price = -3.661e6 + 1818 * year -0.15 * mileage - 79.31 * mpg + 12180 * engineSize\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\n\n#Using the predict() function associated with the 'model' object to make predictions of car price on test (unknown) data\npred_price = model.predict(testf)#Note that the predict() function finds the predictor 'engineSize' in the testf dataframe, and plugs its values in the regression equation for prediction.\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.scatterplot(x = testp.price, y = pred_price)\n#In case of a perfect prediction, all the points must lie on the line x = y.\nsns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='orange') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\n\nText(0, 0.5, 'Predicted price')\n\n\n\n\n\nThe prediction looks better as compared to the one with simple linear regression. This is because we have four predictors to help explain the variation in car price, instead of just one in the case of simple linear regression. Also, all the predictors have a significant relationship with price as evident from their p-values. Thus, all four of them are contributing in explaining the variation. Note the higher values of R2 as compared to the one in the case of simple linear regression.\nWhat is the RMSE of the predicted car price?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9956.82497993548\n\n\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n9563.74782917604\n\n\n\nsns.scatterplot(x = model.fittedvalues, y=model.resid,color = 'orange')\nsns.lineplot(x = [pred_price.min(),pred_price.max()],y = [0,0],color = 'blue')\nplt.xlabel('Predicted price')\nplt.ylabel('Residual')\n\nText(0, 0.5, 'Residual')\n\n\n\n\n\nWill the explained variation (R-squared) in car price always increase if we add a variable?\nShould we keep on adding variables as long as the explained variation (R-squared) is increasing?\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nnp.random.seed(1)\ntrain['rand_col'] = np.random.rand(train.shape[0])\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize+rand_col', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          price        R-squared:             0.661 \n\n\n  Model:                   OLS         Adj. R-squared:        0.660 \n\n\n  Method:             Least Squares    F-statistic:           1928. \n\n\n  Date:             Tue, 27 Dec 2022   Prob (F-statistic):    0.00  \n\n\n  Time:                 01:07:38       Log-Likelihood:      -52497. \n\n\n  No. Observations:        4960        AIC:                1.050e+05\n\n\n  Df Residuals:            4954        BIC:                1.050e+05\n\n\n  Df Model:                   5                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n                coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept  -3.662e+06  1.49e+05   -24.600  0.000 -3.95e+06 -3.37e+06\n\n\n  year        1818.1672    73.753    24.652  0.000  1673.578  1962.756\n\n\n  mileage       -0.1474     0.009   -16.809  0.000    -0.165    -0.130\n\n\n  mpg          -79.2837     9.338    -8.490  0.000   -97.591   -60.976\n\n\n  engineSize  1.218e+04   189.972    64.109  0.000  1.18e+04  1.26e+04\n\n\n  rand_col     451.1226   471.897     0.956  0.339  -474.004  1376.249\n\n\n\n\n  Omnibus:       2451.728   Durbin-Watson:         0.541 \n\n\n  Prob(Omnibus):   0.000    Jarque-Bera (JB):   31040.331\n\n\n  Skew:            2.046    Prob(JB):               0.00 \n\n\n  Kurtosis:       14.552    Cond. No.           3.83e+07 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nAdding a variable with random values to the model (rand_col) increased the explained variation (R-squared). This is because the model has one more parameter to tune to reduce the residual squared error (RSS). However, the p-value of rand_col suggests that its coefficient is zero. Thus, using the model with rand_col may give poorer performance on unknown data, as compared to the model without rand_col. This implies that it is not a good idea to blindly add variables in the model to increase R-squared."
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix A â Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here"
  }
]