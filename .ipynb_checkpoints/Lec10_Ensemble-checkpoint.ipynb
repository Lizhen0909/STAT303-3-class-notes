{
 "cells": [
  {
   "cell_type": "raw",
   "id": "040e9002",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Ensemble modeling\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "    toc-depth: 4\n",
    "    jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157b0a4",
   "metadata": {},
   "source": [
    "Ensembling models can help reduce error by leveraging the diversity and collective wisdom of multiple models. When ensembling, several individual models are trained independently and their predictions are combined to make the final prediction.\n",
    "\n",
    "We have already seen examples of ensemble models in chapters 5 - 13. The ensembled models may reduce error by reducing the bias *(boosting)* and / or reducing the variance *(bagging / random forests / boosting)*.\n",
    "\n",
    "However, in this chapter we'll ensemble different types of models, instead of the same type of model. We may ensemble a linear regression model, a random forest, a gradient boosting model, and as many different types of models as we wish. \n",
    "\n",
    "Below are a couple of reasons why ensembling models can be effective in reducing error:\n",
    "\n",
    "1. **Bias reduction:** Different models may have different biases and the ensemble can help mitigate the individual biases, leading to a more generalized and accurate prediction. For example, consider that one model has a positive bias, and another model has a negative bias for the same instance. By averaging or combining the predictions of the two models, the biases may cancel out.\n",
    "\n",
    "2. **Variance reduction:** As seen in the case of random forests and bagged trees, by averaging or combining the predictions of multiple models, the ensemble can reduce the overall variance and improve the accuracy of the final prediction. Note that for variance reduction, the models should have a low correlation *(recall the variance reduction formula of random forests)*.\n",
    "\n",
    "Mathematically also, we can show the effectiveness of an ensemble model. Let's consider the case of regression, and let the predictors be denoted as $X$, and the response as $Y$. Let $f_1, ..., f_m$ be individual models. The expected MSE of an ensemble can be written as:\n",
    "\n",
    "$$ E(MSE_{Ensemble}) = E\\bigg[\\bigg( \\frac{1}{m} \\sum_{i = 1}^{m} f_i(X) - Y \\bigg)^2 \\bigg] = \\frac{1}{m^2} \\sum_{i = 1}^{m} E \\bigg[\\big(f_i(X) - Y\\big)^2 \\bigg] + \\frac{1}{m^2} \\sum_{i \\ne j} E\\bigg[\\big(f_i(X) - Y\\big)\\big(f_j(X) - Y\\big) \\bigg]$$\n",
    "\n",
    "$$ \\implies E(MSE_{Ensemble}) = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m E \\bigg[\\big(f_i(X) - Y\\big)^2 \\bigg]\\bigg) + \\frac{1}{m^2} \\sum_{i \\ne j} E\\bigg[\\big(f_i(X) - Y\\big)\\big(f_j(X) - Y\\big) \\bigg]$$\n",
    "\n",
    "$$ \\implies E(MSE_{Ensemble}) = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m MSE_{f_i}\\bigg) + \\frac{1}{m^2} \\sum_{i \\ne j} E\\bigg[\\big(f_i(X) - Y\\big)\\big(f_j(X) - Y\\big) \\bigg]$$\n",
    "\n",
    "If $f_1, ..., f_m$ are unbiased, then,\n",
    "\n",
    "$$ E(MSE_{Ensemble}) = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m MSE_{f_i}\\bigg) + \\frac{1}{m^2} \\sum_{i \\ne j} Cov(f_i(X), f_j(X))$$\n",
    "\n",
    "Assuming the **models are uncorrelated** *(i.e., they have a zero correlation)*, the second term *(covariance of $f_i(.)$ and $f_j(.)$)* reduces to zero, and the expected MSE of the ensemble reduces to:\n",
    "\n",
    "$$\n",
    "MSE_{Ensemble} = \\frac{1}{m}\\bigg(\\frac{1}{m} \\sum_{i=1}^m MSE_{f_i}\\bigg)\n",
    "$$ {#eq-ensemble}\n",
    "\n",
    "Thus, the expected MSE of an ensemble model with uncorrelated models is much smaller than the average MSE of all the models. Unless there is a model that is much better than the rest of the models, the MSE of the ensemble model is likely to be lower than the MSE of the individual models. However, there is no guarantee that the MSE of the ensemble model will be lower than the MSE of the individual models. Consider an extreme case where only one of the models have a zero MSE. The MSE of this model will be lower than the expected MSE of the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f819f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score,train_test_split, GridSearchCV, ParameterGrid, \\\n",
    "StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingRegressor, VotingClassifier, StackingRegressor, \\\n",
    "StackingClassifier, GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor, \\\n",
    "BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import itertools as it\n",
    "import time as time\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9036ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carID</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>year</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>tax</th>\n",
       "      <th>mpg</th>\n",
       "      <th>engineSize</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18473</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2020</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>11</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.3282</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15064</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2019</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>10813</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.0430</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18268</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2020</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>6</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.4379</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18480</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2017</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>18895</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>51.5140</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18492</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2015</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>62953</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>160</td>\n",
       "      <td>51.4903</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carID brand      model  year transmission  mileage fuelType  tax      mpg  \\\n",
       "0  18473   bmw   6 Series  2020    Semi-Auto       11   Diesel  145  53.3282   \n",
       "1  15064   bmw   6 Series  2019    Semi-Auto    10813   Diesel  145  53.0430   \n",
       "2  18268   bmw   6 Series  2020    Semi-Auto        6   Diesel  145  53.4379   \n",
       "3  18480   bmw   6 Series  2017    Semi-Auto    18895   Diesel  145  51.5140   \n",
       "4  18492   bmw   6 Series  2015    Automatic    62953   Diesel  160  51.4903   \n",
       "\n",
       "   engineSize  price  \n",
       "0         3.0  37980  \n",
       "1         3.0  33980  \n",
       "2         3.0  36850  \n",
       "3         3.0  25998  \n",
       "4         3.0  18990  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the same datasets as used for linear regression in STAT303-2, \n",
    "#so that we can compare the non-linear models with linear regression\n",
    "trainf = pd.read_csv('./Datasets/Car_features_train.csv')\n",
    "trainp = pd.read_csv('./Datasets/Car_prices_train.csv')\n",
    "testf = pd.read_csv('./Datasets/Car_features_test.csv')\n",
    "testp = pd.read_csv('./Datasets/Car_prices_test.csv')\n",
    "train = pd.merge(trainf,trainp)\n",
    "test = pd.merge(testf,testp)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6b5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['mileage','mpg','year','engineSize']]\n",
    "Xtest = test[['mileage','mpg','year','engineSize']]\n",
    "y = train['price']\n",
    "ytest = test['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c964f9",
   "metadata": {},
   "source": [
    "## Ensembling regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb4c98",
   "metadata": {},
   "source": [
    "### Voting Regressor\n",
    "Here, we will combine the predictions of different models. The function `VotingRegressor()` averages the predictions of all the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9730a85",
   "metadata": {},
   "source": [
    "Below are the individual models tuned in the previous chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "834366ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for AdaBoost =  5693.165811600585\n",
      "RMSE for Random forest =  5642.45839697972\n",
      "RMSE for XGBoost =  5497.553788113875\n",
      "RMSE for Gradient Boosting =  5405.787029062213\n",
      "RMSE for LightGBM =  5355.964600884197\n",
      "RMSE for CatBoost =  5271.104736146779\n"
     ]
    }
   ],
   "source": [
    "#Tuned AdaBoost model from Section 7.2.4\n",
    "model_ada = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,\n",
    "                    learning_rate=1.0,  random_state=1).fit(X, y)\n",
    "print(\"RMSE for AdaBoost = \", np.sqrt(mean_squared_error(model_ada.predict(Xtest), ytest)))\n",
    "\n",
    "#Tuned Random forest model from Section 6.1.2\n",
    "model_rf = RandomForestRegressor(n_estimators=300, random_state=1,\n",
    "                        n_jobs=-1, max_features=2).fit(X, y)\n",
    "print(\"RMSE for Random forest = \", np.sqrt(mean_squared_error(model_rf.predict(Xtest), ytest)))\n",
    "\n",
    "# Tuned XGBoost model from Section 9.2.6\n",
    "model_xgb = xgb.XGBRegressor(random_state=1,max_depth=8,n_estimators=1000, subsample = 0.75, colsample_bytree = 1.0,\n",
    "                                         learning_rate = 0.01,reg_lambda=1, gamma = 100).fit(X, y)\n",
    "print(\"RMSE for XGBoost = \", np.sqrt(mean_squared_error(model_xgb.predict(Xtest), ytest)))\n",
    "\n",
    "#Tuned gradient boosting model from Section 8.2.5\n",
    "model_gb = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n",
    "                         random_state=1,loss='huber').fit(X, y)\n",
    "print(\"RMSE for Gradient Boosting = \", np.sqrt(mean_squared_error(model_gb.predict(Xtest), ytest)))\n",
    "\n",
    "# Tuned Light GBM model from Section 13.1.1\n",
    "model_lgbm = LGBMRegressor(subsample = 0.5, reg_lambda = 0, reg_alpha = 100, boosting_type = 'goss',\n",
    "            num_leaves = 31, n_estimators = 500, learning_rate = 0.05, colsample_bytree = 1.0,\n",
    "                          top_rate = 0.5).fit(X, y)\n",
    "print(\"RMSE for LightGBM = \", np.sqrt(mean_squared_error(model_lgbm.predict(Xtest), ytest)))\n",
    "\n",
    "# Tuned CatBoost model from Section 13.2.3\n",
    "model_cat = CatBoostRegressor(subsample=0.5, num_leaves=40, n_estimators=500, max_depth=10, \n",
    "                              verbose = False, learning_rate = 0.05, colsample_bylevel=0.75, \n",
    "                              grow_policy='Lossguide', random_state = 1).fit(X, y)\n",
    "print(\"RMSE for CatBoost = \", np.sqrt(mean_squared_error(model_cat.predict(Xtest), ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6712f89",
   "metadata": {},
   "source": [
    "Note that we **don't need to fit** the models **individually** before fitting them simultaneously in the voting ensemble. If we fit them individual, it will unnecessarily **waste time**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22dde3e",
   "metadata": {},
   "source": [
    "Let us ensemble the models using the voting ensemble with equal weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ae9a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model RMSE =  5259.899392611916\n",
      "Time taken =  0.21 minutes\n"
     ]
    }
   ],
   "source": [
    "#Voting ensemble: Averaging the predictions of all models\n",
    "\n",
    "#Tuned AdaBoost model from Section 7.2.4\n",
    "model_ada = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=10),\n",
    "                    n_estimators=50,learning_rate=1.0,  random_state=1)\n",
    "\n",
    "#Tuned Random forest model from Section 6.1.2\n",
    "model_rf = RandomForestRegressor(n_estimators=300, random_state=1,\n",
    "                        n_jobs=-1, max_features=2)\n",
    "\n",
    "# Tuned XGBoost model from Section 9.2.6\n",
    "model_xgb = xgb.XGBRegressor(random_state=1,max_depth=8,n_estimators=1000, subsample = 0.75, \n",
    "                colsample_bytree = 1.0, learning_rate = 0.01,reg_lambda=1, gamma = 100)\n",
    "\n",
    "#Tuned gradient boosting model from Section 8.2.5\n",
    "model_gb = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n",
    "                         random_state=1,loss='huber')\n",
    "\n",
    "# Tuned CatBoost model from Section 13.2.3\n",
    "model_cat = CatBoostRegressor(subsample=0.5, num_leaves=40, n_estimators=500, max_depth=10,\n",
    "                             learning_rate = 0.05, colsample_bylevel=0.75, grow_policy='Lossguide',\n",
    "                             random_state=1, verbose = False)\n",
    "\n",
    "# Tuned Light GBM model from Section 13.1.1\n",
    "model_lgbm = LGBMRegressor(subsample = 0.5, reg_lambda = 0, reg_alpha = 100, boosting_type = 'goss',\n",
    "                           num_leaves = 31, n_estimators = 500, learning_rate = 0.05, \n",
    "                           colsample_bytree = 1.0, top_rate = 0.5)\n",
    "\n",
    "start_time = time.time()\n",
    "en = VotingRegressor(estimators = [('xgb',model_xgb),('ada',model_ada),('rf',model_rf),\n",
    "                    ('gb',model_gb), ('cat', model_cat), ('lgbm', model_lgbm)], n_jobs = -1)\n",
    "en.fit(X,y)\n",
    "print(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n",
    "print(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7f1b8",
   "metadata": {},
   "source": [
    "As expected, RMSE of the ensembled model is less than that of each of the individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37142d",
   "metadata": {},
   "source": [
    "Note that the RMSE can be **further improved** by **removing** the **weaker models** from the ensemble. Let us remove the three weakest models - XGBoost, Random forest, and AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f716f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model RMSE =  5191.814866810768\n",
      "Time taken =  0.18 minutes\n"
     ]
    }
   ],
   "source": [
    "#Voting ensemble: Averaging the predictions of all models\n",
    "\n",
    "start_time = time.time()\n",
    "en = VotingRegressor(estimators = [('gb',model_gb), ('cat', model_cat), ('lgbm', model_lgbm)], n_jobs = -1)\n",
    "en.fit(X,y)\n",
    "print(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n",
    "print(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aeaea6",
   "metadata": {},
   "source": [
    "### Stacking Regressor\n",
    "Stacking is a more sophisticated method of ensembling models. The method is as follows:\n",
    "\n",
    "1. The training data is split into *K* folds. Each of the *K* folds serves as a test data in one of the *K* iterations, and the rest of the folds serve as train data. \n",
    "\n",
    "2. Each model is used to make predictions on each of the *K* folds, after being trained on the remaining *K-1* folds. In this manner, each model predicts the response on each train data point - when that train data point was not used to train the model.\n",
    "\n",
    "3. Predictions at each training data points are generated by each model in step 2 (the above step). These predictions are now used as predictors to train a meta-model (referred by the argument `final_estimator`), with the original response as the response. The meta-model (or `final_estimator`) learns to combine predictions of different models to make a better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3bf9d3",
   "metadata": {},
   "source": [
    "#### Metamodel: Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1551cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression metamodel RMSE =  5220.456280327686\n",
      "Time taken =  2.03 minutes\n"
     ]
    }
   ],
   "source": [
    "#Stacking using LinearRegression as the metamodel\n",
    "en = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n",
    "                                     ('gb', model_gb), ('cat', model_cat), ('lgbm', model_lgbm)],\n",
    "                     final_estimator=LinearRegression(),                                          \n",
    "                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\n",
    "start_time = time.time()\n",
    "en.fit(X,y)\n",
    "print(\"Linear regression metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n",
    "print(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2fdff36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05502964,  0.14566665,  0.01093624,  0.30478283,  0.57403909,\n",
       "       -0.07057344])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Co-efficients of the meta-model\n",
    "en.final_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de216c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0198810182715363"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(en.final_estimator_.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc823e84",
   "metadata": {},
   "source": [
    "Note the above coefficients of the meta-model. The model gives the **highest weight** to the **gradient boosting** model *(with **huber** loss)*, and the **catboost** model, and the **lowest weight** to the relatively weak **random forest** model. \n",
    "\n",
    "Also, note that the **coefficients need not sum to one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7836ef5",
   "metadata": {},
   "source": [
    "Let us try improving the RMSE further by removing the weaker models from the ensemble. Let us remove the three weakest models based on the size of their coefficients in the linear regression metamodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7edc7e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression metamodel RMSE =  5205.225710180056\n",
      "Time taken =  1.36 minutes\n"
     ]
    }
   ],
   "source": [
    "#Stacking using LinearRegression as the metamodel\n",
    "en = StackingRegressor(estimators = [('gb', model_gb), ('cat', model_cat), ('ada', model_ada)],\n",
    "                     final_estimator=LinearRegression(),                                          \n",
    "                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\n",
    "start_time = time.time()\n",
    "en.fit(X,y)\n",
    "print(\"Linear regression metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n",
    "print(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b983606",
   "metadata": {},
   "source": [
    "The metamodel accuracy **improves further**, when **strong models** are ensembled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bc62655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31824119, 0.54231032, 0.15998634])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Co-efficients of the meta-model\n",
    "en.final_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44a9662e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.020537847948332"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(en.final_estimator_.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f3cad",
   "metadata": {},
   "source": [
    "#### Metamodel: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "85ed301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso metamodel RMSE =  5206.021083501416\n",
      "Time taken =  2.05 minutes\n"
     ]
    }
   ],
   "source": [
    "#Stacking using Lasso as the metamodel\n",
    "en = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n",
    "                        ('gb', model_gb),('cat', model_cat), ('lgbm', model_lgbm) ],\n",
    "                     final_estimator = LassoCV(),                                          \n",
    "                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\n",
    "start_time = time.time()\n",
    "en.fit(X,y)\n",
    "print(\"Lasso metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n",
    "print(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "404a2480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03524446,  0.15077605,  0.        ,  0.30392268,  0.52946243,\n",
       "       -0.        ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coefficients of the lasso metamodel\n",
    "en.final_estimator_.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71aa9b6",
   "metadata": {},
   "source": [
    "Note that lasso **reduces the weight** of the **weak random forest** model, and **light gbm** model to **0**. Even though light GBM is a strong model, it may be **correlated or collinear** with XGBoost, or other models, and hence is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc33dec",
   "metadata": {},
   "source": [
    "Note that as lasso performs **model selection** on its own, removing models with zero coefficients or weights does not make a difference, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96e28f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso metamodel RMSE =  5205.93233977352\n",
      "Time taken =  1.79 minutes\n"
     ]
    }
   ],
   "source": [
    "#Stacking using Lasso as the metamodel\n",
    "en = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),\n",
    "                        ('gb', model_gb),('cat', model_cat) ],\n",
    "                     final_estimator = LassoCV(),                                          \n",
    "                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\n",
    "start_time = time.time()\n",
    "en.fit(X,y)\n",
    "print(\"Lasso metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n",
    "print(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "028b0bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03415944, 0.15053122, 0.30464838, 0.53006297])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coefficients of the lasso metamodel\n",
    "en.final_estimator_.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a305387",
   "metadata": {},
   "source": [
    "#### Metamodel: Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8ab8f",
   "metadata": {},
   "source": [
    "A highly flexible model such as a random forest may not be a good choice for ensembling correlated models. However, let us tune the random forest meta model, and check its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8ac28bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken =  12.08 minutes\n"
     ]
    }
   ],
   "source": [
    "# Tuning hyperparameter of the random forest meta-model\n",
    "start_time = time.time()\n",
    "oob_score_i = []\n",
    "for i in range(1, 7):\n",
    "    en = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n",
    "                        ('gb', model_gb),('cat', model_cat), ('lgbm', model_lgbm)],\n",
    "                     final_estimator = RandomForestRegressor(max_features = i, oob_score = True),                                          \n",
    "                    cv = KFold(n_splits = 5, shuffle = True, random_state=1)).fit(X,y)\n",
    "    oob_score_i.append(en.final_estimator_.oob_score_)\n",
    "print(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c54723a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of max_features = 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal value of max_features =\", np.array(oob_score_i).argmax() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2ae9b9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest metamodel RMSE =  5441.9155087961\n",
      "Time taken =  1.71 minutes\n"
     ]
    }
   ],
   "source": [
    "# Training the tuned random forest metamodel\n",
    "start_time = time.time()\n",
    "en = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),\n",
    "                    ('rf', model_rf), ('gb', model_gb),('cat', model_cat), \n",
    "                    ('lgbm', model_lgbm)],\n",
    "                final_estimator = RandomForestRegressor(max_features = 1, \n",
    "                n_estimators=500), cv = KFold(n_splits = 5, shuffle = True, \n",
    "                random_state=1)).fit(X,y)\n",
    "print(\"Random Forest metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n",
    "print(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d74b9",
   "metadata": {},
   "source": [
    "Note that highly flexible models may not be needed when the predictors are highly correlated with the response. However, in some cases, they may be useful, as in the classification example in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e62ca",
   "metadata": {},
   "source": [
    "#### Metamodel: CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2cc9879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest metamodel RMSE =  5828.803609683251\n",
      "Time taken =  1.66 minutes\n"
     ]
    }
   ],
   "source": [
    "#Stacking using MARS as the meta-model\n",
    "en = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),\n",
    "                        ('gb', model_gb),('cat', model_cat), ('lgbm', model_lgbm)],\n",
    "                     final_estimator = CatBoostRegressor(verbose = False),                                          \n",
    "                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\n",
    "start_time = time.time()\n",
    "en.fit(X,y)\n",
    "print(\"Random Forest metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n",
    "print(\"Time taken = \", np.round((time.time() - start_time)/60,2), \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875d383",
   "metadata": {},
   "source": [
    "## Ensembling classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d96bc36",
   "metadata": {},
   "source": [
    "We'll ensemble models for predicting accuracy of identifying people having a heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8490f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Datasets/Heart.csv')\n",
    "data.dropna(inplace = True)\n",
    "#Response variable\n",
    "y = pd.get_dummies(data['AHD'])['Yes']\n",
    "\n",
    "#Creating a dataframe for predictors with dummy variables replacing the categorical variables\n",
    "X = data.drop(columns = ['AHD','ChestPain','Thal'])\n",
    "X = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\n",
    "\n",
    "#Creating train and test datasets\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f1ec4",
   "metadata": {},
   "source": [
    "Let us tune the individual models first.\n",
    "\n",
    "### AdaBoost {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbf61bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.871494 using {'base_estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 0.01, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Tuning Adaboost for maximizing accuracy\n",
    "model = AdaBoostClassifier(random_state=1)\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100,200,500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\n",
    "grid['base_estimator'] = [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), \n",
    "                          DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=4)]\n",
    "# define the evaluation procedure\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',refit='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(Xtrain, ytrain)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbcde10",
   "metadata": {},
   "source": [
    "### Gradient Boosting {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d979116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.871954 using {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 100, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Tuning gradient boosting for maximizing accuracy\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100,200,500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\n",
    "grid['max_depth'] = [1,2,3,4,5]\n",
    "grid['subsample'] = [0.5,1.0]\n",
    "# define the evaluation procedure\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',refit='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(Xtrain, ytrain)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345949c",
   "metadata": {},
   "source": [
    "### XGBoost {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0d4f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n",
      "{'gamma': 0, 'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 25, 'reg_lambda': 0, 'scale_pos_weight': 1.25} 0.872183908045977\n",
      "Time taken =  0.9524135629336039  minutes\n"
     ]
    }
   ],
   "source": [
    "# Tuning XGBoost for maximizing accuracy\n",
    "start_time = time.time()\n",
    "param_grid = {'n_estimators':[25, 100,250,500],\n",
    "                'max_depth': [4, 6 ,8],\n",
    "              'learning_rate': [0.01,0.1,0.2],\n",
    "               'gamma': [0, 1, 10, 100],\n",
    "               'reg_lambda':[0, 10, 100],\n",
    "               'subsample': [0.5, 0.75, 1.0]\n",
    "                'scale_pos_weight':[1.25,1.5,1.75]#Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances).\n",
    "             }\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n",
    "optimal_params = GridSearchCV(estimator=xgb.XGBClassifier(random_state=1),\n",
    "                             param_grid = param_grid,\n",
    "                             scoring = 'accuracy',\n",
    "                             verbose = 1,\n",
    "                             n_jobs=-1,\n",
    "                             cv = cv)\n",
    "optimal_params.fit(Xtrain,ytrain)\n",
    "print(optimal_params.best_params_,optimal_params.best_score_)\n",
    "print(\"Time taken = \", (time.time()-start_time)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "54ccc812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost accuracy =  0.7986577181208053\n",
      "Random forest accuracy =  0.8120805369127517\n",
      "Gradient boost accuracy =  0.7986577181208053\n",
      "XGBoost model accuracy =  0.7785234899328859\n"
     ]
    }
   ],
   "source": [
    "#Tuned Adaboost model\n",
    "model_ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=200, \n",
    "                               random_state=1,learning_rate=0.01).fit(Xtrain, ytrain)    \n",
    "test_accuracy_ada = model_ada.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n",
    "    \n",
    "#Tuned Random forest model from Section 6.3\n",
    "model_rf = RandomForestClassifier(n_estimators=500, random_state=1,max_features=3,\n",
    "                        n_jobs=-1,oob_score=False).fit(Xtrain, ytrain)\n",
    "test_accuracy_rf = model_rf.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n",
    "    \n",
    "#Tuned gradient boosting model\n",
    "model_gb = GradientBoostingClassifier(n_estimators=100, random_state=1,max_depth=4,learning_rate=1.0,\n",
    "                                     subsample = 1.0).fit(Xtrain, ytrain)\n",
    "test_accuracy_gb = model_gb.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n",
    "\n",
    "#Tuned XGBoost model\n",
    "model_xgb = xgb.XGBClassifier(random_state=1,gamma=0,learning_rate = 0.2,max_depth=4,\n",
    "                              n_estimators = 25,reg_lambda = 0,scale_pos_weight=1.25).fit(Xtrain,ytrain)\n",
    "test_accuracy_xgb = model_xgb.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n",
    "\n",
    "print(\"Adaboost accuracy = \",test_accuracy_ada)\n",
    "print(\"Random forest accuracy = \",test_accuracy_rf)\n",
    "print(\"Gradient boost accuracy = \",test_accuracy_gb)\n",
    "print(\"XGBoost model accuracy = \",test_accuracy_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd653a7",
   "metadata": {},
   "source": [
    "### Voting classifier - hard voting\n",
    "In this type of ensembling, the predicted class is the one predicted by the majority of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d30cc7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.825503355704698"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)])\n",
    "ensemble_model.fit(Xtrain,ytrain)\n",
    "ensemble_model.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5915b57",
   "metadata": {},
   "source": [
    "Note that the prediction accuracy of the ensemble is higher than the prediction accuracy of each of the individual models on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a5d02",
   "metadata": {},
   "source": [
    "### Voting classifier - soft voting\n",
    "In this type of ensembling, the predicted class is the one based on the average predicted probabilities of all the classifiers. The threshold probability is 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e7c0f301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7919463087248322"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n",
    "                                 voting='soft')\n",
    "ensemble_model.fit(Xtrain,ytrain)\n",
    "ensemble_model.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c70bed",
   "metadata": {},
   "source": [
    "Note that soft voting will be good only for well calibrated classifiers, i.e., all the classifiers must have probabilities at the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a4c3f4",
   "metadata": {},
   "source": [
    "### Stacking classifier\n",
    "Conceptually, the idea is similar to that of Stacking regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "20cd5cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7986577181208053"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Logistic regression as the meta model (final_estimator)\n",
    "ensemble_model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n",
    "                                   final_estimator=LogisticRegression(random_state=1,max_iter=10000),n_jobs=-1,\n",
    "                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1))\n",
    "ensemble_model.fit(Xtrain,ytrain)\n",
    "ensemble_model.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b48ecdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81748051, 1.28663164, 1.64593342, 1.50947087]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coefficients of the logistic regression metamodel\n",
    "ensemble_model.final_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "57693cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8322147651006712"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using random forests as the meta model (final_estimator). Note that random forest will require tuning\n",
    "ensemble_model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n",
    "                                   final_estimator=RandomForestClassifier(n_estimators=500, max_features=1,\n",
    "                                                                          random_state=1,oob_score=True),n_jobs=-1,\n",
    "                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1))\n",
    "ensemble_model.fit(Xtrain,ytrain)\n",
    "ensemble_model.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd67695",
   "metadata": {},
   "source": [
    "Note that a complex `final_estimator` such as random forest will require tuning. In the above case, the `max_features` argument of random forests has been tuned to obtain the maximum OOB score. The tuning is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc5a6852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken =  0.33713538646698  minutes\n",
      "max accuracy =  0.8445945945945946\n",
      "Best value of max_features=  1\n"
     ]
    }
   ],
   "source": [
    "#Tuning the random forest parameters\n",
    "start_time = time.time()\n",
    "oob_score = {}\n",
    "\n",
    "i=0\n",
    "for pr in range(1,5):\n",
    "    model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n",
    "                                   final_estimator=RandomForestClassifier(n_estimators=500, max_features=pr,\n",
    "                                    random_state=1,oob_score=True),n_jobs=-1,\n",
    "                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)).fit(Xtrain, ytrain)\n",
    "    oob_score[pr] = model.final_estimator_.oob_score_\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"time taken = \", (end_time-start_time)/60, \" minutes\")\n",
    "print(\"max accuracy = \", np.max(list(oob_score.values())))\n",
    "print(\"Best value of max_features= \", np.argmax(list(oob_score.values()))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4af7c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.8445945945945946,\n",
       " 2: 0.831081081081081,\n",
       " 3: 0.8378378378378378,\n",
       " 4: 0.831081081081081}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The final predictor (metamodel) - random forest obtains the maximum oob_score for max_features = 1\n",
    "oob_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954fa55",
   "metadata": {},
   "source": [
    "### Tuning all models simultaneously\n",
    "\n",
    "Individual model hyperparameters can be tuned simultaneously while ensembling them with a `VotingClassifier()`. However, this approach can be too expensive for even moderately-sized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the param grid with the names of the models as prefixes\n",
    "\n",
    "model_ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier())\n",
    "model_rf = RandomForestClassifier()\n",
    "model_gb = GradientBoostingClassifier()\n",
    "model_xgb = xgb.XGBClassifier()\n",
    "\n",
    "ensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)])\n",
    "\n",
    "hp_grid = dict()\n",
    "\n",
    "# XGBoost\n",
    "hp_grid['xgb__n_estimators'] = [25, 100,250,50]\n",
    "hp_grid['xgb__max_depth'] = [4, 6 ,8]\n",
    "hp_grid['xgb__learning_rate'] = [0.01, 0.1, 1.0]\n",
    "hp_grid['xgb__gamma'] = [0, 1, 10, 100]\n",
    "hp_grid['xgb__reg_lambda'] = [0, 1, 10, 100]\n",
    "hp_grid['xgb__subsample'] = [0, 1, 10, 100]\n",
    "hp_grid['xgb__scale_pos_weight'] = [1.0, 1.25, 1.5]\n",
    "hp_grid['xgb__colsample_bytree'] = [0.5, 0.75, 1.0]\n",
    "\n",
    "# AdaBoost\n",
    "hp_grid['ada__n_estimators'] = [10, 50, 100,200,500]\n",
    "hp_grid['ada__base_estimator__max_depth'] = [1, 3, 5]\n",
    "hp_grid['ada__learning_rate'] = [0.01, 0.1, 0.2]\n",
    "\n",
    "# Random Forest\n",
    "hp_grid['rf__n_estimators'] = [100]\n",
    "hp_grid['rf__max_features'] = [3, 6, 9, 12, 15]\n",
    "\n",
    "# GradBoost\n",
    "hp_grid['gb__n_estimators'] = [10, 50, 100,200,500]\n",
    "hp_grid['gb__max_depth'] = [1, 3, 5]\n",
    "hp_grid['gb__learning_rate'] = [0.01, 0.1, 0.2, 1.0]\n",
    "hp_grid['gb__subsample'] = [0.5, 0.75, 1.0]\n",
    "\n",
    "start_time = time.time()\n",
    "grid = RandomizedSearchCV(ensemble_model, hp_grid, cv=5, scoring='accuracy', verbose = True,\n",
    "                         n_iter = 100, n_jobs=-1).fit(Xtrain, ytrain)\n",
    "print(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "97a2b9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8120805369127517"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d0e4b",
   "metadata": {},
   "source": [
    "## Ensembling models based on different sets of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3ec65",
   "metadata": {},
   "source": [
    "Generally, the tree-based models such as CatBoost, and XGBoost are the most accurate, while other models, such as bagging, random forests, KNN, and linear models, may not be as accurate. Thus, sometimes, the weaker models, despite bringing-in diversity in the model ensemble may deteriorate the ensemble accuracy due to their poor individual performace *(check sldies for technical details)*. Thus, sometimes, another approach to bring in model diversity is to develop strong models based on different sets of predictors, and ensemble them.\n",
    "\n",
    "Different feature selection methods *(such as Lasso, `feature_importance_` attribute of tree-based methods, stepwise k-fold feature selection, etc.)*, may be used to obtain different sets of important features, strong models can be tuned on these sets, and then ensembled. Even though the models may be of the same type, the different sets of predictors will help bring-in the element of diversity in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46e1cc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carID</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>year</th>\n",
       "      <th>transmission</th>\n",
       "      <th>mileage</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>tax</th>\n",
       "      <th>mpg</th>\n",
       "      <th>engineSize</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18473</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2020</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>11</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.3282</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15064</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2019</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>10813</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.0430</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18268</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2020</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>6</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>53.4379</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18480</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2017</td>\n",
       "      <td>Semi-Auto</td>\n",
       "      <td>18895</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>145</td>\n",
       "      <td>51.5140</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18492</td>\n",
       "      <td>bmw</td>\n",
       "      <td>6 Series</td>\n",
       "      <td>2015</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>62953</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>160</td>\n",
       "      <td>51.4903</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carID brand      model  year transmission  mileage fuelType  tax      mpg  \\\n",
       "0  18473   bmw   6 Series  2020    Semi-Auto       11   Diesel  145  53.3282   \n",
       "1  15064   bmw   6 Series  2019    Semi-Auto    10813   Diesel  145  53.0430   \n",
       "2  18268   bmw   6 Series  2020    Semi-Auto        6   Diesel  145  53.4379   \n",
       "3  18480   bmw   6 Series  2017    Semi-Auto    18895   Diesel  145  51.5140   \n",
       "4  18492   bmw   6 Series  2015    Automatic    62953   Diesel  160  51.4903   \n",
       "\n",
       "   engineSize  price  \n",
       "0         3.0  37980  \n",
       "1         3.0  33980  \n",
       "2         3.0  36850  \n",
       "3         3.0  25998  \n",
       "4         3.0  18990  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainf = pd.read_csv('./Datasets/Car_features_train.csv')\n",
    "trainp = pd.read_csv('./Datasets/Car_prices_train.csv')\n",
    "testf = pd.read_csv('./Datasets/Car_features_test.csv')\n",
    "testp = pd.read_csv('./Datasets/Car_prices_test.csv')\n",
    "train = pd.merge(trainf,trainp)\n",
    "test = pd.merge(testf,testp)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92fc7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['mileage','mpg','year','engineSize']]\n",
    "Xtest = test[['mileage','mpg','year','engineSize']]\n",
    "y = train['price']\n",
    "ytest = test['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f840883",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_set = PolynomialFeatures(2, include_bias = False)\n",
    "X_poly = poly_set.fit_transform(X)\n",
    "X_poly = pd.DataFrame(X_poly, columns=poly_set.get_feature_names_out())\n",
    "X_poly.columns = X_poly.columns.str.replace(\"^\", \"_\", regex=True)\n",
    "Xtest_poly = poly_set.fit_transform(Xtest)\n",
    "Xtest_poly = pd.DataFrame(Xtest_poly, columns=poly_set.get_feature_names_out())\n",
    "Xtest_poly.columns = Xtest_poly.columns.str.replace(\"^\", \"_\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3064ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_set1 = ['mileage','mpg', 'year','engineSize']\n",
    "col_set2 = X_poly.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef1adf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CatBoostRegressor(verbose=False)\n",
    "gb = GradientBoostingRegressor(loss = 'huber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc9b09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe1 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('cat1_transform', 'passthrough', col_set1)], remainder='drop')),\n",
    "    ('cat1', cat)\n",
    "])\n",
    "\n",
    "cat_pipe2 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('cat2_transform', 'passthrough', col_set2)], remainder='drop')),\n",
    "    ('cat2', cat)\n",
    "])\n",
    "\n",
    "gb_pipe1 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('gb1_transform', 'passthrough', col_set1)], remainder='drop')),\n",
    "    ('gb1', gb)\n",
    "])\n",
    "\n",
    "gb_pipe2 = Pipeline([\n",
    "    ('column_transformer', ColumnTransformer([('gb2_transform', 'passthrough', col_set2)], remainder='drop')),\n",
    "    ('gb2', gb)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a5baa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_new = StackingRegressor(estimators = [('cat1', cat_pipe1),('cat2', cat_pipe2),\n",
    "                                        ('gb1', gb_pipe1), ('gb2', gb_pipe2)],\n",
    "                     final_estimator=LinearRegression(),                                          \n",
    "                    cv = KFold(n_splits = 15, shuffle = True, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd0016f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(cv=KFold(n_splits=15, random_state=1, shuffle=True),\n",
       "                  estimators=[(&#x27;cat1&#x27;,\n",
       "                               Pipeline(steps=[(&#x27;column_transformer&#x27;,\n",
       "                                                ColumnTransformer(transformers=[(&#x27;cat1_transform&#x27;,\n",
       "                                                                                 &#x27;passthrough&#x27;,\n",
       "                                                                                 [&#x27;mileage&#x27;,\n",
       "                                                                                  &#x27;mpg&#x27;,\n",
       "                                                                                  &#x27;year&#x27;,\n",
       "                                                                                  &#x27;engineSize&#x27;])])),\n",
       "                                               (&#x27;cat1&#x27;,\n",
       "                                                &lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;)])),\n",
       "                              (&#x27;cat2&#x27;,\n",
       "                               Pipeline(steps=[(&#x27;column_transformer&#x27;,...\n",
       "                               Pipeline(steps=[(&#x27;column_transformer&#x27;,\n",
       "                                                ColumnTransformer(transformers=[(&#x27;gb2_transform&#x27;,\n",
       "                                                                                 &#x27;passthrough&#x27;,\n",
       "                                                                                 Index([&#x27;mileage&#x27;, &#x27;mpg&#x27;, &#x27;year&#x27;, &#x27;engineSize&#x27;, &#x27;mileage_2&#x27;, &#x27;mileage mpg&#x27;,\n",
       "       &#x27;mileage year&#x27;, &#x27;mileage engineSize&#x27;, &#x27;mpg_2&#x27;, &#x27;mpg year&#x27;,\n",
       "       &#x27;mpg engineSize&#x27;, &#x27;year_2&#x27;, &#x27;year engineSize&#x27;, &#x27;engineSize_2&#x27;],\n",
       "      dtype=&#x27;object&#x27;))])),\n",
       "                                               (&#x27;gb2&#x27;,\n",
       "                                                GradientBoostingRegressor(loss=&#x27;huber&#x27;))]))],\n",
       "                  final_estimator=LinearRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(cv=KFold(n_splits=15, random_state=1, shuffle=True),\n",
       "                  estimators=[(&#x27;cat1&#x27;,\n",
       "                               Pipeline(steps=[(&#x27;column_transformer&#x27;,\n",
       "                                                ColumnTransformer(transformers=[(&#x27;cat1_transform&#x27;,\n",
       "                                                                                 &#x27;passthrough&#x27;,\n",
       "                                                                                 [&#x27;mileage&#x27;,\n",
       "                                                                                  &#x27;mpg&#x27;,\n",
       "                                                                                  &#x27;year&#x27;,\n",
       "                                                                                  &#x27;engineSize&#x27;])])),\n",
       "                                               (&#x27;cat1&#x27;,\n",
       "                                                &lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;)])),\n",
       "                              (&#x27;cat2&#x27;,\n",
       "                               Pipeline(steps=[(&#x27;column_transformer&#x27;,...\n",
       "                               Pipeline(steps=[(&#x27;column_transformer&#x27;,\n",
       "                                                ColumnTransformer(transformers=[(&#x27;gb2_transform&#x27;,\n",
       "                                                                                 &#x27;passthrough&#x27;,\n",
       "                                                                                 Index([&#x27;mileage&#x27;, &#x27;mpg&#x27;, &#x27;year&#x27;, &#x27;engineSize&#x27;, &#x27;mileage_2&#x27;, &#x27;mileage mpg&#x27;,\n",
       "       &#x27;mileage year&#x27;, &#x27;mileage engineSize&#x27;, &#x27;mpg_2&#x27;, &#x27;mpg year&#x27;,\n",
       "       &#x27;mpg engineSize&#x27;, &#x27;year_2&#x27;, &#x27;year engineSize&#x27;, &#x27;engineSize_2&#x27;],\n",
       "      dtype=&#x27;object&#x27;))])),\n",
       "                                               (&#x27;gb2&#x27;,\n",
       "                                                GradientBoostingRegressor(loss=&#x27;huber&#x27;))]))],\n",
       "                  final_estimator=LinearRegression())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>cat1</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;cat1_transform&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;mileage&#x27;, &#x27;mpg&#x27;, &#x27;year&#x27;, &#x27;engineSize&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat1_transform</label><div class=\"sk-toggleable__content\"><pre>[&#x27;mileage&#x27;, &#x27;mpg&#x27;, &#x27;year&#x27;, &#x27;engineSize&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>cat2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;cat2_transform&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 Index([&#x27;mileage&#x27;, &#x27;mpg&#x27;, &#x27;year&#x27;, &#x27;engineSize&#x27;, &#x27;mileage_2&#x27;, &#x27;mileage mpg&#x27;,\n",
       "       &#x27;mileage year&#x27;, &#x27;mileage engineSize&#x27;, &#x27;mpg_2&#x27;, &#x27;mpg year&#x27;,\n",
       "       &#x27;mpg engineSize&#x27;, &#x27;year_2&#x27;, &#x27;year engineSize&#x27;, &#x27;engineSize_2&#x27;],\n",
       "      dtype=&#x27;object&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat2_transform</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;mileage&#x27;, &#x27;mpg&#x27;, &#x27;year&#x27;, &#x27;engineSize&#x27;, &#x27;mileage_2&#x27;, &#x27;mileage mpg&#x27;,\n",
       "       &#x27;mileage year&#x27;, &#x27;mileage engineSize&#x27;, &#x27;mpg_2&#x27;, &#x27;mpg year&#x27;,\n",
       "       &#x27;mpg engineSize&#x27;, &#x27;year_2&#x27;, &#x27;year engineSize&#x27;, &#x27;engineSize_2&#x27;],\n",
       "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0&gt;</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>gb1</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;gb1_transform&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;mileage&#x27;, &#x27;mpg&#x27;, &#x27;year&#x27;, &#x27;engineSize&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">gb1_transform</label><div class=\"sk-toggleable__content\"><pre>[&#x27;mileage&#x27;, &#x27;mpg&#x27;, &#x27;year&#x27;, &#x27;engineSize&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(loss=&#x27;huber&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>gb2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;gb2_transform&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 Index([&#x27;mileage&#x27;, &#x27;mpg&#x27;, &#x27;year&#x27;, &#x27;engineSize&#x27;, &#x27;mileage_2&#x27;, &#x27;mileage mpg&#x27;,\n",
       "       &#x27;mileage year&#x27;, &#x27;mileage engineSize&#x27;, &#x27;mpg_2&#x27;, &#x27;mpg year&#x27;,\n",
       "       &#x27;mpg engineSize&#x27;, &#x27;year_2&#x27;, &#x27;year engineSize&#x27;, &#x27;engineSize_2&#x27;],\n",
       "      dtype=&#x27;object&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">gb2_transform</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;mileage&#x27;, &#x27;mpg&#x27;, &#x27;year&#x27;, &#x27;engineSize&#x27;, &#x27;mileage_2&#x27;, &#x27;mileage mpg&#x27;,\n",
       "       &#x27;mileage year&#x27;, &#x27;mileage engineSize&#x27;, &#x27;mpg_2&#x27;, &#x27;mpg year&#x27;,\n",
       "       &#x27;mpg engineSize&#x27;, &#x27;year_2&#x27;, &#x27;year engineSize&#x27;, &#x27;engineSize_2&#x27;],\n",
       "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(loss=&#x27;huber&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingRegressor(cv=KFold(n_splits=15, random_state=1, shuffle=True),\n",
       "                  estimators=[('cat1',\n",
       "                               Pipeline(steps=[('column_transformer',\n",
       "                                                ColumnTransformer(transformers=[('cat1_transform',\n",
       "                                                                                 'passthrough',\n",
       "                                                                                 ['mileage',\n",
       "                                                                                  'mpg',\n",
       "                                                                                  'year',\n",
       "                                                                                  'engineSize'])])),\n",
       "                                               ('cat1',\n",
       "                                                <catboost.core.CatBoostRegressor object at 0x000002CAF5410DF0>)])),\n",
       "                              ('cat2',\n",
       "                               Pipeline(steps=[('column_transformer',...\n",
       "                               Pipeline(steps=[('column_transformer',\n",
       "                                                ColumnTransformer(transformers=[('gb2_transform',\n",
       "                                                                                 'passthrough',\n",
       "                                                                                 Index(['mileage', 'mpg', 'year', 'engineSize', 'mileage_2', 'mileage mpg',\n",
       "       'mileage year', 'mileage engineSize', 'mpg_2', 'mpg year',\n",
       "       'mpg engineSize', 'year_2', 'year engineSize', 'engineSize_2'],\n",
       "      dtype='object'))])),\n",
       "                                               ('gb2',\n",
       "                                                GradientBoostingRegressor(loss='huber'))]))],\n",
       "                  final_estimator=LinearRegression())"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_new.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0576b3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5185.376722607323"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(en_new.predict(Xtest_poly), ytest, squared = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594e4d8e",
   "metadata": {},
   "source": [
    "Note that the above model does better on test data than all the models developed so far. Using different sets of predictors introduces diversity in the ensemble, while using \"strong\" models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
