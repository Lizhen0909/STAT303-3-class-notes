[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science III with python (Class notes)",
    "section": "",
    "text": "Preface\nThese are class notes for the course STAT303-3. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThe course notes are currently being written, and will continue to being developed as the course progresses (just like the class notes last quarter). Please report any typos / mistakes / inconsistencies / issues with the class notes / class presentations in your comments here. Thank you!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html",
    "href": "L1_Scikit-learn.html",
    "title": "1  Introduction to scikit-learn",
    "section": "",
    "text": "1.1 Splitting data into train and test\nLet us create train and test datasets for developing a model to predict if a person has diabetes.\n# Creating training and test data\n    # 80-20 split, which is usual - 70-30 split is also fine, 90-10 is fine if the dataset is large\n    # random_state to set a random seed for the splitting - reproducible results\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the complete dataset.\n#Proportion of 0s and 1s in the complete data\ny.value_counts()/y.shape\n\n0    0.651042\n1    0.348958\nName: Outcome, dtype: float64\nLet us find the proportion of classes (‘having diabetes’ (\\(y = 1\\)) or ‘not having diabetes’ (\\(y = 0\\))) in the train dataset.\n#Proportion of 0s and 1s in train data\ny_train.value_counts()/y_train.shape\n\n0    0.644951\n1    0.355049\nName: Outcome, dtype: float64\n#Proportion of 0s and 1s in test data\ny_test.value_counts()/y_test.shape\n\n0    0.675325\n1    0.324675\nName: Outcome, dtype: float64\nWe observe that the proportion of 0s and 1s in the train and test dataset are slightly different from that in the complete data. In order for these datasets to be more representative of the population, they should have a proportion of 0s and 1s similar to that in the complete dataset. This is especially critical in case of imbalanced datasets, where one class is represented by a significantly smaller number of instances than the other(s).\nWhen training a classification model on an imbalanced dataset, the model might not learn enough about the minority class, which can lead to poor generalization performance on new data. This happens because the model is biased towards the majority class, and it might even predict all instances as belonging to the majority class.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "href": "L1_Scikit-learn.html#splitting-data-into-train-and-test",
    "title": "1  Introduction to scikit-learn",
    "section": "",
    "text": "1.1.1 Stratified splitting\nWe will use the argument stratify to obtain a proportion of 0s and 1s in the train and test datasets that is similar to the proportion in the complete `data.\n\n#Stratified train-test split\nX_train_stratified, X_test_stratified, y_train_stratified,\\\ny_test_stratified = train_test_split(X, y, test_size = 0.2, random_state = 45, stratify=y)\n\n\n#Proportion of 0s and 1s in train data with stratified split\ny_train_stratified.value_counts()/y_train.shape\n\n0    0.651466\n1    0.348534\nName: Outcome, dtype: float64\n\n\n\n#Proportion of 0s and 1s in test data with stratified split\ny_test_stratified.value_counts()/y_test.shape\n\n0    0.649351\n1    0.350649\nName: Outcome, dtype: float64\n\n\nThe proportion of the classes in the stratified split mimics the proportion in the complete dataset more closely.\nBy using stratified splitting, we ensure that both the train and test data sets have the same proportion of instances from each class, which means that the model will see enough instances from the minority class during training. This, in turn, helps the model learn to distinguish between the classes better, leading to better performance on new data.\nThus, stratified splitting helps to ensure that the model sees enough instances from each class during training, which can improve the model’s ability to generalize to new data, particularly in cases where one class is underrepresented in the dataset.\nLet us develop a logistic regression model for predicting if a person has diabetes.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#scaling-data",
    "href": "L1_Scikit-learn.html#scaling-data",
    "title": "1  Introduction to scikit-learn",
    "section": "1.2 Scaling data",
    "text": "1.2 Scaling data\nIn certain models, it may be important to scale data for various reasons. In a logistic regression model, scaling can help with model convergence. Scikit-learn uses a method known as gradient-descent (not in scope of the syllabus of this course) to obtain a solution. In case the predictors have different orders of magnitude, the algorithm may fail to converge. In such cases, it is useful to standardize the predictors so that all of them are at the same scale.\n\n# With linear/logistic regression in scikit-learn, especially when the predictors have different orders \n# of magn., scaling is necessary. This is to enable the training algo. which we did not cover. (Gradient Descent)\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test) # Do NOT refit the scaler with the test data, just transform it.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#fitting-a-model",
    "href": "L1_Scikit-learn.html#fitting-a-model",
    "title": "1  Introduction to scikit-learn",
    "section": "1.3 Fitting a model",
    "text": "1.3 Fitting a model\nLet us fit a logistic regression model for predicting if a person has diabetes. Let us try fitting a model with the un-scaled data.\n\n# Create a model object - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train, y_train)\n\nC:\\Users\\akl0407\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\nNote that the model with the un-scaled predictors fails to converge. Check out the data X_train to see that this may be probably due to the predictors have different orders of magnitude. For example, the predictor DiabetesPedigreeFunction has values in [0.078, 2.42], while the predictor Insulin has values in [0, 800].\nLet us fit the model to the scaled data.\n\n# Create a model - not trained yet\nlogreg = LogisticRegression()\n\n# Train the model\nlogreg.fit(X_train_scaled, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\nThe model converges to a solution with the scaled data!\nThe coefficients of the model can be returned with the coef_ attribute of the LogisticRegression() object. However, the output is not as well formatted as in the case of the statsmodels library since sklearn is developed primarily for the purpose of prediction, and not inference.\n\n# Use coef_ to return the coefficients - only log reg inference you can do with sklearn\nprint(logreg.coef_) \n\n[[ 0.32572891  1.20110566 -0.32046591  0.06849882 -0.21727131  0.72619528\n   0.40088897  0.29698818]]",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#computing-performance-metrics",
    "href": "L1_Scikit-learn.html#computing-performance-metrics",
    "title": "1  Introduction to scikit-learn",
    "section": "1.4 Computing performance metrics",
    "text": "1.4 Computing performance metrics\n\n1.4.1 Accuracy\nLet us test the model prediction accuracy on the test data. We’ll demonstrate two different functions that can be used to compute model accuracy - accuracy_score(), and score().\nThe accuracy_score() function from the metrics module of the sklearn library is general, and can be used for any classification model. We’ll use it along with the predict() method of the LogisticRegression() object, which returns the predicted class based on a threshold probability of 0.5.\n\n# Get the predicted classes first\ny_pred = logreg.predict(X_test_scaled)\n\n# Use the predicted and true classes for accuracy\nprint(accuracy_score(y_pred, y_test)*100) \n\n73.37662337662337\n\n\nThe score() method of the LogisticRegression() object can be used to compute the accuracy only for a logistic regression model. Note that for a LinearRegression() object, the score() method will return the model \\(R\\)-squared.\n\n# Use .score with test predictors and response to get the accuracy\n# Implements the same thing under the hood\nprint(logreg.score(X_test_scaled, y_test)*100)  \n\n73.37662337662337\n\n\n\n\n1.4.2 ROC-AUC\nThe roc_curve() and auc() functions from the metrics module of the sklearn library can be used to compute the ROC-AUC, or the area under the ROC curve. Note that for computing ROC-AUC, we need the predicted probability, instead of the predicted class. Thus, we’ll use the predict_proba() method of the LogisticRegression() object, which returns the predicted probability for the observation to belong to each of the classes, instead of using the predict() method, which returns the predicted class based on threshold probability of 0.5.\n\n#Computing the predicted probability for the observation to belong to the positive class (y=1);\n#The 2nd column in the output of predict_proba() consists of the probability of the observation to \n#belong to the positive class (y=1)\ny_pred_prob = logreg.predict_proba(X_test_scaled)[:,1] \n\n#Using the predicted probability computed above to find ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test, y_pred_prob)\nprint(auc(fpr, tpr))# AUC of ROC\n\n0.7923076923076922\n\n\n\n\n1.4.3 Confusion matrix & precision-recall\nThe confusion_matrix(), precision_score(), and recall_score() functions from the metrics module of the sklearn library can be used to compute the confusion matrix, precision, and recall respectively.\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\n\n\n\n\n\n\n\n\nprint(\"Precision: \", precision_score(y_test, y_pred))\nprint(\"Recall: \", recall_score(y_test, y_pred))\n\nPrecision:  0.6046511627906976\nRecall:  0.52\n\n\nLet us compute the performance metrics if we develop the model using stratified splitting.\n\n# Developing the model with stratified splitting\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n#Computing the accuracy\ny_pred_stratified = logreg.predict(X_test_stratified_scaled)\nprint(\"Accuracy: \",accuracy_score(y_pred_stratified, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_stratified))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_stratified))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_stratified), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  78.57142857142857\nROC-AUC:  0.8505555555555556\nPrecision:  0.7692307692307693\nRecall:  0.5555555555555556\n\n\n\n\n\n\n\n\n\nThe model with the stratified train-test split has a better performance as compared to the other model on all the performance metrics!",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "L1_Scikit-learn.html#tuning-the-model-hyperparameters",
    "href": "L1_Scikit-learn.html#tuning-the-model-hyperparameters",
    "title": "1  Introduction to scikit-learn",
    "section": "1.5 Tuning the model hyperparameters",
    "text": "1.5 Tuning the model hyperparameters\nA hyperparameter (among others) that can be trained in a logistic regression model is the regularization parameter.\nWe may also wish to tune the decision threshold probability. Note that the decision threshold probability is not considered a hyperparameter of the model. Hyperparameters are model parameters that are set prior to training and cannot be directly adjusted by the model during training. Examples of hyperparameters in a logistic regression model include the regularization parameter, and the type of shrinkage penalty - lasso / ridge. These hyperparameters are typically optimized through a separate tuning process, such as cross-validation or grid search, before training the final model.\nThe performance metrics can be computed using a desired value of the threshold probability. Let us compute the performance metrics for a desired threshold probability of 0.3.\n\n# Performance metrics computation for a desired threshold probability of 0.3\ndesired_threshold = 0.3\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob &gt; desired_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  75.32467532467533\nROC-AUC:  0.8505555555555556\nPrecision:  0.6111111111111112\nRecall:  0.8148148148148148\n\n\n\n\n\n\n\n\n\n\n1.5.1 Tuning decision threshold probability\nSuppose we wish to find the optimal decision threshold probability to maximize accuracy. Note that we cannot use the test dataset to optimize model hyperparameters, as that may lead to overfitting on the test data. We’ll use \\(K\\)-fold cross validation on train data to find the optimal decision threshold probability.\nWe’ll use the cross_val_predict() function from the model_selection module of sklearn to compute the \\(K\\)-fold cross validated predicted probabilities. Note that this function simplifies the task of manually creating the \\(K\\)-folds, training the model \\(K\\)-times, and computing the predicted probabilities on each of the \\(K\\)-folds. Thereafter, the predicted probabilities will be used to find the optimal threshold probability that maximizes the classification accuracy.\n\nhyperparam_vals = np.arange(0,1.01,0.01)\naccuracy_iter = []\n\npredicted_probability = cross_val_predict(LogisticRegression(), X_train_stratified_scaled, \n                                              y_train_stratified, cv = 5, method = 'predict_proba')\n\nfor threshold_prob in hyperparam_vals:\n    predicted_class = predicted_probability[:,1] &gt; threshold_prob\n    predicted_class = predicted_class.astype(int)\n\n    #Computing the accuracy\n    accuracy = accuracy_score(predicted_class, y_train_stratified)*100\n    accuracy_iter.append(accuracy)\n\nLet us visualize the accuracy with change in decision threshold probability.\n\n# Accuracy vs decision threshold probability\nsns.scatterplot(x = hyperparam_vals, y = accuracy_iter)\nplt.xlabel('Decision threshold probability')\nplt.ylabel('Average 5-fold CV accuracy');\n\n\n\n\n\n\n\n\nThe optimal decision threshold probability is the one that maximizes the \\(K\\)-fold cross validation accuracy.\n\n# Optimal decision threshold probability\nhyperparam_vals[accuracy_iter.index(max(accuracy_iter))]\n\n0.46\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.46\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob &gt; desired_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8505555555555556\nPrecision:  0.7804878048780488\nRecall:  0.5925925925925926\n\n\n\n\n\n\n\n\n\nModel performance on test data has improved with the optimal decision threshold probability.\n\n\n1.5.2 Tuning the regularization parameter\nThe LogisticRegression() method has a default L2 regularization penalty, which means ridge regression.C is \\(1/\\lambda\\), where \\(\\lambda\\) is the hyperparameter that is multiplied with the ridge penalty. C is 1 by default.\n\naccuracy_iter = []\nhyperparam_vals = 10**np.linspace(-3.5, 1)\n\nfor c_val in hyperparam_vals: # For each possible C value in your grid\n    logreg_model = LogisticRegression(C=c_val) # Create a model with the C value\n    \n    accuracy_iter.append(cross_val_score(logreg_model, X_train_stratified_scaled, y_train_stratified,\n                                      scoring='accuracy', cv=5)) # Find the cv results\n\n\nplt.plot(hyperparam_vals, np.mean(np.array(accuracy_iter), axis=1))\nplt.xlabel('C')\nplt.ylabel('Average 5-fold CV accuracy')\nplt.xscale('log')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Optimal value of the regularization parameter 'C'\noptimal_C = hyperparam_vals[np.argmax(np.array(accuracy_iter).mean(axis=1))]\noptimal_C\n\n0.11787686347935879\n\n\n\n# Developing the model with stratified splitting and optimal 'C'\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg = LogisticRegression(C = optimal_C)\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n#Computing the accuracy\ny_pred_stratified = logreg.predict(X_test_stratified_scaled)\nprint(\"Accuracy: \",accuracy_score(y_pred_stratified, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_stratified))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_stratified))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_stratified), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  78.57142857142857\nROC-AUC:  0.8516666666666666\nPrecision:  0.7837837837837838\nRecall:  0.5370370370370371\n\n\n\n\n\n\n\n\n\n\n\n1.5.3 Tuning the decision threshold probability and the regularization parameter simultaneously\n\nthreshold_hyperparam_vals = np.arange(0,1.01,0.01)\nC_hyperparam_vals = 10**np.linspace(-3.5, 1)\naccuracy_iter = pd.DataFrame({'threshold':[], 'C':[], 'accuracy':[]})\niter_number = 0\n\nfor c_val in C_hyperparam_vals:\n    predicted_probability = cross_val_predict(LogisticRegression(C = c_val), X_train_stratified_scaled, \n                                                  y_train_stratified, cv = 5, method = 'predict_proba')\n\n    for threshold_prob in threshold_hyperparam_vals:\n        predicted_class = predicted_probability[:,1] &gt; threshold_prob\n        predicted_class = predicted_class.astype(int)\n\n        #Computing the accuracy\n        accuracy = accuracy_score(predicted_class, y_train_stratified)*100\n        accuracy_iter.loc[iter_number, 'threshold'] = threshold_prob\n        accuracy_iter.loc[iter_number, 'C'] = c_val\n        accuracy_iter.loc[iter_number, 'accuracy'] = accuracy\n        iter_number = iter_number + 1\n\n\n# Parameters for highest accuracy\noptimal_C = accuracy_iter.sort_values(by = 'accuracy', ascending = False).iloc[0,:]['C']\noptimal_threshold = accuracy_iter.sort_values(by = 'accuracy', ascending = False).iloc[0, :]['threshold']\n\n#Optimal decision threshold probability\nprint(\"Optimal decision threshold = \", optimal_threshold)\n\n#Optimal C\nprint(\"Optimal C = \", optimal_C)\n\nOptimal decision threshold =  0.46\nOptimal C =  4.291934260128778\n\n\n\n# Developing the model with stratified splitting, optimal decision threshold probability, and optimal 'C'\n\n#Scaling data\nscaler = StandardScaler().fit(X_train_stratified)\nX_train_stratified_scaled = scaler.transform(X_train_stratified)\nX_test_stratified_scaled = scaler.transform(X_test_stratified) \n\n# Training the model\nlogreg = LogisticRegression(C = optimal_C)\nlogreg.fit(X_train_stratified_scaled, y_train_stratified)\n\n# Performance metrics computation for the optimal threshold probability\ny_pred_stratified_prob = logreg.predict_proba(X_test_stratified_scaled)[:,1]\n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred_desired_threshold = y_pred_stratified_prob &gt; optimal_threshold\ny_pred_desired_threshold = y_pred_desired_threshold.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred_desired_threshold, y_test_stratified)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(y_test_stratified, y_pred_stratified_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(y_test_stratified, y_pred_desired_threshold))\nprint(\"Recall: \", recall_score(y_test_stratified, y_pred_desired_threshold))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test_stratified, y_pred_desired_threshold), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8509259259259259\nPrecision:  0.7804878048780488\nRecall:  0.5925925925925926\n\n\n\n\n\n\n\n\n\nLater in the course, we’ll see the sklearn function GridSearchCV, which is used to optimize several model hyperparameters simultaneously with \\(K\\)-fold cross validation, while avoiding for loops.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to scikit-learn</span>"
    ]
  },
  {
    "objectID": "Bias_variance_code.html",
    "href": "Bias_variance_code.html",
    "title": "2  Bias-variance tradeoff",
    "section": "",
    "text": "2.1 Simple model (Less flexible)\nLet us consider a linear regression model as the less-flexible (or relatively simple) model.\nWe will first simulate the test dataset for which we will compute the bias and variance.\nnp.random.seed(101)\n\n# Simulating predictor values of test data\nxtest = np.random.uniform(-15, 10, 200)\n\n# Assuming the true mean response is square of the predictor value\nfxtest = xtest**2\n\n# Simulating test response by adding noise to the true mean response\nytest = fxtest + np.random.normal(0, 10, 200)\n\n# We will find bias and variance using a linear regression model for prediction\nmodel = LinearRegression()\n# Visualizing the data and the true mean response\nsns.scatterplot(x = xtest, y = ytest)\nsns.lineplot(x = xtest, y = fxtest, color = 'grey', linewidth = 2)\n\n# Initializing objects to store predictions and mean squared error\n# of 100 models developed on 100 distinct training datasets samples\npred_test = []; mse_test = []\n\n# Iterating over each of the 100 models\nfor i in range(100):\n    np.random.seed(i)\n    \n    # Simulating the ith training data\n    x = np.random.uniform(-15, 10, 200)\n    fx = x**2\n    y = fx + np.random.normal(0, 10, 200)\n    \n    # Fitting the ith model on the ith training data\n    model.fit(x.reshape(-1,1), y)\n    \n    # Plotting the ith model\n    sns.lineplot(x = x, y = model.predict(x.reshape(-1,1)))\n    \n    # Storing the predictions of the ith model on test data\n    pred_test.append(model.predict(xtest.reshape(-1,1)))\n    \n    # Storing the mean squared error of the ith model on test data\n    mse_test.append(mean_squared_error(model.predict(xtest.reshape(-1,1)), ytest))\nThe above plots show that the 100 models seem to have low variance, but high bias. Note that the bias is low only around a couple of points (x = -10 & x = 5).\nLet us compute the average squared bias over all the test data points.\nmean_pred = np.array(pred_test).mean(axis = 0)\nsq_bias = ((mean_pred - fxtest)**2).mean()\nsq_bias\n\n2042.104126728109\nLet us compute the average variance over all the test data points.\nmean_var = np.array(pred_test).var(axis = 0).mean()\nmean_var\n\n28.37397844429763\nLet us compute the mean squared error over all the test data points.\nnp.array(mse_test).mean()\n\n2201.957555529835\nNote that the mean squared error should be the same as the sum of squared bias, variance, and irreducible error.\nThe sum of squared bias, model variance, and irreducible error is:\nsq_bias + mean_var + 100\n\n2170.4781051724067\nNote that this is approximately, but not exactly, the same as the mean squared error computed above as we are developing a finite number of models, and making predictions on a finite number of test data points.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "Bias_variance_code.html#complex-model-more-flexible",
    "href": "Bias_variance_code.html#complex-model-more-flexible",
    "title": "2  Bias-variance tradeoff",
    "section": "2.2 Complex model (more flexible)",
    "text": "2.2 Complex model (more flexible)\nLet us consider a decion tree as the more flexible model.\n\nnp.random.seed(101)\nxtest = np.random.uniform(-15, 10, 200)\nfxtest = xtest**2\nytest = fxtest + np.random.normal(0, 10, 200)\nmodel = DecisionTreeRegressor()\n\n\nsns.scatterplot(x = xtest, y = ytest)\nsns.lineplot(x = xtest, y = fxtest, color = 'grey', linewidth = 2)\npred_test = []; mse_test = []\nfor i in range(100):\n    np.random.seed(i)\n    x = np.random.uniform(-15, 10, 200)\n    fx = x**2\n    y = fx + np.random.normal(0, 10, 200)\n    model.fit(x.reshape(-1,1), y)\n    sns.lineplot(x = x, y = model.predict(x.reshape(-1,1)))\n    pred_test.append(model.predict(xtest.reshape(-1,1)))\n    mse_test.append(mean_squared_error(model.predict(xtest.reshape(-1,1)), ytest))\n\n\n\n\n\n\n\n\nThe above plots show that the 100 models seem to have high variance, but low bias.\nLet us compute the average squared bias over all the test data points.\n\nmean_pred = np.array(pred_test).mean(axis = 0)\nsq_bias = ((mean_pred - fxtest)**2).mean()\nsq_bias\n\n1.3117561629333938\n\n\nLet us compute the average model variance over all the test data points.\n\nmean_var = np.array(pred_test).var(axis = 0).mean()\nmean_var\n\n102.5226748977198\n\n\nLet us compute the average mean squared error over all the test data points.\n\nnp.array(mse_test).mean()\n\n225.92027460924726\n\n\nNote that the above error is approximately the same as the sum of the squared bias, model variance and the irreducible error.\nNote that the relatively more flexible model has a higher variance, but lower bias as compared to the less flexible linear model. This will typically be the case, but may not be true in all scenarios. We will discuss one such scenario later.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "KNN.html",
    "href": "KNN.html",
    "title": "3  KNN",
    "section": "",
    "text": "3.1 KNN for regression\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\npredictors = ['mpg', 'engineSize', 'year', 'mileage']\n\nX_train = train[predictors]\ny_train = train['price']\n\nX_test = test[predictors]\ny_test = test['price']\nLet us scale data as we are using KNN.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "KNN.html#knn-for-regression",
    "href": "KNN.html#knn-for-regression",
    "title": "3  KNN",
    "section": "",
    "text": "3.1.1 Scaling data\n\n# Scale\nsc = StandardScaler()\n\nsc.fit(X_train)\nX_train_scaled = sc.transform(X_train)\nX_test_scaled = sc.transform(X_test)\n\nLet fit the model and compute the RMSE on test data. If the number of neighbors is not specified, the default value is taken.\n\n\n3.1.2 Fitting and validating model\n\nknn_model = KNeighborsRegressor() \n\nknn_model.fit(X_train_scaled, (y_train))\n\ny_pred = knn_model.predict(X_test_scaled)\ny_pred_train = knn_model.predict(X_train_scaled)\n\nmean_squared_error(y_test, (y_pred), squared=False)\n\n6329.691192885354\n\n\n\nknn_model2 = KNeighborsRegressor(n_neighbors = 5, weights='distance') # Default weights is uniform\n\nknn_model2.fit(X_train_scaled, y_train)\n\ny_pred = knn_model2.predict(X_test_scaled)\n\nmean_squared_error(y_test, y_pred, squared=False)\n\n6063.327598353961\n\n\nThe model seems to fit better than all the linear models in STAT303-2.\n\n\n3.1.3 Hyperparameter tuning\nWe will use cross-validation to find the optimal value of the hyperparameter n_neighbors.\n\nKs = np.arange(1,601)\n\ncv_scores = []\n\nfor K in Ks:\n    model = KNeighborsRegressor(n_neighbors = K, weights='distance')\n    score = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring = 'neg_root_mean_squared_error')\n    cv_scores.append(score)\n\n\nnp.array(cv_scores).shape\n# Each row is a K\n\n(600, 5)\n\n\n\ncv_scores_array = np.array(cv_scores)\n\navg_cv_scores = -cv_scores_array.mean(axis=1)\n\n\nsns.lineplot(x = range(600), y = avg_cv_scores);\nplt.xlabel('K')\nplt.ylabel('5-fold Cross-validated RMSE');\n\n\n\n\n\n\n\n\n\navg_cv_scores.min() # Best CV score\n \nKs[avg_cv_scores.argmin()] # Best hyperparam value\n\n366\n\n\nThe optimal hyperparameter value is 366. Does it seem to be too high?\n\nbest_model = KNeighborsRegressor(n_neighbors = Ks[avg_cv_scores.argmin()], weights='distance')\n\nbest_model.fit(X_train_scaled, y_train)\n\ny_pred = best_model.predict(X_test_scaled)\n\nmean_squared_error(y_test, y_pred, squared=False)\n\n7724.452068618346\n\n\nThe test error with the optimal hyperparameter value based on cross-validation is much higher than that based on the default value of the hyperparameter. Why is that?\nSometimes this may happen by chance due to the specific observations in the \\(k\\) folds. One option is to shuffle the dataset before splitting into folds.\nThe function KFold() can be used to shuffle the data before splitting it into folds.\n\n3.1.3.1 KFold()\n\nkcv = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\nNow, let us again try to find the opimal \\(K\\) for KNN, using the new folds, based on shuffled data.\n\nKs = np.arange(1,601)\n\ncv_scores = []\n\nfor K in Ks:\n    model = KNeighborsRegressor(n_neighbors = K, weights='distance')\n    score = cross_val_score(model, X_train_scaled, y_train, cv = kcv, scoring = 'neg_root_mean_squared_error')\n    cv_scores.append(score)\n\n\ncv_scores_array = np.array(cv_scores)\navg_cv_scores = -cv_scores_array.mean(axis=1)\nsns.lineplot(x = range(600), y = avg_cv_scores);\nplt.xlabel('K')\nplt.ylabel('5-fold Cross-validated RMSE');\n\n\n\n\n\n\n\n\nThe optimal K is:\n\nKs[avg_cv_scores.argmin()]\n\n10\n\n\nRMSE on test data with this optimal value of \\(K\\) is:\n\nknn_model2 = KNeighborsRegressor(n_neighbors = 10, weights='distance') # Default weights is uniform\nknn_model2.fit(X_train_scaled, y_train)\ny_pred = knn_model2.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n6043.889393238132\n\n\nIn order to avoid these errors due the specific observations in the \\(k\\) folds, it will be better to repeat the \\(k\\)-fold cross-validation multiple times, where the data is shuffled after each \\(k\\)-fold cross-validation, so that the cross-validation takes place on new folds for each repetition.\nThe function RepeatedKFold() repeats \\(k\\)-fold cross validation multiple times (10 times by default). Let us use it to have a more robust optimal value of the number of neighbors \\(K\\).\n\n\n3.1.3.2 RepeatedKFold()\n\nkcv = RepeatedKFold(n_splits = 5, random_state = 1)\n\n\nKs = np.arange(1,601)\n\ncv_scores = []\n\nfor K in Ks:\n    model = KNeighborsRegressor(n_neighbors = K, weights='distance')\n    score = cross_val_score(model, X_train_scaled, y_train, cv = kcv, scoring = 'neg_root_mean_squared_error')\n    cv_scores.append(score)\n\n\ncv_scores_array = np.array(cv_scores)\navg_cv_scores = -cv_scores_array.mean(axis=1)\nsns.lineplot(x = range(600), y = avg_cv_scores);\nplt.xlabel('K')\nplt.ylabel('5-fold Cross-validated RMSE');\n\n\n\n\n\n\n\n\nThe optimal K is:\n\nKs[avg_cv_scores.argmin()]\n\n9\n\n\nRMSE on test data with this optimal value of \\(K\\) is:\n\nknn_model2 = KNeighborsRegressor(n_neighbors = 9, weights='distance') # Default weights is uniform\nknn_model2.fit(X_train_scaled, y_train)\ny_pred = knn_model2.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n6051.157910333279\n\n\n\n\n\n3.1.4 KNN hyperparameters\nThe model hyperparameters can be obtained using the get_params() method. Note that there are other hyperparameters to tune in addition to number of neighbors. However, the number of neighbours may be the most influential hyperparameter in most cases.\n\nbest_model.get_params()\n\n{'algorithm': 'auto',\n 'leaf_size': 30,\n 'metric': 'minkowski',\n 'metric_params': None,\n 'n_jobs': None,\n 'n_neighbors': 366,\n 'p': 2,\n 'weights': 'distance'}\n\n\nThe distances and the indices of the nearest K observations to each test observation can be obtained using the kneighbors() method.\n\nbest_model.kneighbors(X_test_scaled, return_distance=True)\n\n# Each row is a test obs\n# The cols are the indices of the K Nearest Neighbors (in the training data) to the test obs\n\n(array([[1.92799060e-02, 1.31899013e-01, 1.89662146e-01, ...,\n         8.38960707e-01, 8.39293053e-01, 8.39947823e-01],\n        [7.07215830e-02, 1.99916181e-01, 2.85592939e-01, ...,\n         1.15445056e+00, 1.15450848e+00, 1.15512897e+00],\n        [1.32608205e-03, 1.43558347e-02, 1.80622215e-02, ...,\n         5.16758453e-01, 5.17378567e-01, 5.17852312e-01],\n        ...,\n        [1.29209535e-02, 1.59187173e-02, 3.67038947e-02, ...,\n         8.48811744e-01, 8.51235616e-01, 8.55044146e-01],\n        [1.84971803e-02, 1.67471541e-01, 1.69374312e-01, ...,\n         7.76743422e-01, 7.76943691e-01, 7.77760930e-01],\n        [4.63762129e-01, 5.88639393e-01, 7.54718535e-01, ...,\n         3.16994824e+00, 3.17126663e+00, 3.17294300e+00]]),\n array([[1639, 1647, 4119, ..., 3175, 2818, 4638],\n        [ 367, 1655, 1638, ..., 2010, 3600,  268],\n        [ 393, 4679, 3176, ..., 4663,  357,  293],\n        ...,\n        [3116, 3736, 3108, ..., 3841, 2668, 2666],\n        [4864, 3540, 4852, ..., 3596, 3605, 4271],\n        [ 435,  729, 4897, ..., 4112, 2401, 2460]], dtype=int64))",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "KNN.html#knn-for-classification",
    "href": "KNN.html#knn-for-classification",
    "title": "3  KNN",
    "section": "3.2 KNN for classification",
    "text": "3.2 KNN for classification\nKNN model for classification can developed and tuned in a similar manner using the sklearn function KNeighborsClassifier()\n\nFor classification, KNeighborsClassifier\nExact same inputs\n\nOne detail: Not common to use even numbers for K in classification because of majority voting\nKs = np.arange(1,41,2) –&gt; To get the odd numbers",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>KNN</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html",
    "href": "Hyperparameter tuning.html",
    "title": "4  Hyperparameter tuning",
    "section": "",
    "text": "4.1 GridSearchCV\nThe function is used to compute the cross-validated score (MSE, RMSE, accuracy, etc.) over a grid of hyperparameter values. This helps avoid nested for() loops if multiple hyperparameter values need to be tuned.\n# GridSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor() # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n    # the keys should be EXACTLY the same as the names of the model inputs\n    # the values should be an array or list of hyperparam values you want to try out\n    \n# 30 K values x 2 weight settings x 3 metric settings = 180 different combinations in this grid\ngrid = {'n_neighbors': np.arange(5, 151, 5), 'weights':['uniform', 'distance'], \n        'metric': ['manhattan', 'euclidean', 'chebyshev']}\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, use it if you \n# have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within the function\ngcv = GridSearchCV(model, grid, cv = kfold, scoring = 'neg_root_mean_squared_error', n_jobs = -1, verbose = 10)\n\n# Fit the models, and cross-validate\ngcv.fit(X_train_scaled, y_train)\n\nFitting 5 folds for each of 180 candidates, totalling 900 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(), n_jobs=-1,\n             param_grid={'metric': ['manhattan', 'euclidean', 'chebyshev'],\n                         'n_neighbors': array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n        70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125, 130,\n       135, 140, 145, 150]),\n                         'weights': ['uniform', 'distance']},\n             scoring='neg_root_mean_squared_error', verbose=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(), n_jobs=-1,\n             param_grid={'metric': ['manhattan', 'euclidean', 'chebyshev'],\n                         'n_neighbors': array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n        70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125, 130,\n       135, 140, 145, 150]),\n                         'weights': ['uniform', 'distance']},\n             scoring='neg_root_mean_squared_error', verbose=10)estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()\nThe optimal estimator based on cross-validation is:\ngcv.best_estimator_\n\nKNeighborsRegressor(metric='manhattan', n_neighbors=10, weights='distance')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor(metric='manhattan', n_neighbors=10, weights='distance')\nThe optimal hyperparameter values (based on those considered in the grid search) are:\ngcv.best_params_\n\n{'metric': 'manhattan', 'n_neighbors': 10, 'weights': 'distance'}\nThe cross-validated root mean squared error for the optimal hyperparameter values is:\n-gcv.best_score_\n\n5740.928686723918\nThe RMSE on test data for the optimal hyperparameter values is:\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n5747.466851437544\nNote that the error is further reduced as compared to the case when we tuned only one hyperparameter in the previous chatper. We must tune all the hyperparameters that can effect prediction accuracy, in order to get the most accurate model.\nThe results for each cross-validation are stored in the cv_results_ attribute.\npd.DataFrame(gcv.cv_results_).head()\n\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_metric\nparam_n_neighbors\nparam_weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.011169\n0.005060\n0.011768\n0.001716\nmanhattan\n5\nuniform\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6781.316742\n-5997.969637\n-6726.786770\n-6488.191029\n-6168.502006\n-6432.553237\n306.558600\n19\n\n\n1\n0.009175\n0.001934\n0.009973\n0.000631\nmanhattan\n5\ndistance\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6449.449369\n-5502.975790\n-6306.888303\n-5780.902979\n-5365.980081\n-5881.239304\n429.577113\n3\n\n\n2\n0.008976\n0.001092\n0.012168\n0.001323\nmanhattan\n10\nuniform\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6668.299079\n-6116.693116\n-6387.505084\n-6564.727623\n-6219.094608\n-6391.263902\n205.856097\n16\n\n\n3\n0.007979\n0.000001\n0.011970\n0.000892\nmanhattan\n10\ndistance\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6331.374493\n-5326.304310\n-5787.179591\n-5809.777811\n-5450.007229\n-5740.928687\n349.872624\n1\n\n\n4\n0.006781\n0.000748\n0.012367\n0.001017\nmanhattan\n15\nuniform\n{'metric': 'manhattan', 'n_neighbors': 15, 'we...\n-6871.063499\n-6412.214411\n-6544.343677\n-7008.348770\n-6488.345118\n-6664.863095\n232.385843\n33\nThese results can be useful to see if other hyperparameter values are almost equally good.\nFor example, the next two best optimal values of the hyperparameter correspond to neighbors being 15 and 5 respectively. As the test error has a high variance, the best hyperparameter values need not necessarily be actually optimal.\npd.DataFrame(gcv.cv_results_).sort_values(by = 'rank_test_score').head()\n\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_metric\nparam_n_neighbors\nparam_weights\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n3\n0.007979\n0.000001\n0.011970\n0.000892\nmanhattan\n10\ndistance\n{'metric': 'manhattan', 'n_neighbors': 10, 'we...\n-6331.374493\n-5326.304310\n-5787.179591\n-5809.777811\n-5450.007229\n-5740.928687\n349.872624\n1\n\n\n5\n0.009374\n0.004829\n0.013564\n0.001850\nmanhattan\n15\ndistance\n{'metric': 'manhattan', 'n_neighbors': 15, 'we...\n-6384.403268\n-5427.978762\n-5742.606651\n-6041.135255\n-5563.240077\n-5831.872803\n344.192700\n2\n\n\n1\n0.009175\n0.001934\n0.009973\n0.000631\nmanhattan\n5\ndistance\n{'metric': 'manhattan', 'n_neighbors': 5, 'wei...\n-6449.449369\n-5502.975790\n-6306.888303\n-5780.902979\n-5365.980081\n-5881.239304\n429.577113\n3\n\n\n7\n0.007977\n0.001092\n0.017553\n0.002054\nmanhattan\n20\ndistance\n{'metric': 'manhattan', 'n_neighbors': 20, 'we...\n-6527.825519\n-5534.609170\n-5860.837805\n-6100.919269\n-5679.403544\n-5940.719061\n349.270714\n4\n\n\n9\n0.007777\n0.000748\n0.019349\n0.003374\nmanhattan\n25\ndistance\n{'metric': 'manhattan', 'n_neighbors': 25, 'we...\n-6620.272336\n-5620.462675\n-5976.406911\n-6181.847891\n-5786.081991\n-6037.014361\n346.791650\n5\nLet us compute the RMSE on test data based on the 2nd and 3rd best hyperparameter values.\nmodel = KNeighborsRegressor(n_neighbors=15, metric='manhattan', weights='distance').fit(X_train_scaled, y_train)\nmean_squared_error(model.predict(X_test_scaled), y_test, squared = False)\n\n5800.418957612656\nmodel = KNeighborsRegressor(n_neighbors=5, metric='manhattan', weights='distance').fit(X_train_scaled, y_train)\nmean_squared_error(model.predict(X_test_scaled), y_test, squared = False)\n\n5722.4859230146685\nWe can see that the RMSE corresponding to the 3rd best hyperparameter value is the least. Due to variance in test errors, it may be a good idea to consider the set of top few best hyperparameter values, instead of just considering the best one.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#randomizedsearchcv",
    "href": "Hyperparameter tuning.html#randomizedsearchcv",
    "title": "4  Hyperparameter tuning",
    "section": "4.2 RandomizedSearchCV()",
    "text": "4.2 RandomizedSearchCV()\nIn case of many possible values of hyperparameters, it may be comptaionally very expensive to use GridSearchCV(). In such cases, RandomizedSearchCV() can be used to compute the cross-validated score on a randomly selected subset of hyperparameter values from the specified grid. The number of values can be fixed by the user, as per the available budget.\n\n# RandomizedSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor() # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n    # the keys should be EXACTLY the same as the names of the model inputs\n    # the values should be an array or list of hyperparam values, or distribution of hyperparameter values\n    \n    \ngrid = {'n_neighbors': range(1, 500), 'weights':['uniform', 'distance'], \n        'metric': ['minkowski'], 'p': uniform(loc=1, scale=10)} #We can specify a distribution \n                                                                #for continuous hyperparameter values\n\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, use it if you \n# have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within the function\ngcv = RandomizedSearchCV(model, param_distributions = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1, verbose = 10)\n\n# Fit the models, and cross-validate\ngcv.fit(X_train_scaled, y_train)\n\nFitting 5 folds for each of 180 candidates, totalling 900 fits\n\n\nRandomizedSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n                   estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n                   param_distributions={'metric': ['minkowski'],\n                                        'n_neighbors': range(1, 500),\n                                        'p': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000226D6E70700&gt;,\n                                        'weights': ['uniform', 'distance']},\n                   random_state=10, scoring='neg_root_mean_squared_error',\n                   verbose=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n                   estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n                   param_distributions={'metric': ['minkowski'],\n                                        'n_neighbors': range(1, 500),\n                                        'p': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000226D6E70700&gt;,\n                                        'weights': ['uniform', 'distance']},\n                   random_state=10, scoring='neg_root_mean_squared_error',\n                   verbose=10)estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()\n\n\n\ngcv.best_params_\n\n{'metric': 'minkowski',\n 'n_neighbors': 3,\n 'p': 1.252639454318171,\n 'weights': 'uniform'}\n\n\n\ngcv.best_score_\n\n-6239.171627183809\n\n\n\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n6176.533397589911\n\n\nNote that in this example, RandomizedSearchCV() helps search for optimal values of the hyperparameter \\(p\\) over a continuous domain space. In this dataset, \\(p = 1\\) seems to be the optimal value. However, if the optimal value was somewhere in the middle of a larger continuous domain space (instead of the boundary of the domain space), and there were several other hyperparameters, some of which were not influencing the response (effect sparsity), RandomizedSearchCV() is likely to be more effective in estimating the optimal value of the continuous hyperparameter.\nThe advantages of RandomizedSearchCV() over GridSearchCV() are:\n\nRandomizedSearchCV() fixes the computational cost in case of large number of hyperparameters / large number of levels of individual hyperparameters. If there are \\(n\\) hyper parameters, each with 3 levels, the number of all possible hyperparameter values will be \\(3^n\\). The computational cost increase exponentially with increase in number of hyperparameters.\nIn case of a hyperparameter having continuous values, the distribution of the hyperparameter can be specified in RandomizedSearchCV().\nIn case of effect sparsity of hyperparameters, i.e., if only a few hyperparameters significantly effect prediction accuracy, RandomizedSearchCV() is likely to consider more unique values of the influential hyperparameters as compared to GridSearchCV(), and is thus likely to provide more optimal hyperparameter values as compared to GridSearchCV(). The figure below shows effect sparsity where there are 2 hyperparameters, but only one of them is associated with the cross-validated score, Here, it is more likely that the optimal cross-validated score will be obtained by RandomizedSearchCV(), as it is evaluating the model on 9 unique values of the relevant hyperparameter, instead of just 3.",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#bayessearchcv",
    "href": "Hyperparameter tuning.html#bayessearchcv",
    "title": "4  Hyperparameter tuning",
    "section": "4.3 BayesSearchCV()",
    "text": "4.3 BayesSearchCV()\nUnlike the grid search and random search, which treat hyperparameter sets independently, the Bayesian optimization is an informed search method, meaning that it learns from previous iterations. The number of trials in this approach is determined by the user.\n\nThe function begins by computing the cross-validated score by randomly selecting a few hyperparameter values from the specified disttribution of hyperparameter values.\nBased on the data of hyperparameter values tested (predictors), and the cross-validated score (the response), a Gaussian process model is developed to estimate the cross-validated score & the uncertainty in the estimate in the entire space of the hyperparameter values\nA criterion that “explores” uncertain regions of the space of hyperparameter values (where it is difficult to predict cross-validated score), and “exploits” promising regions of the space are of hyperparameter values (where the cross-validated score is predicted to minimize) is used to suggest the next hyperparameter value that will potentially minimize the cross-validated score\nCross-validated score is computed at the suggested hyperparameter value, the Gaussian process model is updated, and the previous step is repeated, until a certain number of iterations specified by the user.\n\nTo summarize, instead of blindly testing the model for the specified hyperparameter values (as in GridSearchCV()), or randomly testing the model on certain hyperparameter values (as in RandomizedSearchCV()), BayesSearchCV() smartly tests the model for those hyperparameter values that are likely to reduce the cross-validated score. The algorithm becomes “smarter” as it “learns” more with increasing iterations.\nHere is a nice blog, if you wish to understand more about the Bayesian optimization procedure.\n\n# BayesSearchCV works in three steps:\n\n# 1) Create the model\nmodel = KNeighborsRegressor(metric = 'minkowski') # No inputs defined inside the model\n\n# 2) Create a hyperparameter grid (as a dict)\n# the keys should be EXACTLY the same as the names of the model inputs\n# the values should be the distribution of hyperparameter values. Lists and NumPy arrays can\n# also be used\n    \ngrid = {'n_neighbors': Integer(1, 500), 'weights': Categorical(['uniform', 'distance']), \n       'p': Real(1, 10, prior = 'uniform')} \n\n# 3) Create the Kfold object (Using RepeatedKFold will be more robust, but more expensive, \n# use it if you have the budget)\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\n\n# 4) Create the CV object\n# Look at the documentation to see the order in which the objects must be specified within \n# the function\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n\n# Fit the models, and cross-validate\n\n# Sometimes the Gaussian process model predicting the cross-validated score suggests a \n# \"promising point\" (i.e., set of hyperparameter values) for cross-validation that it has \n# already suggested earlier. In such  a case a warning is raised, and the objective \n# function (i.e., the cross-validation score) is computed at a randomly selected point \n# (as in RandomizedSearchCV()). This feature helps the algorithm explore other regions of\n# the hyperparameter space, rather than only searching in the promising regions. Thus, it \n# balances exploration (of the hyperparameter space) with exploitation (of the promising \n# regions of the hyperparameter space)\n\nwarnings.filterwarnings(\"ignore\")\ngcv.fit(X_train_scaled, y_train)\nwarnings.resetwarnings()\n\nThe optimal hyperparameter values (based on Bayesian search) on the provided distribution of hyperparameter values are:\n\ngcv.best_params_\n\nOrderedDict([('n_neighbors', 9),\n             ('p', 1.0008321732366932),\n             ('weights', 'distance')])\n\n\nThe cross-validated root mean squared error for the optimal hyperparameter values is:\n\n-gcv.best_score_\n\n5756.172382596493\n\n\nThe RMSE on test data for the optimal hyperparameter values is:\n\ny_pred = gcv.predict(X_test_scaled)\nmean_squared_error(y_test, y_pred, squared=False)\n\n5740.432278861367\n\n\n\n4.3.1 Diagonosis of cross-validated score optimization\nBelow are the partial dependence plots of the objective function (i.e., the cross-validated score). The cross-validated score predictions are based on the most recently updated model (i.e., the updated Gaussian Process model at the end of n_iter iterations specified by the user) that predicts the cross-validated score.\nCheck the plot_objective() documentation to interpret the plots.\n\nplot_objective(gcv.optimizer_results_[0],\n                   dimensions=[\"n_neighbors\", \"p\", \"weights\"], size = 3)\nplt.show();\n\n\n\n\n\n\n\n\nThe frequence of individual hyperparameter values considered can also be visualized as below.\n\nfig, ax = plt.subplots(1, 3, figsize = (10, 3))\nplt.subplots_adjust(wspace=0.4)\nplot_histogram(gcv.optimizer_results_[0], 0, ax = ax[0])\nplot_histogram(gcv.optimizer_results_[0], 1, ax = ax[1])\nplot_histogram(gcv.optimizer_results_[0], 2, ax = ax[2])\nplt.show()\n\n\n\n\n\n\n\n\nBelow is the plot showing the minimum cross-validated score computed obtained until ‘n’ hyperparameter values are considered for cross-validation.\n\nplot_convergence(gcv.optimizer_results_)\nplt.show()\n\n\n\n\n\n\n\n\nNote that the cross-validated error is close to the optmial value in the 53rd iteration itself.\nThe cross-validated error at the 53rd iteration is:\n\ngcv.optimizer_results_[0]['func_vals'][53]\n\n5831.87280274334\n\n\nThe hyperparameter values at the 53rd iterations are:\n\ngcv.optimizer_results_[0]['x_iters'][53]\n\n[15, 1.0, 'distance']\n\n\nNote that this is the 2nd most optimal hyperparameter value based on GridSearchCV().\nBelow is the plot showing the cross-validated score computed at each of the 180 hyperparameter values considered for cross-validation. The plot shows that the algorithm seems to explore new regions of the domain space, instead of just exploting the promising ones. There is a balance between exploration and exploitation for finding the optimal hyperparameter values that minimize the objective function (i.e., the function that models the cross-validated score).\n\nsns.lineplot(x = range(1, 181), y = gcv.optimizer_results_[0]['func_vals'])\nplt.xlabel('Iteration')\nplt.ylabel('Cross-validated score')\nplt.show();\n\n\n\n\n\n\n\n\nThe advantages of BayesSearchCV() over GridSearchCV() and RandomizedSearchCV() are:\n\nThe Bayesian Optimization approach gives the benefit that we can give a much larger range of possible values, since over time we identify and exploit the most promising regions and discard the not so promising ones. Plain grid-search would burn computational resources to explore all regions of the domain space with the same granularity, even the not promising ones. Since we search much more effectively in Bayesian search, we can search over a larger domain space.\nBayesSearch CV may help us identify the optimal hyperparameter value in fewer iterations if the Gaussian process model estimating the cross-validated score is relatively accurate. However, this is not certain. Grid and random search are completely uninformed by past evaluations, and as a result, often spend a significant amount of time evaluating “bad” hyperparameters.\nBayesSearch CV is more reliable in cases of a large search space, where random selection may miss sampling values from optimal regions of the search space.\n\nThe disadvantages of BayesSearchCV() over GridSearchCV() and RandomizedSearchCV() are:\n\nBayesSearchCV() has a cost of learning from past data, i.e., updating the model that predicts the cross-validated score after every iteration of evaluating the cross-validated score on a new hyperparameter value. This cost will continue to increase as more and more data is collected. There is no such cost in GridSearchCV() and RandomizedSearchCV() as there is no learning. This implies that each iteration of BayesSearchCV() will take a longer time than each iteration of GridSearchCV() / RandomizedSearchCV(). Thus, even if BayesSearchCV() finds the optimal hyperparameter value in fewer iterations, it may take more time than GridSearchCV() / RandomizedSearchCV() for the same.\nThe success of BayesSearchCV() depends on the predictions and associated uncertainty estimated by the Gaussian process (GP) model that predicts the cross-validated score. The GP model, although works well in general, may not be suitable for certain datasets, or may take a relatively large number of iterations to learn for certain datasets.\n\n\n\n4.3.2 Live monitoring of cross-validated score\nNote that it will be useful monitor the cross-validated score while the Bayesian Search CV code is running, and stop the code as soon as the desired accuracy is reached, or the optimal cross-validated score doesn’t seem to improve. The fit() method of the BayesSeaerchCV() object has a callback argument that can be used as follows:\n\nmodel = KNeighborsRegressor(metric = 'minkowski') # No inputs defined inside the model\ngrid = {'n_neighbors': Integer(1, 500), 'weights': Categorical(['uniform', 'distance']), \n       'p': Real(1, 10, prior = 'uniform')} \n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n\n\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\n\n\ngcv.fit(X_train_scaled, y_train, callback = monitor)\n\n['n_neighbors', 'p', 'weights'] = [9, 1.0008321732366932, 'distance'] 5756.172382596493\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'n_neighbors': Integer(low=1, high=500, prior='uniform', transform='normalize'),\n                             'p': Real(low=1, high=10, prior='uniform', transform='normalize'),\n                             'weights': Categorical(categories=('uniform', 'distance'), prior=None)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=KNeighborsRegressor(), n_iter=180, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'n_neighbors': Integer(low=1, high=500, prior='uniform', transform='normalize'),\n                             'p': Real(low=1, high=10, prior='uniform', transform='normalize'),\n                             'weights': Categorical(categories=('uniform', 'distance'), prior=None)})estimator: KNeighborsRegressorKNeighborsRegressor()KNeighborsRegressorKNeighborsRegressor()",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Hyperparameter tuning.html#cross_validate",
    "href": "Hyperparameter tuning.html#cross_validate",
    "title": "4  Hyperparameter tuning",
    "section": "4.4 cross_validate()",
    "text": "4.4 cross_validate()\nWe have used cross_val_score() and cross_val_predict() so far.\nWhen can we use one over the other?\nThe function cross_validate() is similar to cross_val_score() except that it has the option to return multiple cross-validated metrics, instead of a single one.\nConsider the heart disease classification problem, where the response is target (whether the person has a heart disease or not).\n\ndata = pd.read_csv('Datasets/heart_disease_classification.csv')\ndata.head()\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\n\nLet us pre-process the data.\n\n# First, separate the response and the predictors\ny = data['target']\nX = data.drop('target', axis=1)\n\n\n# Separate the data (X,y) into training and test\n\n# Inputs:\n    # data\n    # train-test ratio\n    # random_state for reproducible code\n    \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20, stratify=y) # 80%-20% split\n\n# stratify=y makes sure the class 0 to class 1 ratio in the training and test sets are kept the same as the entire dataset.\n\n\nmodel = KNeighborsClassifier() \nsc = StandardScaler()\nsc.fit(X_train)\nX_train_scaled = sc.transform(X_train)\nX_test_scaled = sc.transform(X_test)\n\nSuppose we want to take recall above a certain threshold with the highest precision possible. cross_validate() computes the cross-validated score for multiple metrics - rest is the same as cross_val_score().\n\nKs = np.arange(10,200,10)\n\nscores = []\n\nfor K in Ks:\n    model = KNeighborsClassifier(n_neighbors=K) # Keeping distance uniform\n    scores.append(cross_validate(model, X_train_scaled, y_train, cv=5, scoring = ['accuracy','recall', 'precision']))\n\n\nscores\n\n# The output is now a list of dicts - easy to convert to a df\n\ndf_scores = pd.DataFrame(scores) # We need to handle test_recall and test_precision cols\n\ndf_scores['CV_recall'] = df_scores['test_recall'].apply(np.mean)\ndf_scores['CV_precision'] = df_scores['test_precision'].apply(np.mean)\ndf_scores['CV_accuracy'] = df_scores['test_accuracy'].apply(np.mean)\n\ndf_scores.index = Ks # We can set K values as indices for convenience\n\n\n#df_scores\n# What happens as K increases?\n    # Recall increases (not monotonically)\n    # Precision decreases (not monotonically)\n# Why?\n    # Check the class distribution in the data - more obs with class 1\n    # As K gets higher, the majority class overrules (visualized in the slides)\n    # More 1s means less FNs - higher recall\n    # More 1s means more FPs - lower precision\n# Would this be the case for any dataset?\n    # NO!! Depends on what the majority class is!\n\nSuppose we wish to have the maximum possible precision for at least 95% recall.\nThe optimal ‘K’ will be:\n\ndf_scores.loc[df_scores['CV_recall'] &gt; 0.95, 'CV_precision'].idxmax()\n\n120\n\n\nThe cross-validated precision, recall and accuracy for the optimal ‘K’ are:\n\ndf_scores.loc[120, ['CV_recall', 'CV_precision', 'CV_accuracy']]\n\nCV_recall       0.954701\nCV_precision    0.734607\nCV_accuracy     0.785374\nName: 120, dtype: object\n\n\n\nsns.lineplot(x = df_scores.index, y = df_scores.CV_precision, color = 'blue', label = 'precision')\nsns.lineplot(x = df_scores.index, y = df_scores.CV_recall, color = 'red', label = 'recall')\nsns.lineplot(x = df_scores.index, y = df_scores.CV_accuracy, color = 'green', label = 'accuracy')\nplt.ylabel('Metric')\nplt.xlabel('K')\nplt.show()",
    "crumbs": [
      "Sklearn; Bias & Variance; KNN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "Lec3_RegressionTrees.html",
    "href": "Lec3_RegressionTrees.html",
    "title": "5  Regression trees",
    "section": "",
    "text": "5.1 Building a regression tree\nDevelop a regression tree to predict car price based on mileage\nX = train['mileage']\ny = train['price']\n#Defining the object to build a regression tree\nmodel = DecisionTreeRegressor(random_state=1, max_depth=3) \n\n#Fitting the regression tree to the data\nmodel.fit(X.values.reshape(-1,1), y)\n\nDecisionTreeRegressor(max_depth=3, random_state=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=3, random_state=1)\n#Visualizing the regression tree\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('car_price_tree.png')\nImage(graph.create_png())\n#prediction on test data\npred=model.predict(test[['mileage']].values)\n#RMSE on test data\nnp.sqrt(mean_squared_error(test.price, pred))\n\n13764.798425410803\n#Visualizing the model fit\nXtest = np.linspace(min(X), max(X), 100)\npred_test = model.predict(Xtest.reshape(-1,1))\nsns.scatterplot(x = 'mileage', y = 'price', data = train, color = 'orange')\nsns.lineplot(x = Xtest, y = pred_test, color = 'blue');\nAll cars falling within the same terminal node have the same predicted price, which is seen as flat line segments in the above model curve.\nDevelop a regression tree to predict car price based on mileage, mpg, engineSize and year\nX = train[['mileage','mpg','year','engineSize']]\nmodel = DecisionTreeRegressor(random_state=1, max_depth=3) \nmodel.fit(X, y)\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage','mpg','year','engineSize'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('car_price_tree.png')\nImage(graph.create_png())\nThe model can also be visualized in the text format as below.\nprint(export_text(model))\n\n|--- feature_3 &lt;= 2.75\n|   |--- feature_2 &lt;= 2018.50\n|   |   |--- feature_3 &lt;= 1.75\n|   |   |   |--- value: [9912.24]\n|   |   |--- feature_3 &gt;  1.75\n|   |   |   |--- value: [16599.03]\n|   |--- feature_2 &gt;  2018.50\n|   |   |--- feature_3 &lt;= 1.90\n|   |   |   |--- value: [19363.81]\n|   |   |--- feature_3 &gt;  1.90\n|   |   |   |--- value: [31919.42]\n|--- feature_3 &gt;  2.75\n|   |--- feature_2 &lt;= 2017.50\n|   |   |--- feature_0 &lt;= 53289.00\n|   |   |   |--- value: [31004.63]\n|   |   |--- feature_0 &gt;  53289.00\n|   |   |   |--- value: [15255.91]\n|   |--- feature_2 &gt;  2017.50\n|   |   |--- feature_1 &lt;= 21.79\n|   |   |   |--- value: [122080.00]\n|   |   |--- feature_1 &gt;  21.79\n|   |   |   |--- value: [49350.79]",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "Lec3_RegressionTrees.html#optimizing-parameters-to-improve-the-regression-tree",
    "href": "Lec3_RegressionTrees.html#optimizing-parameters-to-improve-the-regression-tree",
    "title": "5  Regression trees",
    "section": "5.2 Optimizing parameters to improve the regression tree",
    "text": "5.2 Optimizing parameters to improve the regression tree\nLet us find the optimal depth of the tree and the number of terminal nodes (leaves) by cross validation.\n\n5.2.1 Range of hyperparameter values\nFirst, we’ll find the minimum and maximum possible values of the depth and leaves, and then find the optimal value in that range.\n\nmodel = DecisionTreeRegressor(random_state=1) \nmodel.fit(X, y)\n\nprint(\"Maximum tree depth =\", model.get_depth())\n\nprint(\"Maximum leaves =\", model.get_n_leaves())\n\nMaximum tree depth = 29\nMaximum leaves = 4845\n\n\n\n\n5.2.2 Cross validation: Coarse grid\nWe’ll use the sklearn function GridSearchCV to find the optimal hyperparameter values over a grid of possible values. By default, GridSearchCV returns the optimal hyperparameter values based on the coefficient of determination \\(R^2\\). However, the scoring argument of the function can be used to find the optimal parameters based on several different criteria as mentioned in the scoring-parameter documentation.\n\n#Finding cross-validation error for trees \nparameters = {'max_depth':range(2,30, 3),'max_leaf_nodes':range(2,4900, 100)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv)\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \n\nFitting 5 folds for each of 490 candidates, totalling 2450 fits\n0.8433100904754441 {'max_depth': 11, 'max_leaf_nodes': 302}\n\n\nLet us find the optimal hyperparameters based on root mean squared error (RMSE), instead of \\(R^2\\). Let us compute \\(R^2\\) as well during cross validation, as we can compute multiple performance metrics using the scoring argument. However, when computing multiple performance metrics, we will need to specify the performance metric used to find the optimal hyperparameters with the refit argument.\n\n#Finding cross-validation error for trees \nparameters = {'max_depth':range(2,30, 3),'max_leaf_nodes':range(2,4900, 100)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv,\n                    scoring=['neg_root_mean_squared_error', 'r2'], refit = 'neg_root_mean_squared_error')\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \n\nFitting 5 folds for each of 490 candidates, totalling 2450 fits\n-6475.329183576911 {'max_depth': 11, 'max_leaf_nodes': 302}\n\n\nNote that as the GridSearchCV function maximizes the performance metric to find the optimal hyperparameters, we are maximizing the negative root mean squared error (neg_root_mean_squared_error), and the function returns the optimal negative mean squared error.\nLet us visualize the mean squared error based on the hyperparameter values. We’ll use the cross validation results stored in the cv_results_ attribute of the GridSearchCV fit() object.\n\n#Detailed results of k-fold cross validation\ncv_results = pd.DataFrame(model.cv_results_)\ncv_results.head()\n\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_max_depth\nparam_max_leaf_nodes\nparams\nsplit0_test_neg_root_mean_squared_error\nsplit1_test_neg_root_mean_squared_error\nsplit2_test_neg_root_mean_squared_error\n...\nstd_test_neg_root_mean_squared_error\nrank_test_neg_root_mean_squared_error\nsplit0_test_r2\nsplit1_test_r2\nsplit2_test_r2\nsplit3_test_r2\nsplit4_test_r2\nmean_test_r2\nstd_test_r2\nrank_test_r2\n\n\n\n\n0\n0.010178\n7.531409e-04\n0.003791\n0.000415\n2\n2\n{'max_depth': 2, 'max_leaf_nodes': 2}\n-13729.979521\n-13508.807583\n-12792.941600\n...\n390.725290\n481\n0.314894\n0.329197\n0.394282\n0.361007\n0.382504\n0.356377\n0.030333\n481\n\n\n1\n0.009574\n1.758238e-03\n0.003782\n0.000396\n2\n102\n{'max_depth': 2, 'max_leaf_nodes': 102}\n-10586.885662\n-11230.674720\n-10682.195189\n...\n321.837965\n433\n0.592662\n0.536369\n0.577671\n0.568705\n0.612407\n0.577563\n0.025368\n433\n\n\n2\n0.009774\n7.458305e-04\n0.003590\n0.000488\n2\n202\n{'max_depth': 2, 'max_leaf_nodes': 202}\n-10586.885662\n-11230.674720\n-10682.195189\n...\n321.837965\n433\n0.592662\n0.536369\n0.577671\n0.568705\n0.612407\n0.577563\n0.025368\n433\n\n\n3\n0.009568\n4.953541e-04\n0.003391\n0.000489\n2\n302\n{'max_depth': 2, 'max_leaf_nodes': 302}\n-10586.885662\n-11230.674720\n-10682.195189\n...\n321.837965\n433\n0.592662\n0.536369\n0.577671\n0.568705\n0.612407\n0.577563\n0.025368\n433\n\n\n4\n0.008976\n6.843901e-07\n0.003192\n0.000399\n2\n402\n{'max_depth': 2, 'max_leaf_nodes': 402}\n-10586.885662\n-11230.674720\n-10682.195189\n...\n321.837965\n433\n0.592662\n0.536369\n0.577671\n0.568705\n0.612407\n0.577563\n0.025368\n433\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n\nfig, axes = plt.subplots(1,2,figsize=(14,5))\nplt.subplots_adjust(wspace=0.2)\naxes[0].plot(cv_results.param_max_depth, (-cv_results.mean_test_neg_root_mean_squared_error), 'o')\naxes[0].set_ylim([6200, 7500])\naxes[0].set_xlabel('Depth')\naxes[0].set_ylabel('K-fold RMSE')\naxes[1].plot(cv_results.param_max_leaf_nodes, (-cv_results.mean_test_neg_root_mean_squared_error), 'o')\naxes[1].set_ylim([6200, 7500])\naxes[1].set_xlabel('Leaves')\naxes[1].set_ylabel('K-fold RMSE');\n\n\n\n\n\n\n\n\nWe observe that for a depth of around 8-14, and number of leaves within 1000, we get the lowest \\(K\\)-fold RMSE. So, we should do a finer search in that region to obtain more precise hyperparameter values.\n\n\n5.2.3 Cross validation: Finer grid\n\n#Finding cross-validation error for trees\nstart_time = tm.time()\nparameters = {'max_depth':range(8,15),'max_leaf_nodes':range(2,1000)}\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\nmodel = GridSearchCV(DecisionTreeRegressor(random_state=1), parameters, n_jobs=-1,verbose=1,cv=cv,\n                    scoring = 'neg_root_mean_squared_error')\nmodel.fit(X, y)\nprint (model.best_score_, model.best_params_) \nprint(\"Time taken =\", round((tm.time() - start_time)/60), \"minutes\")\n\nFitting 5 folds for each of 6986 candidates, totalling 34930 fits\n-6414.468922119372 {'max_depth': 10, 'max_leaf_nodes': 262}\nTime taken = 2 minutes\n\n\nFrom the above cross-validation, the optimal hyperparameter values are max_depth = 10 and max_leaf_nodes = 262. Note that the cross-validation score with finer grid is only slightly lower than the course grid. However, depending on the dataset, the finer grid may lead to more benefit.\n\n#Developing the tree based on optimal hyperparameters found by cross-validation\nmodel = DecisionTreeRegressor(random_state=1, max_depth=10,max_leaf_nodes=262) \nmodel.fit(X, y)\n\nDecisionTreeRegressor(max_depth=10, max_leaf_nodes=262, random_state=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=10, max_leaf_nodes=262, random_state=1)\n\n\n\n#RMSE on test data\nXtest = test[['mileage','mpg','year','engineSize']]\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n6921.0404660552895\n\n\nThe RMSE for the decision tree is lower than that of linear regression models with these four predictors. This may be probably due to car price having a highly non-linear association with the predictors.\nNote that we may also use RandomizedSearchCV() or BayesSearchCV() to optimze the hyperparameters.\nPredictor importance: The importance of a predictor is computed as the (normalized) total reduction of the criterion (SSE in case of regression trees) brought by that predictor.\nWarning: impurity-based feature importances can be misleading for high cardinality features (many unique values) Source: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor.feature_importances_\nWhy?\nBecause high cardinality predictors will tend to overfit. When the predictors have high cardinality, it means they form little groups (in the leaf nodes) and then the model “learns” the individuals, instead of “learning” the general trend. The higher the cardinality of the predictor, the more prone is the model to overfitting.\n\nmodel.feature_importances_\n\narray([0.04490344, 0.15882336, 0.29739951, 0.49887369])\n\n\nEngine size is the most important predictor, followed by year, which is followed by mpg, and mileage is the least important predictor.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "Lec3_RegressionTrees.html#cost-complexity-pruning",
    "href": "Lec3_RegressionTrees.html#cost-complexity-pruning",
    "title": "5  Regression trees",
    "section": "5.3 Cost complexity pruning",
    "text": "5.3 Cost complexity pruning\nWhile optimizing parameters above, we optimized them within a range that we thought was reasonable. While doing so, we restricted ourselves to considering only a subset of the unpruned tree. Thus, we could have missed out on finding the optimal tree (or the best model).\nWith cost complexity pruning, we first develop an unpruned tree without any restrictions. Then, using cross validation, we find the optimal value of the tuning parameter \\(\\alpha\\). All the non-terminal nodes for which \\(\\alpha_{eff}\\) is smaller that the optimal \\(\\alpha\\) will be pruned. You will need to check out the link below to understand this better.\nCheck out a detailed explanation of how cost complexity pruning is implemented in sklearn at: https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning\nHere are some informative visualizations that will help you understand what is happening in cost complexity pruning: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n\nmodel = DecisionTreeRegressor(random_state = 1)#model without any restrictions\npath= model.cost_complexity_pruning_path(X,y)# Compute the pruning path during Minimal Cost-Complexity Pruning.\n\n\nalphas=path['ccp_alphas']\n\n\nlen(alphas)\n\n4126\n\n\n\nstart_time = tm.time()\ncv = KFold(n_splits = 5,shuffle=True,random_state=1)\ntree = GridSearchCV(DecisionTreeRegressor(random_state=1), param_grid = {'ccp_alpha':alphas}, \n                     scoring = 'neg_mean_squared_error',n_jobs=-1,verbose=1,cv=cv)\ntree.fit(X, y)\nprint (tree.best_score_, tree.best_params_)\nprint(\"Time taken =\",round((tm.time()-start_time)/60), \"minutes\")\n\nFitting 5 folds for each of 4126 candidates, totalling 20630 fits\n-44150619.209031895 {'ccp_alpha': 143722.94076639024}\nTime taken = 2 minutes\n\n\nThe code took 2 minutes to run on a dataset of about 5000 observations and 4 predictors.\n\nmodel = DecisionTreeRegressor(ccp_alpha=143722.94076639024,random_state=1)\nmodel.fit(X, y)\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(test.price, pred))\n\n7306.592294294368\n\n\nThe RMSE for the decision tree with cost complexity pruning is lower than that of linear regression models and spline regression models (including MARS), with these four predictors. However, it is higher than the one obtained with tuning tree parameters using grid search (shown previously). Cost complexity pruning considers a completely unpruned tree unlike the ‘grid search’ method of searching over a grid of hyperparameters such as max_depth and max_leaf_nodes, and thus may seem to be more comprehensive than the ‘grid search’ approach. However, both the approaches may consider trees that are not considered by the other approach, and thus either one may provide a more accurate model. Depending on the grid of parameters chosen for cross validation, the grid search method may be more or less comprehensive than cost complexity pruning.\n\ngridcv_results = pd.DataFrame(tree.cv_results_)\ncv_error = -gridcv_results['mean_test_score']\n\n\n#Visualizing the 5-fold cross validation error vs alpha\nplt.plot(alphas,cv_error)\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('K-fold MSE');\n\n\n\n\n\n\n\n\n\n#Zooming in the above visualization to see the alpha where the 5-fold cross validation error is minimizing\nplt.plot(alphas[0:4093],cv_error[0:4093])\nplt.xlabel('alpha')\nplt.ylabel('K-fold MSE');\n\n\n\n\n\n\n\n\n\n5.3.1 Depth vs alpha; Node counts vs alpha\n\nstime = time.time()\ntrees=[]\nfor i in alphas:\n    tree = DecisionTreeRegressor(ccp_alpha=i,random_state=1)\n    tree.fit(X, train['price'])\n    trees.append(tree)\nprint(time.time()-stime)\n\n268.10325384140015\n\n\nThis code takes 4.5 minutes to run\n\nnode_counts = [clf.tree_.node_count for clf in trees]\ndepth = [clf.tree_.max_depth for clf in trees]\n\n\nfig, ax = plt.subplots(1, 2,figsize=(10,6))\nax[0].plot(alphas[0:4093], node_counts[0:4093], marker=\"o\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(alphas[0:4093], depth[0:4093], marker=\"o\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\n#fig.tight_layout()\n\nText(0.5, 1.0, 'Depth vs alpha')\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Train and test accuracies (R-squared) vs alpha\n\ntrain_scores = [clf.score(X, y) for clf in trees]\ntest_scores = [clf.score(Xtest, test.price) for clf in trees]\n\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(alphas[0:4093], train_scores[0:4093], marker=\"o\", label=\"train\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax.plot(alphas[0:4093], test_scores[0:4093], marker=\"o\", label=\"test\", drawstyle=\"steps-post\")#Plotting the zoomed-in plot (ignoring very high alphas), otherwise it is hard to see the trend\nax.legend()\nplt.show()",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression trees</span>"
    ]
  },
  {
    "objectID": "Lec4_ClassificationTree.html",
    "href": "Lec4_ClassificationTree.html",
    "title": "6  Classification trees",
    "section": "",
    "text": "6.1 Building a classification tree\nDevelop a classification tree to predict if a person has diabetes.\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n#Defining the object to build a classification tree\nmodel = DecisionTreeClassifier(random_state=1, max_depth=3) \n\n#Fitting the regression tree to the data\nmodel.fit(X, y)\n\nDecisionTreeClassifier(max_depth=3, random_state=1)\n#Visualizing the regression tree\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =X.columns,precision=2)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n#graph.write_png('car_price_tree.png')\nImage(graph.create_png())\n# Performance metrics computation \n\n#Computing the accuracy\ny_pred = model.predict(Xtest)\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\ny_pred_prob = model.predict_proba(Xtest)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  73.37662337662337\nROC-AUC:  0.8349197955226512\nPrecision:  0.7777777777777778\nRecall:  0.45901639344262296",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Lec4_ClassificationTree.html#optimizing-hyperparameters-to-optimize-performance",
    "href": "Lec4_ClassificationTree.html#optimizing-hyperparameters-to-optimize-performance",
    "title": "6  Classification trees",
    "section": "6.2 Optimizing hyperparameters to optimize performance",
    "text": "6.2 Optimizing hyperparameters to optimize performance\nIn case of diabetes, it is important to reduce FNR (False negative rate) or maximize recall. This is because if a person has diabetes, the consequences of predicting that they don’t have diabetes can be much worse than the other way round.\nLet us find the optimal depth of the tree and the number of terminal nods (leaves) that minimizes the FNR or maximizes recall.\nFind the maximum values of depth and number of leaves.\n\n#Defining the object to build a regression tree\nmodel = DecisionTreeClassifier(random_state=1) \n\n#Fitting the regression tree to the data\nmodel.fit(X, y)\n\nDecisionTreeClassifier(random_state=1)\n\n\n\n# Maximum number of leaves\nmodel.get_n_leaves()\n\n118\n\n\n\n# Maximum depth\nmodel.get_depth()\n\n14\n\n\n\n#Defining parameters and the range of values over which to optimize\nparam_grid = {    \n    'max_depth': range(2,14),\n    'max_leaf_nodes': range(2,118),\n    'max_features': range(1, 9)\n}\n\n\n#Grid search to optimize parameter values\n\nstart_time = time.time()\nskf = StratifiedKFold(n_splits=5)#The folds are made by preserving the percentage of samples for each class.\n\n#Minimizing FNR is equivalent to maximizing recall\ngrid_search = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, scoring=['precision','recall'], \n                           refit=\"recall\", cv=skf, n_jobs=-1, verbose = True)\ngrid_search.fit(X, y)\n\n# make the predictions\ny_pred = grid_search.predict(Xtest)\n\nprint('Train accuracy : %.3f'%grid_search.best_estimator_.score(X, y))\nprint('Test accuracy : %.3f'%grid_search.best_estimator_.score(Xtest, ytest))\nprint('Best recall Through Grid Search : %.3f'%grid_search.best_score_)\n\nprint('Best params for recall')\nprint(grid_search.best_params_)\n\nprint(\"Time taken =\", round((time.time() - start_time)), \"seconds\")\n\nFitting 5 folds for each of 11136 candidates, totalling 55680 fits\nTrain accuracy : 0.785\nTest accuracy : 0.675\nBest recall Through Grid Search : 0.658\nBest params for recall\n{'max_depth': 4, 'max_features': 2, 'max_leaf_nodes': 8}\nTime taken = 70 seconds",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Lec4_ClassificationTree.html#optimizing-the-decision-threshold-probability",
    "href": "Lec4_ClassificationTree.html#optimizing-the-decision-threshold-probability",
    "title": "6  Classification trees",
    "section": "6.3 Optimizing the decision threshold probability",
    "text": "6.3 Optimizing the decision threshold probability\nNote that decision threshold probability is not tuned with GridSearchCV because GridSearchCV is a technique used for hyperparameter tuning in machine learning models, and the decision threshold probability is not a hyperparameter of the model.\nThe decision threshold is set to 0.5 by default during hyperparameter tuning with GridSearchCV.\nGridSearchCV is used to tune hyperparameters that control the internal settings of a machine learning model, such as learning rate, regularization strength, and maximum tree depth, among others. These hyperparameters affect the model’s internal behavior and performance. On the other hand, the decision threshold is an external parameter that is used to interpret the model’s output and make predictions based on the predicted probabilities.\nTo tune the decision threshold, one typically needs to manually adjust it after the model has been trained and evaluated using a specific set of hyperparameter values. This can be done using methods, which involve evaluating the model’s performance at different decision threshold values and selecting the one that best meets the desired trade-off between false positives and false negatives based on the specific problem requirements.\nAs the recall will always be 100% for a decision threshold probability of zero, we’ll find a decision threshold probability that balances recall with another performance metric such as precision, false positive rate, accuracy, etc. Below are a couple of examples that show we can balance recall with (1) precision or (2) false positive rate.\n\n6.3.1 Balancing recall with precision\nWe can find a threshold probability that balances recall with precision.\n\nmodel = DecisionTreeClassifier(random_state=1, max_depth = 4, max_leaf_nodes=8, max_features=2).fit(X, y)\n\n# Note that we are using the cross-validated predicted probabilities, instead of directly using the \n# predicted probabilities on train data, as the model may be overfitting on the train data, and \n# may lead to misleading results\ncross_val_ypred = cross_val_predict(DecisionTreeClassifier(random_state=1, max_depth = 4, \n                                                        max_leaf_nodes=8, max_features=2), X, \n                                              y, cv = 5, method = 'predict_proba')\n\np, r, thresholds = precision_recall_curve(y, cross_val_ypred[:,1])\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\n\n# Thresholds with precision and recall\nnp.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\n\narray([[0.08196721, 0.33713355, 1.        ],\n       [0.09045226, 0.34982332, 0.95652174],\n       [0.09248555, 0.36641221, 0.92753623],\n       [0.0964467 , 0.39293139, 0.91304348],\n       [0.1       , 0.42105263, 0.88888889],\n       [0.10810811, 0.42298851, 0.88888889],\n       [0.10869565, 0.42857143, 0.88405797],\n       [0.12820513, 0.48378378, 0.8647343 ],\n       [0.14285714, 0.48219178, 0.85024155],\n       [0.18518519, 0.48618785, 0.85024155],\n       [0.2       , 0.48611111, 0.84541063],\n       [0.20512821, 0.48876404, 0.84057971],\n       [0.20833333, 0.49418605, 0.82125604],\n       [0.21276596, 0.49411765, 0.8115942 ],\n       [0.22916667, 0.50151976, 0.79710145],\n       [0.23684211, 0.51582278, 0.78743961],\n       [0.27777778, 0.52786885, 0.77777778],\n       [0.3015873 , 0.54794521, 0.77294686],\n       [0.36      , 0.56554307, 0.7294686 ],\n       [0.3697479 , 0.56692913, 0.69565217],\n       [0.37931034, 0.58974359, 0.66666667],\n       [0.54954955, 0.59130435, 0.65700483],\n       [0.55172414, 0.59798995, 0.57487923],\n       [0.55882353, 0.59893048, 0.5410628 ],\n       [0.58823529, 0.6091954 , 0.51207729],\n       [0.61904762, 0.6       , 0.47826087],\n       [0.62337662, 0.60431655, 0.4057971 ],\n       [0.63461538, 0.59130435, 0.32850242],\n       [0.69354839, 0.59803922, 0.29468599],\n       [0.69642857, 0.59493671, 0.22705314],\n       [0.70149254, 0.56338028, 0.19323671],\n       [0.71153846, 0.61403509, 0.16908213],\n       [0.75609756, 0.5952381 , 0.12077295],\n       [0.76363636, 0.55555556, 0.09661836],\n       [0.76470588, 0.59090909, 0.06280193],\n       [0.875     , 0.66666667, 0.03864734],\n       [0.94117647, 0.66666667, 0.02898551],\n       [1.        , 0.6       , 0.01449275]])\n\n\nSuppose, we wish to have at least 80% recall, with the highest possible precision. Then, based on the precision-recall curve (or the table above), we should have a decision threshold probability of 0.21.\nLet’s assess the model’s performance on test data with a threshold probability of 0.21.\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.21\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  72.72727272727273\nROC-AUC:  0.7544509078089194\nPrecision:  0.611764705882353\nRecall:  0.8524590163934426\n\n\n\n\n\n\n\n\n\n\n\n6.3.2 Balancing recall with false positive rate\nSuppose we wish to balance recall with false positive rate. We can optimize the model to maximize ROC-AUC, and then choose a point on the ROC-curve that balances recall with the false positive rate.\n\n# Defining parameters and the range of values over which to optimize\nparam_grid = {    \n    'max_depth': range(2,14),\n    'max_leaf_nodes': range(2,118),\n    'max_features': range(1, 9)\n}\n\n\n#Grid search to optimize parameter values\n\nstart_time = time.time()\nskf = StratifiedKFold(n_splits=5)#The folds are made by preserving the percentage of samples for each class.\n\n#Minimizing FNR is equivalent to maximizing recall\ngrid_search = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, scoring=['precision','recall',\n                            'roc_auc'], refit=\"roc_auc\", cv=skf, n_jobs=-1, verbose = True)\ngrid_search.fit(X, y)\n\n# make the predictions\ny_pred = grid_search.predict(Xtest)\n\nprint('Best params for recall')\nprint(grid_search.best_params_)\n\nprint(\"Time taken =\", round((time.time() - start_time)), \"seconds\")\n\nFitting 5 folds for each of 11136 candidates, totalling 55680 fits\nBest params for recall\n{'max_depth': 6, 'max_features': 2, 'max_leaf_nodes': 9}\nTime taken = 72 seconds\n\n\n\nmodel = DecisionTreeClassifier(random_state=1, max_depth = 6, max_leaf_nodes=9, max_features=2).fit(X, y)\n\n\ncross_val_ypred = cross_val_predict(DecisionTreeClassifier(random_state=1, max_depth = 6, \n                                                           max_leaf_nodes=9, max_features=2), X, \n                                              y, cv = 5, method = 'predict_proba')\n\nfpr, tpr, auc_thresholds = roc_curve(y, cross_val_ypred[:,1])\nprint(auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot(fpr, tpr, 'o', color = 'blue')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(y, cross_val_ypred[:,1])\nplot_roc_curve(fpr, tpr)\n\n0.7605075431162388\n\n\n\n\n\n\n\n\n\n\n# Thresholds with TPR and FPR\nall_thresholds = np.concatenate([auc_thresholds.reshape(-1,1), tpr.reshape(-1,1), fpr.reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,1]&gt;0.8,:]\n# As the values in 'recall_more_than_80' are arranged in increasing order of recall and decreasing threshold,\n# the first value will provide the maximum threshold probability for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the minimum possible FPR\nrecall_more_than_80[0]\n\narray([0.21276596, 0.80676329, 0.39066339])\n\n\nSuppose, we wish to have at least 80% recall, with the lowest possible precision. Then, based on the ROC-AUC curve, we should have a decision threshold probability of 0.21.\nLet’s assess the model’s performance on test data with a threshold probability of 0.21.\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.21\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  71.42857142857143\nROC-AUC:  0.7618543980257358\nPrecision:  0.6075949367088608\nRecall:  0.7868852459016393",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Lec4_ClassificationTree.html#cost-complexity-pruning",
    "href": "Lec4_ClassificationTree.html#cost-complexity-pruning",
    "title": "6  Classification trees",
    "section": "6.4 Cost complexity pruning",
    "text": "6.4 Cost complexity pruning\nJust as we did cost complexity pruning in a regression tree, we can do it to optimize the model for a classification tree.\n\nmodel = DecisionTreeClassifier(random_state = 1)#model without any restrictions\npath= model.cost_complexity_pruning_path(X,y)# Compute the pruning path during Minimal Cost-Complexity Pruning.\n\n\nalphas=path['ccp_alphas']\nlen(alphas)\n\n58\n\n\n\n#Grid search to optimize parameter values\n\nskf = StratifiedKFold(n_splits=5)\ngrid_search = GridSearchCV(DecisionTreeClassifier(random_state = 1), param_grid = {'ccp_alpha':alphas}, \n                                                  scoring=['precision','recall','accuracy'], \n                                                  refit=\"recall\", cv=skf, n_jobs=-1, verbose = True)\ngrid_search.fit(X, y)\n\n# make the predictions\ny_pred = grid_search.predict(Xtest)\n\nprint('Best params for recall')\nprint(grid_search.best_params_)\n\nFitting 5 folds for each of 58 candidates, totalling 290 fits\nBest params for recall\n{'ccp_alpha': 0.010561291712538737}\n\n\n\n# Model with the optimal value of 'ccp_alpha'\nmodel = DecisionTreeClassifier(ccp_alpha=0.01435396,random_state=1)\nmodel.fit(X, y)\n\nDecisionTreeClassifier(ccp_alpha=0.01435396, random_state=1)\n\n\nNow we can tune the decision threshold probability to balance recall with another performance metrics as shown earlier in Section 4.3.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification trees</span>"
    ]
  },
  {
    "objectID": "Lec4_Bagging.html",
    "href": "Lec4_Bagging.html",
    "title": "7  Bagging",
    "section": "",
    "text": "7.1 Bagging regression trees\nBag regression trees to develop a model to predict car price using the predictors mileage,mpg,year,and engineSize.\n#Bagging the results of 10 decision trees to predict car price\nmodel = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=1,\n                        n_jobs=-1).fit(X, y)\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5752.0779571060875\nThe RMSE has reduced a lot by averaging the predictions of 10 trees. The RMSE for a single tree model with optimized parameters was around 7000.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "Lec4_Bagging.html#bagging-regression-trees",
    "href": "Lec4_Bagging.html#bagging-regression-trees",
    "title": "7  Bagging",
    "section": "",
    "text": "7.1.1 Model accuracy vs number of trees\nHow does the model accuracy vary with the number of trees?\nAs we increase the number of trees, it will tend to reduce the variance of individual trees leading to a more accurate prediction.\n\n#Finding model accuracy vs number of trees\nwarnings.filterwarnings(\"ignore\")\noob_rsquared={};test_rsquared={};oob_rmse={};test_rmse = {}\nfor i in np.linspace(10,400,40,dtype=int):\n    model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=i, random_state=1,\n                        n_jobs=-1,oob_score=True).fit(X, y)\n    oob_rsquared[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_rsquared[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n    oob_rmse[i]=np.sqrt(mean_squared_error(model.oob_prediction_,y))\n    test_rmse[i]=np.sqrt(mean_squared_error(model.predict(Xtest),ytest))\nwarnings.resetwarnings()\n    \n# The hidden warning is: \"Some inputs do not have OOB scores. This probably means too few \n# estimators were used to compute any reliable oob estimates.\" This warning will appear\n# in case of small number of estimators. In such a case, some observations may be use\n# by all the estimators, and their OOB score can't be computed\n\nAs we are bagging only 10 trees in the first iteration, some of the observations are selected in every bootstrapped sample, and thus they don’t have an out-of-bag error, which is producing the warning. For every observation to have an out-of-bag error, the number of trees must be sufficiently large.\nLet us visualize the out-of-bag (OOB) R-squared and R-squared on test data vs the number of trees.\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),label = 'Out of bag R-squared')\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),'o',color = 'blue')\nplt.plot(test_rsquared.keys(),test_rsquared.values(), label = 'Test data R-squared')\nplt.xlabel('Number of trees')\nplt.ylabel('Rsquared')\nplt.legend();\n\n\n\n\n\n\n\n\nThe out-of-bag R-squared initially increases, and then stabilizes after a certain number of trees (around 150 in this case). Note that increasing the number of trees further will not lead to overfitting. However, increasing the number of trees will increase the computations. Thus, we don’t need to develop more trees once the R-squared stabilizes.\n\n#Visualizing out-of-bag RMSE and test data RMSE\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rmse.keys(),oob_rmse.values(),label = 'Out of bag RMSE')\nplt.plot(oob_rmse.keys(),oob_rmse.values(),'o',color = 'blue')\nplt.plot(test_rmse.keys(),test_rmse.values(), label = 'Test data RMSE')\nplt.xlabel('Number of trees')\nplt.ylabel('RMSE')\nplt.legend()\n\n\n\n\n\n\n\n\nA similar trend can be seen by plotting out-of-bag RMSE and test RMSE. Note that RMSE is proportional to R-squared. We only need to visualize one of RMSE or R-squared to find the optimal number of trees.\n\n#Bagging with 150 trees\nmodel = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=150, random_state=1,\n                        oob_score=True,n_jobs=-1).fit(X, y)\n\n\n#OOB R-squared\nmodel.oob_score_\n\n0.897561533100511\n\n\n\n#RMSE on test data\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(test.price, pred))\n\n5673.756466489405\n\n\n\n\n7.1.2 Optimizing bagging hyperparameters using grid search\nMore parameters of a bagged regression tree model can be optimized using the typical approach of k-fold cross validation over a grid of parameter values.\nNote that we don’t need to tune the number of trees in bagging as we know that the higher the number of trees, the lower will be the expected MSE. So, we will tune all the hyperparameters for a fixed number of trees. Once we have obtained the optimal hyperparameter values, we’ll keep increasing the number of trees until the gains are neglible.\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'base_estimator': [DecisionTreeRegressor(random_state = 1),LinearRegression()],#Comparing bagging with a linear regression model as well\n          'n_estimators': [100],\n          'max_samples': [0.5,1.0],\n          'max_features': [0.5,1.0],\n          'bootstrap': [True, False],\n          'bootstrap_features': [True, False]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\nbagging_regressor_grid = GridSearchCV(BaggingRegressor(random_state=1, n_jobs=-1), \n                                      param_grid =params, cv=cv, n_jobs=-1, verbose=1)\nbagging_regressor_grid.fit(X, y)\n\nprint('Train R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X, y))\nprint('Test R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(Xtest, ytest))\nprint('Best R^2 Score Through Grid Search : %.3f'%bagging_regressor_grid.best_score_)\nprint('Best Parameters : ',bagging_regressor_grid.best_params_)\n\nFitting 5 folds for each of 32 candidates, totalling 160 fits\nTrain R^2 Score : 0.986\nTest R^2 Score : 0.882\nBest R^2 Score Through Grid Search : 0.892\nBest Parameters :  {'base_estimator': DecisionTreeRegressor(random_state=1), 'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 100}\n\n\nYou may use the object bagging_regressor_grid to directly make the prediction.\n\nnp.sqrt(mean_squared_error(test.price, bagging_regressor_grid.predict(Xtest)))\n\n5708.308794847089\n\n\nNote that once the model has been tuned and the optimal hyperparameters identified, we can keep increasing the number of trees until it ceases to benefit.\n\n#Model with optimal hyperparameters and increased number of trees\nmodel = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=500, random_state=1,\n                        oob_score=True,n_jobs=-1,bootstrap_features=False,bootstrap=True,\n                        max_features=1.0,max_samples=1.0).fit(X, y)\n\n\n#RMSE on test data\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5624.685464926517",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "Lec4_Bagging.html#bagging-for-classification",
    "href": "Lec4_Bagging.html#bagging-for-classification",
    "title": "7  Bagging",
    "section": "7.2 Bagging for classification",
    "text": "7.2 Bagging for classification\nBag classification tree models to predict if a person has diabetes.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n#Bagging the results of 10 decision trees to predict car price\nmodel = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=150, random_state=1,\n                        n_jobs=-1).fit(X, y)\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.23\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  76.62337662337663\nROC-AUC:  0.8766084963863917\nPrecision:  0.6404494382022472\nRecall:  0.9344262295081968\n\n\n\n\n\n\n\n\n\nAs a result of bagging, we obtain a model (with a threshold probabiltiy cutoff of 0.23) that has a better performance on test data in terms of almost all the metrics - accuracy, precision (comparable performance), recall, and ROC-AUC, as compared the single tree classification model (with a threshold probability cutoff of 0.23). Note that we have not yet tuned the model using GridSearchCv here, which is shown towards the end of this chapter.\n\n7.2.1 Model accuracy vs number of trees\n\n#Finding model accuracy vs number of trees\noob_accuracy={};test_accuracy={};oob_rmse={};test_rmse = {}\nfor i in np.linspace(10,400,40,dtype=int):\n    model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=i, random_state=1,\n                        n_jobs=-1,oob_score=True).fit(X, y)\n    oob_accuracy[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_accuracy[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:640: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n  warn(\"Some inputs do not have OOB scores. \"\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:644: RuntimeWarning: invalid value encountered in true_divide\n  oob_decision_function = (predictions /\n\n\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),label = 'Out of bag accuracy')\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),'o',color = 'blue')\nplt.plot(test_accuracy.keys(),test_accuracy.values(), label = 'Test data accuracy')\nplt.xlabel('Number of trees')\nplt.ylabel('Rsquared')\nplt.legend()\n\n\n\n\n\n\n\n\n\n#ROC curve on training data\nypred = model.predict_proba(X)[:, 1]\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nprint(auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nplot_roc_curve(fpr, tpr)\n\n1.0\n\n\n\n\n\n\n\n\n\nNote that there is perfect separation in train data as ROC-AUC = 1. This shows that the model is probably overfitting. However, this also shows that, despite the reduced variance (as compared to a single tree), the bagged tree model is flexibly enough to perfectly separate the classes.\n\n#ROC curve on test data\nypred = model.predict_proba(Xtest)[:, 1]\nfpr, tpr, auc_thresholds = roc_curve(ytest, ypred)\nprint(\"ROC-AUC = \",auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(ytest, ypred)\nplot_roc_curve(fpr, tpr)\n\nROC-AUC =  0.8781949585757096\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Optimizing bagging hyperparameters using grid search\nMore parameters of a bagged classification tree model can be optimized using the typical approach of k-fold cross validation over a grid of parameter values.\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'base_estimator': [DecisionTreeClassifier(random_state = 1),LogisticRegression()],#Comparing bagging with a linear regression model as well\n          'n_estimators': [150,200,250],\n          'max_samples': [0.5,1.0],\n          'max_features': [0.5,1.0],\n          'bootstrap': [True, False],\n          'bootstrap_features': [True, False]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\nbagging_classifier_grid = GridSearchCV(BaggingClassifier(random_state=1, n_jobs=-1), \n                                      param_grid =params, cv=cv, n_jobs=-1, verbose=1,\n                                      scoring = ['precision', 'recall'], refit='recall')\nbagging_classifier_grid.fit(X, y)\n\nprint('Train accuracy : %.3f'%bagging_classifier_grid.best_estimator_.score(X, y))\nprint('Test accuracy : %.3f'%bagging_classifier_grid.best_estimator_.score(Xtest, ytest))\nprint('Best accuracy Through Grid Search : %.3f'%bagging_classifier_grid.best_score_)\nprint('Best Parameters : ',bagging_classifier_grid.best_params_)\n\nFitting 5 folds for each of 96 candidates, totalling 480 fits\nTrain accuracy : 1.000\nTest accuracy : 0.786\nBest accuracy Through Grid Search : 0.573\nBest Parameters :  {'base_estimator': DecisionTreeClassifier(random_state=1), 'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200}\n\n\n\n\n7.2.3 Tuning the decision threshold probability\nWe’ll find a decision threshold probability that balances recall with precision.\n\nmodel = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=1), n_estimators=200, \n                          random_state=1,max_features=1.0, oob_score=True,\n                        max_samples=1.0,n_jobs=-1,bootstrap=True,bootstrap_features=False).fit(X, y)\n\nAs the model is overfitting on the train data, it will not be a good idea to tune the decision threshold probability based on the precision-recall curve on train data, as shown in the figure below.\n\nypred = model.predict_proba(X)[:,1]\np, r, thresholds = precision_recall_curve(y, ypred)\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\nInstead, we should make the precision-recall curve using the out-of-bag predictions, as shown below. The method oob_decision_function_ provides the predicted probability.\n\nypred = model.oob_decision_function_[:,1]\np, r, thresholds = precision_recall_curve(y, ypred)\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\n\n# Thresholds with precision and recall\nall_thresholds = np.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,2]&gt;0.8,:]\n# As the values in 'recall_more_than_80' are arranged in decreasing order of recall and increasing threshold,\n# the last value will provide the maximum threshold probability for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the maximum possible precision\nrecall_more_than_80[recall_more_than_80.shape[0]-1]\n\narray([0.2804878 , 0.53205128, 0.80193237])\n\n\nSuppose, we wish to have at least 80% recall, with the highest possible precision. Then, based on the precision-recall curve, we should have a decision threshold probability of 0.28.\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.28\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.22077922077922\nROC-AUC:  0.8802221047065044\nPrecision:  0.6705882352941176\nRecall:  0.9344262295081968\n\n\n\n\n\n\n\n\n\nNote that this model has a better performance than the untuned bagged model earlier, and the single tree classification model, as expected.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bagging</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html",
    "href": "Bagging (OOB vs K-fold cross-validation).html",
    "title": "8  Bagging (addendum)",
    "section": "",
    "text": "8.1 Tree without tuning\nmodel = DecisionTreeRegressor()\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n-np.mean(cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv = cv))\n\n7056.960817154941\nparam_grid = {'max_depth': Integer(2, 30)}\ngcv = BayesSearchCV(model, search_spaces = param_grid, cv = cv, n_iter = 40, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\ngcv.fit(X, y, callback = monitor)    \n\n['max_depth'] = [10] 6341.1481858990355\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=DecisionTreeRegressor(), n_iter=40, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'max_depth': Integer(low=2, high=30, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=DecisionTreeRegressor(), n_iter=40, n_jobs=-1,\n              random_state=10, scoring='neg_root_mean_squared_error',\n              search_spaces={'max_depth': Integer(low=2, high=30, prior='uniform', transform='normalize')})estimator: DecisionTreeRegressorDecisionTreeRegressor()DecisionTreeRegressorDecisionTreeRegressor()",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#performance-of-tree-improves-with-tuning",
    "href": "Bagging (OOB vs K-fold cross-validation).html#performance-of-tree-improves-with-tuning",
    "title": "8  Bagging (addendum)",
    "section": "8.2 Performance of tree improves with tuning",
    "text": "8.2 Performance of tree improves with tuning\n\nmodel = DecisionTreeRegressor(max_depth=10)\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n-np.mean(cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv = cv))\n\n6442.494300778735",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#bagging-tuned-trees",
    "href": "Bagging (OOB vs K-fold cross-validation).html#bagging-tuned-trees",
    "title": "8  Bagging (addendum)",
    "section": "8.3 Bagging tuned trees",
    "text": "8.3 Bagging tuned trees\n\nmodel = BaggingRegressor(DecisionTreeRegressor(max_depth = 10), oob_score=True, n_estimators = 100).fit(X, y)\nmean_squared_error(model.oob_prediction_, y, squared = False)\n\n5354.357809020438",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#bagging-untuned-trees",
    "href": "Bagging (OOB vs K-fold cross-validation).html#bagging-untuned-trees",
    "title": "8  Bagging (addendum)",
    "section": "8.4 Bagging untuned trees",
    "text": "8.4 Bagging untuned trees\n\nmodel = BaggingRegressor(DecisionTreeRegressor(), oob_score=True, n_estimators = 100).fit(X, y)\nmean_squared_error(model.oob_prediction_, y, squared = False)\n\n5248.720845665685\n\n\nWhy is bagging tuned trees worse than bagging untuned trees?\nIn the tuned tree here, the reduction in variance by controlling maximum depth resulted in an increas in bias of indivudual trees. Bagging trees only reduces the variance, but not the bias of the indivudal trees. Thus, bagging high bias models will result in a high-bias model, while bagging high variance models may result in a low variance model if the models are not highly correlated.\nBagging tuned models may provide a better performance as compared to bagging untuned models if the reduction in variance of the individual models is high enough to overshadow the increase in bias, and increase in pairwise correlation of the individual models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#tuning-bagged-model---oob",
    "href": "Bagging (OOB vs K-fold cross-validation).html#tuning-bagged-model---oob",
    "title": "8  Bagging (addendum)",
    "section": "8.5 Tuning bagged model - OOB",
    "text": "8.5 Tuning bagged model - OOB\n\nparam_grid1 = {'max_samples': [0.25, 0.5, 0.75, 1.0],\n             'max_features': [2, 3, 4],\n             'bootstrap_features': [True, False]}\nparam_grid2 = {'max_samples': [0.25, 0.5, 0.75, 1.0],\n             'max_features': [1],\n              'bootstrap_features': [False]}\nparam_list1 = list(it.product(*[values for key, values in param_grid1.items()]))\nparam_list2 = list(it.product(*[values for key, values in param_grid2.items()]))\nparam_list = param_list1 + param_list2\n\n\noob_score_pr = []\nfor pr in param_list:\n    model = BaggingRegressor(DecisionTreeRegressor(), max_samples=pr[0], max_features=pr[1],\n                            bootstrap_features=pr[2], n_jobs = -1, oob_score=True, n_estimators = 50).fit(X, y)\n    oob_score_pr.append(mean_squared_error(model.oob_prediction_, y, squared=False))\n\nWhat is the benefit of OOB validation to tune hyperparameters in bagging?\nIt is much cheaper than \\(k\\)-fold cross-validation, as only \\(1/k\\) of the models are trained with OOB validation as compared to \\(k\\)-fold cross-validation. However, the cost of training individual models is lower in \\(k\\)-fold cross-validation as models are trained on a smaller dataset. Typically, OOB will be faster than \\(k\\)-fold cross-validation. The higher the value of \\(k\\), the more faster OOB validation will be as compared to \\(k\\)-fold cross-validation.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#tuning-without-k-fold-cross-validation",
    "href": "Bagging (OOB vs K-fold cross-validation).html#tuning-without-k-fold-cross-validation",
    "title": "8  Bagging (addendum)",
    "section": "8.6 Tuning without k-fold cross-validation",
    "text": "8.6 Tuning without k-fold cross-validation\nWhen hyperparameters can be tuned with OOB validation, what is the benefit of using k-fold cross-validation?\n\nHyperparameters cannot be tuned over continuous spaces with OOB validation.\nOOB score is not computed if samping is done without replacement (bootstrap = False). Thus, for tuning the bootstrap hyperparameter, \\(k\\)-fold cross-validation will need to be used.\n\n\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\n\n\nparam_grid = {'max_samples': Real(0.2, 1.0),\n             'max_features': Integer(1, 4),\n             'bootstrap_features': [True, False],\n              'bootstrap': [True, False]}\ngcv = BayesSearchCV(BaggingRegressor(DecisionTreeRegressor(), bootstrap=False), \n                    search_spaces = param_grid, cv = cv, n_jobs = -1,\n                  scoring='neg_root_mean_squared_error')\n\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ngcv.fit(X, y, callback=monitor)\n\n['bootstrap', 'bootstrap_features', 'max_features', 'max_samples'] = [True, False, 4, 0.8061354588503475] 5561.064432968422\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=BaggingRegressor(bootstrap=False,\n                                         estimator=DecisionTreeRegressor()),\n              n_jobs=-1, scoring='neg_root_mean_squared_error',\n              search_spaces={'bootstrap': [True, False],\n                             'bootstrap_features': [True, False],\n                             'max_features': Integer(low=1, high=4, prior='uniform', transform='normalize'),\n                             'max_samples': Real(low=0.2, high=1.0, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=BaggingRegressor(bootstrap=False,\n                                         estimator=DecisionTreeRegressor()),\n              n_jobs=-1, scoring='neg_root_mean_squared_error',\n              search_spaces={'bootstrap': [True, False],\n                             'bootstrap_features': [True, False],\n                             'max_features': Integer(low=1, high=4, prior='uniform', transform='normalize'),\n                             'max_samples': Real(low=0.2, high=1.0, prior='uniform', transform='normalize')})estimator: BaggingRegressorBaggingRegressor(bootstrap=False, estimator=DecisionTreeRegressor())estimator: DecisionTreeRegressorDecisionTreeRegressor()DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nplot_histogram(gcv.optimizer_results_[0],0)\n\n\n\n\n\n\n\n\n\nplot_objective(gcv.optimizer_results_[0])",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#warm-start",
    "href": "Bagging (OOB vs K-fold cross-validation).html#warm-start",
    "title": "8  Bagging (addendum)",
    "section": "8.7 warm start",
    "text": "8.7 warm start\nWhat is the purpose of warm_start?\nThe purpose of warm_start is to avoid developing trees from scratch, and incrementally add trees to monitor the validation error. However, note that OOB score is not computed with warm_start. Thus, a validation set approach will need to be adopted to tune number of trees.\nA cheaper approach to tune number of estimators is to just use trial and error, and stop increasing once the cross-validation error / OOB error / validation set error stabilizes.\n\nmodel = BaggingRegressor(DecisionTreeRegressor(), oob_score=False, n_estimators = 5,\n                        warm_start=True).fit(X, y)\nrmse = []\nfor i in range(10, 200, 10):\n    model.n_estimators = i\n    model.fit(X, y)\n    rmse.append(mean_squared_error(model.predict(Xtest), ytest, squared=False))\n    sns.lineplot(x = range(10, i+1, 10), y = rmse)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Bagging (OOB vs K-fold cross-validation).html#bagging-knn",
    "href": "Bagging (OOB vs K-fold cross-validation).html#bagging-knn",
    "title": "8  Bagging (addendum)",
    "section": "8.8 Bagging KNN",
    "text": "8.8 Bagging KNN\nShould we bag a tuned KNN model or an untuned one?\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nmodel = KNeighborsRegressor(n_neighbors=9) # optimal neigbors\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n-np.mean(cross_val_score((model), X_scaled, y, cv = cv, \n                         scoring='neg_root_mean_squared_error', n_jobs = -1))\n\n6972.997277781689\n\n\n\nmodel = KNeighborsRegressor(n_neighbors=1)\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n-np.mean(cross_val_score(BaggingRegressor(model), X_scaled, y, cv = cv, \n                         scoring='neg_root_mean_squared_error', n_jobs = -1))\n\n6254.305462266355\n\n\n\nmodel = BaggingRegressor(DecisionTreeRegressor(), n_estimators=5, warm_start=True)\nmodel.fit(X, y)\nrmse = []\nfor i in range(10, 200,10):\n    model.n_estimators = i\n    model.fit(X, y)\n    rmse.append(mean_squared_error(model.predict(Xtest), ytest, squared=False))\n    sns.lineplot(x = range(10, i + 1, 10), y = rmse)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bagging (addendum)</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html",
    "href": "Lec6_RandomForest.html",
    "title": "9  Random Forest",
    "section": "",
    "text": "9.1 Random Forest for regression\nNow, let us visualize small trees with the random forest algorithm to see if a predictor dominates all the trees.\n#Averaging the results of 10 decision trees, while randomly considering sqrt(4)=2 predictors at each node\n#to split, to predict car price\nmodel = RandomForestRegressor(n_estimators=10, random_state=1,max_features=\"sqrt\",max_depth=3,\n                        n_jobs=-1).fit(X, y)\n#Change the index of model.estimators_[index] to visualize the 10 random forest trees, one at a time\ndot_data = StringIO()\nexport_graphviz(model.estimators_[4], out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names =['mileage','mpg','year','engineSize'],precision=0)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n#graph.write_png('car_price_tree.png')\nImage(graph.create_png())\nAs two of the four predictors are randomly selected for splitting each node, engineSize no longer seems to dominate the trees. This will tend to reduce correlation among trees, thereby reducing the prediction variance, which in turn will tend to improve prediction accuracy.\n#Averaging the results of 10 decision trees, while randomly considering sqrt(4)=2 predictors at each node\n#to split, to predict car price\nmodel = RandomForestRegressor(n_estimators=10, random_state=1,max_features=\"sqrt\",\n                        n_jobs=-1).fit(X, y)\nmodel.feature_importances_\n\narray([0.16370584, 0.35425511, 0.18552673, 0.29651232])\nNote that the feature importance of engineSize is reduced in random forests (as compared to bagged trees), and it no longer dominates the trees.\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5856.022395768459\nThe RMSE is similar to that obtained by bagging. We will discuss the comparison later.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-for-regression",
    "href": "Lec6_RandomForest.html#random-forest-for-regression",
    "title": "9  Random Forest",
    "section": "",
    "text": "9.1.1 Model accuracy vs number of trees\nHow does the model accuracy vary with the number of trees?\nAs we increase the number of trees, it will tend to reduce the variance of individual trees leading to a more accurate prediction.\n\n#Finding model accuracy vs number of trees\nwarnings.filterwarnings(\"ignore\")\noob_rsquared={};test_rsquared={};oob_rmse={};test_rmse = {}\n\nfor i in np.linspace(10,400,40,dtype=int):\n    model = RandomForestRegressor(n_estimators=i, random_state=1,max_features=\"sqrt\",\n                        n_jobs=-1,oob_score=True).fit(X, y)\n    oob_rsquared[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_rsquared[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n    oob_rmse[i]=np.sqrt(mean_squared_error(model.oob_prediction_,y))\n    test_rmse[i]=np.sqrt(mean_squared_error(model.predict(Xtest),ytest))\n\nwarnings.resetwarnings()\n    \n# The hidden warning is: \"Some inputs do not have OOB scores. This probably means too few \n# estimators were used to compute any reliable oob estimates.\" This warning will appear\n# in case of small number of estimators. In such a case, some observations may be use\n# by all the estimators, and their OOB score can't be computed\n\nAs we are ensemble only 10 trees in the first iteration, some of the observations are selected in every bootstrapped sample, and thus they don’t have an out-of-bag error, which is producing the warning. For every observation to have an out-of-bag error, the number of trees must be sufficiently large.\nLet us visualize the out-of-bag (OOB) R-squared and R-squared on test data vs the number of trees.\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),label = 'Out of bag R-squared')\nplt.plot(oob_rsquared.keys(),oob_rsquared.values(),'o',color = 'blue')\nplt.plot(test_rsquared.keys(),test_rsquared.values(), label = 'Test data R-squared')\nplt.xlabel('Number of trees')\nplt.ylabel('Rsquared')\nplt.legend();\n\n\n\n\n\n\n\n\nThe out-of-bag \\(R\\)-squared initially increases, and then stabilizes after a certain number of trees (around 200 in this case). Note that increasing the number of trees further will not lead to overfitting. However, increasing the number of trees will increase the computations. Thus, the number of trees developed should be the number beyond which the \\(R\\)-squared stabilizes.\n\n#Visualizing out-of-bag RMSE and test data RMSE\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_rmse.keys(),oob_rmse.values(),label = 'Out of bag RMSE')\nplt.plot(oob_rmse.keys(),oob_rmse.values(),'o',color = 'blue')\nplt.plot(test_rmse.keys(),test_rmse.values(), label = 'Test data RMSE')\nplt.xlabel('Number of trees')\nplt.ylabel('RMSE')\nplt.legend();\n\n\n\n\n\n\n\n\nA similar trend can be seen by plotting out-of-bag RMSE and test RMSE. Note that RMSE is proportional to R-squared. You only need to visualize one of RMSE or \\(R\\)-squared to find the optimal number of trees.\n\n#Bagging with 150 trees\nmodel = RandomForestRegressor(n_estimators=200, random_state=1,max_features=\"sqrt\",\n                        oob_score=True,n_jobs=-1).fit(X, y)\n\n\n#OOB R-squared\nmodel.oob_score_\n\n0.8998265006519903\n\n\n\n#RMSE on test data\npred = model.predict(Xtest)\nnp.sqrt(mean_squared_error(test.price, pred))\n\n5647.195064555622\n\n\n\n\n9.1.2 Tuning random forest\nThe Random forest object has options to set parameters such as depth, leaves, minimum number of observations in a leaf etc., for individual trees. These parameters are useful to prune a decision tree model consisting of a single tree, in order to avoid overfitting due to high variance of an unpruned tree.\nPruning individual trees in random forests is not likely to add much value, since averaging a sufficient number of unpruned trees reduces the variance of the trees, which enhances prediction accuracy. Pruning individual trees is unlikely to further reduce the prediction variance.\nHere is a comment from page 596 of the The Elements of Statistical Learning that supports the above statement: Segal (2004) demonstrates small gains in performance by controlling the depths of the individual trees grown in random forests. Our experience is that using full-grown trees seldom costs much, and results in one less tuning parameter.\nBelow we attempt to optimize parameters that prune individual trees. However, as expected, it does not result in a substantial increase in prediction accuracy.\nAlso, note that we don’t need to tune the number of trees in random forest with GridSearchCV. As we know the prediction accuracy will keep increasing with number of trees, we can tune the other hyperparameters with a constant value for the number of trees.\n\nmodel.estimators_[0].get_n_leaves()\n\n3086\n\n\n\nmodel.estimators_[0].get_depth()\n\n29\n\n\nCoarse grid search\n\n#Optimizing with OOB score takes half the time as compared to cross validation. \n#The number of models developed with OOB score tuning is one-fifth of the number of models developed with\n#5-fold cross validation\nstart_time = time.time()\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'max_depth': [5, 10, 15, 20, 25, 30],\n          'max_leaf_nodes':[600, 1200, 1800, 2400, 3000],\n          'max_features': [1,2,3,4]}\n\nparam_list=list(it.product(*(params[Name] for Name in params)))\n\noob_score = [0]*len(param_list)\ni=0\nfor pr in param_list:\n    model = RandomForestRegressor(random_state=1,oob_score=True,verbose=False,\n                    n_estimators = 100, max_depth=pr[0],\n                    max_leaf_nodes=pr[1], max_features=pr[2], n_jobs=-1).fit(X,y)\n    oob_score[i] = mean_squared_error(model.oob_prediction_, y, squared=False)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"Best params = \", param_list[np.argmin(oob_score)])\nprint(\"Optimal OOB validation RMSE = \", np.min(oob_score))\n\ntime taken =  1.230358862876892  minutes\nBest params =  (15, 1800, 3)\nOptimal OOB validation RMSE =  5243.408784594606\n\n\nFiner grid search\nBased on the coarse grid search, hyperparameters will be tuned in a finer grid around the optimal hyperparamter values obtained.\n\n#Optimizing with OOB score takes half the time as compared to cross validation. \n#The number of models developed with OOB score tuning is one-fifth of the number of models developed with\n#5-fold cross validation\nstart_time = time.time()\n\nn_samples = train.shape[0]\nn_features = train.shape[1]\n\nparams = {'max_depth': [12, 15, 18],\n          'max_leaf_nodes':[1600, 1800, 2000],\n          'max_features': [1,2,3,4]}\n\nparam_list=list(it.product(*(params[Name] for Name in params)))\n\noob_score = [0]*len(param_list)\ni=0\nfor pr in param_list:\n    model = RandomForestRegressor(random_state=1,oob_score=True,verbose=False,\n             n_estimators = 100, max_depth=pr[0], max_leaf_nodes=pr[1],\n                    max_features=pr[2], n_jobs=-1).fit(X,y)\n    oob_score[i] = mean_squared_error(model.oob_prediction_, y, squared=False)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"Best params = \", param_list[np.argmin(oob_score)])\nprint(\"Optimal OOB validation RMSE = \", np.min(oob_score))\n\ntime taken =  0.4222299337387085  minutes\nBest params =  (15, 1800, 3)\nBest score =  5243.408784594606\n\n\n\n#Model with optimal parameters\nmodel = RandomForestRegressor(n_estimators = 100, random_state=1, max_leaf_nodes = 1800, max_depth = 15,\n                        oob_score=True,n_jobs=-1, max_features=3).fit(X, y)\n\n\n#RMSE on test data\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5671.410705964455\n\n\nOptimizing depth and leaves of individual trees didn’t improve the prediction accuracy of the model. Important parameters to optimize in random forests will be the number of trees (n_estimators), and number of predictors considered at each split (max_features). However, sometimes individual pruning of trees may be useful. This may happen when the increase in bias in individual trees (when pruned) is lesser than the decrease in variance of the tree. However, if the pairwise correlation coefficient \\(\\rho\\) of the trees increases by a certain extent on pruning, pruning may again be not useful.\n\n#Tuning only n_estimators and max_features produces similar results\nstart_time = time.time()\nparams = {'max_features': [1,2,3,4]}\n\nparam_list=list(it.product(*(params[Name] for Name in params)))\n\noob_score = [0]*len(param_list)\ni=0\nfor pr in param_list:\n    model = RandomForestRegressor(random_state=1,oob_score=True,verbose=False,\n                      n_estimators = 100, max_features=pr[0], n_jobs=-1).fit(X,y)\n    oob_score[i] = mean_squared_error(model.oob_prediction_, y, squared=False)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"Best params = \", param_list[np.argmin(oob_score)])\nprint(\"Optimal OOB validation RMSE = \", np.min(oob_score))\n\ntime taken =  0.02856200933456421  minutes\nBest params =  (3,)\nBest score (R-squared) =  5252.291978670057\n\n\n\n#Model with optimal parameters\nmodel = RandomForestRegressor(n_estimators=100, random_state=1,\n                        n_jobs=-1, max_features=3).fit(X, y)\nnp.sqrt(mean_squared_error(test.price, model.predict(Xtest)))\n\n5656.561522632323\n\n\nConsidering hyperparameters involving pruning, we observe a marginal decrease in the out-of-bag RMSE. Thus, other hyperparameters (such as max_features and max_samples) must be prioritized for tuning over hyperparameters involving pruning.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-for-classification",
    "href": "Lec6_RandomForest.html#random-forest-for-classification",
    "title": "9  Random Forest",
    "section": "9.2 Random forest for classification",
    "text": "9.2 Random forest for classification\nRandom forest model to predict if a person has diabetes.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n#Ensembling the results of 10 decision trees\nmodel = RandomForestClassifier(n_estimators=200, random_state=1,max_features=\"sqrt\",n_jobs=-1).fit(X, y)\n\n\n#Feature importance for Random forest\nnp.mean([tree.feature_importances_ for tree in model.estimators_],axis=0)\n\narray([0.08380406, 0.25403736, 0.09000104, 0.07151063, 0.07733353,\n       0.16976023, 0.12289303, 0.13066012])\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.23\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  72.72727272727273\nROC-AUC:  0.8744050766790058\nPrecision:  0.6021505376344086\nRecall:  0.9180327868852459\n\n\n\n\n\n\n\n\n\nThe model obtained above is similar to the one obtained by bagging. We’ll discuss the comparison later.\n\n9.2.1 Model accuracy vs number of trees\n\n#Finding model accuracy vs number of trees\noob_accuracy={};test_accuracy={};oob_precision={}; test_precision = {}\nfor i in np.linspace(50,500,45,dtype=int):\n    model = RandomForestClassifier(n_estimators=i, random_state=1,max_features=\"sqrt\",n_jobs=-1,oob_score=True).fit(X, y)\n    oob_accuracy[i]=model.oob_score_  #Returns the out-of_bag R-squared of the model\n    test_accuracy[i]=model.score(Xtest,ytest) #Returns the test R-squared of the model\n    oob_pred = (model.oob_decision_function_[:,1]&gt;=0.5).astype(int)     \n    oob_precision[i] = precision_score(y, oob_pred)\n    test_pred = model.predict(Xtest)\n    test_precision[i] = precision_score(ytest, test_pred)\n\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),label = 'Out of bag accuracy')\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),'o',color = 'blue')\nplt.plot(test_accuracy.keys(),test_accuracy.values(), label = 'Test data accuracy')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Classification accuracy')\nplt.legend();\n\n\n\n\n\n\n\n\nWe can also plot other metrics of interest such as out-of-bag precision vs number of trees.\n\n#Precision vs number of trees\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_precision.keys(),oob_precision.values(),label = 'Out of bag precision')\nplt.plot(oob_precision.keys(),oob_precision.values(),'o',color = 'blue')\nplt.plot(test_precision.keys(),test_precision.values(), label = 'Test data precision')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Precision')\nplt.legend();\n\n\n\n\n\n\n\n\n\n\n9.2.2 Tuning random forest\nHere we tune the number of predictors to be considered at each node for the split to maximize recall.\n\nstart_time = time.time()\n\nparams = {'n_estimators': [500],\n          'max_features': range(1,9),\n         }\n\nparam_list=list(it.product(*(params[Name] for Name in list(params.keys()))))\noob_recall = [0]*len(param_list)\n\ni=0\nfor pr in param_list:\n    model = RandomForestClassifier(random_state=1,oob_score=True,verbose=False,n_estimators = pr[0],\n                                  max_features=pr[1], n_jobs=-1).fit(X,y)\n    \n    oob_pred = (model.oob_decision_function_[:,1]&gt;=0.5).astype(int)     \n    oob_recall[i] = recall_score(y, oob_pred)\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"max recall = \", np.max(oob_recall))\nprint(\"params= \", param_list[np.argmax(oob_recall)])\n\ntime taken =  0.08032723267873128  minutes\nmax recall =  0.5990338164251208\nparams=  (500, 8)\n\n\n\nmodel = RandomForestClassifier(random_state=1,n_jobs=-1,max_features=8,n_estimators=500).fit(X, y)\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = 0.23\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  76.62337662337663\nROC-AUC:  0.8787237793054822\nPrecision:  0.6404494382022472\nRecall:  0.9344262295081968\n\n\n\n\n\n\n\n\n\n\nmodel.feature_importances_\n\narray([0.069273  , 0.31211579, 0.08492953, 0.05225877, 0.06179047,\n       0.17732674, 0.12342981, 0.1188759 ])",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#random-forest-vs-bagging",
    "href": "Lec6_RandomForest.html#random-forest-vs-bagging",
    "title": "9  Random Forest",
    "section": "9.3 Random forest vs Bagging",
    "text": "9.3 Random forest vs Bagging\nWe saw in the above examples that the performance of random forest was similar to that of bagged trees. This may happen in some cases including but not limited to:\n\nAll the predictors are more or less equally important, and the bagged trees are not highly correlated.\nOne of the predictors dominates the trees, resulting in highly correlated trees. However, each of the highly correlated trees have high prediction accuracy, leading to overall high prediction accuracy of the bagged trees despite the high correlation.\n\nWhen can random forests perform poorly: When the number of variables is large, but the fraction of relevant variables small, random forests are likely to perform poorly with small \\(m\\) (fraction of predictors considered for each split). At each split the chance can be small that the relevant variables will be selected. - Elements of Statistical Learning, page 596.\nHowever, in general, random forests are expected to decorrelate and improve the bagged trees.\nLet us consider a classification example.\n\ndata = pd.read_csv('Heart.csv')\ndata.dropna(inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\n\nAge\nSex\nChestPain\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nThal\nAHD\n\n\n\n\n0\n63\n1\ntypical\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\nfixed\nNo\n\n\n1\n67\n1\nasymptomatic\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\nnormal\nYes\n\n\n2\n67\n1\nasymptomatic\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\nreversable\nYes\n\n\n3\n37\n1\nnonanginal\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\nnormal\nNo\n\n\n4\n41\n0\nnontypical\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\nnormal\nNo\n\n\n\n\n\n\n\n\nIn the above dataset, we wish to predict if a person has acquired heart disease (AHD = ‘Yes’), based on their symptoms.\n\n#Response variable\ny = pd.get_dummies(data['AHD'])['Yes']\n\n#Creating a dataframe for predictors with dummy variables replacing the categorical variables\nX = data.drop(columns = ['AHD','ChestPain','Thal'])\nX = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\nX.head()\n\n\n\n\n\n\n\n\n\nAge\nSex\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nasymptomatic\nnonanginal\nnontypical\ntypical\nfixed\nnormal\nreversable\n\n\n\n\n0\n63\n1\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\n0\n0\n0\n1\n1\n0\n0\n\n\n1\n67\n1\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\n1\n0\n0\n0\n0\n1\n0\n\n\n2\n67\n1\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\n1\n0\n0\n0\n0\n0\n1\n\n\n3\n37\n1\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\n0\n1\n0\n0\n0\n1\n0\n\n\n4\n41\n0\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\nX.shape\n\n(297, 18)\n\n\n\n#Creating train and test datasets\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec6_RandomForest.html#tuning-random-forest-2",
    "href": "Lec6_RandomForest.html#tuning-random-forest-2",
    "title": "9  Random Forest",
    "section": "Tuning random forest",
    "text": "Tuning random forest\n\n#Tuning the random forest parameters\nstart_time = time.time()\n\noob_score = {}\n\ni=0\nfor pr in range(1,19):\n    model = RandomForestClassifier(random_state=1,oob_score=True,verbose=False,n_estimators = 500,\n                                  max_features=pr, n_jobs=-1).fit(X,y)\n    oob_score[i] = model.oob_score_\n    i=i+1\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"max accuracy = \", np.max(list(oob_score.values())))\nprint(\"Best value of max_features= \", np.argmax(list(oob_score.values()))+1)\n\ntime taken =  0.21557459433873494  minutes\nmax accuracy =  0.8249158249158249\nBest value of max_features=  3\n\n\n\nsns.scatterplot(x = oob_score.keys(),y = oob_score.values())\nplt.xlabel('Max features')\nplt.ylabel('Classification accuracy')\n\nText(0, 0.5, 'Classification accuracy')\n\n\n\n\n\n\n\n\n\nNote that as the value of max_features is increasing, the accuracy is decreasing. This is probably due to the trees getting correlated as we consider more predictors for each split.\n\n#Finding model accuracy vs number of trees\noob_accuracy={};test_accuracy={};\noob_accuracy2={};test_accuacy2={};\n\nfor i in np.linspace(100,500,40,dtype=int):\n    #Bagging\n    model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=i, random_state=1,\n                        n_jobs=-1,oob_score=True).fit(Xtrain, ytrain)\n    oob_accuracy[i]=model.oob_score_  #Returns the out-of-bag classification accuracy of the model\n    test_accuracy[i]=model.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n    \n    #Random forest\n    model2 = RandomForestClassifier(n_estimators=i, random_state=1,max_features=3,\n                        n_jobs=-1,oob_score=True).fit(Xtrain, ytrain)\n    oob_accuracy2[i]=model2.oob_score_  #Returns the out-of-bag classification accuracy of the model\n    test_accuacy2[i]=model2.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n   \n\n\n#Feature importance for bagging\nnp.mean([tree.feature_importances_ for tree in model.estimators_],axis=0)\n\narray([0.04381883, 0.05913479, 0.08585651, 0.07165678, 0.00302965,\n       0.00903484, 0.05890448, 0.01223421, 0.072461  , 0.01337919,\n       0.17495662, 0.18224651, 0.00527156, 0.00953965, 0.00396654,\n       0.00163193, 0.09955286, 0.09332406])\n\n\nNote that no predictor is too important to consider. That’s why a small value of three for max_features is likely to decorrelate trees without compromising the quality of predictions.\n\nplt.rcParams.update({'font.size': 15})\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),label = 'Bagging OOB')\nplt.plot(oob_accuracy.keys(),oob_accuracy.values(),'o',color = 'blue')\nplt.plot(test_accuracy.keys(),test_accuracy.values(), label = 'Bagging test accuracy')\n\nplt.plot(oob_accuracy2.keys(),oob_accuracy2.values(),label = 'RF OOB')\nplt.plot(oob_accuracy2.keys(),oob_accuracy2.values(),'o',color = 'green')\nplt.plot(test_accuacy2.keys(),test_accuacy2.values(), label = 'RF test accuracy')\n\nplt.xlabel('Number of trees')\nplt.ylabel('Classification accuracy')\nplt.legend(bbox_to_anchor=(0, -0.15, 1, 0), loc=2, ncol=2, mode=\"expand\", borderaxespad=0)\n\n\n\n\n\n\n\n\nIn the above example we observe that random forest does improve over bagged trees in terms of classification accuracy. Unlike the previous two examples, the optimal value of max_features for random forests is much smaller than the total number of available predictors, thereby making the random forest model much different than the bagged tree model.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html",
    "href": "Lec7_AdaBoost.html",
    "title": "10  Adaptive Boosting",
    "section": "",
    "text": "10.1 Hyperparameters\nThere are 3 important parameters to tune in AdaBoost:\nLet us visualize the accuracy of AdaBoost when we independently tweak each of the above parameters.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\nfrom sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\nrecall_score, precision_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold\nfrom sklearn.ensemble import BaggingRegressor,BaggingClassifier,AdaBoostRegressor,AdaBoostClassifier, \\\nRandomForestRegressor\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nimport itertools as it\nimport time as time\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_convergence\nimport warnings\nfrom IPython import display\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\nX = train[['mileage','mpg','year','engineSize']]\nXtest = test[['mileage','mpg','year','engineSize']]\ny = train['price']\nytest = test['price']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html#hyperparameters",
    "href": "Lec7_AdaBoost.html#hyperparameters",
    "title": "10  Adaptive Boosting",
    "section": "",
    "text": "Number of trees\nDepth of each tree\nLearning rate",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html#adaboost-for-regression",
    "href": "Lec7_AdaBoost.html#adaboost-for-regression",
    "title": "10  Adaptive Boosting",
    "section": "10.2 AdaBoost for regression",
    "text": "10.2 AdaBoost for regression\n\n10.2.1 Number of trees vs cross validation error\nAs the number of trees increases, the prediction bias will decrease, and the prediction variance will increase. Thus, there will be an optimal number of trees that minimizes the prediction error.\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [2, 5, 10, 50, 100, 500, 1000]\n    for n in n_trees:\n        models[str(n)] = AdaBoostRegressor(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15);\n\n&gt;2 9190.253 (757.408)\n&gt;5 8583.629 (341.406)\n&gt;10 8814.328 (248.891)\n&gt;50 10763.138 (465.677)\n&gt;100 11217.783 (602.642)\n&gt;500 11336.088 (763.288)\n&gt;1000 11390.043 (752.446)\n\n\n\n\n\n\n\n\n\n\n\n10.2.2 Depth of tree vs cross validation error\nAs the depth of each weak learner (decision tree) increases, the complexity of the weak learner will increase. As the complexity increases, the prediction bias will decrease, while the prediction variance will increase. Thus, there will be an optimal depth for each weak learner that minimizes the prediction error.\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define base model\n        base = DecisionTreeRegressor(max_depth=i)\n        # define ensemble model\n        models[str(i)] = AdaBoostRegressor(base_estimator=base,n_estimators=50)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15);\n\n&gt;1 12798.764 (490.538)\n&gt;2 11031.451 (465.520)\n&gt;3 10739.302 (636.517)\n&gt;4 9491.714 (466.764)\n&gt;5 7184.489 (324.484)\n&gt;6 6181.533 (411.394)\n&gt;7 5746.902 (407.451)\n&gt;8 5587.726 (473.619)\n&gt;9 5526.291 (541.512)\n&gt;10 5444.928 (554.170)\n&gt;11 5321.725 (455.899)\n&gt;12 5279.581 (492.785)\n&gt;13 5494.982 (393.469)\n&gt;14 5423.982 (488.564)\n&gt;15 5369.485 (441.799)\n&gt;16 5536.739 (409.166)\n&gt;17 5511.002 (517.384)\n&gt;18 5510.922 (478.285)\n&gt;19 5482.119 (465.565)\n&gt;20 5667.969 (468.964)\n\n\n\n\n\n\n\n\n\n\n\n10.2.3 Learning rate vs cross validation error\nThe optimal learning rate will depend on the number of trees, and vice-versa. If the learning rate is too low, it will take several trees to “learn” the response. If the learning rate is high, the response will be “learned” quickly (with fewer) trees. Learning too quickly will be prone to overfitting, while learning too slowly will be computationally expensive. Thus, there will be an optimal learning rate to minimize the prediction error.\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = AdaBoostRegressor(learning_rate=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15);\n\n&gt;0.1 8291.9 (452.4)\n&gt;0.2 8475.7 (465.3)\n&gt;0.3 8648.5 (458.8)\n&gt;0.4 8995.5 (438.6)\n&gt;0.5 9376.1 (388.2)\n&gt;0.6 9655.3 (551.8)\n&gt;0.7 9877.3 (319.8)\n&gt;0.8 10466.8 (528.3)\n&gt;0.9 10728.9 (386.8)\n&gt;1.0 10720.2 (410.6)\n&gt;1.1 11043.9 (432.5)\n&gt;1.2 10602.5 (570.0)\n&gt;1.3 11058.8 (362.1)\n&gt;1.4 11022.7 (616.0)\n&gt;1.5 11252.5 (839.3)\n&gt;1.6 11195.3 (604.5)\n&gt;1.7 11206.3 (636.1)\n&gt;1.8 11569.1 (674.6)\n&gt;1.9 11232.3 (605.6)\n&gt;2.0 11581.0 (824.8)\n\n\n\n\n\n\n\n\n\n\n\n10.2.4 Tuning AdaBoost for regression\nAs the optimal value of the parameters depend on each other, we need to optimize them simultaneously.\n\nmodel = AdaBoostRegressor(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['estimator'] = [DecisionTreeRegressor(max_depth=3), DecisionTreeRegressor(max_depth=5), \n                          DecisionTreeRegressor(max_depth=10),DecisionTreeRegressor(max_depth=15)]\n# define the evaluation procedure\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='neg_root_mean_squared_error')\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (-grid_result.best_score_, grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nBest: 5346.490675 using {'estimator': DecisionTreeRegressor(max_depth=10), 'learning_rate': 1.0, 'n_estimators': 50}\n\n\nNote that for tuning max_depth of the base estimator - decision tree, we specified 4 different base estimators with different depths. However, there is a more concise way to do that. We can specify the max_depth of the estimator by adding a double underscore “__” between the estimator and the hyperparameter that we wish to tune (max_depth here), and then specify its potential values in the grid itself as shown below. However, we’ll then need to add DecisionTreeRegressor() as the estimator within the AdaBoostRegressor() function.\n\nmodel = AdaBoostRegressor(random_state=1, estimator = DecisionTreeRegressor(random_state=1))\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['estimator__max_depth'] = [3, 5, 10, 15]\n# define the evaluation procedure\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='neg_root_mean_squared_error')\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (-grid_result.best_score_, grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nBest: 5346.490675 using {'estimator__max_depth': 10, 'learning_rate': 1.0, 'n_estimators': 50}\n\n\nThe BayesSearchCV() approach also coverges to a slightly different set of optimal hyperparameter values. However, it gives a similar cross-validated RMSE. This is possible. There may be multiple hyperparameter values that are different from each other, but similar in performance. It may be a good idea to ensemble models based on these two distinct set of hyperparameter values that give an equally accurate model.\n\nmodel = AdaBoostRegressor(estimator=DecisionTreeRegressor()) \ngrid = dict()\ngrid['n_estimators'] = Integer(2, 1000)\ngrid['learning_rate'] = Real(0.0001, 1.0)\ngrid['estimator__max_depth'] = Integer(1, 20)\n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 180, random_state = 10,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\ngcv.fit(X, y, callback = monitor)\n\n['estimator__max_depth', 'learning_rate', 'n_estimators'] = [13, 1.0, 570] 5325.017602505734\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=AdaBoostRegressor(estimator=DecisionTreeRegressor()),\n              n_iter=180, n_jobs=-1, random_state=10,\n              scoring='neg_root_mean_squared_error',\n              search_spaces={'estimator__max_depth': Integer(low=1, high=20, prior='uniform', transform='normalize'),\n                             'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=1000, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=AdaBoostRegressor(estimator=DecisionTreeRegressor()),\n              n_iter=180, n_jobs=-1, random_state=10,\n              scoring='neg_root_mean_squared_error',\n              search_spaces={'estimator__max_depth': Integer(low=1, high=20, prior='uniform', transform='normalize'),\n                             'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=1000, prior='uniform', transform='normalize')})estimator: AdaBoostRegressorAdaBoostRegressor(estimator=DecisionTreeRegressor())estimator: DecisionTreeRegressorDecisionTreeRegressor()DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nplot_objective(gcv.optimizer_results_[0],\n                   dimensions=['estimator__max_depth', 'learning_rate', 'n_estimators'], size = 3)\nplt.show();\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 3, figsize = (10, 3))\nplt.subplots_adjust(wspace=0.4)\nplot_histogram(gcv.optimizer_results_[0], 0, ax = ax[0])\nplot_histogram(gcv.optimizer_results_[0], 1, ax = ax[1])\nplot_histogram(gcv.optimizer_results_[0], 2, ax = ax[2])\nplt.show()\n\n\n\n\n\n\n\n\n\n#Model based on the optimal hyperparameters\nmodel = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,learning_rate=1.0,\n                         random_state=1).fit(X,y)\n\n\n#RMSE of the optimized model on test data\npred1=model.predict(Xtest)\nprint(\"AdaBoost model RMSE = \", np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nAdaBoost model RMSE =  5693.165811600585\n\n\n\n#Model based on the optimal hyperparameters\nmodel = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=13),n_estimators=570,learning_rate=1.0,\n                         random_state=1).fit(X,y)\n\n\n#RMSE of the optimized model on test data\npred2=model.predict(Xtest)\nprint(\"AdaBoost model RMSE = \", np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nAdaBoost model RMSE =  5434.852990644646\n\n\n\nmodel = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\npred3 = model.predict(Xtest)\nprint(\"Random Forest model RMSE = \", np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nRandom Forest model RMSE =  5642.45839697972\n\n\n\n#Ensemble modeling\npred = 0.33*pred1+0.33*pred2 + 0.34*pred3\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5402.832128650372\n\n\nCombined, the random forest model and the Adaboost models do better than each of the individual models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec7_AdaBoost.html#adaboost-for-classification",
    "href": "Lec7_AdaBoost.html#adaboost-for-classification",
    "title": "10  Adaptive Boosting",
    "section": "10.3 AdaBoost for classification",
    "text": "10.3 AdaBoost for classification\nBelow is the AdaBoost implementation on a classification problem. The takeaways are the same as that of the regression problem above.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n10.3.1 Number of trees vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [10, 50, 100, 500, 1000, 5000]\n    for n in n_trees:\n        models[str(n)] = AdaBoostClassifier(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;10 0.718 (0.060)\n&gt;50 0.751 (0.051)\n&gt;100 0.748 (0.053)\n&gt;500 0.690 (0.045)\n&gt;1000 0.694 (0.048)\n&gt;5000 0.691 (0.044)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Depth of each tree vs cross validation accuracy\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define base model\n        base = DecisionTreeClassifier(max_depth=i)\n        # define ensemble model\n        models[str(i)] = AdaBoostClassifier(estimator=base)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 0.751 (0.051)\n&gt;2 0.699 (0.063)\n&gt;3 0.696 (0.062)\n&gt;4 0.707 (0.055)\n&gt;5 0.713 (0.021)\n&gt;6 0.710 (0.061)\n&gt;7 0.733 (0.057)\n&gt;8 0.738 (0.044)\n&gt;9 0.727 (0.053)\n&gt;10 0.738 (0.065)\n&gt;11 0.748 (0.048)\n&gt;12 0.699 (0.044)\n&gt;13 0.738 (0.047)\n&gt;14 0.697 (0.041)\n&gt;15 0.697 (0.052)\n&gt;16 0.692 (0.052)\n&gt;17 0.702 (0.056)\n&gt;18 0.702 (0.045)\n&gt;19 0.700 (0.040)\n&gt;20 0.696 (0.042)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n10.3.3 Learning rate vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = AdaBoostClassifier(learning_rate=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.1 0.749 (0.052)\n&gt;0.2 0.743 (0.050)\n&gt;0.3 0.731 (0.057)\n&gt;0.4 0.736 (0.053)\n&gt;0.5 0.733 (0.062)\n&gt;0.6 0.738 (0.058)\n&gt;0.7 0.741 (0.056)\n&gt;0.8 0.741 (0.049)\n&gt;0.9 0.736 (0.048)\n&gt;1.0 0.741 (0.035)\n&gt;1.1 0.734 (0.037)\n&gt;1.2 0.736 (0.038)\n&gt;1.3 0.731 (0.057)\n&gt;1.4 0.728 (0.041)\n&gt;1.5 0.730 (0.036)\n&gt;1.6 0.720 (0.038)\n&gt;1.7 0.707 (0.045)\n&gt;1.8 0.730 (0.024)\n&gt;1.9 0.712 (0.033)\n&gt;2.0 0.454 (0.191)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n10.3.4 Tuning AdaBoost Classifier hyperparameters\n\nmodel = AdaBoostClassifier(random_state=1, estimator = DecisionTreeClassifier())\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['estimator__max_depth'] = [1, 2, 3, 4]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                          verbose = True)\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n#for mean, stdev, param in zip(means, stds, params):\n#    print(\"%f (%f) with: %r\" % (mean, stdev, param)\n\nFitting 5 folds for each of 100 candidates, totalling 500 fits\nBest: 0.763934 using {'estimator__max_depth': 3, 'learning_rate': 0.01, 'n_estimators': 200}\n\n\n\n\n10.3.5 Tuning the decision threshold probability\nWe’ll find a decision threshold probability that balances recall with precision.\n\n#Model based on the optimal parameters\nmodel = AdaBoostClassifier(random_state=1, estimator = DecisionTreeClassifier(max_depth=3),learning_rate=0.01,\n                          n_estimators=200).fit(X,y)\n\n# Note that we are using the cross-validated predicted probabilities, instead of directly using the \n# predicted probabilities on train data, as the model may be overfitting on the train data, and \n# may lead to misleading results\ncross_val_ypred = cross_val_predict(AdaBoostClassifier(random_state=1,base_estimator = DecisionTreeClassifier(max_depth=3),learning_rate=0.01,\n                          n_estimators=200), X, y, cv = 5, method = 'predict_proba')\n\np, r, thresholds = precision_recall_curve(y, cross_val_ypred[:,1])\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\n\n# Thresholds with precision and recall\nall_thresholds = np.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,2]&gt;0.8,:]\n# As the values in 'recall_more_than_80' are arranged in decreasing order of recall and increasing threshold,\n# the last value will provide the maximum threshold probability for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the maximum possible precision\nrecall_more_than_80[recall_more_than_80.shape[0]-1]\n\narray([0.33488762, 0.50920245, 0.80193237])\n\n\n\n#Optimal decision threshold probability\nthres = recall_more_than_80[recall_more_than_80.shape[0]-1][0]\nthres\n\n0.3348876199649718\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = thres\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  79.87012987012987\nROC-AUC:  0.8884188260179798\nPrecision:  0.6875\nRecall:  0.9016393442622951\n\n\n\n\n\n\n\n\n\nThe above model is similar to the one obtained with bagging / random forest. However, adaptive boosting may lead to better classification performance as compared to bagging / random forest.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adaptive Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html",
    "href": "Lec8_Gradient_Boosting.html",
    "title": "11  Gradient Boosting",
    "section": "",
    "text": "11.1 Hyperparameters\nThere are 5 important parameters to tune in Gradient boosting:\nLet us visualize the accuracy of Gradient boosting when we independently tweak each of the above parameters.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\nfrom sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\nrecall_score, precision_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nimport itertools as it\nimport time as time\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_convergence\nimport warnings\nfrom IPython import display\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\nX = train[['mileage','mpg','year','engineSize']]\nXtest = test[['mileage','mpg','year','engineSize']]\ny = train['price']\nytest = test['price']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html#hyperparameters",
    "href": "Lec8_Gradient_Boosting.html#hyperparameters",
    "title": "11  Gradient Boosting",
    "section": "",
    "text": "Number of trees\nDepth of each tree\nLearning rate\nSubsample fraction\nMaximum features",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html#gradient-boosting-for-regression",
    "href": "Lec8_Gradient_Boosting.html#gradient-boosting-for-regression",
    "title": "11  Gradient Boosting",
    "section": "11.2 Gradient boosting for regression",
    "text": "11.2 Gradient boosting for regression\n\n11.2.1 Number of trees vs cross validation error\nAs per the documentation, Gradient boosting is fairly robust (as compared to AdaBoost) to over-fitting (why?) so a large number usually results in better performance. Note that the number of trees still need to be tuned for optimal performance.\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [2, 5, 10, 50, 100, 500, 1000, 2000, 5000]\n    for n in n_trees:\n        models[str(n)] = GradientBoostingRegressor(n_estimators=n,random_state=1,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;2 14927.566 (179.475)\n&gt;5 12743.148 (189.408)\n&gt;10 10704.199 (226.234)\n&gt;50 6869.066 (278.885)\n&gt;100 6354.656 (270.097)\n&gt;500 5515.622 (424.516)\n&gt;1000 5515.251 (427.767)\n&gt;2000 5600.041 (389.687)\n&gt;5000 5854.168 (362.223)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n11.2.2 Depth of tree vs cross validation error\nAs the depth of each weak learner (decision tree) increases, the complexity of the weak learner will increase. As the complexity increases, the prediction bias will decrease, while the prediction variance will increase. Thus, there will be an optimal depth of each weak learner that minimizes the prediction error.\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define ensemble model\n        models[str(i)] = GradientBoostingRegressor(n_estimators=50,random_state=1,max_depth=i,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 9693.731 (810.090)\n&gt;2 7682.569 (489.841)\n&gt;3 6844.225 (536.792)\n&gt;4 5972.203 (538.693)\n&gt;5 5664.563 (497.882)\n&gt;6 5329.130 (404.330)\n&gt;7 5210.934 (461.038)\n&gt;8 5197.204 (494.957)\n&gt;9 5227.975 (478.789)\n&gt;10 5299.782 (446.509)\n&gt;11 5433.822 (451.673)\n&gt;12 5617.946 (509.797)\n&gt;13 5876.424 (542.981)\n&gt;14 6030.507 (560.447)\n&gt;15 6125.914 (643.852)\n&gt;16 6294.784 (672.646)\n&gt;17 6342.327 (677.050)\n&gt;18 6372.418 (791.068)\n&gt;19 6456.471 (741.693)\n&gt;20 6503.622 (759.193)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n11.2.3 Learning rate vs cross validation error\nThe optimal learning rate will depend on the number of trees, and vice-versa. If the learning rate is too low, it will take several trees to “learn” the response. If the learning rate is high, the response will be “learned” quickly (with fewer) trees. Learning too quickly will be prone to overfitting, while learning too slowly will be computationally expensive. Thus, there will be an optimal learning rate to minimize the prediction error.\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = GradientBoostingRegressor(learning_rate=i,random_state=1,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.1 6329.8 (450.7)\n&gt;0.2 5942.9 (454.8)\n&gt;0.3 5618.4 (490.8)\n&gt;0.4 5665.9 (577.3)\n&gt;0.5 5783.5 (561.7)\n&gt;0.6 5773.8 (500.3)\n&gt;0.7 5875.5 (565.7)\n&gt;0.8 5878.5 (540.5)\n&gt;0.9 6214.4 (594.3)\n&gt;1.0 5986.1 (601.5)\n&gt;1.1 6216.5 (395.3)\n&gt;1.2 6667.5 (657.2)\n&gt;1.3 6717.4 (594.4)\n&gt;1.4 7048.4 (531.7)\n&gt;1.5 7265.0 (742.0)\n&gt;1.6 7404.4 (868.2)\n&gt;1.7 7425.8 (606.3)\n&gt;1.8 8283.0 (1345.3)\n&gt;1.9 8872.2 (1137.9)\n&gt;2.0 17713.3 (865.3)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n11.2.4 Subsampling vs cross validation error\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for s in np.arange(0.25, 1.1, 0.25):\n        key = '%.2f' % s\n        models[key] = GradientBoostingRegressor(random_state=1,subsample=s,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.2f (%.2f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Subsample',fontsize=15)\n\n&gt;0.25 6219.59 (569.97)\n&gt;0.50 6178.28 (501.87)\n&gt;0.75 6141.96 (432.66)\n&gt;1.00 6329.79 (450.72)\n\n\nText(0.5, 0, 'Subsample')\n\n\n\n\n\n\n\n\n\n\n\n11.2.5 Maximum features vs cross-validation error\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for s in np.arange(0.25, 1.1, 0.25):\n        key = '%.2f' % s\n        models[key] = GradientBoostingRegressor(random_state=1,max_features=s,loss='huber')\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.2f (%.2f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Maximum features',fontsize=15)\n\n&gt;0.25 6654.27 (567.72)\n&gt;0.50 6373.92 (538.53)\n&gt;0.75 6325.55 (470.41)\n&gt;1.00 6329.79 (450.72)\n\n\nText(0.5, 0, 'Maximum features')\n\n\n\n\n\n\n\n\n\n\n\n11.2.6 Tuning Gradient boosting for regression\nAs the optimal value of the parameters depend on each other, we need to optimize them simultaneously.\n\nstart_time = time.time()\nmodel = GradientBoostingRegressor(random_state=1,loss='huber')\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['max_depth'] = [3,5,8,10,12,15]\n\n# define the evaluation procedure\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='neg_mean_squared_error',\n                          verbose = True)\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (np.sqrt(-grid_result.best_score_), grid_result.best_params_))\n# summarize all scores that were evaluated\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n#for mean, stdev, param in zip(means, stds, params):\n#    print(\"%f (%f) with: %r\" % (mean, stdev, param)\nprint(\"Time taken = \",(time.time()-start_time)/60,\" minutes\")\n\nBest: 5190.765919 using {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 100}\nTime taken =  46.925597019990285  minutes\n\n\nNote that the code takes 46 minutes to run. In case of a lot of hyperparameters, RandomizedSearchCV may be preferred to trade-off between optimality of the solution and computational cost.\n\nmodel = GradientBoostingRegressor(random_state=1, loss='huber') \ngrid = dict()\ngrid['n_estimators'] = Integer(2, 1000)\ngrid['learning_rate'] = Real(0.0001, 1.0)\ngrid['max_leaf_nodes'] = Integer(4, 5000)\ngrid['subsample'] = Real(0.1, 1)\ngrid['max_features'] = Real(0.1, 1)\n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 100, random_state = 1,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nparas = list(gcv.search_spaces.keys())\nparas.sort()\nstart_time = time.time()\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    print(\"Time so far = \", np.round((time.time()-start_time)/60), \"minutes\")\n    sns.lineplot(cv_values)\n    plt.show()\ngcv.fit(X, y, callback = monitor)\n\n['learning_rate', 'max_features', 'max_leaf_nodes', 'n_estimators', 'subsample'] = [0.23102084158310995, 0.315075948850183, 5000, 817, 1.0] 5360.92726695485\nTime so far =  21.0 minutes\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=GradientBoostingRegressor(loss='huber', random_state=1),\n              n_iter=100, n_jobs=-1, random_state=1,\n              scoring='neg_root_mean_squared_error',\n              search_spaces={'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'max_features': Real(low=0.1, high=1, prior='uniform', transform='normalize'),\n                             'max_leaf_nodes': Integer(low=4, high=5000, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=1000, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.1, high=1, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=GradientBoostingRegressor(loss='huber', random_state=1),\n              n_iter=100, n_jobs=-1, random_state=1,\n              scoring='neg_root_mean_squared_error',\n              search_spaces={'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'max_features': Real(low=0.1, high=1, prior='uniform', transform='normalize'),\n                             'max_leaf_nodes': Integer(low=4, high=5000, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=1000, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.1, high=1, prior='uniform', transform='normalize')})estimator: GradientBoostingRegressorGradientBoostingRegressor(loss='huber', random_state=1)GradientBoostingRegressorGradientBoostingRegressor(loss='huber', random_state=1)\n\n\n\n#Model based on the optimal parameters\nmodel = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n                         random_state=1,loss='huber').fit(X,y)\n\n\n#RMSE of the optimized model on test data\nprint(\"Gradient boost RMSE = \",np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nGradient boost RMSE =  5405.787029062213\n\n\n\n#Model based on the optimal parameters\nmodel_bayes = GradientBoostingRegressor(max_leaf_nodes=5000,n_estimators=817,learning_rate=0.23, max_features=0.31,\n                         random_state=1,subsample=1.0,loss='huber').fit(X,y)\n\n\n#RMSE of the optimized model on test data\nprint(\"Gradient boost RMSE = \",np.sqrt(mean_squared_error(model_bayes.predict(Xtest),ytest)))\n\nGradient boost RMSE =  5734.200307094321\n\n\n\n#Let us combine the Gradient boost model with other models\nmodel2 = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,learning_rate=1.0,\n                         random_state=1).fit(X,y)\nprint(\"AdaBoost RMSE = \",np.sqrt(mean_squared_error(model2.predict(Xtest),ytest)))\nmodel3 = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\nprint(\"Random Forest RMSE = \",np.sqrt(mean_squared_error(model3.predict(Xtest),ytest)))\n\nAdaBoost RMSE =  5693.165811600585\nRandom Forest RMSE =  5642.45839697972\n\n\n\n#Ensemble model\npred1=model.predict(Xtest)#Gradient boost\npred2=model2.predict(Xtest)#Adaboost\npred3=model3.predict(Xtest)#Random forest\npred = 0.34*pred1+0.33*pred2+0.33*pred3 #Higher weight to the better model\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5364.478227748279\n\n\n\n\n11.2.7 Ensemble modeling (for regression models)\n\n#Ensemble model\npred1=model.predict(Xtest)#Gradient boost\npred2=model2.predict(Xtest)#Adaboost\npred3=model3.predict(Xtest)#Random forest\npred = 0.6*pred1+0.2*pred2+0.2*pred3 #Higher weight to the better model\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5323.119083375402\n\n\nCombined, the random forest model, gradient boost and the Adaboost model do better than each of the individual models.\nNote that ideally we should do K-fold cross validation to figure out the optimal weights. We’ll learn about ensembling techniques later in the course.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html#gradient-boosting-for-classification",
    "href": "Lec8_Gradient_Boosting.html#gradient-boosting-for-classification",
    "title": "11  Gradient Boosting",
    "section": "11.3 Gradient boosting for classification",
    "text": "11.3 Gradient boosting for classification\nBelow is the Gradient boost implementation on a classification problem. The takeaways are the same as that of the regression problem above.\n\ntrain = pd.read_csv('./Datasets/diabetes_train.csv')\ntest = pd.read_csv('./Datasets/diabetes_test.csv')\n\n\nX = train.drop(columns = 'Outcome')\nXtest = test.drop(columns = 'Outcome')\ny = train['Outcome']\nytest = test['Outcome']\n\n\n11.3.1 Number of trees vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [10, 50, 100, 500, 1000, 5000]\n    for n in n_trees:\n        models[str(n)] = GradientBoostingClassifier(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;10 0.738 (0.031)\n&gt;50 0.748 (0.054)\n&gt;100 0.722 (0.075)\n&gt;500 0.707 (0.066)\n&gt;1000 0.712 (0.075)\n&gt;5000 0.697 (0.061)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n11.3.2 Depth of each tree vs cross validation accuracy\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define ensemble model\n        models[str(i)] = GradientBoostingClassifier(random_state=1,max_depth=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 0.746 (0.040)\n&gt;2 0.744 (0.046)\n&gt;3 0.722 (0.075)\n&gt;4 0.743 (0.049)\n&gt;5 0.738 (0.046)\n&gt;6 0.741 (0.047)\n&gt;7 0.735 (0.057)\n&gt;8 0.736 (0.051)\n&gt;9 0.728 (0.055)\n&gt;10 0.710 (0.050)\n&gt;11 0.697 (0.061)\n&gt;12 0.681 (0.056)\n&gt;13 0.709 (0.047)\n&gt;14 0.702 (0.048)\n&gt;15 0.705 (0.048)\n&gt;16 0.700 (0.042)\n&gt;17 0.699 (0.048)\n&gt;18 0.697 (0.050)\n&gt;19 0.696 (0.042)\n&gt;20 0.697 (0.048)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n11.3.3 Learning rate vs cross validation accuracy\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in np.arange(0.1, 2.1, 0.1):\n        key = '%.1f' % i\n        models[key] = GradientBoostingClassifier(learning_rate=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Accuracy',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.1 0.747 (0.044)\n&gt;0.2 0.736 (0.028)\n&gt;0.3 0.726 (0.039)\n&gt;0.4 0.730 (0.034)\n&gt;0.5 0.726 (0.041)\n&gt;0.6 0.722 (0.043)\n&gt;0.7 0.717 (0.050)\n&gt;0.8 0.713 (0.033)\n&gt;0.9 0.694 (0.045)\n&gt;1.0 0.695 (0.032)\n&gt;1.1 0.718 (0.034)\n&gt;1.2 0.692 (0.045)\n&gt;1.3 0.708 (0.042)\n&gt;1.4 0.704 (0.050)\n&gt;1.5 0.702 (0.028)\n&gt;1.6 0.700 (0.050)\n&gt;1.7 0.694 (0.044)\n&gt;1.8 0.650 (0.075)\n&gt;1.9 0.551 (0.163)\n&gt;2.0 0.484 (0.123)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n11.3.4 Tuning Gradient boosting Classifier\n\nstart_time = time.time()\nmodel = GradientBoostingClassifier(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['max_depth'] = [1,2,3,4,5]\ngrid['subsample'] = [0.5,1.0]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, verbose = True, scoring = 'recall')\n# execute the grid search\ngrid_result = grid_search.fit(X, y)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nprint(\"Time taken = \", time.time() - start_time, \"seconds\")\n\nFitting 5 folds for each of 250 candidates, totalling 1250 fits\nBest: 0.701045 using {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}\nTime taken =  32.46394085884094\n\n\n\n#Model based on the optimal parameters\nmodel = GradientBoostingClassifier(random_state=1,max_depth=3,learning_rate=0.1,subsample=0.5,\n                          n_estimators=200).fit(X,y)\n\n# Note that we are using the cross-validated predicted probabilities, instead of directly using the \n# predicted probabilities on train data, as the model may be overfitting on the train data, and \n# may lead to misleading results\ncross_val_ypred = cross_val_predict(GradientBoostingClassifier(random_state=1,max_depth=3,\n                                                               learning_rate=0.1,subsample=0.5,\n                          n_estimators=200), X, y, cv = 5, method = 'predict_proba')\n\np, r, thresholds = precision_recall_curve(y, cross_val_ypred[:,1])\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.plot(thresholds, precisions[:-1], \"o\", color = 'blue')\n    plt.plot(thresholds, recalls[:-1], \"o\", color = 'green')\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\n\n# Thresholds with precision and recall\nall_thresholds = np.concatenate([thresholds.reshape(-1,1), p[:-1].reshape(-1,1), r[:-1].reshape(-1,1)], axis = 1)\nrecall_more_than_80 = all_thresholds[all_thresholds[:,2]&gt;0.8,:]\n# As the values in 'recall_more_than_80' are arranged in decreasing order of recall and increasing threshold,\n# the last value will provide the maximum threshold probability for the recall to be more than 80%\n# We wish to find the maximum threshold probability to obtain the maximum possible precision\nrecall_more_than_80[recall_more_than_80.shape[0]-1]\n\narray([0.18497144, 0.53205128, 0.80193237])\n\n\n\n#Optimal decision threshold probability\nthres = recall_more_than_80[recall_more_than_80.shape[0]-1][0]\nthres\n\n0.18497143500912738\n\n\n\n# Performance metrics computation for the optimum decision threshold probability\ndesired_threshold = thres\n\ny_pred_prob = model.predict_proba(Xtest)[:,1] \n\n# Classifying observations in the positive class (y = 1) if the predicted probability is greater\n# than the desired decision threshold probability\ny_pred = y_pred_prob &gt; desired_threshold\ny_pred = y_pred.astype(int)\n\n#Computing the accuracy\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), \n                  columns=['Predicted 0', 'Predicted 1'], index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  77.92207792207793\nROC-AUC:  0.8704389212057112\nPrecision:  0.6626506024096386\nRecall:  0.9016393442622951\n\n\n\n\n\n\n\n\n\nThe model seems to be similar to the Adaboost model. However, gradient boosting algorithms with robust loss functions can perform better than Adaboost in the presence of outliers (in terms of response) in the data.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec8_Gradient_Boosting.html#faster-algorithms-and-tuning-tips",
    "href": "Lec8_Gradient_Boosting.html#faster-algorithms-and-tuning-tips",
    "title": "11  Gradient Boosting",
    "section": "11.4 Faster algorithms and tuning tips",
    "text": "11.4 Faster algorithms and tuning tips\nCheck out HistGradientBoostingRegressor() and HistGradientBoostingClassifier() for a faster gradient boosting algorithm for big datasets (more than 10,000 observations).\nCheck out tips for faster hyperparameter tuning, such as tuning max_leaf_nodes instead of max_depth here.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html",
    "href": "Lec9_XGBoost.html",
    "title": "12  XGBoost",
    "section": "",
    "text": "12.1 Hyperparameters\nThe following are some of the important hyperparameters to tune in XGBoost:\nHowever, there are other hyperparameters that can be tuned as well. Check out the list of all hyperparameters in the XGBoost documentation.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\nfrom sklearn.metrics import mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\nrecall_score, precision_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.ensemble import VotingRegressor, VotingClassifier, StackingRegressor, StackingClassifier, GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.linear_model import LinearRegression,LogisticRegression, LassoCV, RidgeCV, ElasticNetCV\nfrom sklearn.neighbors import KNeighborsRegressor\nimport itertools as it\nimport time as time\nimport xgboost as xgb\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_convergence\nimport warnings\nfrom IPython import display\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\nX = train[['mileage','mpg','year','engineSize']]\nXtest = test[['mileage','mpg','year','engineSize']]\ny = train['price']\nytest = test['price']",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html#hyperparameters",
    "href": "Lec9_XGBoost.html#hyperparameters",
    "title": "12  XGBoost",
    "section": "",
    "text": "Number of trees (n_estimators)\nDepth of each tree (max_depth)\nLearning rate (learning_rate)\nSampling observations / predictors (subsample for observations, colsample_bytree for predictors)\nRegularization parameters (reg_lambda & gamma)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html#xgboost-for-regression",
    "href": "Lec9_XGBoost.html#xgboost-for-regression",
    "title": "12  XGBoost",
    "section": "12.2 XGBoost for regression",
    "text": "12.2 XGBoost for regression\n\n12.2.1 Number of trees vs cross validation error\nAs the number of trees increase, the prediction bias will decrease. Like gradient boosting is relatively robust (as compared to AdaBoost) to over-fitting (why?) so a large number usually results in better performance. Note that the number of trees still need to be tuned for optimal performance.\n\ndef get_models():\n    models = dict()\n    # define number of trees to consider\n    n_trees = [5, 10, 50, 100, 500, 1000, 2000, 5000]\n    for n in n_trees:\n        models[str(n)] = xgb.XGBRegressor(n_estimators=n,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Number of trees',fontsize=15)\n\n&gt;5 7961.485 (192.906)\n&gt;10 5837.134 (217.986)\n&gt;50 5424.788 (263.890)\n&gt;100 5465.396 (237.938)\n&gt;500 5608.350 (235.903)\n&gt;1000 5635.159 (236.664)\n&gt;2000 5642.669 (236.192)\n&gt;5000 5643.411 (236.074)\n\n\nText(0.5, 0, 'Number of trees')\n\n\n\n\n\n\n\n\n\n\n\n12.2.2 Depth of tree vs cross validation error\nAs the depth of each weak learner (decision tree) increases, the complexity of the weak learner will increase. As the complexity increases, the prediction bias will decrease, while the prediction variance will increase. Thus, there will be an optimal depth of each weak learner that minimizes the prediction error.\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # explore depths from 1 to 10\n    for i in range(1,21):\n        # define ensemble model\n        models[str(i)] = xgb.XGBRegressor(random_state=1,max_depth=i)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Depth of each tree',fontsize=15)\n\n&gt;1 7541.827 (545.951)\n&gt;2 6129.425 (393.357)\n&gt;3 5647.783 (454.318)\n&gt;4 5438.481 (453.726)\n&gt;5 5358.074 (379.431)\n&gt;6 5281.675 (383.848)\n&gt;7 5495.163 (459.356)\n&gt;8 5399.145 (380.437)\n&gt;9 5469.563 (384.004)\n&gt;10 5461.549 (416.630)\n&gt;11 5443.210 (432.863)\n&gt;12 5546.447 (412.097)\n&gt;13 5532.414 (369.131)\n&gt;14 5556.761 (362.746)\n&gt;15 5540.366 (452.612)\n&gt;16 5586.004 (451.199)\n&gt;17 5563.137 (464.344)\n&gt;18 5594.919 (480.221)\n&gt;19 5641.226 (451.713)\n&gt;20 5616.462 (417.405)\n\n\nText(0.5, 0, 'Depth of each tree')\n\n\n\n\n\n\n\n\n\n\n\n12.2.3 Learning rate vs cross validation error\nThe optimal learning rate will depend on the number of trees, and vice-versa. If the learning rate is too low, it will take several trees to “learn” the response. If the learning rate is high, the response will be “learned” quickly (with fewer) trees. Learning too quickly will be prone to overfitting, while learning too slowly will be computationally expensive. Thus, there will be an optimal learning rate to minimize the prediction error.\n\ndef get_models():\n    models = dict()\n    # explore learning rates from 0.1 to 2 in 0.1 increments\n    for i in [0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.8,1.0]:\n        key = '%.4f' % i\n        models[key] = xgb.XGBRegressor(learning_rate=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('Learning rate',fontsize=15)\n\n&gt;0.0100 12223.8 (636.7)\n&gt;0.0500 5298.5 (383.5)\n&gt;0.1000 5236.3 (397.5)\n&gt;0.2000 5221.5 (347.5)\n&gt;0.3000 5281.7 (383.8)\n&gt;0.4000 5434.1 (364.6)\n&gt;0.5000 5537.0 (471.9)\n&gt;0.6000 5767.4 (478.5)\n&gt;0.8000 6132.7 (472.5)\n&gt;1.0000 6593.6 (408.9)\n\n\nText(0.5, 0, 'Learning rate')\n\n\n\n\n\n\n\n\n\n\n\n12.2.4 Regularization (reg_lambda) vs cross validation error\nThe parameter reg_lambda penalizes the L2 norm of the leaf scores. For example, in case of classification, it will penalize the summation of the square of log odds of the predicted probability. This penalization will tend to reduce the log odds, thereby reducing the tendency to overfit. “Reducing the log odds” in layman terms will mean not being overly sure about the prediction.\nWithout regularization, the algorithm will be closer to the gradient boosting algorithm. Regularization may provide some additional boost to prediction accuracy by reducing over-fitting. In the example below, regularization with reg_lambda=1 turns out to be better than no regularization (reg_lambda=0)*. Of course, too much regularization may increase bias so much such that it leads to a decrease in prediction accuracy.\n\ndef get_models():\n    models = dict()\n    # explore 'reg_lambda' from 0.1 to 2 in 0.1 increments\n    for i in [0,0.5,1.0,1.5,2,10,100]:\n        key = '%.4f' % i\n        models[key] = xgb.XGBRegressor(reg_lambda=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('reg_lambda',fontsize=15)\n\n&gt;0.0000 5359.2 (317.0)\n&gt;0.5000 5382.7 (363.1)\n&gt;1.0000 5281.7 (383.8)\n&gt;1.5000 5348.0 (383.9)\n&gt;2.0000 5336.4 (426.6)\n&gt;10.0000 5410.9 (521.9)\n&gt;100.0000 5801.1 (563.7)\n\n\nText(0.5, 0, 'reg_lambda')\n\n\n\n\n\n\n\n\n\n\n\n12.2.5 Regularization (gamma) vs cross validation error\nThe parameter gamma penalizes the tree based on the number of leaves. This is similar to the parameter alpha of cost complexity pruning. As gamma increases, more leaves will be pruned. Note that the previous parameter reg_lambda penalizes the leaf score, but does not prune the tree.\nWithout regularization, the algorithm will be closer to the gradient boosting algorithm. Regularization may provide some additional boost to prediction accuracy by reducing over-fitting. However, in the example below, no regularization (in terms of gamma=0) turns out to be better than a non-zero regularization. (reg_lambda=0).\n\ndef get_models():\n    models = dict()\n    # explore gamma from 0.1 to 2 in 0.1 increments\n    for i in [0,10,1e2,1e3,1e4,1e5,1e6,1e7,1e8,1e9]:\n        key = '%.4f' % i\n        models[key] = xgb.XGBRegressor(gamma=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = KFold(n_splits=10, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1))\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    # store the results\n    results.append(scores)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.1f (%.1f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.ylabel('Cross validation error',fontsize=15)\nplt.xlabel('gamma',fontsize=15)\n#ax.set_xticklabels(x.astype(int))\nplt.xticks(ticks=plt.xticks()[0].astype(int), labels=np.round([0,10,1e2,1e3,1e4,1e5,1e6,1e7,1e8,1e9]),\n          rotation = 30);\n\n&gt;0.0000 5281.7 (383.8)\n&gt;10.0000 5281.7 (383.8)\n&gt;100.0000 5281.7 (383.8)\n&gt;1000.0000 5291.8 (381.8)\n&gt;10000.0000 5295.7 (370.2)\n&gt;100000.0000 5293.0 (402.5)\n&gt;1000000.0000 5322.2 (368.9)\n&gt;10000000.0000 5273.7 (409.8)\n&gt;100000000.0000 5345.4 (373.9)\n&gt;1000000000.0000 5932.3 (397.6)\n\n\n\n\n\n\n\n\n\n\n\n12.2.6 Tuning XGboost regressor\nAlong with max_depth, learning_rate, and n_estimators, here we tune reg_lambda - the regularization parameter for penalizing the tree predictions.\n\n#K-fold cross validation to find optimal parameters for XGBoost\nstart_time = time.time()\nparam_grid = {'max_depth': [4,6,8],\n              'learning_rate': [0.01, 0.05, 0.1],\n               'reg_lambda':[0, 1, 10],\n                'n_estimators':[100, 500, 1000],\n                'gamma': [0, 10, 100],\n                'subsample': [0.5, 0.75, 1.0],\n                'colsample_bytree': [0.5, 0.75, 1.0]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = RandomizedSearchCV(estimator=xgb.XGBRegressor(random_state=1),                                                       \n                             param_distributions = param_grid, n_iter = 200,\n                             verbose = 1,\n                             n_jobs=-1,\n                             cv = cv)\noptimal_params.fit(X,y)\nprint(\"Optimal parameter values =\", optimal_params.best_params_)\nprint(\"Optimal cross validation R-squared = \",optimal_params.best_score_)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\nOptimal parameter values = {'subsample': 0.75, 'reg_lambda': 1, 'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.01, 'gamma': 100, 'colsample_bytree': 1.0}\nOptimal cross validation R-squared =  0.9002580404500382\nTime taken =  4  minutes\n\n\n\n#RMSE based on the optimal parameter values\nnp.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))\n\n5497.553788113875\n\n\nLet us use Bayes search to tune the model.\n\nmodel = xgb.XGBRegressor(random_state = 1) \n\ngrid = {'max_leaves': Integer(4, 5000),\n              'learning_rate': Real(0.0001, 1.0),\n               'reg_lambda':Real(0, 1e4),\n                'n_estimators':Integer(2, 2000),\n                'gamma': Real(0, 1e11),\n                'subsample': Real(0.1,1.0),\n                'colsample_bytree': Real(0.1, 1.0)}\n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 100, random_state = 1,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\ngcv.fit(X, y, callback = monitor)\n\n['colsample_bytree', 'gamma', 'learning_rate', 'max_leaves', 'n_estimators', 'reg_lambda', 'subsample'] = [0.8455872906441244, 0.0, 0.9137620583590551, 802, 1023, 1394.0140479620452, 0.7987263539365876] 5340.272253124142\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=XGBRegressor(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False, eval_metric=None,\n                                     feature_types=None, gamma=None,\n                                     gpu_id=None, grow_policy=None,\n                                     importance_type=None,\n                                     inte...\n                             'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'max_leaves': Integer(low=4, high=5000, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=2000, prior='uniform', transform='normalize'),\n                             'reg_lambda': Real(low=0, high=10000.0, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.1, high=1.0, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=XGBRegressor(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False, eval_metric=None,\n                                     feature_types=None, gamma=None,\n                                     gpu_id=None, grow_policy=None,\n                                     importance_type=None,\n                                     inte...\n                             'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'max_leaves': Integer(low=4, high=5000, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=2000, prior='uniform', transform='normalize'),\n                             'reg_lambda': Real(low=0, high=10000.0, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.1, high=1.0, prior='uniform', transform='normalize')})estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=1, ...)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=1, ...)\n\n\n\nmodel1 = xgb.XGBRegressor(random_state = 1, colsample_bytree = 0.85, gamma = 0, learning_rate = 0.91, \n                          max_leaves = 802, n_estimators = 1023, reg_lambda = 1394, subsample = 0.8).fit(X, y)\n\n\nnp.sqrt(mean_squared_error(model1.predict(Xtest),ytest))\n\n5466.076861800755\n\n\nWe got a different set of optimal hyperparameters with Bayes search. Thus, ensembling the model based on the two sets of hyperparameters is likely to improve the accuracy over the individual models.\n\nmodel2 = xgb.XGBRegressor(random_state = 1, colsample_bytree = 1.0, gamma = 100, learning_rate = 0.01, \n                          max_depth = 8, n_estimators = 1000, reg_lambda = 1, subsample = 0.75).fit(X, y)\n\n\nnp.sqrt(mean_squared_error(0.5*model1.predict(Xtest)+0.5*model2.predict(Xtest),ytest))\n\n5393.379834226845\n\n\n\n\n12.2.7 Early stopping with XGBoost\nIf we have a test dataset (or we can further split the train data into a smaller train and test data), we can use it with the early_stopping_rounds argument of XGBoost, where it will stop growing trees once the model accuracy fails to increase for a certain number of consecutive iterations, given as early_stopping_rounds.\n\nX_train_sub, X_test_sub, y_train_sub, y_test_sub = \\\ntrain_test_split(X, y, test_size = 0.2, random_state = 45)\n\n\nmodel = xgb.XGBRegressor(random_state = 1, max_depth = 8, learning_rate = 0.01,\n                        n_estimators = 20000,reg_lambda = 1, gamma = 100, subsample = 0.75, colsample_bytree = 1.0)\nmodel.fit(X_train_sub, y_train_sub, eval_set = ([(X_test_sub, y_test_sub)]), early_stopping_rounds = 250)\n\nThe results of the code are truncated to save space. A snapshot of the beginning and end of the results is below. The algorithm keeps adding trees to the model until the RMSE ceases to decrease for 250 consecutive iterations.\n\n\n\n\n\n\nprint(\"XGBoost RMSE = \",np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nXGBoost RMSE =  5508.787454011525\n\n\nLet us further reduce the learning rate to 0.001 and see if the accuracy increases further on the test data. We’ll use the early_stopping_rounds argument to stop growing trees once the accuracy fails to increase for 250 consecutive iterations.\n\nmodel = xgb.XGBRegressor(random_state = 1, max_depth = 8, learning_rate = 0.001,\n                        n_estimators = 20000,reg_lambda = 1, gamma = 100, subsample = 0.75, colsample_bytree = 1.0)\nmodel.fit(X_train_sub, y_train_sub, eval_set = ([(X_test_sub, y_test_sub)]), early_stopping_rounds = 250)\n\n\n\n\n\n\n\nprint(\"XGBoost RMSE = \",np.sqrt(mean_squared_error(model.predict(Xtest),ytest)))\n\nXGBoost RMSE =  5483.518711988693\n\n\nNote that the accuracy on this test data has further increased with a lower learning rate.\nLet us combine the XGBoost model with other tuned models from earlier chapters.\n\n#Tuned AdaBoost model from Section 7.2.4\nmodel_ada = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,learning_rate=1.0,\n                         random_state=1).fit(X,y)\nprint(\"AdaBoost RMSE = \", np.sqrt(mean_squared_error(model_ada.predict(Xtest),ytest)))\n\n#Tuned Random forest model from Section 6.1.2\nmodel_rf = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\nprint(\"Random Forest RMSE = \",np.sqrt(mean_squared_error(model_rf.predict(Xtest),ytest)))\n\n#Tuned gradient boosting model from Section 8.2.5\nmodel_gb = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n                         random_state=1,loss='huber').fit(X,y)\nprint(\"Gradient boost RMSE = \",np.sqrt(mean_squared_error(model_gb.predict(Xtest),ytest)))\n\nAdaBoost RMSE =  5693.165811600585\nRandom Forest RMSE =  5642.45839697972\nGradient boost RMSE =  5405.787029062213\n\n\n\n#Ensemble model\npred_xgb = model.predict(Xtest)    #XGBoost\npred_ada = model_ada.predict(Xtest)#AdaBoost\npred_rf = model_rf.predict(Xtest)  #Random Forest\npred_gb = model_gb.predict(Xtest)  #Gradient boost\npred = 0.25*pred_xgb + 0.25*pred_ada + 0.25*pred_rf + 0.25*pred_gb #Option 1 - All models are equally weighted\n#pred = 0.15*pred1+0.15*pred2+0.15*pred3+0.55*pred4 #Option 2 - Higher weight to the better model\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(pred,ytest)))\n\nEnsemble model RMSE =  5352.145010078119\n\n\nCombined, the random forest model, gradient boost, XGBoost and the Adaboost model do better than each of the individual models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Lec9_XGBoost.html#xgboost-for-classification",
    "href": "Lec9_XGBoost.html#xgboost-for-classification",
    "title": "12  XGBoost",
    "section": "12.3 XGBoost for classification",
    "text": "12.3 XGBoost for classification\n\ndata = pd.read_csv('./Datasets/Heart.csv')\ndata.dropna(inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\n\nAge\nSex\nChestPain\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nThal\nAHD\n\n\n\n\n0\n63\n1\ntypical\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\nfixed\nNo\n\n\n1\n67\n1\nasymptomatic\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\nnormal\nYes\n\n\n2\n67\n1\nasymptomatic\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\nreversable\nYes\n\n\n3\n37\n1\nnonanginal\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\nnormal\nNo\n\n\n4\n41\n0\nnontypical\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\nnormal\nNo\n\n\n\n\n\n\n\n\n\n#Response variable\ny = pd.get_dummies(data['AHD'])['Yes']\n\n#Creating a dataframe for predictors with dummy varibles replacing the categorical variables\nX = data.drop(columns = ['AHD','ChestPain','Thal'])\nX = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\nX.head()\n\n\n\n\n\n\n\n\n\nAge\nSex\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nasymptomatic\nnonanginal\nnontypical\ntypical\nfixed\nnormal\nreversable\n\n\n\n\n0\n63\n1\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\n0\n0\n0\n1\n1\n0\n0\n\n\n1\n67\n1\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\n1\n0\n0\n0\n0\n1\n0\n\n\n2\n67\n1\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\n1\n0\n0\n0\n0\n0\n1\n\n\n3\n37\n1\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\n0\n1\n0\n0\n0\n1\n0\n\n\n4\n41\n0\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\n#Creating train and test datasets\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)\n\nXGBoost has an additional parameter for classification: scale_pos_weight\nGradients are used as the basis for fitting subsequent trees added to boost or correct errors made by the existing state of the ensemble of decision trees.\nThe scale_pos_weight value is used to scale the gradient for the positive class.\nThis has the effect of scaling errors made by the model during training on the positive class and encourages the model to over-correct them. In turn, this can help the model achieve better performance when making predictions on the positive class. Pushed too far, it may result in the model overfitting the positive class at the cost of worse performance on the negative class or both classes.\nAs such, the scale_pos_weight hyperparameter can be used to train a class-weighted or cost-sensitive version of XGBoost for imbalanced classification.\nA sensible default value to set for the scale_pos_weight hyperparameter is the inverse of the class distribution. For example, for a dataset with a 1 to 100 ratio for examples in the minority to majority classes, the scale_pos_weight can be set to 100. This will give classification errors made by the model on the minority class (positive class) 100 times more impact, and in turn, 100 times more correction than errors made on the majority class.\nReference\n\nstart_time = time.time()\nparam_grid = {'n_estimators':[25,100,500],\n                'max_depth': [6,7,8],\n              'learning_rate': [0.01,0.1,0.2],\n               'gamma': [0.1,0.25,0.5],\n               'reg_lambda':[0,0.01,0.001],\n                'scale_pos_weight':[1.25,1.5,1.75]#Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances).\n             }\n\ncv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = GridSearchCV(estimator=xgb.XGBClassifier(objective = 'binary:logistic',random_state=1,\n                                                         use_label_encoder=False),\n                             param_grid = param_grid,\n                             scoring = 'accuracy',\n                             verbose = 1,\n                             n_jobs=-1,\n                             cv = cv)\noptimal_params.fit(Xtrain,ytrain)\nprint(optimal_params.best_params_,optimal_params.best_score_)\nprint(\"Time taken = \", (time.time()-start_time)/60, \" minutes\")\n\nFitting 5 folds for each of 729 candidates, totalling 3645 fits\n[22:00:02] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n{'gamma': 0.25, 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 25, 'reg_lambda': 0.01, 'scale_pos_weight': 1.5} 0.872183908045977\n\n\n\ncv_results=pd.DataFrame(optimal_params.cv_results_)\ncv_results.sort_values(by = 'mean_test_score',ascending=False)[0:5]\n\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_gamma\nparam_learning_rate\nparam_max_depth\nparam_n_estimators\nparam_reg_lambda\nparam_scale_pos_weight\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n409\n0.111135\n0.017064\n0.005629\n0.000737\n0.25\n0.2\n6\n25\n0.01\n1.5\n{'gamma': 0.25, 'learning_rate': 0.2, 'max_dep...\n0.866667\n0.766667\n0.9\n0.931034\n0.896552\n0.872184\n0.05656\n1\n\n\n226\n0.215781\n0.007873\n0.005534\n0.001615\n0.1\n0.2\n8\n100\n0\n1.5\n{'gamma': 0.1, 'learning_rate': 0.2, 'max_dept...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n290\n1.391273\n0.107808\n0.007723\n0.006286\n0.25\n0.01\n7\n500\n0\n1.75\n{'gamma': 0.25, 'learning_rate': 0.01, 'max_de...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n266\n1.247463\n0.053597\n0.006830\n0.002728\n0.25\n0.01\n6\n500\n0.01\n1.75\n{'gamma': 0.25, 'learning_rate': 0.01, 'max_de...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n269\n1.394361\n0.087307\n0.005530\n0.001718\n0.25\n0.01\n6\n500\n0.001\n1.75\n{'gamma': 0.25, 'learning_rate': 0.01, 'max_de...\n0.833333\n0.766667\n0.9\n0.931034\n0.896552\n0.865517\n0.05874\n2\n\n\n\n\n\n\n\n\n\n#Function to compute confusion matrix and prediction accuracy on test/train data\ndef confusion_matrix_data(data,actual_values,model,cutoff=0.5):\n#Predict the values using the Logit model\n    pred_values = model.predict_proba(data)[:,1]\n# Specify the bins\n    bins=np.array([0,cutoff,1])\n#Confusion matrix\n    cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n    cm_df = pd.DataFrame(cm)\n    cm_df.columns = ['Predicted 0','Predicted 1']\n    cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n# Calculate the accuracy\n    accuracy = 100*(cm[0,0]+cm[1,1])/cm.sum()\n    fnr = 100*(cm[1,0])/(cm[1,0]+cm[1,1])\n    precision = 100*(cm[1,1])/(cm[0,1]+cm[1,1])\n    fpr = 100*(cm[0,1])/(cm[0,0]+cm[0,1])\n    tpr = 100*(cm[1,1])/(cm[1,0]+cm[1,1])\n    print(\"Accuracy = \", accuracy)\n    print(\"Precision = \", precision)\n    print(\"FNR = \", fnr)\n    print(\"FPR = \", fpr)\n    print(\"TPR or Recall = \", tpr)\n    print(\"Confusion matrix = \\n\", cm_df)\n    return (\" \")\n\n\nmodel4 = xgb.XGBClassifier(objective = 'binary:logistic',random_state=1,gamma=0.25,learning_rate = 0.01,max_depth=6,\n                              n_estimators = 500,reg_lambda = 0.01,scale_pos_weight=1.75)\nmodel4.fit(Xtrain,ytrain)\nmodel4.score(Xtest,ytest)\n\n0.7718120805369127\n\n\n\n#Computing the accuracy\ny_pred = model4.predict(Xtest)\nprint(\"Accuracy: \",accuracy_score(y_pred, ytest)*100)  \n\n#Computing the ROC-AUC\ny_pred_prob = model4.predict_proba(Xtest)[:,1]\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred_prob)\nprint(\"ROC-AUC: \",auc(fpr, tpr))# AUC of ROC\n\n#Computing the precision and recall\nprint(\"Precision: \", precision_score(ytest, y_pred))\nprint(\"Recall: \", recall_score(ytest, y_pred))\n\n#Confusion matrix\ncm = pd.DataFrame(confusion_matrix(ytest, y_pred), columns=['Predicted 0', 'Predicted 1'], \n            index = ['Actual 0', 'Actual 1'])\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g');\n\nAccuracy:  77.18120805369128\nROC-AUC:  0.8815070986530761\nPrecision:  0.726027397260274\nRecall:  0.7910447761194029\n\n\n\n\n\n\n\n\n\nIf we increase the value of scale_pos_weight, the model will focus on classifying positives more correctly. This will increase the recall (true positive rate) since the focus is on identifying all positives. However, this will lead to identifying positives aggressively, and observations ‘similar’ to observations of the positive class will also be predicted as positive resulting in an increase in false positives and a decrease in precision. See the trend below as we increase the value of scale_pos_weight.\n\n12.3.1 Precision & recall vs scale_pos_weight\n\ndef get_models():\n    models = dict()\n    # explore 'scale_pos_weight' from 0.1 to 2 in 0.1 increments\n    for i in [0,1,10,1e2,1e3,1e4,1e5,1e6,1e7,1e8,1e9]:\n        key = '%.0f' % i\n        models[key] = xgb.XGBClassifier(objective = 'binary:logistic',scale_pos_weight=i,random_state=1)\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n    # evaluate the model and collect the results\n    scores_recall = cross_val_score(model, X, y, scoring='recall', cv=cv, n_jobs=-1)\n    scores_precision = cross_val_score(model, X, y, scoring='precision', cv=cv, n_jobs=-1)\n    return list([scores_recall,scores_precision])\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults_recall, results_precision, names = list(), list(), list()\nfor name, model in models.items():\n    # evaluate the model\n    scores = evaluate_model(model, X, y)\n    scores_recall = scores[0]\n    scores_precision = scores[1]\n    # store the results\n    results_recall.append(scores_recall)\n    results_precision.append(scores_precision)\n    names.append(name)\n    # summarize the performance along the way\n    print('&gt;%s %.2f (%.2f)' % (name, np.mean(scores_recall), np.std(scores_recall)))\n# plot model performance for comparison\nplt.figure(figsize=(7, 7))\nsns.set(font_scale = 1.5)\npdata = pd.DataFrame(results_precision)\npdata.columns = list(['p1','p2','p3','p4','p5'])\npdata['metric'] = 'precision'\nrdata = pd.DataFrame(results_recall)\nrdata.columns = list(['p1','p2','p3','p4','p5'])\nrdata['metric'] = 'recall'\npr_data = pd.concat([pdata,rdata])\npr_data.reset_index(drop=False,inplace= True)\n#sns.boxplot(x=\"day\", y=\"total_bill\", hue=\"time\",pr_data=tips, linewidth=2.5)\npr_data_melt=pr_data.melt(id_vars = ['index','metric'])\npr_data_melt['index']=pr_data_melt['index']-1\npr_data_melt['index'] = pr_data_melt['index'].astype('str')\npr_data_melt.replace(to_replace='-1',value =  '-inf',inplace=True)\nsns.boxplot(x='index', y=\"value\", hue=\"metric\", data=pr_data_melt, linewidth=2.5)\nplt.xlabel('$log_{10}$(scale_pos_weight)',fontsize=15)\nplt.ylabel('Precision / Recall ',fontsize=15)\nplt.legend(loc=\"lower right\", frameon=True, fontsize=15)\n\n&gt;0 0.00 (0.00)\n&gt;1 0.77 (0.13)\n&gt;10 0.81 (0.09)\n&gt;100 0.85 (0.11)\n&gt;1000 0.85 (0.10)\n&gt;10000 0.90 (0.06)\n&gt;100000 0.90 (0.08)\n&gt;1000000 0.90 (0.06)\n&gt;10000000 0.91 (0.10)\n&gt;100000000 0.96 (0.03)\n&gt;1000000000 1.00 (0.00)",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "Lec11_More boosting models.html",
    "href": "Lec11_More boosting models.html",
    "title": "13  LightGBM and CatBoost",
    "section": "",
    "text": "13.1 LightGBM\nLightGBM is a gradient boosting decision tree algorithm developed by Microsoft in 2017. LightGBM outperforms XGBoost in terms of compuational speed, and provides comparable accuracy in general. The following two key features in LightGBM that make it faster than XGBoost:\n1. Gradient-based One-Side Sampling (GOSS): Recall, in gradient boosting, we fit trees on the gradient of the loss function (refer the gradient boosting algorithm in section 10.10.2 of the book, Elements of Statistical Learning):\n\\[r_m = -\\bigg[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}  \\bigg]_{f = f_{m-1}}. \\]\nObservations that correspond to relatively larger gradients contribute more to minimizing the loss function as compared to observations with smaller gradients. The algorithm down-samples the observations with small gradients, while selecting all the observations with large gradients. As observations with large gradients contribute the most to the reduction in loss function when considering a split, the accuracy of loss reduction estimate is maintained even with a reduced sample size. This leads to similar performance in terms of prediction accuracy while reducing computation speed due to reduction in sample size to fit trees.\n2. Exclusive feature bundling (EFB): This is useful when there are a lot of predictors, but the predictor space is sparse, i.e., most of the values are zero for several predictors, and the predictors rarely take non-zero values simultaneously. This can typically happen in case of a lot of dummy variables in the data. In such a case, the predictors are bundled to create a single predictor.\nIn the example below you can see that feature1 and feature2 are mutually exclusive. In order to achieve non overlapping buckets we add bundle size of feature1 to feature2. This makes sure that non zero data points of bundled features (feature1 and feature2) reside in different buckets. In feature_bundle buckets 1 to 4 contains non zero instances of feature1 and buckets 5,6 contain non zero instances of feature2 (Reference).\nRead the LightGBM paper for more details.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "Lec11_More boosting models.html#lightgbm",
    "href": "Lec11_More boosting models.html#lightgbm",
    "title": "13  LightGBM and CatBoost",
    "section": "",
    "text": "feature1\nfeature2\nfeature_bundle\n\n\n\n\n0\n2\n6\n\n\n0\n1\n5\n\n\n0\n2\n6\n\n\n1\n0\n1\n\n\n2\n0\n2\n\n\n3\n0\n3\n\n\n4\n0\n4\n\n\n\n\n\n13.1.1 LightGBM for regression\nLet us tune a lightGBM model for regression for our problem of predicting car price. We’ll use the function LGBMRegressor. For classification problems, LGBMClassifier can be used. Note that we are using the GOSS algorithm to downsample observations with smaller gradients.\n\n#K-fold cross validation to find optimal parameters for LightGBM regressor\nstart_time = time.time()\nparam_grid = {'num_leaves': [20, 31, 40],\n              'learning_rate': [0.01, 0.05, 0.1],\n               'reg_lambda':[0, 10, 100],\n                'n_estimators':[100, 500, 1000],\n                'reg_alpha': [0, 10, 100],\n                'subsample': [0.5, 0.75, 1.0],\n                'colsample_bytree': [0.5, 0.75, 1.0]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = RandomizedSearchCV(estimator=LGBMRegressor(boosting_type = 'goss'),                                                       \n                             param_distributions = param_grid, n_iter = 200,\n                             verbose = 1, scoring='neg_root_mean_squared_error',\n                             n_jobs=-1,random_state=1,\n                             cv = cv)\noptimal_params.fit(X,y)\nprint(\"Optimal parameter values =\", optimal_params.best_params_)\nprint(\"Optimal cross validation RMSE = \",optimal_params.best_score_)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\nOptimal parameter values = {'subsample': 1.0, 'reg_lambda': 10, 'reg_alpha': 0, 'num_leaves': 20, 'n_estimators': 1000, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\nOptimal cross validation R-squared =  -5670.309021679375\nTime taken =  1  minutes\n\n\n\n#RMSE based on the optimal parameter values of a LighGBM Regressor model\nnp.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))\n\n5614.374498193448\n\n\nNote that downsampling of small-gradient observations leads to faster execution time, but potentially by compromising some accuracy. We can expect to improve the accuracy by increasing the top_rate or the other_rate hyperparameters, but at an increased computational cost. In the cross-validation below, we have increased the top_rate to 0.5 from the default value of 0.2.\n\n#K-fold cross validation to find optimal parameters for LightGBM regressor\nstart_time = time.time()\nparam_grid = {'num_leaves': [20, 31, 40],\n              'learning_rate': [0.01, 0.05, 0.1],\n               'reg_lambda':[0, 10, 100],\n                'n_estimators':[100, 500, 1000],\n                'reg_alpha': [0, 10, 100],\n                'subsample': [0.5, 0.75, 1.0],\n                'colsample_bytree': [0.5, 0.75, 1.0]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = RandomizedSearchCV(estimator=LGBMRegressor(boosting_type = 'goss', top_rate = 0.5),                                                       \n                             param_distributions = param_grid, n_iter = 200,\n                             verbose = 1, scoring='neg_root_mean_squared_error',\n                             n_jobs=-1,random_state=1,\n                             cv = cv)\noptimal_params.fit(X,y)\nprint(\"Optimal parameter values =\", optimal_params.best_params_)\nprint(\"Optimal cross validation RMSE = \",optimal_params.best_score_)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\nOptimal parameter values = {'subsample': 0.5, 'reg_lambda': 0, 'reg_alpha': 100, 'num_leaves': 31, 'n_estimators': 500, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\nOptimal cross validation R-squared =  -5436.062435616846\nTime taken =  1  minutes\n\n\n\n#RMSE based on the optimal parameter values of a LighGBM Regressor model\nnp.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))\n\n5355.964600884197\n\n\nNote that the cross-validated RMSE has reduced. However, this is at an increased computational expense. In the simulations below, we compare the time taken to train models with increasing values of the top_rate hyperparameter.\n\ntime_list = []\nfor i in range(50):\n    start_time = time.time()\n    model = LGBMRegressor(boosting_type = 'goss', top_rate = 0.2, n_jobs=-1).fit(X, y)\n    time_list.append(time.time()-start_time)\n\n\ntime_list2 = []\nfor i in range(50):\n    start_time = time.time()\n    model = LGBMRegressor(boosting_type = 'goss', top_rate = 0.5, n_jobs=-1).fit(X, y)\n    time_list2.append(time.time()-start_time)\n\n\ntime_list3 = []\nfor i in range(50):\n    start_time = time.time()\n    model = LGBMRegressor(boosting_type = 'goss', top_rate = 0.8, n_jobs=-1).fit(X, y)\n    time_list3.append(time.time()-start_time)\n\n\nax = sns.boxplot([time_list, time_list2, time_list3]);\nax.set_xticklabels([0.2, 0.5, 0.75]);\nplt.ylabel('Time');\nplt.xlabel('top_rate');\nplt.xticks(rotation = 45);\n\n\n\n\n\n\n\n\n\n\n13.1.2 LightGBM vs XGBoost\nLightGBM model took 2 minutes for a random search with 1000 fits as compared to 7 minutes for an XGBoost model with 1000 fits on the same data (as shown below). In terms of prediction accuracy, we observe that the accuracy of LightGBM on test (unseen) data is comparable to that of XGBoost.\n\n#K-fold cross validation to find optimal parameters for XGBoost\nstart_time = time.time()\nparam_grid = {'max_depth': [4,6,8],\n              'learning_rate': [0.01, 0.05, 0.1],\n               'reg_lambda':[0, 1, 10],\n                'n_estimators':[100, 500, 1000],\n                'gamma': [0, 10, 100],\n                'subsample': [0.5, 0.75, 1.0],\n                'colsample_bytree': [0.5, 0.75, 1.0]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = RandomizedSearchCV(estimator=xgb.XGBRegressor(),                                                       \n                             param_distributions = param_grid, n_iter = 200,\n                             verbose = 1, scoring = 'neg_root_mean_squared_error',\n                             n_jobs=-1,random_state = 1,\n                             cv = cv)\noptimal_params.fit(X,y)\nprint(\"Optimal parameter values =\", optimal_params.best_params_)\nprint(\"Optimal cross validation R-squared = \",optimal_params.best_score_)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\nOptimal parameter values = {'subsample': 0.75, 'reg_lambda': 1, 'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.01, 'gamma': 100, 'colsample_bytree': 1.0}\nOptimal cross validation R-squared =  -5178.8689594137295\nTime taken =  7  minutes\n\n\n\n#RMSE based on the optimal parameter values\nnp.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))\n\n5420.661056398766",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "Lec11_More boosting models.html#catboost",
    "href": "Lec11_More boosting models.html#catboost",
    "title": "13  LightGBM and CatBoost",
    "section": "13.2 CatBoost",
    "text": "13.2 CatBoost\nCatBoost is a gradient boosting algorithm developed by Yandex (Russian Google) in 2017. Like LightGBM, CatBoost is also faster than XGBoost in training. However, unlike LightGBM, the authors have claimed that it outperforms both LightGBM and XGBoost in terms of prediction accuracy as well.\nThe key feature of CatBoost that address the issue with the gradient boosting procedure is the idea of ordered boosting. Classic boosting algorithms are prone to overfitting on small/noisy datasets due to a problem known as prediction shift. Recall, in gradient boosting, we fit trees on the gradient of the loss function (refer the gradient boosting algorithm in section 10.10.2 of the book, Elements of Statistical Learning):\n\\[r_m = -\\bigg[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}  \\bigg]_{f = f_{m-1}}. \\]\nWhen calculating the gradient estimate of an observation, these algorithms use the same observations that the model was built with, thus having no chances of experiencing unseen data. CatBoost, on the other hand, uses the concept of ordered boosting, a permutation-driven approach to train model on a subset of data while calculating residuals on another subset, thus preventing “target leakage” and overfitting. The residuals of an observation are computed based on a model developed on the previous observations, where the observations are randomly shuffled at each iteration, i.e., for each tree.\nThus, the gradient of the loss function is based on test (unseen) data, instead of the data on which the model has been trained, which improves the generalizability of the model, and avoids overfitting on train data.\nThe authors have also shown that CatBoost performs better than XGBoost and LightGBM without tuning, i.e., with default hyperparameter settings.\nRead the CatBoost paper for more details.\nHere is a good blog listing the key features of CatBoost.\n\n13.2.1 CatBoost for regression\nWe’ll use the function CatBoostRegressor for regression. For classification problems CatBoostClassifier can be used.\nLet us check the performance of CatBoostRegressor() without tuning, i.e., with default hyperparameter settings.\n\nmodel_cat = CatBoostRegressor(verbose=0).fit(X, y)\n\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\nnp.mean(-cross_val_score(CatBoostRegressor(verbose=0), X, y, cv = cv, n_jobs = -1, \n                scoring='neg_root_mean_squared_error'))\n\n5035.972129299527\n\n\n\nnp.sqrt(mean_squared_error(model_cat.predict(Xtest),ytest))\n\n5288.82153844634\n\n\nEven with default hyperparameter settings, CatBoost has outperformed both XGBoost and LightGBM in terms of cross-validated RMSE, and RMSE on test data for our example of predicting car prices.\n\n\n13.2.2 CatBoost vs XGBoost\nLet us see the performance of XGBoost with default hyperparameter settings.\n\nmodel_xgb = xgb.XGBRFRegressor().fit(X, y)\nnp.mean(-cross_val_score(xgb.XGBRFRegressor(), X, y, cv = cv, n_jobs = -1, \n                scoring='neg_root_mean_squared_error'))\n\n6273.043859096154\n\n\n\nnp.sqrt(mean_squared_error(model_xgb.predict(Xtest),ytest))\n\n6821.745153860935\n\n\nXGBoost performance deteriorates showing that hyperparameter tuning is more important in XGBoost.\nLet us see the performance of LightGBM with default hyperparameter settings.\n\nmodel_lgbm = LGBMRegressor().fit(X, y)\nnp.mean(-cross_val_score(LGBMRegressor(), X, y, cv = cv, n_jobs = -1, \n                scoring='neg_root_mean_squared_error'))\n\n5562.149251902867\n\n\n\nnp.sqrt(mean_squared_error(model_lgbm.predict(Xtest),ytest))\n\n5494.0777923513515\n\n\nLightGBM’s default hyperparameter settings also seem to be more robust as compared to those of XGBoost.\n\n\n13.2.3 Tuning CatBoostRegressor\nThe CatBoost hyperparameters can be tuned just like the XGBoost hyperparameters. However, there is some difference in the hyperparameters of both the packages. For example, reg_alpha (the L1 penalization on weights of leaves) and colsample_bytree (subsample ratio of columns when constructing each tree) hyperparameters are not there in CatBoost.\n\n#K-fold cross validation to find optimal parameters for CatBoost regressor\nstart_time = time.time()\nparam_grid = {'max_depth': [4,6,8, 10],\n              'num_leaves': [20, 31, 40, 60],\n              'learning_rate': [0.01, 0.05, 0.1],\n               'reg_lambda':[0, 10, 100],\n                'n_estimators':[500, 1000, 1500],\n                'subsample': [0.5, 0.75, 1.0],\n             'colsample_bylevel': [0.25, 0.5, 0.75, 1.0]}\n\ncv = KFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = RandomizedSearchCV(estimator=CatBoostRegressor(random_state=1, verbose=False, \n                            grow_policy='Lossguide'),                                                       \n                             param_distributions = param_grid, n_iter = 200,\n                             verbose = 1,random_state = 1, scoring='neg_root_mean_squared_error',\n                             n_jobs=-1,\n                             cv = cv)\noptimal_params.fit(X,y)\nprint(\"Optimal parameter values =\", optimal_params.best_params_)\nprint(\"Optimal cross validation RMSE = \",optimal_params.best_score_)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\nOptimal parameter values = {'subsample': 0.5, 'reg_lambda': 0, 'num_leaves': 40, 'n_estimators': 500, 'max_depth': 10, 'learning_rate': 0.05, 'colsample_bylevel': 0.75}\nOptimal cross validation RMSE =  -4993.129407810791\nTime taken =  23  minutes\n\n\n\n#RMSE based on the optimal parameter values\nnp.sqrt(mean_squared_error(optimal_params.best_estimator_.predict(Xtest),ytest))\n\n5249.434282204398\n\n\nIt takes 2 minutes to tune CatBoost, which is higher than LightGBM and lesser than XGBoost. CatBoost falls in between LightGBM and XGBoost in terms of speed. However, it is likely to be more accurate than XGBoost and LighGBM, and likely to require lesser tuning as compared to XGBoost.\n\nmodel = CatBoostRegressor(grow_policy='Lossguide') \n\ngrid = {'num_leaves': Integer(4, 64),\n              'learning_rate': Real(0.0001, 1.0),\n               'reg_lambda':Real(0, 1e4),\n                'n_estimators':Integer(2, 2000),\n                'subsample': Real(0.1,1.0),\n                'colsample_bylevel': Real(0.1, 1.0)}\n\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 1)\ngcv = BayesSearchCV(model, search_spaces = grid, cv = kfold, n_iter = 200, random_state = 1,\n                         scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nparas = list(gcv.search_spaces.keys())\nparas.sort()\n\ndef monitor(optim_result):\n    cv_values = pd.Series(optim_result['func_vals']).cummin()\n    display.clear_output(wait = True)\n    min_ind = pd.Series(optim_result['func_vals']).argmin()\n    print(paras, \"=\", optim_result['x_iters'][min_ind], pd.Series(optim_result['func_vals']).min())\n    sns.lineplot(cv_values)\n    plt.show()\ngcv.fit(X, y, callback = monitor)\n\n['colsample_bylevel', 'learning_rate', 'n_estimators', 'num_leaves', 'reg_lambda', 'subsample'] = [0.3745508446405472, 0.1000958551500621, 2000, 11, 0.0, 0.3877212027881348] 5132.537839676808\n0:  learn: 15586.6547227    total: 7.88ms   remaining: 15.8s\n1:  learn: 14594.4802869    total: 16.4ms   remaining: 16.4s\n2:  learn: 14594.4802869    total: 17.2ms   remaining: 11.5s\n3:  learn: 13743.4503923    total: 20.1ms   remaining: 10s\n4:  learn: 13266.2414822    total: 24.9ms   remaining: 9.94s\n5:  learn: 12498.3369959    total: 28.1ms   remaining: 9.33s\n6:  learn: 12129.2561319    total: 30.3ms   remaining: 8.63s\n7:  learn: 11505.6411010    total: 32.3ms   remaining: 8.03s\n8:  learn: 11505.6411010    total: 32.7ms   remaining: 7.23s\n9:  learn: 11021.2139091    total: 34.6ms   remaining: 6.89s\n10: learn: 10442.9678139    total: 37ms remaining: 6.69s\n11: learn: 9947.6741148 total: 39.3ms   remaining: 6.51s\n12: learn: 9619.4595819 total: 41ms remaining: 6.27s\n13: learn: 9259.8855899 total: 42.9ms   remaining: 6.08s\n14: learn: 9259.8855899 total: 43.4ms   remaining: 5.74s\n15: learn: 8939.7710703 total: 45.3ms   remaining: 5.62s\n16: learn: 8711.0852634 total: 47.3ms   remaining: 5.52s\n17: learn: 8604.6071027 total: 49.1ms   remaining: 5.41s\n18: learn: 8438.7728051 total: 50.8ms   remaining: 5.3s\n19: learn: 8274.7829809 total: 53.2ms   remaining: 5.26s\n20: learn: 8042.3020027 total: 55.9ms   remaining: 5.27s\n21: learn: 7876.9560283 total: 57.5ms   remaining: 5.17s\n22: learn: 7710.6772232 total: 59.5ms   remaining: 5.12s\n23: learn: 7571.7455782 total: 61.1ms   remaining: 5.03s\n24: learn: 7440.8874033 total: 63ms remaining: 4.98s\n25: learn: 7316.5070355 total: 65ms remaining: 4.93s\n26: learn: 7190.4998886 total: 66.8ms   remaining: 4.88s\n27: learn: 7126.4859430 total: 68.2ms   remaining: 4.8s\n28: learn: 7126.4859430 total: 68.6ms   remaining: 4.66s\n29: learn: 7016.0377180 total: 70.1ms   remaining: 4.6s\n30: learn: 7000.1845553 total: 70.7ms   remaining: 4.49s\n31: learn: 6884.2153374 total: 72.3ms   remaining: 4.45s\n32: learn: 6884.2153374 total: 72.7ms   remaining: 4.33s\n33: learn: 6859.6936709 total: 74.3ms   remaining: 4.29s\n34: learn: 6859.6936709 total: 74.7ms   remaining: 4.19s\n35: learn: 6784.1411674 total: 76.4ms   remaining: 4.17s\n36: learn: 6784.1411674 total: 76.9ms   remaining: 4.08s\n37: learn: 6759.2775251 total: 77.8ms   remaining: 4.01s\n38: learn: 6699.5901664 total: 79.7ms   remaining: 4.01s\n39: learn: 6618.8189493 total: 81.6ms   remaining: 4s\n40: learn: 6588.5585061 total: 83.2ms   remaining: 3.97s\n41: learn: 6576.3802378 total: 84.1ms   remaining: 3.92s\n42: learn: 6576.3802378 total: 84.5ms   remaining: 3.85s\n43: learn: 6576.3802378 total: 84.9ms   remaining: 3.77s\n44: learn: 6576.3802378 total: 85.3ms   remaining: 3.71s\n45: learn: 6554.9758286 total: 86.5ms   remaining: 3.67s\n46: learn: 6525.5802913 total: 88.1ms   remaining: 3.66s\n47: learn: 6466.1035176 total: 89.1ms   remaining: 3.62s\n48: learn: 6436.3855462 total: 90.1ms   remaining: 3.59s\n49: learn: 6425.6798093 total: 91.3ms   remaining: 3.56s\n50: learn: 6415.6723682 total: 92.3ms   remaining: 3.53s\n51: learn: 6370.9376356 total: 95ms remaining: 3.56s\n52: learn: 6362.2269483 total: 96.1ms   remaining: 3.53s\n53: learn: 6270.8381417 total: 97.5ms   remaining: 3.51s\n54: learn: 6224.2584318 total: 98.8ms   remaining: 3.49s\n55: learn: 6196.9126269 total: 100ms    remaining: 3.48s\n56: learn: 6159.9795842 total: 102ms    remaining: 3.48s\n57: learn: 6095.2640049 total: 104ms    remaining: 3.48s\n58: learn: 6068.5568780 total: 105ms    remaining: 3.47s\n59: learn: 6045.8539265 total: 107ms    remaining: 3.45s\n60: learn: 6014.2089991 total: 108ms    remaining: 3.44s\n61: learn: 6014.2089991 total: 109ms    remaining: 3.4s\n62: learn: 5991.4276619 total: 110ms    remaining: 3.4s\n63: learn: 5960.8755319 total: 112ms    remaining: 3.4s\n64: learn: 5939.4519328 total: 114ms    remaining: 3.39s\n65: learn: 5937.5977651 total: 114ms    remaining: 3.35s\n66: learn: 5914.5991789 total: 116ms    remaining: 3.34s\n67: learn: 5893.7012597 total: 117ms    remaining: 3.34s\n68: learn: 5881.6048683 total: 119ms    remaining: 3.33s\n69: learn: 5857.2384791 total: 121ms    remaining: 3.33s\n70: learn: 5856.3744823 total: 121ms    remaining: 3.3s\n71: learn: 5844.6720134 total: 123ms    remaining: 3.29s\n72: learn: 5808.4057279 total: 125ms    remaining: 3.29s\n73: learn: 5796.1301234 total: 126ms    remaining: 3.29s\n74: learn: 5774.0161859 total: 128ms    remaining: 3.29s\n75: learn: 5771.0218306 total: 129ms    remaining: 3.27s\n76: learn: 5771.0218306 total: 129ms    remaining: 3.23s\n77: learn: 5754.9516948 total: 131ms    remaining: 3.23s\n78: learn: 5753.1841839 total: 132ms    remaining: 3.22s\n79: learn: 5750.6624545 total: 134ms    remaining: 3.21s\n80: learn: 5730.0398718 total: 136ms    remaining: 3.21s\n81: learn: 5730.0398718 total: 136ms    remaining: 3.18s\n82: learn: 5724.0710370 total: 138ms    remaining: 3.18s\n83: learn: 5709.8421847 total: 139ms    remaining: 3.18s\n84: learn: 5695.5072261 total: 141ms    remaining: 3.18s\n85: learn: 5681.9956673 total: 143ms    remaining: 3.18s\n86: learn: 5660.6016053 total: 145ms    remaining: 3.18s\n87: learn: 5643.4061588 total: 147ms    remaining: 3.18s\n88: learn: 5635.8928486 total: 148ms    remaining: 3.18s\n89: learn: 5614.4729570 total: 149ms    remaining: 3.17s\n90: learn: 5607.0906414 total: 151ms    remaining: 3.17s\n91: learn: 5606.5748917 total: 151ms    remaining: 3.14s\n92: learn: 5593.2044205 total: 153ms    remaining: 3.14s\n93: learn: 5581.7260489 total: 154ms    remaining: 3.13s\n94: learn: 5568.1299183 total: 156ms    remaining: 3.12s\n95: learn: 5568.1299183 total: 156ms    remaining: 3.1s\n96: learn: 5568.1299183 total: 157ms    remaining: 3.07s\n97: learn: 5547.7453457 total: 158ms    remaining: 3.07s\n98: learn: 5536.8538716 total: 160ms    remaining: 3.07s\n99: learn: 5533.3512293 total: 162ms    remaining: 3.07s\n100:    learn: 5531.6988001 total: 163ms    remaining: 3.06s\n101:    learn: 5521.6827472 total: 164ms    remaining: 3.06s\n102:    learn: 5512.1275625 total: 166ms    remaining: 3.05s\n103:    learn: 5501.2710735 total: 167ms    remaining: 3.04s\n104:    learn: 5483.1332945 total: 169ms    remaining: 3.04s\n105:    learn: 5468.6932573 total: 170ms    remaining: 3.04s\n106:    learn: 5468.6932573 total: 171ms    remaining: 3.02s\n107:    learn: 5466.3196454 total: 172ms    remaining: 3.01s\n108:    learn: 5466.3196454 total: 173ms    remaining: 2.99s\n109:    learn: 5450.8555079 total: 174ms    remaining: 2.99s\n110:    learn: 5450.5222911 total: 175ms    remaining: 2.98s\n111:    learn: 5444.9363205 total: 176ms    remaining: 2.98s\n112:    learn: 5434.7455852 total: 178ms    remaining: 2.98s\n113:    learn: 5434.7455852 total: 179ms    remaining: 2.96s\n114:    learn: 5433.5957428 total: 180ms    remaining: 2.94s\n115:    learn: 5415.6597932 total: 181ms    remaining: 2.94s\n116:    learn: 5415.6597932 total: 181ms    remaining: 2.92s\n117:    learn: 5401.1008140 total: 183ms    remaining: 2.92s\n118:    learn: 5391.8658503 total: 185ms    remaining: 2.92s\n119:    learn: 5380.7927393 total: 186ms    remaining: 2.92s\n120:    learn: 5365.7813769 total: 188ms    remaining: 2.92s\n121:    learn: 5365.7813769 total: 188ms    remaining: 2.9s\n122:    learn: 5354.1319730 total: 191ms    remaining: 2.91s\n123:    learn: 5354.1319730 total: 191ms    remaining: 2.89s\n124:    learn: 5342.7838789 total: 193ms    remaining: 2.89s\n125:    learn: 5342.7838789 total: 193ms    remaining: 2.87s\n126:    learn: 5327.8897475 total: 195ms    remaining: 2.88s\n127:    learn: 5310.2941871 total: 197ms    remaining: 2.88s\n128:    learn: 5306.9281433 total: 198ms    remaining: 2.87s\n129:    learn: 5294.2149974 total: 200ms    remaining: 2.87s\n130:    learn: 5291.2189448 total: 202ms    remaining: 2.88s\n131:    learn: 5285.4079447 total: 203ms    remaining: 2.87s\n132:    learn: 5276.3888293 total: 205ms    remaining: 2.88s\n133:    learn: 5253.7966160 total: 206ms    remaining: 2.88s\n134:    learn: 5242.4363346 total: 208ms    remaining: 2.87s\n135:    learn: 5234.7617159 total: 210ms    remaining: 2.87s\n136:    learn: 5230.7299511 total: 211ms    remaining: 2.87s\n137:    learn: 5228.9494984 total: 212ms    remaining: 2.86s\n138:    learn: 5220.7658686 total: 213ms    remaining: 2.86s\n139:    learn: 5220.7658686 total: 214ms    remaining: 2.84s\n140:    learn: 5220.3928102 total: 214ms    remaining: 2.83s\n141:    learn: 5216.5111017 total: 216ms    remaining: 2.82s\n142:    learn: 5213.4217372 total: 217ms    remaining: 2.82s\n143:    learn: 5211.7652831 total: 218ms    remaining: 2.81s\n144:    learn: 5193.0346592 total: 220ms    remaining: 2.81s\n145:    learn: 5185.9448931 total: 221ms    remaining: 2.81s\n146:    learn: 5182.0516815 total: 223ms    remaining: 2.81s\n147:    learn: 5174.6829049 total: 225ms    remaining: 2.81s\n148:    learn: 5174.6829049 total: 225ms    remaining: 2.79s\n149:    learn: 5166.2911738 total: 226ms    remaining: 2.79s\n150:    learn: 5159.8205215 total: 229ms    remaining: 2.8s\n151:    learn: 5156.3185114 total: 230ms    remaining: 2.8s\n152:    learn: 5156.3185114 total: 231ms    remaining: 2.78s\n153:    learn: 5147.4051798 total: 232ms    remaining: 2.79s\n154:    learn: 5147.4051798 total: 233ms    remaining: 2.77s\n155:    learn: 5147.2407560 total: 234ms    remaining: 2.76s\n156:    learn: 5146.4759564 total: 235ms    remaining: 2.75s\n157:    learn: 5137.3716292 total: 236ms    remaining: 2.75s\n158:    learn: 5121.4193275 total: 238ms    remaining: 2.75s\n159:    learn: 5115.5018273 total: 239ms    remaining: 2.75s\n160:    learn: 5115.5018273 total: 240ms    remaining: 2.74s\n161:    learn: 5110.2699403 total: 242ms    remaining: 2.74s\n162:    learn: 5109.9847477 total: 242ms    remaining: 2.73s\n163:    learn: 5101.1792661 total: 244ms    remaining: 2.73s\n164:    learn: 5094.6570018 total: 246ms    remaining: 2.73s\n165:    learn: 5087.8982851 total: 247ms    remaining: 2.73s\n166:    learn: 5075.5065415 total: 249ms    remaining: 2.73s\n167:    learn: 5073.0923568 total: 250ms    remaining: 2.73s\n168:    learn: 5059.3039228 total: 252ms    remaining: 2.73s\n169:    learn: 5052.5169956 total: 254ms    remaining: 2.73s\n170:    learn: 5048.6517154 total: 255ms    remaining: 2.73s\n171:    learn: 5044.5398469 total: 257ms    remaining: 2.73s\n172:    learn: 5042.2657789 total: 258ms    remaining: 2.73s\n173:    learn: 5027.5698491 total: 260ms    remaining: 2.73s\n174:    learn: 5018.2624339 total: 262ms    remaining: 2.73s\n175:    learn: 5015.4207674 total: 263ms    remaining: 2.73s\n176:    learn: 5004.4326005 total: 265ms    remaining: 2.73s\n177:    learn: 5002.2042902 total: 267ms    remaining: 2.73s\n178:    learn: 4991.5613784 total: 268ms    remaining: 2.73s\n179:    learn: 4986.5919903 total: 270ms    remaining: 2.73s\n180:    learn: 4983.2524252 total: 272ms    remaining: 2.73s\n181:    learn: 4983.0495780 total: 273ms    remaining: 2.72s\n182:    learn: 4972.3553519 total: 274ms    remaining: 2.72s\n183:    learn: 4963.4540764 total: 276ms    remaining: 2.72s\n184:    learn: 4959.3484378 total: 277ms    remaining: 2.72s\n185:    learn: 4953.8290305 total: 278ms    remaining: 2.71s\n186:    learn: 4948.8512443 total: 280ms    remaining: 2.71s\n187:    learn: 4942.0173436 total: 282ms    remaining: 2.71s\n188:    learn: 4935.2163749 total: 283ms    remaining: 2.71s\n189:    learn: 4927.8308601 total: 285ms    remaining: 2.71s\n190:    learn: 4922.7874103 total: 286ms    remaining: 2.71s\n191:    learn: 4921.8930548 total: 287ms    remaining: 2.71s\n192:    learn: 4921.8930548 total: 288ms    remaining: 2.69s\n193:    learn: 4916.1740670 total: 289ms    remaining: 2.69s\n194:    learn: 4910.6747951 total: 291ms    remaining: 2.69s\n195:    learn: 4901.7995007 total: 292ms    remaining: 2.69s\n196:    learn: 4901.7821460 total: 293ms    remaining: 2.68s\n197:    learn: 4897.4539465 total: 295ms    remaining: 2.68s\n198:    learn: 4894.9567494 total: 296ms    remaining: 2.68s\n199:    learn: 4891.2982974 total: 298ms    remaining: 2.68s\n200:    learn: 4886.1187451 total: 299ms    remaining: 2.67s\n201:    learn: 4877.8813207 total: 301ms    remaining: 2.67s\n202:    learn: 4875.4244708 total: 302ms    remaining: 2.67s\n203:    learn: 4870.3140418 total: 304ms    remaining: 2.67s\n204:    learn: 4870.3140418 total: 304ms    remaining: 2.66s\n205:    learn: 4856.6381725 total: 306ms    remaining: 2.66s\n206:    learn: 4856.6381725 total: 306ms    remaining: 2.65s\n207:    learn: 4852.3702566 total: 307ms    remaining: 2.65s\n208:    learn: 4851.5158620 total: 309ms    remaining: 2.64s\n209:    learn: 4847.0222783 total: 310ms    remaining: 2.64s\n210:    learn: 4842.1138566 total: 312ms    remaining: 2.64s\n211:    learn: 4838.7635392 total: 313ms    remaining: 2.64s\n212:    learn: 4826.0451253 total: 315ms    remaining: 2.64s\n213:    learn: 4819.7240280 total: 316ms    remaining: 2.64s\n214:    learn: 4811.2169173 total: 318ms    remaining: 2.64s\n215:    learn: 4811.2169173 total: 319ms    remaining: 2.63s\n216:    learn: 4807.9718531 total: 320ms    remaining: 2.63s\n217:    learn: 4806.7792354 total: 322ms    remaining: 2.63s\n218:    learn: 4804.2533811 total: 323ms    remaining: 2.63s\n219:    learn: 4799.5793423 total: 325ms    remaining: 2.63s\n220:    learn: 4792.8584914 total: 326ms    remaining: 2.63s\n221:    learn: 4787.4430082 total: 328ms    remaining: 2.63s\n222:    learn: 4781.1207856 total: 330ms    remaining: 2.63s\n223:    learn: 4773.1142514 total: 331ms    remaining: 2.62s\n224:    learn: 4771.3835507 total: 332ms    remaining: 2.62s\n225:    learn: 4769.0205967 total: 334ms    remaining: 2.62s\n226:    learn: 4769.0205967 total: 334ms    remaining: 2.61s\n227:    learn: 4765.8353863 total: 336ms    remaining: 2.61s\n228:    learn: 4765.1621035 total: 337ms    remaining: 2.61s\n229:    learn: 4761.9525642 total: 339ms    remaining: 2.61s\n230:    learn: 4760.0450712 total: 341ms    remaining: 2.61s\n231:    learn: 4755.1246652 total: 342ms    remaining: 2.61s\n232:    learn: 4753.8217625 total: 343ms    remaining: 2.6s\n233:    learn: 4747.5145731 total: 345ms    remaining: 2.6s\n234:    learn: 4747.5145731 total: 345ms    remaining: 2.59s\n235:    learn: 4744.6301998 total: 347ms    remaining: 2.59s\n236:    learn: 4738.8864864 total: 348ms    remaining: 2.59s\n237:    learn: 4736.5714509 total: 350ms    remaining: 2.59s\n238:    learn: 4735.0890903 total: 352ms    remaining: 2.59s\n239:    learn: 4728.3682935 total: 354ms    remaining: 2.59s\n240:    learn: 4721.9886699 total: 355ms    remaining: 2.59s\n241:    learn: 4717.3702814 total: 357ms    remaining: 2.59s\n242:    learn: 4714.6835057 total: 359ms    remaining: 2.59s\n243:    learn: 4712.3286406 total: 360ms    remaining: 2.59s\n244:    learn: 4710.3755007 total: 362ms    remaining: 2.59s\n245:    learn: 4710.3755007 total: 362ms    remaining: 2.58s\n246:    learn: 4710.3279402 total: 363ms    remaining: 2.57s\n247:    learn: 4708.7320386 total: 364ms    remaining: 2.57s\n248:    learn: 4708.5308039 total: 365ms    remaining: 2.57s\n249:    learn: 4708.4446512 total: 366ms    remaining: 2.56s\n250:    learn: 4704.9778190 total: 368ms    remaining: 2.56s\n251:    learn: 4703.1372636 total: 369ms    remaining: 2.56s\n252:    learn: 4698.4459674 total: 371ms    remaining: 2.56s\n253:    learn: 4698.4309104 total: 371ms    remaining: 2.55s\n254:    learn: 4696.1765101 total: 373ms    remaining: 2.55s\n255:    learn: 4693.9238815 total: 374ms    remaining: 2.55s\n256:    learn: 4689.8119640 total: 377ms    remaining: 2.56s\n257:    learn: 4689.8119640 total: 378ms    remaining: 2.55s\n258:    learn: 4686.5896296 total: 380ms    remaining: 2.55s\n259:    learn: 4680.2191721 total: 382ms    remaining: 2.56s\n260:    learn: 4675.0591879 total: 385ms    remaining: 2.56s\n261:    learn: 4671.5328062 total: 386ms    remaining: 2.56s\n262:    learn: 4669.4746102 total: 388ms    remaining: 2.56s\n263:    learn: 4666.6833477 total: 389ms    remaining: 2.56s\n264:    learn: 4665.4037114 total: 391ms    remaining: 2.56s\n265:    learn: 4663.4051577 total: 392ms    remaining: 2.56s\n266:    learn: 4663.3353224 total: 393ms    remaining: 2.55s\n267:    learn: 4660.7525508 total: 394ms    remaining: 2.55s\n268:    learn: 4657.2685203 total: 396ms    remaining: 2.54s\n269:    learn: 4657.2685203 total: 396ms    remaining: 2.54s\n270:    learn: 4656.1503206 total: 397ms    remaining: 2.53s\n271:    learn: 4652.0533288 total: 399ms    remaining: 2.54s\n272:    learn: 4648.9435674 total: 401ms    remaining: 2.54s\n273:    learn: 4645.7461085 total: 403ms    remaining: 2.54s\n274:    learn: 4642.4289709 total: 404ms    remaining: 2.53s\n275:    learn: 4635.8782081 total: 406ms    remaining: 2.53s\n276:    learn: 4629.6954381 total: 407ms    remaining: 2.53s\n277:    learn: 4627.1516605 total: 409ms    remaining: 2.53s\n278:    learn: 4620.0534128 total: 410ms    remaining: 2.53s\n279:    learn: 4620.0534128 total: 411ms    remaining: 2.52s\n280:    learn: 4617.4583624 total: 412ms    remaining: 2.52s\n281:    learn: 4615.3063600 total: 413ms    remaining: 2.52s\n282:    learn: 4615.3063600 total: 414ms    remaining: 2.51s\n283:    learn: 4615.3063600 total: 414ms    remaining: 2.5s\n284:    learn: 4615.2984525 total: 415ms    remaining: 2.5s\n285:    learn: 4611.3120573 total: 416ms    remaining: 2.5s\n286:    learn: 4602.2423954 total: 418ms    remaining: 2.5s\n287:    learn: 4600.2687023 total: 420ms    remaining: 2.5s\n288:    learn: 4600.2687023 total: 420ms    remaining: 2.49s\n289:    learn: 4596.0500378 total: 422ms    remaining: 2.49s\n290:    learn: 4596.0500378 total: 422ms    remaining: 2.48s\n291:    learn: 4594.3298144 total: 424ms    remaining: 2.48s\n292:    learn: 4594.3298144 total: 424ms    remaining: 2.47s\n293:    learn: 4591.3635862 total: 426ms    remaining: 2.47s\n294:    learn: 4591.3400551 total: 426ms    remaining: 2.46s\n295:    learn: 4591.3400551 total: 427ms    remaining: 2.46s\n296:    learn: 4587.7544912 total: 428ms    remaining: 2.46s\n297:    learn: 4582.6753402 total: 429ms    remaining: 2.45s\n298:    learn: 4581.1496820 total: 431ms    remaining: 2.45s\n299:    learn: 4580.8360346 total: 432ms    remaining: 2.45s\n300:    learn: 4580.6042904 total: 433ms    remaining: 2.44s\n301:    learn: 4577.5019033 total: 435ms    remaining: 2.44s\n302:    learn: 4577.5019033 total: 435ms    remaining: 2.44s\n303:    learn: 4577.4991453 total: 436ms    remaining: 2.43s\n304:    learn: 4574.0774804 total: 438ms    remaining: 2.43s\n305:    learn: 4567.1656172 total: 440ms    remaining: 2.44s\n306:    learn: 4567.1656172 total: 440ms    remaining: 2.43s\n307:    learn: 4567.1656172 total: 441ms    remaining: 2.42s\n308:    learn: 4563.1270449 total: 442ms    remaining: 2.42s\n309:    learn: 4556.4082285 total: 444ms    remaining: 2.42s\n310:    learn: 4549.4002934 total: 446ms    remaining: 2.42s\n311:    learn: 4548.6445542 total: 447ms    remaining: 2.42s\n312:    learn: 4548.6445542 total: 447ms    remaining: 2.41s\n313:    learn: 4548.6445542 total: 448ms    remaining: 2.4s\n314:    learn: 4543.7698643 total: 449ms    remaining: 2.4s\n315:    learn: 4537.5322728 total: 450ms    remaining: 2.4s\n316:    learn: 4537.5322728 total: 451ms    remaining: 2.39s\n317:    learn: 4535.8866669 total: 452ms    remaining: 2.39s\n318:    learn: 4534.0177274 total: 454ms    remaining: 2.39s\n319:    learn: 4534.0177274 total: 454ms    remaining: 2.38s\n320:    learn: 4532.6563239 total: 456ms    remaining: 2.38s\n321:    learn: 4532.5713131 total: 457ms    remaining: 2.38s\n322:    learn: 4530.5122706 total: 458ms    remaining: 2.38s\n323:    learn: 4530.0043105 total: 460ms    remaining: 2.38s\n324:    learn: 4530.0043105 total: 460ms    remaining: 2.37s\n325:    learn: 4521.2667630 total: 462ms    remaining: 2.37s\n326:    learn: 4521.2667630 total: 462ms    remaining: 2.37s\n327:    learn: 4514.9759966 total: 464ms    remaining: 2.36s\n328:    learn: 4514.5353565 total: 465ms    remaining: 2.36s\n329:    learn: 4514.5353565 total: 466ms    remaining: 2.35s\n330:    learn: 4514.2251824 total: 467ms    remaining: 2.35s\n331:    learn: 4513.4657797 total: 468ms    remaining: 2.35s\n332:    learn: 4508.6585143 total: 469ms    remaining: 2.35s\n333:    learn: 4506.4620472 total: 471ms    remaining: 2.35s\n334:    learn: 4505.0540337 total: 472ms    remaining: 2.35s\n335:    learn: 4501.2064750 total: 474ms    remaining: 2.35s\n336:    learn: 4501.2064750 total: 474ms    remaining: 2.34s\n337:    learn: 4496.9533102 total: 476ms    remaining: 2.34s\n338:    learn: 4495.3477163 total: 478ms    remaining: 2.34s\n339:    learn: 4495.0865050 total: 478ms    remaining: 2.33s\n340:    learn: 4495.0865050 total: 479ms    remaining: 2.33s\n341:    learn: 4494.8878363 total: 480ms    remaining: 2.33s\n342:    learn: 4494.8878363 total: 481ms    remaining: 2.32s\n343:    learn: 4493.9972375 total: 482ms    remaining: 2.32s\n344:    learn: 4487.6187148 total: 483ms    remaining: 2.32s\n345:    learn: 4485.1605053 total: 485ms    remaining: 2.32s\n346:    learn: 4483.9053796 total: 486ms    remaining: 2.31s\n347:    learn: 4483.9053796 total: 486ms    remaining: 2.31s\n348:    learn: 4483.9051133 total: 487ms    remaining: 2.3s\n349:    learn: 4483.6219873 total: 489ms    remaining: 2.3s\n350:    learn: 4478.6575151 total: 490ms    remaining: 2.3s\n351:    learn: 4478.1207406 total: 492ms    remaining: 2.3s\n352:    learn: 4472.9000386 total: 493ms    remaining: 2.3s\n353:    learn: 4472.8453202 total: 494ms    remaining: 2.3s\n354:    learn: 4471.0678218 total: 496ms    remaining: 2.3s\n355:    learn: 4468.5441814 total: 497ms    remaining: 2.29s\n356:    learn: 4465.5049818 total: 498ms    remaining: 2.29s\n357:    learn: 4461.3387165 total: 500ms    remaining: 2.29s\n358:    learn: 4459.7424491 total: 501ms    remaining: 2.29s\n359:    learn: 4455.5656117 total: 503ms    remaining: 2.29s\n360:    learn: 4452.7121694 total: 504ms    remaining: 2.29s\n361:    learn: 4447.3890000 total: 506ms    remaining: 2.29s\n362:    learn: 4447.0423543 total: 507ms    remaining: 2.29s\n363:    learn: 4445.7774126 total: 508ms    remaining: 2.29s\n364:    learn: 4445.7774126 total: 509ms    remaining: 2.28s\n365:    learn: 4444.5254579 total: 511ms    remaining: 2.28s\n366:    learn: 4442.4085722 total: 512ms    remaining: 2.28s\n367:    learn: 4441.9445764 total: 513ms    remaining: 2.27s\n368:    learn: 4439.0150744 total: 514ms    remaining: 2.27s\n369:    learn: 4435.2123964 total: 516ms    remaining: 2.27s\n370:    learn: 4433.5067182 total: 517ms    remaining: 2.27s\n371:    learn: 4425.5447245 total: 519ms    remaining: 2.27s\n372:    learn: 4425.5447245 total: 520ms    remaining: 2.27s\n373:    learn: 4423.8261031 total: 521ms    remaining: 2.26s\n374:    learn: 4423.3283440 total: 522ms    remaining: 2.26s\n375:    learn: 4423.3283440 total: 523ms    remaining: 2.26s\n376:    learn: 4420.3203928 total: 524ms    remaining: 2.26s\n377:    learn: 4419.5474976 total: 526ms    remaining: 2.25s\n378:    learn: 4414.1595475 total: 527ms    remaining: 2.25s\n379:    learn: 4412.2039198 total: 529ms    remaining: 2.25s\n380:    learn: 4403.8910365 total: 530ms    remaining: 2.25s\n381:    learn: 4403.2953039 total: 532ms    remaining: 2.25s\n382:    learn: 4399.9196106 total: 533ms    remaining: 2.25s\n383:    learn: 4398.9201818 total: 535ms    remaining: 2.25s\n384:    learn: 4398.2316231 total: 536ms    remaining: 2.25s\n385:    learn: 4398.2316231 total: 536ms    remaining: 2.24s\n386:    learn: 4397.1150817 total: 538ms    remaining: 2.24s\n387:    learn: 4396.4216007 total: 540ms    remaining: 2.24s\n388:    learn: 4391.6969293 total: 541ms    remaining: 2.24s\n389:    learn: 4389.9434594 total: 543ms    remaining: 2.24s\n390:    learn: 4387.7026310 total: 544ms    remaining: 2.24s\n391:    learn: 4385.6624572 total: 546ms    remaining: 2.24s\n392:    learn: 4384.3285328 total: 547ms    remaining: 2.24s\n393:    learn: 4381.9411758 total: 549ms    remaining: 2.24s\n394:    learn: 4377.9349406 total: 550ms    remaining: 2.23s\n395:    learn: 4375.2340793 total: 552ms    remaining: 2.23s\n396:    learn: 4375.2338690 total: 552ms    remaining: 2.23s\n397:    learn: 4374.5378448 total: 553ms    remaining: 2.23s\n398:    learn: 4374.5378448 total: 554ms    remaining: 2.22s\n399:    learn: 4374.0246587 total: 556ms    remaining: 2.22s\n400:    learn: 4373.0983116 total: 557ms    remaining: 2.22s\n401:    learn: 4371.7726903 total: 558ms    remaining: 2.22s\n402:    learn: 4371.7726903 total: 559ms    remaining: 2.21s\n403:    learn: 4367.9188809 total: 561ms    remaining: 2.22s\n404:    learn: 4367.3837471 total: 563ms    remaining: 2.22s\n405:    learn: 4367.3837471 total: 563ms    remaining: 2.21s\n406:    learn: 4366.3383986 total: 565ms    remaining: 2.21s\n407:    learn: 4364.9695002 total: 566ms    remaining: 2.21s\n408:    learn: 4364.5791148 total: 567ms    remaining: 2.21s\n409:    learn: 4364.5791148 total: 567ms    remaining: 2.2s\n410:    learn: 4360.6650300 total: 569ms    remaining: 2.2s\n411:    learn: 4358.8850197 total: 570ms    remaining: 2.2s\n412:    learn: 4358.0952260 total: 572ms    remaining: 2.2s\n413:    learn: 4358.0952260 total: 572ms    remaining: 2.19s\n414:    learn: 4358.0952260 total: 573ms    remaining: 2.19s\n415:    learn: 4353.9406626 total: 574ms    remaining: 2.19s\n416:    learn: 4353.9406626 total: 574ms    remaining: 2.18s\n417:    learn: 4353.9406626 total: 575ms    remaining: 2.17s\n418:    learn: 4350.8582602 total: 576ms    remaining: 2.17s\n419:    learn: 4348.7656427 total: 578ms    remaining: 2.17s\n420:    learn: 4348.7654323 total: 579ms    remaining: 2.17s\n421:    learn: 4347.2057659 total: 580ms    remaining: 2.17s\n422:    learn: 4345.8380325 total: 582ms    remaining: 2.17s\n423:    learn: 4340.0889391 total: 583ms    remaining: 2.17s\n424:    learn: 4337.3466418 total: 585ms    remaining: 2.17s\n425:    learn: 4333.1806959 total: 586ms    remaining: 2.16s\n426:    learn: 4332.8557929 total: 587ms    remaining: 2.16s\n427:    learn: 4332.8557929 total: 588ms    remaining: 2.16s\n428:    learn: 4331.0378307 total: 589ms    remaining: 2.16s\n429:    learn: 4325.1923800 total: 591ms    remaining: 2.16s\n430:    learn: 4319.7524581 total: 592ms    remaining: 2.16s\n431:    learn: 4316.9973772 total: 594ms    remaining: 2.16s\n432:    learn: 4315.3920928 total: 596ms    remaining: 2.15s\n433:    learn: 4313.5095488 total: 597ms    remaining: 2.15s\n434:    learn: 4312.0378766 total: 599ms    remaining: 2.15s\n435:    learn: 4309.9677471 total: 600ms    remaining: 2.15s\n436:    learn: 4307.8543156 total: 602ms    remaining: 2.15s\n437:    learn: 4306.3912379 total: 603ms    remaining: 2.15s\n438:    learn: 4304.0146016 total: 605ms    remaining: 2.15s\n439:    learn: 4304.0146016 total: 605ms    remaining: 2.15s\n440:    learn: 4304.0146016 total: 606ms    remaining: 2.14s\n441:    learn: 4296.2066898 total: 607ms    remaining: 2.14s\n442:    learn: 4296.2066898 total: 608ms    remaining: 2.13s\n443:    learn: 4296.2066898 total: 608ms    remaining: 2.13s\n444:    learn: 4293.7742302 total: 610ms    remaining: 2.13s\n445:    learn: 4290.6682497 total: 611ms    remaining: 2.13s\n446:    learn: 4290.6682497 total: 612ms    remaining: 2.13s\n447:    learn: 4290.6659104 total: 612ms    remaining: 2.12s\n448:    learn: 4290.6659104 total: 613ms    remaining: 2.12s\n449:    learn: 4290.6659104 total: 613ms    remaining: 2.11s\n450:    learn: 4289.4314861 total: 615ms    remaining: 2.11s\n451:    learn: 4287.6019761 total: 616ms    remaining: 2.11s\n452:    learn: 4284.1460191 total: 618ms    remaining: 2.11s\n453:    learn: 4283.1275688 total: 619ms    remaining: 2.11s\n454:    learn: 4283.1275688 total: 620ms    remaining: 2.1s\n455:    learn: 4282.2886315 total: 621ms    remaining: 2.1s\n456:    learn: 4282.2886315 total: 622ms    remaining: 2.1s\n457:    learn: 4281.3414277 total: 623ms    remaining: 2.1s\n458:    learn: 4280.9302933 total: 624ms    remaining: 2.1s\n459:    learn: 4280.2722045 total: 625ms    remaining: 2.09s\n460:    learn: 4278.2675356 total: 627ms    remaining: 2.09s\n461:    learn: 4276.8834771 total: 628ms    remaining: 2.09s\n462:    learn: 4276.8162392 total: 629ms    remaining: 2.09s\n463:    learn: 4271.9664215 total: 631ms    remaining: 2.09s\n464:    learn: 4268.1650630 total: 633ms    remaining: 2.09s\n465:    learn: 4261.3143626 total: 634ms    remaining: 2.09s\n466:    learn: 4261.3143592 total: 635ms    remaining: 2.08s\n467:    learn: 4255.8005291 total: 636ms    remaining: 2.08s\n468:    learn: 4250.4214698 total: 638ms    remaining: 2.08s\n469:    learn: 4248.4840035 total: 640ms    remaining: 2.08s\n470:    learn: 4247.4707012 total: 642ms    remaining: 2.08s\n471:    learn: 4244.9999349 total: 643ms    remaining: 2.08s\n472:    learn: 4244.8961803 total: 644ms    remaining: 2.08s\n473:    learn: 4243.5136900 total: 646ms    remaining: 2.08s\n474:    learn: 4240.5620812 total: 647ms    remaining: 2.08s\n475:    learn: 4237.5068841 total: 649ms    remaining: 2.08s\n476:    learn: 4235.7372353 total: 650ms    remaining: 2.08s\n477:    learn: 4235.5684329 total: 652ms    remaining: 2.07s\n478:    learn: 4235.2638310 total: 653ms    remaining: 2.07s\n479:    learn: 4234.8174553 total: 655ms    remaining: 2.07s\n480:    learn: 4234.0613475 total: 657ms    remaining: 2.07s\n481:    learn: 4234.0612821 total: 657ms    remaining: 2.07s\n482:    learn: 4230.8662841 total: 659ms    remaining: 2.07s\n483:    learn: 4228.3535703 total: 660ms    remaining: 2.07s\n484:    learn: 4227.2170785 total: 662ms    remaining: 2.07s\n485:    learn: 4227.2037809 total: 663ms    remaining: 2.07s\n486:    learn: 4225.3901041 total: 665ms    remaining: 2.06s\n487:    learn: 4224.7331910 total: 667ms    remaining: 2.06s\n488:    learn: 4218.3517171 total: 668ms    remaining: 2.06s\n489:    learn: 4218.3517171 total: 669ms    remaining: 2.06s\n490:    learn: 4217.8187895 total: 670ms    remaining: 2.06s\n491:    learn: 4215.5984258 total: 671ms    remaining: 2.06s\n492:    learn: 4215.5984258 total: 672ms    remaining: 2.05s\n493:    learn: 4213.8364336 total: 674ms    remaining: 2.05s\n494:    learn: 4213.8364336 total: 674ms    remaining: 2.05s\n495:    learn: 4213.0051294 total: 676ms    remaining: 2.05s\n496:    learn: 4212.8201871 total: 677ms    remaining: 2.05s\n497:    learn: 4210.8474431 total: 678ms    remaining: 2.04s\n498:    learn: 4208.3847884 total: 680ms    remaining: 2.05s\n499:    learn: 4207.8647287 total: 682ms    remaining: 2.04s\n500:    learn: 4205.4937402 total: 683ms    remaining: 2.04s\n501:    learn: 4201.6181630 total: 685ms    remaining: 2.04s\n502:    learn: 4199.5545731 total: 686ms    remaining: 2.04s\n503:    learn: 4199.5545731 total: 687ms    remaining: 2.04s\n504:    learn: 4194.7013059 total: 688ms    remaining: 2.04s\n505:    learn: 4194.7012736 total: 689ms    remaining: 2.03s\n506:    learn: 4194.2513690 total: 690ms    remaining: 2.03s\n507:    learn: 4193.7391858 total: 692ms    remaining: 2.03s\n508:    learn: 4192.8297057 total: 694ms    remaining: 2.03s\n509:    learn: 4192.3261240 total: 695ms    remaining: 2.03s\n510:    learn: 4188.6361916 total: 696ms    remaining: 2.03s\n511:    learn: 4184.1036666 total: 698ms    remaining: 2.03s\n512:    learn: 4176.3244783 total: 700ms    remaining: 2.03s\n513:    learn: 4176.3244783 total: 700ms    remaining: 2.02s\n514:    learn: 4172.9846817 total: 701ms    remaining: 2.02s\n515:    learn: 4168.9849081 total: 703ms    remaining: 2.02s\n516:    learn: 4167.2190260 total: 704ms    remaining: 2.02s\n517:    learn: 4167.2190260 total: 705ms    remaining: 2.02s\n518:    learn: 4166.9911886 total: 706ms    remaining: 2.01s\n519:    learn: 4162.8150043 total: 707ms    remaining: 2.01s\n520:    learn: 4162.4234461 total: 709ms    remaining: 2.01s\n521:    learn: 4161.6787564 total: 711ms    remaining: 2.01s\n522:    learn: 4161.6787564 total: 711ms    remaining: 2.01s\n523:    learn: 4159.0877863 total: 713ms    remaining: 2.01s\n524:    learn: 4158.1609903 total: 714ms    remaining: 2.01s\n525:    learn: 4154.6942835 total: 716ms    remaining: 2.01s\n526:    learn: 4151.0966275 total: 718ms    remaining: 2s\n527:    learn: 4149.3851416 total: 719ms    remaining: 2s\n528:    learn: 4148.7633600 total: 721ms    remaining: 2s\n529:    learn: 4148.2950844 total: 722ms    remaining: 2s\n530:    learn: 4147.3736223 total: 724ms    remaining: 2s\n531:    learn: 4147.3736223 total: 725ms    remaining: 2s\n532:    learn: 4147.3596110 total: 725ms    remaining: 2s\n533:    learn: 4145.0761992 total: 727ms    remaining: 2s\n534:    learn: 4138.0181778 total: 729ms    remaining: 2s\n535:    learn: 4136.2533307 total: 730ms    remaining: 1.99s\n536:    learn: 4135.3564180 total: 732ms    remaining: 1.99s\n537:    learn: 4135.3564180 total: 733ms    remaining: 1.99s\n538:    learn: 4134.6207875 total: 734ms    remaining: 1.99s\n539:    learn: 4130.3626163 total: 736ms    remaining: 1.99s\n540:    learn: 4127.8526230 total: 738ms    remaining: 1.99s\n541:    learn: 4127.8526230 total: 738ms    remaining: 1.99s\n542:    learn: 4126.9499016 total: 740ms    remaining: 1.99s\n543:    learn: 4124.4222877 total: 742ms    remaining: 1.99s\n544:    learn: 4124.4093974 total: 744ms    remaining: 1.99s\n545:    learn: 4120.4906474 total: 745ms    remaining: 1.98s\n546:    learn: 4120.1085313 total: 747ms    remaining: 1.98s\n547:    learn: 4118.5497982 total: 748ms    remaining: 1.98s\n548:    learn: 4118.5497982 total: 749ms    remaining: 1.98s\n549:    learn: 4116.7736360 total: 750ms    remaining: 1.98s\n550:    learn: 4114.8590115 total: 752ms    remaining: 1.98s\n551:    learn: 4113.8718524 total: 754ms    remaining: 1.98s\n552:    learn: 4113.6509912 total: 755ms    remaining: 1.97s\n553:    learn: 4112.7943829 total: 756ms    remaining: 1.97s\n554:    learn: 4112.7943829 total: 757ms    remaining: 1.97s\n555:    learn: 4111.8971678 total: 758ms    remaining: 1.97s\n556:    learn: 4108.7174163 total: 760ms    remaining: 1.97s\n557:    learn: 4108.0979840 total: 762ms    remaining: 1.97s\n558:    learn: 4107.4667399 total: 763ms    remaining: 1.97s\n559:    learn: 4107.0891159 total: 765ms    remaining: 1.97s\n560:    learn: 4106.3886713 total: 767ms    remaining: 1.97s\n561:    learn: 4106.3851242 total: 768ms    remaining: 1.96s\n562:    learn: 4106.3797848 total: 768ms    remaining: 1.96s\n563:    learn: 4106.3797848 total: 769ms    remaining: 1.96s\n564:    learn: 4106.3797848 total: 769ms    remaining: 1.95s\n565:    learn: 4106.3797848 total: 770ms    remaining: 1.95s\n566:    learn: 4104.3466739 total: 772ms    remaining: 1.95s\n567:    learn: 4102.6349434 total: 773ms    remaining: 1.95s\n568:    learn: 4093.9566800 total: 775ms    remaining: 1.95s\n569:    learn: 4089.1688460 total: 777ms    remaining: 1.95s\n570:    learn: 4088.2412857 total: 778ms    remaining: 1.95s\n571:    learn: 4088.2412857 total: 779ms    remaining: 1.94s\n572:    learn: 4087.4126865 total: 781ms    remaining: 1.94s\n573:    learn: 4087.3246347 total: 784ms    remaining: 1.95s\n574:    learn: 4079.1728530 total: 786ms    remaining: 1.95s\n575:    learn: 4076.9756189 total: 788ms    remaining: 1.95s\n576:    learn: 4074.5129417 total: 790ms    remaining: 1.95s\n577:    learn: 4073.6434006 total: 792ms    remaining: 1.95s\n578:    learn: 4073.6434006 total: 793ms    remaining: 1.95s\n579:    learn: 4073.1058334 total: 795ms    remaining: 1.95s\n580:    learn: 4070.4631078 total: 797ms    remaining: 1.95s\n581:    learn: 4068.7561071 total: 799ms    remaining: 1.95s\n582:    learn: 4067.6543142 total: 801ms    remaining: 1.95s\n583:    learn: 4064.0010837 total: 802ms    remaining: 1.95s\n584:    learn: 4062.6699329 total: 804ms    remaining: 1.95s\n585:    learn: 4061.8289513 total: 806ms    remaining: 1.95s\n586:    learn: 4058.8888104 total: 808ms    remaining: 1.94s\n587:    learn: 4057.5685823 total: 810ms    remaining: 1.94s\n588:    learn: 4056.3428995 total: 812ms    remaining: 1.94s\n589:    learn: 4051.0984948 total: 814ms    remaining: 1.94s\n590:    learn: 4050.7321529 total: 816ms    remaining: 1.94s\n591:    learn: 4048.6650531 total: 818ms    remaining: 1.95s\n592:    learn: 4047.2165603 total: 820ms    remaining: 1.95s\n593:    learn: 4043.7914684 total: 822ms    remaining: 1.94s\n594:    learn: 4043.2525797 total: 823ms    remaining: 1.94s\n595:    learn: 4043.2525797 total: 824ms    remaining: 1.94s\n596:    learn: 4042.6610138 total: 826ms    remaining: 1.94s\n597:    learn: 4039.9999717 total: 828ms    remaining: 1.94s\n598:    learn: 4034.3252839 total: 830ms    remaining: 1.94s\n599:    learn: 4034.3252839 total: 830ms    remaining: 1.94s\n600:    learn: 4034.3252839 total: 831ms    remaining: 1.93s\n601:    learn: 4034.2830758 total: 832ms    remaining: 1.93s\n602:    learn: 4033.9653700 total: 834ms    remaining: 1.93s\n603:    learn: 4033.9606019 total: 834ms    remaining: 1.93s\n604:    learn: 4028.7695295 total: 836ms    remaining: 1.93s\n605:    learn: 4028.4370609 total: 838ms    remaining: 1.93s\n606:    learn: 4026.1362055 total: 840ms    remaining: 1.93s\n607:    learn: 4025.1194150 total: 842ms    remaining: 1.93s\n608:    learn: 4021.1161232 total: 844ms    remaining: 1.93s\n609:    learn: 4020.5508077 total: 846ms    remaining: 1.93s\n610:    learn: 4020.5508077 total: 846ms    remaining: 1.92s\n611:    learn: 4020.3832148 total: 847ms    remaining: 1.92s\n612:    learn: 4016.1194956 total: 849ms    remaining: 1.92s\n613:    learn: 4013.4009443 total: 851ms    remaining: 1.92s\n614:    learn: 4012.3927298 total: 853ms    remaining: 1.92s\n615:    learn: 4012.3875832 total: 853ms    remaining: 1.92s\n616:    learn: 4012.2092547 total: 855ms    remaining: 1.92s\n617:    learn: 4010.9373855 total: 856ms    remaining: 1.91s\n618:    learn: 4010.6712533 total: 858ms    remaining: 1.91s\n619:    learn: 4007.0184514 total: 859ms    remaining: 1.91s\n620:    learn: 4007.0184514 total: 860ms    remaining: 1.91s\n621:    learn: 4006.3237895 total: 862ms    remaining: 1.91s\n622:    learn: 4004.8793407 total: 864ms    remaining: 1.91s\n623:    learn: 4003.9471214 total: 865ms    remaining: 1.91s\n624:    learn: 4000.7887982 total: 867ms    remaining: 1.91s\n625:    learn: 4000.4795677 total: 868ms    remaining: 1.91s\n626:    learn: 4000.3438616 total: 870ms    remaining: 1.9s\n627:    learn: 3999.1296902 total: 871ms    remaining: 1.9s\n628:    learn: 3998.0867078 total: 873ms    remaining: 1.9s\n629:    learn: 3998.0867078 total: 873ms    remaining: 1.9s\n630:    learn: 3995.4813310 total: 875ms    remaining: 1.9s\n631:    learn: 3995.4813310 total: 876ms    remaining: 1.9s\n632:    learn: 3994.7973506 total: 878ms    remaining: 1.9s\n633:    learn: 3994.4255478 total: 879ms    remaining: 1.89s\n634:    learn: 3993.8426501 total: 881ms    remaining: 1.89s\n635:    learn: 3992.4595577 total: 883ms    remaining: 1.89s\n636:    learn: 3991.2065592 total: 885ms    remaining: 1.89s\n637:    learn: 3990.5372097 total: 887ms    remaining: 1.89s\n638:    learn: 3990.5313149 total: 887ms    remaining: 1.89s\n639:    learn: 3989.2375720 total: 889ms    remaining: 1.89s\n640:    learn: 3987.6399593 total: 891ms    remaining: 1.89s\n641:    learn: 3986.0680229 total: 893ms    remaining: 1.89s\n642:    learn: 3986.0680229 total: 893ms    remaining: 1.89s\n643:    learn: 3985.3611248 total: 895ms    remaining: 1.88s\n644:    learn: 3984.9141762 total: 897ms    remaining: 1.88s\n645:    learn: 3978.4243745 total: 899ms    remaining: 1.88s\n646:    learn: 3978.0748821 total: 900ms    remaining: 1.88s\n647:    learn: 3975.0948892 total: 902ms    remaining: 1.88s\n648:    learn: 3973.7336182 total: 904ms    remaining: 1.88s\n649:    learn: 3973.5694895 total: 906ms    remaining: 1.88s\n650:    learn: 3973.5694895 total: 906ms    remaining: 1.88s\n651:    learn: 3973.2035512 total: 908ms    remaining: 1.88s\n652:    learn: 3971.3748963 total: 910ms    remaining: 1.88s\n653:    learn: 3970.8273230 total: 912ms    remaining: 1.88s\n654:    learn: 3970.6661701 total: 913ms    remaining: 1.87s\n655:    learn: 3970.3188222 total: 915ms    remaining: 1.87s\n656:    learn: 3967.7537638 total: 917ms    remaining: 1.87s\n657:    learn: 3965.3162844 total: 919ms    remaining: 1.87s\n658:    learn: 3963.3457135 total: 922ms    remaining: 1.88s\n659:    learn: 3960.6737011 total: 924ms    remaining: 1.88s\n660:    learn: 3957.5551840 total: 927ms    remaining: 1.88s\n661:    learn: 3954.5284019 total: 928ms    remaining: 1.88s\n662:    learn: 3954.2685083 total: 930ms    remaining: 1.88s\n663:    learn: 3953.7975769 total: 936ms    remaining: 1.88s\n664:    learn: 3952.5579480 total: 939ms    remaining: 1.89s\n665:    learn: 3952.5579480 total: 940ms    remaining: 1.88s\n666:    learn: 3952.5579480 total: 941ms    remaining: 1.88s\n667:    learn: 3952.0530212 total: 943ms    remaining: 1.88s\n668:    learn: 3951.0236652 total: 946ms    remaining: 1.88s\n669:    learn: 3948.8482121 total: 948ms    remaining: 1.88s\n670:    learn: 3948.1039576 total: 950ms    remaining: 1.88s\n671:    learn: 3948.1039576 total: 951ms    remaining: 1.88s\n672:    learn: 3947.8466720 total: 952ms    remaining: 1.88s\n673:    learn: 3944.7760408 total: 954ms    remaining: 1.88s\n674:    learn: 3944.3794626 total: 956ms    remaining: 1.88s\n675:    learn: 3943.8828460 total: 959ms    remaining: 1.88s\n676:    learn: 3939.4803751 total: 962ms    remaining: 1.88s\n677:    learn: 3937.0226901 total: 964ms    remaining: 1.88s\n678:    learn: 3935.6062093 total: 966ms    remaining: 1.88s\n679:    learn: 3931.6908549 total: 968ms    remaining: 1.88s\n680:    learn: 3931.6908549 total: 969ms    remaining: 1.88s\n681:    learn: 3931.4780236 total: 970ms    remaining: 1.88s\n682:    learn: 3931.4780236 total: 971ms    remaining: 1.87s\n683:    learn: 3930.1802631 total: 973ms    remaining: 1.87s\n684:    learn: 3930.1802631 total: 974ms    remaining: 1.87s\n685:    learn: 3930.1802631 total: 974ms    remaining: 1.87s\n686:    learn: 3929.9546359 total: 975ms    remaining: 1.86s\n687:    learn: 3929.9546359 total: 976ms    remaining: 1.86s\n688:    learn: 3929.5200873 total: 978ms    remaining: 1.86s\n689:    learn: 3924.1504149 total: 980ms    remaining: 1.86s\n690:    learn: 3924.1504149 total: 981ms    remaining: 1.86s\n691:    learn: 3924.1504149 total: 981ms    remaining: 1.85s\n692:    learn: 3923.3728173 total: 984ms    remaining: 1.86s\n693:    learn: 3923.3728173 total: 985ms    remaining: 1.85s\n694:    learn: 3921.6096814 total: 987ms    remaining: 1.85s\n695:    learn: 3920.3917066 total: 989ms    remaining: 1.85s\n696:    learn: 3919.1748239 total: 991ms    remaining: 1.85s\n697:    learn: 3918.6781013 total: 993ms    remaining: 1.85s\n698:    learn: 3915.5786090 total: 995ms    remaining: 1.85s\n699:    learn: 3914.9386661 total: 997ms    remaining: 1.85s\n700:    learn: 3914.1044266 total: 999ms    remaining: 1.85s\n701:    learn: 3914.1044266 total: 1000ms   remaining: 1.85s\n702:    learn: 3911.5461909 total: 1s   remaining: 1.85s\n703:    learn: 3909.7858547 total: 1s   remaining: 1.85s\n704:    learn: 3908.5848398 total: 1s   remaining: 1.85s\n705:    learn: 3908.5846612 total: 1.01s    remaining: 1.84s\n706:    learn: 3906.3544545 total: 1.01s    remaining: 1.84s\n707:    learn: 3902.5421111 total: 1.01s    remaining: 1.84s\n708:    learn: 3900.6794103 total: 1.01s    remaining: 1.84s\n709:    learn: 3900.1145635 total: 1.01s    remaining: 1.84s\n710:    learn: 3898.7988768 total: 1.02s    remaining: 1.84s\n711:    learn: 3897.7890023 total: 1.02s    remaining: 1.84s\n712:    learn: 3897.3063669 total: 1.02s    remaining: 1.84s\n713:    learn: 3896.0709359 total: 1.02s    remaining: 1.84s\n714:    learn: 3893.2716970 total: 1.03s    remaining: 1.84s\n715:    learn: 3893.2716970 total: 1.03s    remaining: 1.84s\n716:    learn: 3892.7696855 total: 1.03s    remaining: 1.84s\n717:    learn: 3891.8609727 total: 1.03s    remaining: 1.84s\n718:    learn: 3891.8609727 total: 1.03s    remaining: 1.84s\n719:    learn: 3886.5432527 total: 1.03s    remaining: 1.84s\n720:    learn: 3885.5890724 total: 1.03s    remaining: 1.84s\n721:    learn: 3885.2509517 total: 1.04s    remaining: 1.83s\n722:    learn: 3884.8683416 total: 1.04s    remaining: 1.83s\n723:    learn: 3883.8658058 total: 1.04s    remaining: 1.83s\n724:    learn: 3882.8769674 total: 1.04s    remaining: 1.83s\n725:    learn: 3880.1551434 total: 1.04s    remaining: 1.83s\n726:    learn: 3878.8486183 total: 1.05s    remaining: 1.83s\n727:    learn: 3878.0455602 total: 1.05s    remaining: 1.83s\n728:    learn: 3878.0455602 total: 1.05s    remaining: 1.83s\n729:    learn: 3875.8100129 total: 1.05s    remaining: 1.83s\n730:    learn: 3875.8100129 total: 1.05s    remaining: 1.83s\n731:    learn: 3874.7519439 total: 1.05s    remaining: 1.82s\n732:    learn: 3873.0181722 total: 1.05s    remaining: 1.82s\n733:    learn: 3872.6475110 total: 1.06s    remaining: 1.82s\n734:    learn: 3870.6422449 total: 1.06s    remaining: 1.82s\n735:    learn: 3869.4474087 total: 1.06s    remaining: 1.82s\n736:    learn: 3867.0168014 total: 1.06s    remaining: 1.82s\n737:    learn: 3865.3121541 total: 1.06s    remaining: 1.82s\n738:    learn: 3864.3253068 total: 1.07s    remaining: 1.82s\n739:    learn: 3863.3321651 total: 1.07s    remaining: 1.82s\n740:    learn: 3863.0244965 total: 1.07s    remaining: 1.82s\n741:    learn: 3861.8958492 total: 1.07s    remaining: 1.82s\n742:    learn: 3861.7968837 total: 1.07s    remaining: 1.82s\n743:    learn: 3861.7962967 total: 1.07s    remaining: 1.81s\n744:    learn: 3860.4491412 total: 1.08s    remaining: 1.81s\n745:    learn: 3860.4491412 total: 1.08s    remaining: 1.81s\n746:    learn: 3859.3516675 total: 1.08s    remaining: 1.81s\n747:    learn: 3858.3562986 total: 1.08s    remaining: 1.81s\n748:    learn: 3858.3562986 total: 1.08s    remaining: 1.8s\n749:    learn: 3858.3490771 total: 1.08s    remaining: 1.8s\n750:    learn: 3858.2502493 total: 1.08s    remaining: 1.8s\n751:    learn: 3857.4697682 total: 1.08s    remaining: 1.8s\n752:    learn: 3857.4659809 total: 1.08s    remaining: 1.8s\n753:    learn: 3856.9435214 total: 1.09s    remaining: 1.8s\n754:    learn: 3856.9435214 total: 1.09s    remaining: 1.79s\n755:    learn: 3856.2765714 total: 1.09s    remaining: 1.79s\n756:    learn: 3855.7600481 total: 1.09s    remaining: 1.79s\n757:    learn: 3852.6969291 total: 1.09s    remaining: 1.79s\n758:    learn: 3852.6730764 total: 1.09s    remaining: 1.79s\n759:    learn: 3852.2047553 total: 1.09s    remaining: 1.79s\n760:    learn: 3851.5653409 total: 1.1s remaining: 1.79s\n761:    learn: 3851.5508266 total: 1.1s remaining: 1.78s\n762:    learn: 3848.7387610 total: 1.1s remaining: 1.78s\n763:    learn: 3847.7080161 total: 1.1s remaining: 1.78s\n764:    learn: 3847.7080161 total: 1.1s remaining: 1.78s\n765:    learn: 3847.6293776 total: 1.1s remaining: 1.78s\n766:    learn: 3847.6293776 total: 1.1s remaining: 1.77s\n767:    learn: 3847.6289207 total: 1.1s remaining: 1.77s\n768:    learn: 3845.8710962 total: 1.11s    remaining: 1.77s\n769:    learn: 3845.3032079 total: 1.11s    remaining: 1.77s\n770:    learn: 3844.5649758 total: 1.11s    remaining: 1.77s\n771:    learn: 3843.7347054 total: 1.11s    remaining: 1.77s\n772:    learn: 3841.6978237 total: 1.11s    remaining: 1.77s\n773:    learn: 3841.0971126 total: 1.12s    remaining: 1.77s\n774:    learn: 3837.4084642 total: 1.12s    remaining: 1.77s\n775:    learn: 3837.0934050 total: 1.12s    remaining: 1.76s\n776:    learn: 3836.7762376 total: 1.12s    remaining: 1.76s\n777:    learn: 3836.2799013 total: 1.12s    remaining: 1.76s\n778:    learn: 3834.5565153 total: 1.12s    remaining: 1.76s\n779:    learn: 3832.9932344 total: 1.13s    remaining: 1.76s\n780:    learn: 3829.3338033 total: 1.13s    remaining: 1.76s\n781:    learn: 3828.5601788 total: 1.13s    remaining: 1.76s\n782:    learn: 3828.5601788 total: 1.13s    remaining: 1.76s\n783:    learn: 3828.5601744 total: 1.13s    remaining: 1.75s\n784:    learn: 3827.5366664 total: 1.13s    remaining: 1.75s\n785:    learn: 3826.4151057 total: 1.13s    remaining: 1.75s\n786:    learn: 3825.7586832 total: 1.14s    remaining: 1.75s\n787:    learn: 3825.0938251 total: 1.14s    remaining: 1.75s\n788:    learn: 3824.9906903 total: 1.14s    remaining: 1.75s\n789:    learn: 3824.5655529 total: 1.14s    remaining: 1.75s\n790:    learn: 3822.6288381 total: 1.14s    remaining: 1.75s\n791:    learn: 3822.3783984 total: 1.15s    remaining: 1.75s\n792:    learn: 3820.2943393 total: 1.15s    remaining: 1.75s\n793:    learn: 3820.0646286 total: 1.15s    remaining: 1.74s\n794:    learn: 3819.9664245 total: 1.15s    remaining: 1.74s\n795:    learn: 3818.9084625 total: 1.15s    remaining: 1.74s\n796:    learn: 3818.5750938 total: 1.15s    remaining: 1.74s\n797:    learn: 3817.5402599 total: 1.16s    remaining: 1.74s\n798:    learn: 3816.2278347 total: 1.16s    remaining: 1.74s\n799:    learn: 3812.7380606 total: 1.16s    remaining: 1.74s\n800:    learn: 3812.7380606 total: 1.16s    remaining: 1.74s\n801:    learn: 3812.7380606 total: 1.16s    remaining: 1.73s\n802:    learn: 3811.5718740 total: 1.16s    remaining: 1.73s\n803:    learn: 3811.3369065 total: 1.16s    remaining: 1.73s\n804:    learn: 3811.1293690 total: 1.16s    remaining: 1.73s\n805:    learn: 3811.1183005 total: 1.17s    remaining: 1.73s\n806:    learn: 3808.8433205 total: 1.17s    remaining: 1.73s\n807:    learn: 3808.6095214 total: 1.17s    remaining: 1.72s\n808:    learn: 3807.7139243 total: 1.17s    remaining: 1.72s\n809:    learn: 3807.5754253 total: 1.17s    remaining: 1.72s\n810:    learn: 3807.4133638 total: 1.17s    remaining: 1.72s\n811:    learn: 3805.7109740 total: 1.18s    remaining: 1.72s\n812:    learn: 3804.7089862 total: 1.18s    remaining: 1.72s\n813:    learn: 3802.8930509 total: 1.18s    remaining: 1.72s\n814:    learn: 3802.2797474 total: 1.18s    remaining: 1.72s\n815:    learn: 3800.9263311 total: 1.18s    remaining: 1.72s\n816:    learn: 3796.9644567 total: 1.19s    remaining: 1.72s\n817:    learn: 3796.9644567 total: 1.19s    remaining: 1.71s\n818:    learn: 3794.6321824 total: 1.19s    remaining: 1.71s\n819:    learn: 3793.7138810 total: 1.19s    remaining: 1.71s\n820:    learn: 3793.4292740 total: 1.19s    remaining: 1.71s\n821:    learn: 3793.4255576 total: 1.19s    remaining: 1.71s\n822:    learn: 3792.1343626 total: 1.19s    remaining: 1.71s\n823:    learn: 3790.4684901 total: 1.2s remaining: 1.71s\n824:    learn: 3790.0355937 total: 1.2s remaining: 1.71s\n825:    learn: 3789.5560775 total: 1.2s remaining: 1.7s\n826:    learn: 3787.8915354 total: 1.2s remaining: 1.7s\n827:    learn: 3787.5419872 total: 1.2s remaining: 1.7s\n828:    learn: 3784.9113968 total: 1.2s remaining: 1.7s\n829:    learn: 3784.9113968 total: 1.21s    remaining: 1.7s\n830:    learn: 3784.5295880 total: 1.21s    remaining: 1.7s\n831:    learn: 3784.5061352 total: 1.21s    remaining: 1.7s\n832:    learn: 3783.9559552 total: 1.21s    remaining: 1.7s\n833:    learn: 3783.2083733 total: 1.21s    remaining: 1.69s\n834:    learn: 3782.5879048 total: 1.21s    remaining: 1.69s\n835:    learn: 3778.7531050 total: 1.22s    remaining: 1.69s\n836:    learn: 3778.3606445 total: 1.22s    remaining: 1.69s\n837:    learn: 3777.5169044 total: 1.22s    remaining: 1.69s\n838:    learn: 3776.0781037 total: 1.22s    remaining: 1.69s\n839:    learn: 3774.1576914 total: 1.22s    remaining: 1.69s\n840:    learn: 3773.0961327 total: 1.23s    remaining: 1.69s\n841:    learn: 3769.1849336 total: 1.23s    remaining: 1.69s\n842:    learn: 3769.1849336 total: 1.23s    remaining: 1.69s\n843:    learn: 3768.5544138 total: 1.23s    remaining: 1.68s\n844:    learn: 3767.0540299 total: 1.23s    remaining: 1.68s\n845:    learn: 3766.5195021 total: 1.23s    remaining: 1.68s\n846:    learn: 3766.5195021 total: 1.23s    remaining: 1.68s\n847:    learn: 3766.4835237 total: 1.23s    remaining: 1.68s\n848:    learn: 3766.3146637 total: 1.24s    remaining: 1.68s\n849:    learn: 3766.3146637 total: 1.24s    remaining: 1.67s\n850:    learn: 3760.7193853 total: 1.24s    remaining: 1.67s\n851:    learn: 3760.2822988 total: 1.24s    remaining: 1.67s\n852:    learn: 3760.2822988 total: 1.24s    remaining: 1.67s\n853:    learn: 3760.1005594 total: 1.24s    remaining: 1.67s\n854:    learn: 3758.8854406 total: 1.24s    remaining: 1.67s\n855:    learn: 3758.2299296 total: 1.25s    remaining: 1.67s\n856:    learn: 3755.6860428 total: 1.25s    remaining: 1.66s\n857:    learn: 3754.4660145 total: 1.25s    remaining: 1.66s\n858:    learn: 3753.1371758 total: 1.25s    remaining: 1.66s\n859:    learn: 3752.4535790 total: 1.25s    remaining: 1.66s\n860:    learn: 3752.2871783 total: 1.25s    remaining: 1.66s\n861:    learn: 3751.5256924 total: 1.26s    remaining: 1.66s\n862:    learn: 3751.5209414 total: 1.26s    remaining: 1.66s\n863:    learn: 3751.2839240 total: 1.26s    remaining: 1.66s\n864:    learn: 3750.8896543 total: 1.26s    remaining: 1.65s\n865:    learn: 3748.9138530 total: 1.26s    remaining: 1.65s\n866:    learn: 3748.7446691 total: 1.26s    remaining: 1.65s\n867:    learn: 3748.2933914 total: 1.27s    remaining: 1.65s\n868:    learn: 3747.7771132 total: 1.27s    remaining: 1.65s\n869:    learn: 3747.4756519 total: 1.27s    remaining: 1.65s\n870:    learn: 3746.8798693 total: 1.27s    remaining: 1.65s\n871:    learn: 3746.3395775 total: 1.27s    remaining: 1.65s\n872:    learn: 3746.1296952 total: 1.27s    remaining: 1.65s\n873:    learn: 3743.5685571 total: 1.28s    remaining: 1.65s\n874:    learn: 3741.4152266 total: 1.28s    remaining: 1.64s\n875:    learn: 3741.2402215 total: 1.28s    remaining: 1.64s\n876:    learn: 3741.0882207 total: 1.28s    remaining: 1.64s\n877:    learn: 3740.1415409 total: 1.28s    remaining: 1.64s\n878:    learn: 3740.1413814 total: 1.28s    remaining: 1.64s\n879:    learn: 3738.9862194 total: 1.29s    remaining: 1.64s\n880:    learn: 3737.5337821 total: 1.29s    remaining: 1.64s\n881:    learn: 3737.5337821 total: 1.29s    remaining: 1.63s\n882:    learn: 3737.1581488 total: 1.29s    remaining: 1.63s\n883:    learn: 3736.7844566 total: 1.29s    remaining: 1.63s\n884:    learn: 3736.7844566 total: 1.29s    remaining: 1.63s\n885:    learn: 3736.7841652 total: 1.29s    remaining: 1.63s\n886:    learn: 3736.5406219 total: 1.3s remaining: 1.63s\n887:    learn: 3736.1152642 total: 1.3s remaining: 1.63s\n888:    learn: 3734.4118333 total: 1.3s remaining: 1.62s\n889:    learn: 3733.9724930 total: 1.3s remaining: 1.62s\n890:    learn: 3733.8000485 total: 1.3s remaining: 1.62s\n891:    learn: 3733.4601714 total: 1.3s remaining: 1.62s\n892:    learn: 3733.1641953 total: 1.31s    remaining: 1.62s\n893:    learn: 3733.0912557 total: 1.31s    remaining: 1.62s\n894:    learn: 3731.9732853 total: 1.31s    remaining: 1.62s\n895:    learn: 3731.4078934 total: 1.31s    remaining: 1.62s\n896:    learn: 3730.4823594 total: 1.31s    remaining: 1.62s\n897:    learn: 3730.4018892 total: 1.32s    remaining: 1.61s\n898:    learn: 3729.7012273 total: 1.32s    remaining: 1.61s\n899:    learn: 3729.2725886 total: 1.32s    remaining: 1.61s\n900:    learn: 3728.4962656 total: 1.32s    remaining: 1.61s\n901:    learn: 3728.2322230 total: 1.32s    remaining: 1.61s\n902:    learn: 3727.5962340 total: 1.32s    remaining: 1.61s\n903:    learn: 3726.0272566 total: 1.33s    remaining: 1.61s\n904:    learn: 3725.9333842 total: 1.33s    remaining: 1.61s\n905:    learn: 3724.8243608 total: 1.33s    remaining: 1.6s\n906:    learn: 3724.2181171 total: 1.33s    remaining: 1.6s\n907:    learn: 3724.2181171 total: 1.33s    remaining: 1.6s\n908:    learn: 3724.1997717 total: 1.33s    remaining: 1.6s\n909:    learn: 3722.5087637 total: 1.33s    remaining: 1.6s\n910:    learn: 3722.5087637 total: 1.33s    remaining: 1.6s\n911:    learn: 3722.3746981 total: 1.34s    remaining: 1.59s\n912:    learn: 3722.0076978 total: 1.34s    remaining: 1.59s\n913:    learn: 3721.3351238 total: 1.34s    remaining: 1.59s\n914:    learn: 3721.3351238 total: 1.34s    remaining: 1.59s\n915:    learn: 3719.7788144 total: 1.34s    remaining: 1.59s\n916:    learn: 3718.5757182 total: 1.34s    remaining: 1.59s\n917:    learn: 3717.0143275 total: 1.35s    remaining: 1.59s\n918:    learn: 3716.7013645 total: 1.35s    remaining: 1.58s\n919:    learn: 3716.7013645 total: 1.35s    remaining: 1.58s\n920:    learn: 3716.1538803 total: 1.35s    remaining: 1.58s\n921:    learn: 3715.6253818 total: 1.35s    remaining: 1.58s\n922:    learn: 3715.0529522 total: 1.35s    remaining: 1.58s\n923:    learn: 3714.5005523 total: 1.36s    remaining: 1.58s\n924:    learn: 3712.1508975 total: 1.36s    remaining: 1.58s\n925:    learn: 3711.6039564 total: 1.36s    remaining: 1.58s\n926:    learn: 3710.1385822 total: 1.36s    remaining: 1.58s\n927:    learn: 3708.9893699 total: 1.36s    remaining: 1.57s\n928:    learn: 3708.9768362 total: 1.36s    remaining: 1.57s\n929:    learn: 3708.9709846 total: 1.36s    remaining: 1.57s\n930:    learn: 3708.4679536 total: 1.37s    remaining: 1.57s\n931:    learn: 3707.2311610 total: 1.37s    remaining: 1.57s\n932:    learn: 3707.0972053 total: 1.37s    remaining: 1.57s\n933:    learn: 3706.8547678 total: 1.37s    remaining: 1.57s\n934:    learn: 3704.6011174 total: 1.37s    remaining: 1.56s\n935:    learn: 3702.5238173 total: 1.38s    remaining: 1.56s\n936:    learn: 3702.4828780 total: 1.38s    remaining: 1.56s\n937:    learn: 3702.4828780 total: 1.38s    remaining: 1.56s\n938:    learn: 3702.3967813 total: 1.38s    remaining: 1.56s\n939:    learn: 3702.0342639 total: 1.38s    remaining: 1.56s\n940:    learn: 3701.1780133 total: 1.38s    remaining: 1.55s\n941:    learn: 3700.7531695 total: 1.38s    remaining: 1.55s\n942:    learn: 3700.4658106 total: 1.39s    remaining: 1.55s\n943:    learn: 3699.2095210 total: 1.39s    remaining: 1.55s\n944:    learn: 3699.0562756 total: 1.39s    remaining: 1.55s\n945:    learn: 3699.0562756 total: 1.39s    remaining: 1.55s\n946:    learn: 3698.2595417 total: 1.39s    remaining: 1.55s\n947:    learn: 3695.5636725 total: 1.39s    remaining: 1.55s\n948:    learn: 3694.5819843 total: 1.4s remaining: 1.55s\n949:    learn: 3694.3462767 total: 1.4s remaining: 1.54s\n950:    learn: 3693.6615322 total: 1.4s remaining: 1.54s\n951:    learn: 3692.5638974 total: 1.4s remaining: 1.54s\n952:    learn: 3692.5638974 total: 1.4s remaining: 1.54s\n953:    learn: 3685.4910277 total: 1.4s remaining: 1.54s\n954:    learn: 3685.2059786 total: 1.4s remaining: 1.54s\n955:    learn: 3684.2455121 total: 1.41s    remaining: 1.54s\n956:    learn: 3683.7580229 total: 1.41s    remaining: 1.53s\n957:    learn: 3681.9457611 total: 1.41s    remaining: 1.53s\n958:    learn: 3681.5652050 total: 1.41s    remaining: 1.53s\n959:    learn: 3681.5652050 total: 1.41s    remaining: 1.53s\n960:    learn: 3680.2135370 total: 1.41s    remaining: 1.53s\n961:    learn: 3680.1767027 total: 1.42s    remaining: 1.53s\n962:    learn: 3679.8473836 total: 1.42s    remaining: 1.53s\n963:    learn: 3677.4207960 total: 1.42s    remaining: 1.52s\n964:    learn: 3677.4207960 total: 1.42s    remaining: 1.52s\n965:    learn: 3677.4188338 total: 1.42s    remaining: 1.52s\n966:    learn: 3677.4188338 total: 1.42s    remaining: 1.52s\n967:    learn: 3676.9216619 total: 1.42s    remaining: 1.52s\n968:    learn: 3675.3089857 total: 1.42s    remaining: 1.51s\n969:    learn: 3675.2688544 total: 1.43s    remaining: 1.51s\n970:    learn: 3674.0677886 total: 1.43s    remaining: 1.51s\n971:    learn: 3673.5915745 total: 1.43s    remaining: 1.51s\n972:    learn: 3673.3693200 total: 1.43s    remaining: 1.51s\n973:    learn: 3672.7873480 total: 1.43s    remaining: 1.51s\n974:    learn: 3671.8253357 total: 1.43s    remaining: 1.51s\n975:    learn: 3668.8709628 total: 1.44s    remaining: 1.51s\n976:    learn: 3668.8709628 total: 1.44s    remaining: 1.5s\n977:    learn: 3668.7627721 total: 1.44s    remaining: 1.5s\n978:    learn: 3667.4786633 total: 1.44s    remaining: 1.5s\n979:    learn: 3666.5861168 total: 1.44s    remaining: 1.5s\n980:    learn: 3666.3023313 total: 1.44s    remaining: 1.5s\n981:    learn: 3666.1529594 total: 1.44s    remaining: 1.5s\n982:    learn: 3664.1631253 total: 1.45s    remaining: 1.5s\n983:    learn: 3663.8921268 total: 1.45s    remaining: 1.5s\n984:    learn: 3663.5891686 total: 1.45s    remaining: 1.49s\n985:    learn: 3663.3478513 total: 1.45s    remaining: 1.49s\n986:    learn: 3663.3443644 total: 1.45s    remaining: 1.49s\n987:    learn: 3662.4739468 total: 1.45s    remaining: 1.49s\n988:    learn: 3659.7979207 total: 1.46s    remaining: 1.49s\n989:    learn: 3656.7774930 total: 1.46s    remaining: 1.49s\n990:    learn: 3655.5815035 total: 1.46s    remaining: 1.49s\n991:    learn: 3655.3199530 total: 1.46s    remaining: 1.48s\n992:    learn: 3653.0603803 total: 1.46s    remaining: 1.48s\n993:    learn: 3653.0603803 total: 1.46s    remaining: 1.48s\n994:    learn: 3652.4369568 total: 1.46s    remaining: 1.48s\n995:    learn: 3651.9836020 total: 1.47s    remaining: 1.48s\n996:    learn: 3649.7055373 total: 1.47s    remaining: 1.48s\n997:    learn: 3649.3310304 total: 1.47s    remaining: 1.48s\n998:    learn: 3648.6113397 total: 1.47s    remaining: 1.47s\n999:    learn: 3648.1952207 total: 1.47s    remaining: 1.47s\n1000:   learn: 3647.6434249 total: 1.48s    remaining: 1.47s\n1001:   learn: 3647.4992176 total: 1.48s    remaining: 1.47s\n1002:   learn: 3647.4992176 total: 1.48s    remaining: 1.47s\n1003:   learn: 3645.5814467 total: 1.48s    remaining: 1.47s\n1004:   learn: 3645.1610774 total: 1.48s    remaining: 1.47s\n1005:   learn: 3645.1130043 total: 1.48s    remaining: 1.47s\n1006:   learn: 3643.9491608 total: 1.48s    remaining: 1.46s\n1007:   learn: 3641.4554180 total: 1.49s    remaining: 1.46s\n1008:   learn: 3640.8504319 total: 1.49s    remaining: 1.46s\n1009:   learn: 3640.7715577 total: 1.49s    remaining: 1.46s\n1010:   learn: 3640.6094922 total: 1.49s    remaining: 1.46s\n1011:   learn: 3638.4023054 total: 1.49s    remaining: 1.46s\n1012:   learn: 3636.7756918 total: 1.49s    remaining: 1.46s\n1013:   learn: 3635.7462844 total: 1.5s remaining: 1.45s\n1014:   learn: 3635.4468132 total: 1.5s remaining: 1.45s\n1015:   learn: 3635.3983898 total: 1.5s remaining: 1.45s\n1016:   learn: 3635.1676270 total: 1.5s remaining: 1.45s\n1017:   learn: 3634.6068208 total: 1.5s remaining: 1.45s\n1018:   learn: 3633.7610803 total: 1.5s remaining: 1.45s\n1019:   learn: 3632.7942984 total: 1.51s    remaining: 1.45s\n1020:   learn: 3630.7096665 total: 1.51s    remaining: 1.45s\n1021:   learn: 3629.6153402 total: 1.51s    remaining: 1.45s\n1022:   learn: 3629.5740763 total: 1.51s    remaining: 1.44s\n1023:   learn: 3629.3666657 total: 1.51s    remaining: 1.44s\n1024:   learn: 3628.5367924 total: 1.51s    remaining: 1.44s\n1025:   learn: 3627.3209578 total: 1.52s    remaining: 1.44s\n1026:   learn: 3627.0698097 total: 1.52s    remaining: 1.44s\n1027:   learn: 3624.6297907 total: 1.52s    remaining: 1.44s\n1028:   learn: 3624.2137850 total: 1.52s    remaining: 1.44s\n1029:   learn: 3623.3317266 total: 1.52s    remaining: 1.43s\n1030:   learn: 3623.0141339 total: 1.52s    remaining: 1.43s\n1031:   learn: 3622.7357887 total: 1.53s    remaining: 1.43s\n1032:   learn: 3622.3153971 total: 1.53s    remaining: 1.43s\n1033:   learn: 3621.7364889 total: 1.53s    remaining: 1.43s\n1034:   learn: 3621.2690431 total: 1.53s    remaining: 1.43s\n1035:   learn: 3619.7498802 total: 1.53s    remaining: 1.43s\n1036:   learn: 3617.4767168 total: 1.54s    remaining: 1.43s\n1037:   learn: 3617.4767168 total: 1.54s    remaining: 1.42s\n1038:   learn: 3616.3999831 total: 1.54s    remaining: 1.42s\n1039:   learn: 3616.3999831 total: 1.54s    remaining: 1.42s\n1040:   learn: 3616.3999831 total: 1.54s    remaining: 1.42s\n1041:   learn: 3615.6974691 total: 1.54s    remaining: 1.42s\n1042:   learn: 3613.3498141 total: 1.54s    remaining: 1.42s\n1043:   learn: 3613.3498141 total: 1.54s    remaining: 1.41s\n1044:   learn: 3612.5324436 total: 1.55s    remaining: 1.41s\n1045:   learn: 3611.6863959 total: 1.55s    remaining: 1.41s\n1046:   learn: 3611.6110633 total: 1.55s    remaining: 1.41s\n1047:   learn: 3611.3409241 total: 1.55s    remaining: 1.41s\n1048:   learn: 3609.1333550 total: 1.55s    remaining: 1.41s\n1049:   learn: 3607.2325683 total: 1.55s    remaining: 1.41s\n1050:   learn: 3606.8956123 total: 1.56s    remaining: 1.41s\n1051:   learn: 3606.0445751 total: 1.56s    remaining: 1.4s\n1052:   learn: 3604.8280679 total: 1.56s    remaining: 1.4s\n1053:   learn: 3603.0442265 total: 1.56s    remaining: 1.4s\n1054:   learn: 3603.0414058 total: 1.56s    remaining: 1.4s\n1055:   learn: 3602.5215798 total: 1.56s    remaining: 1.4s\n1056:   learn: 3601.7869199 total: 1.57s    remaining: 1.4s\n1057:   learn: 3601.4903778 total: 1.57s    remaining: 1.4s\n1058:   learn: 3601.4903778 total: 1.57s    remaining: 1.39s\n1059:   learn: 3600.3736078 total: 1.57s    remaining: 1.39s\n1060:   learn: 3599.3800476 total: 1.57s    remaining: 1.39s\n1061:   learn: 3599.1596399 total: 1.57s    remaining: 1.39s\n1062:   learn: 3598.7990757 total: 1.57s    remaining: 1.39s\n1063:   learn: 3597.4087900 total: 1.58s    remaining: 1.39s\n1064:   learn: 3597.2450173 total: 1.58s    remaining: 1.39s\n1065:   learn: 3596.9727837 total: 1.58s    remaining: 1.39s\n1066:   learn: 3596.3506297 total: 1.58s    remaining: 1.38s\n1067:   learn: 3595.2715205 total: 1.58s    remaining: 1.38s\n1068:   learn: 3593.1006259 total: 1.59s    remaining: 1.38s\n1069:   learn: 3592.5959892 total: 1.59s    remaining: 1.38s\n1070:   learn: 3592.5883966 total: 1.59s    remaining: 1.38s\n1071:   learn: 3592.5343152 total: 1.59s    remaining: 1.38s\n1072:   learn: 3592.4848819 total: 1.59s    remaining: 1.38s\n1073:   learn: 3592.4848819 total: 1.59s    remaining: 1.38s\n1074:   learn: 3591.5370427 total: 1.6s remaining: 1.37s\n1075:   learn: 3591.1534654 total: 1.6s remaining: 1.37s\n1076:   learn: 3591.0191414 total: 1.6s remaining: 1.37s\n1077:   learn: 3590.7011409 total: 1.6s remaining: 1.37s\n1078:   learn: 3590.3736424 total: 1.6s remaining: 1.37s\n1079:   learn: 3589.1039583 total: 1.61s    remaining: 1.37s\n1080:   learn: 3588.7743905 total: 1.61s    remaining: 1.37s\n1081:   learn: 3588.5977644 total: 1.61s    remaining: 1.36s\n1082:   learn: 3588.2087809 total: 1.61s    remaining: 1.36s\n1083:   learn: 3587.7235973 total: 1.61s    remaining: 1.36s\n1084:   learn: 3587.3342363 total: 1.62s    remaining: 1.36s\n1085:   learn: 3587.2197299 total: 1.62s    remaining: 1.36s\n1086:   learn: 3587.2138956 total: 1.62s    remaining: 1.36s\n1087:   learn: 3586.5718200 total: 1.62s    remaining: 1.36s\n1088:   learn: 3586.3580947 total: 1.62s    remaining: 1.36s\n1089:   learn: 3585.3864547 total: 1.62s    remaining: 1.36s\n1090:   learn: 3581.8700961 total: 1.63s    remaining: 1.35s\n1091:   learn: 3581.4482743 total: 1.63s    remaining: 1.35s\n1092:   learn: 3581.4247096 total: 1.63s    remaining: 1.35s\n1093:   learn: 3581.0372065 total: 1.63s    remaining: 1.35s\n1094:   learn: 3579.1062612 total: 1.63s    remaining: 1.35s\n1095:   learn: 3579.1062612 total: 1.63s    remaining: 1.35s\n1096:   learn: 3578.4765793 total: 1.64s    remaining: 1.35s\n1097:   learn: 3578.2423896 total: 1.64s    remaining: 1.35s\n1098:   learn: 3578.1712965 total: 1.64s    remaining: 1.34s\n1099:   learn: 3575.3026950 total: 1.64s    remaining: 1.34s\n1100:   learn: 3575.1259486 total: 1.64s    remaining: 1.34s\n1101:   learn: 3573.3194123 total: 1.65s    remaining: 1.34s\n1102:   learn: 3573.3081655 total: 1.65s    remaining: 1.34s\n1103:   learn: 3573.2685013 total: 1.65s    remaining: 1.34s\n1104:   learn: 3573.2685013 total: 1.65s    remaining: 1.34s\n1105:   learn: 3572.4575298 total: 1.65s    remaining: 1.33s\n1106:   learn: 3571.8274230 total: 1.65s    remaining: 1.33s\n1107:   learn: 3571.6342525 total: 1.66s    remaining: 1.33s\n1108:   learn: 3571.0126053 total: 1.66s    remaining: 1.33s\n1109:   learn: 3569.9617541 total: 1.66s    remaining: 1.33s\n1110:   learn: 3569.7106094 total: 1.66s    remaining: 1.33s\n1111:   learn: 3569.5235690 total: 1.66s    remaining: 1.33s\n1112:   learn: 3566.0157600 total: 1.66s    remaining: 1.33s\n1113:   learn: 3564.8566264 total: 1.67s    remaining: 1.32s\n1114:   learn: 3563.8169358 total: 1.67s    remaining: 1.32s\n1115:   learn: 3563.1418067 total: 1.67s    remaining: 1.32s\n1116:   learn: 3562.7999182 total: 1.67s    remaining: 1.32s\n1117:   learn: 3562.6120048 total: 1.67s    remaining: 1.32s\n1118:   learn: 3561.7899379 total: 1.68s    remaining: 1.32s\n1119:   learn: 3561.5663789 total: 1.68s    remaining: 1.32s\n1120:   learn: 3561.5653418 total: 1.68s    remaining: 1.32s\n1121:   learn: 3560.3072786 total: 1.68s    remaining: 1.31s\n1122:   learn: 3559.8587073 total: 1.68s    remaining: 1.31s\n1123:   learn: 3559.8570120 total: 1.68s    remaining: 1.31s\n1124:   learn: 3559.6594204 total: 1.69s    remaining: 1.31s\n1125:   learn: 3557.8153776 total: 1.69s    remaining: 1.31s\n1126:   learn: 3557.6496824 total: 1.69s    remaining: 1.31s\n1127:   learn: 3557.6496824 total: 1.69s    remaining: 1.31s\n1128:   learn: 3557.6496824 total: 1.69s    remaining: 1.3s\n1129:   learn: 3557.2345494 total: 1.69s    remaining: 1.3s\n1130:   learn: 3556.5439816 total: 1.7s remaining: 1.3s\n1131:   learn: 3555.8897573 total: 1.7s remaining: 1.3s\n1132:   learn: 3555.5143977 total: 1.7s remaining: 1.3s\n1133:   learn: 3554.2410994 total: 1.7s remaining: 1.3s\n1134:   learn: 3554.0107634 total: 1.71s    remaining: 1.3s\n1135:   learn: 3553.9615743 total: 1.71s    remaining: 1.3s\n1136:   learn: 3551.0941172 total: 1.71s    remaining: 1.3s\n1137:   learn: 3550.3466574 total: 1.71s    remaining: 1.3s\n1138:   learn: 3550.0225315 total: 1.72s    remaining: 1.3s\n1139:   learn: 3550.0225315 total: 1.72s    remaining: 1.29s\n1140:   learn: 3548.6873160 total: 1.72s    remaining: 1.29s\n1141:   learn: 3547.3571996 total: 1.72s    remaining: 1.29s\n1142:   learn: 3546.2323811 total: 1.73s    remaining: 1.29s\n1143:   learn: 3545.2744841 total: 1.73s    remaining: 1.29s\n1144:   learn: 3545.1597357 total: 1.73s    remaining: 1.29s\n1145:   learn: 3545.1597357 total: 1.73s    remaining: 1.29s\n1146:   learn: 3544.9196089 total: 1.73s    remaining: 1.29s\n1147:   learn: 3542.6693975 total: 1.74s    remaining: 1.29s\n1148:   learn: 3542.6693975 total: 1.74s    remaining: 1.29s\n1149:   learn: 3542.0170374 total: 1.74s    remaining: 1.29s\n1150:   learn: 3541.9987886 total: 1.74s    remaining: 1.29s\n1151:   learn: 3541.4564659 total: 1.75s    remaining: 1.29s\n1152:   learn: 3537.7902406 total: 1.75s    remaining: 1.29s\n1153:   learn: 3535.9440933 total: 1.75s    remaining: 1.29s\n1154:   learn: 3535.3618591 total: 1.76s    remaining: 1.29s\n1155:   learn: 3534.0038064 total: 1.76s    remaining: 1.29s\n1156:   learn: 3528.8624480 total: 1.77s    remaining: 1.29s\n1157:   learn: 3528.8624480 total: 1.77s    remaining: 1.28s\n1158:   learn: 3528.8624480 total: 1.77s    remaining: 1.28s\n1159:   learn: 3528.5705897 total: 1.77s    remaining: 1.28s\n1160:   learn: 3527.7039807 total: 1.77s    remaining: 1.28s\n1161:   learn: 3526.8718806 total: 1.78s    remaining: 1.28s\n1162:   learn: 3525.2002199 total: 1.78s    remaining: 1.28s\n1163:   learn: 3524.7384686 total: 1.78s    remaining: 1.28s\n1164:   learn: 3521.6204101 total: 1.78s    remaining: 1.28s\n1165:   learn: 3521.0234755 total: 1.8s remaining: 1.29s\n1166:   learn: 3516.6497800 total: 1.8s remaining: 1.29s\n1167:   learn: 3516.0855787 total: 1.81s    remaining: 1.29s\n1168:   learn: 3515.7976760 total: 1.81s    remaining: 1.29s\n1169:   learn: 3515.5704574 total: 1.81s    remaining: 1.29s\n1170:   learn: 3515.1427513 total: 1.82s    remaining: 1.29s\n1171:   learn: 3515.0602849 total: 1.82s    remaining: 1.28s\n1172:   learn: 3514.7638701 total: 1.82s    remaining: 1.28s\n1173:   learn: 3513.5698655 total: 1.82s    remaining: 1.28s\n1174:   learn: 3513.0935427 total: 1.82s    remaining: 1.28s\n1175:   learn: 3512.7069115 total: 1.83s    remaining: 1.28s\n1176:   learn: 3512.2088626 total: 1.83s    remaining: 1.28s\n1177:   learn: 3511.8630839 total: 1.83s    remaining: 1.28s\n1178:   learn: 3510.1118571 total: 1.83s    remaining: 1.28s\n1179:   learn: 3509.9803469 total: 1.84s    remaining: 1.28s\n1180:   learn: 3509.6869391 total: 1.84s    remaining: 1.27s\n1181:   learn: 3507.4650152 total: 1.84s    remaining: 1.27s\n1182:   learn: 3506.7078692 total: 1.84s    remaining: 1.27s\n1183:   learn: 3504.7656726 total: 1.85s    remaining: 1.27s\n1184:   learn: 3503.8159906 total: 1.85s    remaining: 1.27s\n1185:   learn: 3503.8159906 total: 1.85s    remaining: 1.27s\n1186:   learn: 3503.6273305 total: 1.85s    remaining: 1.27s\n1187:   learn: 3503.6273305 total: 1.85s    remaining: 1.27s\n1188:   learn: 3502.9832157 total: 1.85s    remaining: 1.26s\n1189:   learn: 3502.7191578 total: 1.86s    remaining: 1.26s\n1190:   learn: 3501.7102875 total: 1.86s    remaining: 1.26s\n1191:   learn: 3501.6303065 total: 1.86s    remaining: 1.26s\n1192:   learn: 3501.6303065 total: 1.86s    remaining: 1.26s\n1193:   learn: 3500.2342721 total: 1.86s    remaining: 1.26s\n1194:   learn: 3498.5046870 total: 1.87s    remaining: 1.26s\n1195:   learn: 3497.4228105 total: 1.87s    remaining: 1.26s\n1196:   learn: 3497.4228105 total: 1.87s    remaining: 1.25s\n1197:   learn: 3496.7478212 total: 1.87s    remaining: 1.25s\n1198:   learn: 3495.8283729 total: 1.88s    remaining: 1.25s\n1199:   learn: 3495.8227030 total: 1.88s    remaining: 1.25s\n1200:   learn: 3495.8195457 total: 1.88s    remaining: 1.25s\n1201:   learn: 3495.8111827 total: 1.88s    remaining: 1.25s\n1202:   learn: 3495.3319656 total: 1.88s    remaining: 1.25s\n1203:   learn: 3494.7155581 total: 1.88s    remaining: 1.25s\n1204:   learn: 3494.7155581 total: 1.88s    remaining: 1.24s\n1205:   learn: 3494.4485050 total: 1.89s    remaining: 1.24s\n1206:   learn: 3493.8914014 total: 1.89s    remaining: 1.24s\n1207:   learn: 3493.4939211 total: 1.89s    remaining: 1.24s\n1208:   learn: 3493.3499219 total: 1.89s    remaining: 1.24s\n1209:   learn: 3492.0621516 total: 1.89s    remaining: 1.24s\n1210:   learn: 3491.9988867 total: 1.9s remaining: 1.24s\n1211:   learn: 3491.9983620 total: 1.9s remaining: 1.23s\n1212:   learn: 3489.3417180 total: 1.9s remaining: 1.23s\n1213:   learn: 3489.1380101 total: 1.9s remaining: 1.23s\n1214:   learn: 3489.1380101 total: 1.9s remaining: 1.23s\n1215:   learn: 3489.1380101 total: 1.9s remaining: 1.23s\n1216:   learn: 3489.1340465 total: 1.91s    remaining: 1.23s\n1217:   learn: 3489.1340465 total: 1.91s    remaining: 1.22s\n1218:   learn: 3489.1340465 total: 1.91s    remaining: 1.22s\n1219:   learn: 3489.0039633 total: 1.91s    remaining: 1.22s\n1220:   learn: 3489.0039633 total: 1.91s    remaining: 1.22s\n1221:   learn: 3488.9564392 total: 1.91s    remaining: 1.22s\n1222:   learn: 3488.9564392 total: 1.92s    remaining: 1.22s\n1223:   learn: 3488.9564392 total: 1.92s    remaining: 1.21s\n1224:   learn: 3487.9908790 total: 1.92s    remaining: 1.21s\n1225:   learn: 3487.7033151 total: 1.92s    remaining: 1.21s\n1226:   learn: 3487.4716292 total: 1.92s    remaining: 1.21s\n1227:   learn: 3487.4421957 total: 1.93s    remaining: 1.21s\n1228:   learn: 3487.4417232 total: 1.93s    remaining: 1.21s\n1229:   learn: 3486.1161129 total: 1.93s    remaining: 1.21s\n1230:   learn: 3485.3868887 total: 1.93s    remaining: 1.21s\n1231:   learn: 3485.0605288 total: 1.94s    remaining: 1.21s\n1232:   learn: 3485.0067723 total: 1.94s    remaining: 1.21s\n1233:   learn: 3483.1809236 total: 1.94s    remaining: 1.2s\n1234:   learn: 3481.3149412 total: 1.94s    remaining: 1.2s\n1235:   learn: 3481.1425501 total: 1.95s    remaining: 1.2s\n1236:   learn: 3481.1425501 total: 1.95s    remaining: 1.2s\n1237:   learn: 3480.8921921 total: 1.95s    remaining: 1.2s\n1238:   learn: 3480.3199413 total: 1.95s    remaining: 1.2s\n1239:   learn: 3480.3199413 total: 1.96s    remaining: 1.2s\n1240:   learn: 3479.3737682 total: 1.96s    remaining: 1.2s\n1241:   learn: 3478.0905011 total: 1.96s    remaining: 1.2s\n1242:   learn: 3476.7956632 total: 1.96s    remaining: 1.2s\n1243:   learn: 3476.3331213 total: 1.97s    remaining: 1.2s\n1244:   learn: 3476.0137821 total: 1.97s    remaining: 1.2s\n1245:   learn: 3473.3929457 total: 1.97s    remaining: 1.19s\n1246:   learn: 3473.3929457 total: 1.97s    remaining: 1.19s\n1247:   learn: 3468.8674239 total: 1.98s    remaining: 1.19s\n1248:   learn: 3468.5034134 total: 1.98s    remaining: 1.19s\n1249:   learn: 3468.5034134 total: 1.98s    remaining: 1.19s\n1250:   learn: 3468.3679050 total: 1.98s    remaining: 1.19s\n1251:   learn: 3468.3679050 total: 1.98s    remaining: 1.18s\n1252:   learn: 3467.6336042 total: 1.98s    remaining: 1.18s\n1253:   learn: 3466.0650105 total: 1.99s    remaining: 1.18s\n1254:   learn: 3465.5514242 total: 1.99s    remaining: 1.18s\n1255:   learn: 3464.7760892 total: 1.99s    remaining: 1.18s\n1256:   learn: 3464.7760892 total: 1.99s    remaining: 1.18s\n1257:   learn: 3464.7052528 total: 1.99s    remaining: 1.17s\n1258:   learn: 3464.2389179 total: 1.99s    remaining: 1.17s\n1259:   learn: 3464.2389179 total: 1.99s    remaining: 1.17s\n1260:   learn: 3463.7080189 total: 2s   remaining: 1.17s\n1261:   learn: 3463.6448661 total: 2s   remaining: 1.17s\n1262:   learn: 3463.4723039 total: 2s   remaining: 1.17s\n1263:   learn: 3463.4214435 total: 2s   remaining: 1.17s\n1264:   learn: 3462.4528015 total: 2.01s    remaining: 1.17s\n1265:   learn: 3462.3418798 total: 2.01s    remaining: 1.16s\n1266:   learn: 3461.0735834 total: 2.01s    remaining: 1.16s\n1267:   learn: 3460.9924441 total: 2.01s    remaining: 1.16s\n1268:   learn: 3460.8876125 total: 2.02s    remaining: 1.16s\n1269:   learn: 3460.4933662 total: 2.02s    remaining: 1.16s\n1270:   learn: 3460.4919842 total: 2.02s    remaining: 1.16s\n1271:   learn: 3460.4035048 total: 2.02s    remaining: 1.16s\n1272:   learn: 3460.4035048 total: 2.02s    remaining: 1.15s\n1273:   learn: 3459.3359575 total: 2.02s    remaining: 1.15s\n1274:   learn: 3457.8199183 total: 2.02s    remaining: 1.15s\n1275:   learn: 3457.4863670 total: 2.03s    remaining: 1.15s\n1276:   learn: 3455.8452690 total: 2.03s    remaining: 1.15s\n1277:   learn: 3455.0689443 total: 2.03s    remaining: 1.15s\n1278:   learn: 3455.0689443 total: 2.03s    remaining: 1.14s\n1279:   learn: 3454.6749412 total: 2.03s    remaining: 1.14s\n1280:   learn: 3453.5278084 total: 2.04s    remaining: 1.14s\n1281:   learn: 3453.3104756 total: 2.04s    remaining: 1.14s\n1282:   learn: 3453.0127466 total: 2.04s    remaining: 1.14s\n1283:   learn: 3452.9312517 total: 2.04s    remaining: 1.14s\n1284:   learn: 3452.6055727 total: 2.04s    remaining: 1.14s\n1285:   learn: 3451.5120403 total: 2.04s    remaining: 1.13s\n1286:   learn: 3450.9189052 total: 2.04s    remaining: 1.13s\n1287:   learn: 3450.9154103 total: 2.05s    remaining: 1.13s\n1288:   learn: 3449.6982402 total: 2.05s    remaining: 1.13s\n1289:   learn: 3449.0524982 total: 2.05s    remaining: 1.13s\n1290:   learn: 3448.8385412 total: 2.05s    remaining: 1.13s\n1291:   learn: 3448.8385412 total: 2.05s    remaining: 1.12s\n1292:   learn: 3448.6875173 total: 2.05s    remaining: 1.12s\n1293:   learn: 3447.7991746 total: 2.06s    remaining: 1.12s\n1294:   learn: 3446.8332555 total: 2.06s    remaining: 1.12s\n1295:   learn: 3446.7066740 total: 2.06s    remaining: 1.12s\n1296:   learn: 3446.7066740 total: 2.06s    remaining: 1.12s\n1297:   learn: 3440.0124138 total: 2.06s    remaining: 1.11s\n1298:   learn: 3439.1136582 total: 2.06s    remaining: 1.11s\n1299:   learn: 3438.8334759 total: 2.06s    remaining: 1.11s\n1300:   learn: 3437.5049112 total: 2.07s    remaining: 1.11s\n1301:   learn: 3436.4869249 total: 2.07s    remaining: 1.11s\n1302:   learn: 3436.3743441 total: 2.07s    remaining: 1.11s\n1303:   learn: 3436.3743441 total: 2.07s    remaining: 1.1s\n1304:   learn: 3436.3743441 total: 2.07s    remaining: 1.1s\n1305:   learn: 3436.3743441 total: 2.07s    remaining: 1.1s\n1306:   learn: 3435.9674258 total: 2.07s    remaining: 1.1s\n1307:   learn: 3435.7246142 total: 2.07s    remaining: 1.1s\n1308:   learn: 3435.6276167 total: 2.08s    remaining: 1.09s\n1309:   learn: 3434.9562900 total: 2.08s    remaining: 1.09s\n1310:   learn: 3433.9987724 total: 2.08s    remaining: 1.09s\n1311:   learn: 3433.6087305 total: 2.08s    remaining: 1.09s\n1312:   learn: 3433.4416265 total: 2.08s    remaining: 1.09s\n1313:   learn: 3433.4416265 total: 2.08s    remaining: 1.09s\n1314:   learn: 3432.1607968 total: 2.08s    remaining: 1.08s\n1315:   learn: 3432.1607968 total: 2.08s    remaining: 1.08s\n1316:   learn: 3431.2901737 total: 2.09s    remaining: 1.08s\n1317:   learn: 3429.9883872 total: 2.09s    remaining: 1.08s\n1318:   learn: 3429.9284206 total: 2.09s    remaining: 1.08s\n1319:   learn: 3429.8485066 total: 2.09s    remaining: 1.08s\n1320:   learn: 3429.8485066 total: 2.09s    remaining: 1.07s\n1321:   learn: 3428.8791224 total: 2.09s    remaining: 1.07s\n1322:   learn: 3428.1362123 total: 2.1s remaining: 1.07s\n1323:   learn: 3426.5251308 total: 2.1s remaining: 1.07s\n1324:   learn: 3426.2746341 total: 2.1s remaining: 1.07s\n1325:   learn: 3426.2746341 total: 2.1s remaining: 1.07s\n1326:   learn: 3426.2746341 total: 2.1s remaining: 1.06s\n1327:   learn: 3425.9170980 total: 2.1s remaining: 1.06s\n1328:   learn: 3425.3611482 total: 2.1s remaining: 1.06s\n1329:   learn: 3424.4659138 total: 2.1s remaining: 1.06s\n1330:   learn: 3423.9350526 total: 2.1s remaining: 1.06s\n1331:   learn: 3422.9492344 total: 2.11s    remaining: 1.06s\n1332:   learn: 3421.7893589 total: 2.11s    remaining: 1.05s\n1333:   learn: 3421.5134698 total: 2.11s    remaining: 1.05s\n1334:   learn: 3421.2251437 total: 2.11s    remaining: 1.05s\n1335:   learn: 3420.8226376 total: 2.11s    remaining: 1.05s\n1336:   learn: 3420.5664151 total: 2.12s    remaining: 1.05s\n1337:   learn: 3420.5664151 total: 2.12s    remaining: 1.05s\n1338:   learn: 3420.5664151 total: 2.12s    remaining: 1.04s\n1339:   learn: 3420.5664151 total: 2.12s    remaining: 1.04s\n1340:   learn: 3419.8780256 total: 2.12s    remaining: 1.04s\n1341:   learn: 3419.0746453 total: 2.12s    remaining: 1.04s\n1342:   learn: 3419.0746453 total: 2.12s    remaining: 1.04s\n1343:   learn: 3419.0734552 total: 2.12s    remaining: 1.03s\n1344:   learn: 3418.4145722 total: 2.12s    remaining: 1.03s\n1345:   learn: 3418.1149355 total: 2.12s    remaining: 1.03s\n1346:   learn: 3417.9532152 total: 2.13s    remaining: 1.03s\n1347:   learn: 3417.5387633 total: 2.13s    remaining: 1.03s\n1348:   learn: 3416.8310578 total: 2.13s    remaining: 1.03s\n1349:   learn: 3416.8310578 total: 2.13s    remaining: 1.02s\n1350:   learn: 3416.8310578 total: 2.13s    remaining: 1.02s\n1351:   learn: 3416.8310578 total: 2.13s    remaining: 1.02s\n1352:   learn: 3416.2403108 total: 2.13s    remaining: 1.02s\n1353:   learn: 3415.5193738 total: 2.13s    remaining: 1.02s\n1354:   learn: 3414.9793795 total: 2.13s    remaining: 1.02s\n1355:   learn: 3414.8846548 total: 2.14s    remaining: 1.01s\n1356:   learn: 3414.6135055 total: 2.14s    remaining: 1.01s\n1357:   learn: 3414.4637963 total: 2.14s    remaining: 1.01s\n1358:   learn: 3414.1424451 total: 2.14s    remaining: 1.01s\n1359:   learn: 3414.1235906 total: 2.14s    remaining: 1.01s\n1360:   learn: 3413.8978249 total: 2.15s    remaining: 1.01s\n1361:   learn: 3413.8978249 total: 2.15s    remaining: 1s\n1362:   learn: 3413.2717749 total: 2.15s    remaining: 1s\n1363:   learn: 3413.1662130 total: 2.15s    remaining: 1s\n1364:   learn: 3413.1662130 total: 2.15s    remaining: 1s\n1365:   learn: 3412.5539553 total: 2.15s    remaining: 999ms\n1366:   learn: 3412.5539553 total: 2.15s    remaining: 997ms\n1367:   learn: 3412.2162145 total: 2.15s    remaining: 995ms\n1368:   learn: 3411.7199549 total: 2.16s    remaining: 994ms\n1369:   learn: 3410.7213858 total: 2.16s    remaining: 992ms\n1370:   learn: 3410.7213858 total: 2.16s    remaining: 990ms\n1371:   learn: 3410.7130116 total: 2.16s    remaining: 989ms\n1372:   learn: 3409.6714120 total: 2.16s    remaining: 987ms\n1373:   learn: 3407.0469234 total: 2.16s    remaining: 985ms\n1374:   learn: 3406.8334734 total: 2.16s    remaining: 984ms\n1375:   learn: 3405.7944436 total: 2.17s    remaining: 982ms\n1376:   learn: 3404.3862481 total: 2.17s    remaining: 981ms\n1377:   learn: 3404.1123839 total: 2.17s    remaining: 979ms\n1378:   learn: 3403.9273859 total: 2.17s    remaining: 978ms\n1379:   learn: 3403.8005365 total: 2.17s    remaining: 976ms\n1380:   learn: 3403.4403922 total: 2.17s    remaining: 975ms\n1381:   learn: 3402.6081195 total: 2.17s    remaining: 973ms\n1382:   learn: 3402.1766047 total: 2.18s    remaining: 971ms\n1383:   learn: 3402.0382933 total: 2.18s    remaining: 970ms\n1384:   learn: 3401.9022868 total: 2.18s    remaining: 968ms\n1385:   learn: 3401.9022868 total: 2.18s    remaining: 966ms\n1386:   learn: 3401.4068753 total: 2.18s    remaining: 965ms\n1387:   learn: 3400.9534708 total: 2.18s    remaining: 963ms\n1388:   learn: 3400.0592208 total: 2.19s    remaining: 962ms\n1389:   learn: 3399.9068631 total: 2.19s    remaining: 960ms\n1390:   learn: 3399.7704808 total: 2.19s    remaining: 959ms\n1391:   learn: 3398.3180640 total: 2.19s    remaining: 957ms\n1392:   learn: 3398.1779493 total: 2.19s    remaining: 955ms\n1393:   learn: 3397.4052751 total: 2.19s    remaining: 954ms\n1394:   learn: 3397.3551202 total: 2.19s    remaining: 952ms\n1395:   learn: 3397.3551202 total: 2.2s remaining: 950ms\n1396:   learn: 3397.3032297 total: 2.2s remaining: 949ms\n1397:   learn: 3396.4413072 total: 2.2s remaining: 947ms\n1398:   learn: 3396.1246833 total: 2.2s remaining: 946ms\n1399:   learn: 3396.1246833 total: 2.2s remaining: 944ms\n1400:   learn: 3393.3386727 total: 2.2s remaining: 943ms\n1401:   learn: 3393.0818296 total: 2.21s    remaining: 941ms\n1402:   learn: 3392.3444709 total: 2.21s    remaining: 940ms\n1403:   learn: 3392.3188682 total: 2.21s    remaining: 938ms\n1404:   learn: 3392.3180694 total: 2.21s    remaining: 936ms\n1405:   learn: 3391.6507938 total: 2.21s    remaining: 934ms\n1406:   learn: 3391.3211071 total: 2.21s    remaining: 933ms\n1407:   learn: 3391.3199251 total: 2.21s    remaining: 931ms\n1408:   learn: 3390.2805332 total: 2.22s    remaining: 930ms\n1409:   learn: 3389.8258158 total: 2.22s    remaining: 928ms\n1410:   learn: 3388.7130263 total: 2.22s    remaining: 927ms\n1411:   learn: 3386.4266049 total: 2.22s    remaining: 925ms\n1412:   learn: 3385.1734910 total: 2.22s    remaining: 924ms\n1413:   learn: 3381.9668231 total: 2.23s    remaining: 923ms\n1414:   learn: 3381.3037243 total: 2.23s    remaining: 921ms\n1415:   learn: 3381.2345977 total: 2.23s    remaining: 920ms\n1416:   learn: 3380.8300046 total: 2.23s    remaining: 919ms\n1417:   learn: 3377.9959849 total: 2.23s    remaining: 917ms\n1418:   learn: 3377.2197494 total: 2.24s    remaining: 916ms\n1419:   learn: 3377.0327590 total: 2.24s    remaining: 914ms\n1420:   learn: 3377.0327590 total: 2.24s    remaining: 912ms\n1421:   learn: 3376.5109946 total: 2.24s    remaining: 911ms\n1422:   learn: 3376.5038494 total: 2.24s    remaining: 909ms\n1423:   learn: 3375.7564955 total: 2.24s    remaining: 908ms\n1424:   learn: 3375.6156596 total: 2.25s    remaining: 906ms\n1425:   learn: 3375.4890280 total: 2.25s    remaining: 905ms\n1426:   learn: 3375.1804910 total: 2.25s    remaining: 904ms\n1427:   learn: 3375.1536405 total: 2.25s    remaining: 903ms\n1428:   learn: 3375.0401643 total: 2.25s    remaining: 901ms\n1429:   learn: 3374.4391521 total: 2.26s    remaining: 899ms\n1430:   learn: 3374.1964831 total: 2.26s    remaining: 898ms\n1431:   learn: 3374.1497595 total: 2.26s    remaining: 897ms\n1432:   learn: 3374.1493869 total: 2.26s    remaining: 895ms\n1433:   learn: 3373.8020310 total: 2.26s    remaining: 893ms\n1434:   learn: 3373.8020310 total: 2.26s    remaining: 891ms\n1435:   learn: 3373.8020310 total: 2.26s    remaining: 889ms\n1436:   learn: 3373.8020310 total: 2.26s    remaining: 887ms\n1437:   learn: 3373.7951891 total: 2.27s    remaining: 886ms\n1438:   learn: 3373.7019291 total: 2.27s    remaining: 884ms\n1439:   learn: 3371.3433288 total: 2.27s    remaining: 883ms\n1440:   learn: 3371.3234119 total: 2.27s    remaining: 881ms\n1441:   learn: 3371.0762139 total: 2.27s    remaining: 880ms\n1442:   learn: 3370.8838681 total: 2.28s    remaining: 879ms\n1443:   learn: 3370.1963553 total: 2.28s    remaining: 877ms\n1444:   learn: 3369.8998741 total: 2.28s    remaining: 876ms\n1445:   learn: 3368.6938579 total: 2.28s    remaining: 875ms\n1446:   learn: 3368.4789147 total: 2.28s    remaining: 873ms\n1447:   learn: 3368.3351815 total: 2.29s    remaining: 871ms\n1448:   learn: 3366.8936917 total: 2.29s    remaining: 870ms\n1449:   learn: 3366.8388420 total: 2.29s    remaining: 868ms\n1450:   learn: 3366.7271114 total: 2.29s    remaining: 867ms\n1451:   learn: 3366.7271114 total: 2.29s    remaining: 865ms\n1452:   learn: 3366.7271114 total: 2.29s    remaining: 863ms\n1453:   learn: 3366.7261233 total: 2.29s    remaining: 861ms\n1454:   learn: 3366.5542195 total: 2.29s    remaining: 860ms\n1455:   learn: 3366.4240616 total: 2.3s remaining: 858ms\n1456:   learn: 3365.8210886 total: 2.3s remaining: 857ms\n1457:   learn: 3364.7499193 total: 2.3s remaining: 855ms\n1458:   learn: 3364.7499193 total: 2.3s remaining: 853ms\n1459:   learn: 3364.4529790 total: 2.3s remaining: 852ms\n1460:   learn: 3363.8600341 total: 2.3s remaining: 850ms\n1461:   learn: 3363.7260588 total: 2.31s    remaining: 849ms\n1462:   learn: 3362.7257758 total: 2.31s    remaining: 847ms\n1463:   learn: 3361.8405663 total: 2.31s    remaining: 845ms\n1464:   learn: 3361.3130622 total: 2.31s    remaining: 844ms\n1465:   learn: 3361.3130622 total: 2.31s    remaining: 842ms\n1466:   learn: 3360.9499820 total: 2.31s    remaining: 840ms\n1467:   learn: 3358.8757138 total: 2.31s    remaining: 839ms\n1468:   learn: 3358.8757138 total: 2.31s    remaining: 837ms\n1469:   learn: 3358.4843122 total: 2.32s    remaining: 835ms\n1470:   learn: 3358.1180199 total: 2.32s    remaining: 834ms\n1471:   learn: 3357.6770658 total: 2.32s    remaining: 832ms\n1472:   learn: 3357.5197320 total: 2.32s    remaining: 830ms\n1473:   learn: 3357.2766382 total: 2.32s    remaining: 829ms\n1474:   learn: 3356.2353078 total: 2.32s    remaining: 827ms\n1475:   learn: 3355.3662159 total: 2.33s    remaining: 826ms\n1476:   learn: 3355.1446737 total: 2.33s    remaining: 824ms\n1477:   learn: 3355.1446737 total: 2.33s    remaining: 822ms\n1478:   learn: 3355.1440713 total: 2.33s    remaining: 820ms\n1479:   learn: 3355.1400279 total: 2.33s    remaining: 819ms\n1480:   learn: 3355.1400279 total: 2.33s    remaining: 817ms\n1481:   learn: 3354.4871633 total: 2.33s    remaining: 815ms\n1482:   learn: 3354.4270963 total: 2.33s    remaining: 814ms\n1483:   learn: 3354.4181495 total: 2.33s    remaining: 812ms\n1484:   learn: 3354.1594655 total: 2.34s    remaining: 810ms\n1485:   learn: 3353.2292466 total: 2.34s    remaining: 809ms\n1486:   learn: 3353.2292466 total: 2.34s    remaining: 807ms\n1487:   learn: 3353.1014820 total: 2.34s    remaining: 805ms\n1488:   learn: 3353.0987823 total: 2.34s    remaining: 803ms\n1489:   learn: 3353.0761911 total: 2.34s    remaining: 801ms\n1490:   learn: 3353.0761911 total: 2.34s    remaining: 800ms\n1491:   learn: 3352.9010122 total: 2.34s    remaining: 798ms\n1492:   learn: 3352.6327161 total: 2.35s    remaining: 796ms\n1493:   learn: 3351.5962084 total: 2.35s    remaining: 795ms\n1494:   learn: 3351.5962084 total: 2.35s    remaining: 793ms\n1495:   learn: 3350.1006470 total: 2.35s    remaining: 791ms\n1496:   learn: 3350.1006470 total: 2.35s    remaining: 790ms\n1497:   learn: 3350.0253032 total: 2.35s    remaining: 788ms\n1498:   learn: 3349.5414000 total: 2.35s    remaining: 787ms\n1499:   learn: 3349.4987197 total: 2.35s    remaining: 785ms\n1500:   learn: 3349.2979973 total: 2.36s    remaining: 783ms\n1501:   learn: 3349.1114527 total: 2.36s    remaining: 782ms\n1502:   learn: 3348.9814439 total: 2.36s    remaining: 780ms\n1503:   learn: 3348.2336001 total: 2.36s    remaining: 779ms\n1504:   learn: 3348.2336001 total: 2.36s    remaining: 777ms\n1505:   learn: 3348.2336001 total: 2.36s    remaining: 775ms\n1506:   learn: 3347.9761559 total: 2.36s    remaining: 773ms\n1507:   learn: 3345.8139572 total: 2.37s    remaining: 772ms\n1508:   learn: 3344.4834264 total: 2.37s    remaining: 770ms\n1509:   learn: 3344.0163725 total: 2.37s    remaining: 769ms\n1510:   learn: 3344.0163725 total: 2.37s    remaining: 767ms\n1511:   learn: 3343.9811918 total: 2.37s    remaining: 765ms\n1512:   learn: 3343.8073597 total: 2.37s    remaining: 764ms\n1513:   learn: 3343.5492516 total: 2.37s    remaining: 762ms\n1514:   learn: 3343.5492516 total: 2.37s    remaining: 760ms\n1515:   learn: 3343.0367120 total: 2.38s    remaining: 758ms\n1516:   learn: 3341.9384454 total: 2.38s    remaining: 757ms\n1517:   learn: 3341.2393729 total: 2.38s    remaining: 755ms\n1518:   learn: 3340.1546795 total: 2.38s    remaining: 754ms\n1519:   learn: 3340.1546795 total: 2.38s    remaining: 752ms\n1520:   learn: 3339.2131062 total: 2.38s    remaining: 751ms\n1521:   learn: 3338.8641810 total: 2.38s    remaining: 749ms\n1522:   learn: 3338.8641810 total: 2.38s    remaining: 747ms\n1523:   learn: 3338.8441572 total: 2.39s    remaining: 745ms\n1524:   learn: 3338.7404088 total: 2.39s    remaining: 744ms\n1525:   learn: 3338.5734868 total: 2.39s    remaining: 742ms\n1526:   learn: 3338.1755709 total: 2.39s    remaining: 741ms\n1527:   learn: 3338.1263132 total: 2.39s    remaining: 740ms\n1528:   learn: 3336.8059586 total: 2.4s remaining: 738ms\n1529:   learn: 3336.7759575 total: 2.4s remaining: 737ms\n1530:   learn: 3333.9289410 total: 2.4s remaining: 736ms\n1531:   learn: 3333.9082217 total: 2.4s remaining: 734ms\n1532:   learn: 3333.2865190 total: 2.4s remaining: 732ms\n1533:   learn: 3332.0903192 total: 2.41s    remaining: 731ms\n1534:   learn: 3332.0516926 total: 2.41s    remaining: 729ms\n1535:   learn: 3331.9876738 total: 2.41s    remaining: 728ms\n1536:   learn: 3331.9876738 total: 2.41s    remaining: 726ms\n1537:   learn: 3331.9876738 total: 2.41s    remaining: 725ms\n1538:   learn: 3325.7763364 total: 2.41s    remaining: 723ms\n1539:   learn: 3325.5592471 total: 2.42s    remaining: 722ms\n1540:   learn: 3324.5254327 total: 2.42s    remaining: 720ms\n1541:   learn: 3323.4979834 total: 2.42s    remaining: 719ms\n1542:   learn: 3323.4979834 total: 2.42s    remaining: 717ms\n1543:   learn: 3323.2673846 total: 2.42s    remaining: 715ms\n1544:   learn: 3323.1182052 total: 2.42s    remaining: 714ms\n1545:   learn: 3323.0203677 total: 2.43s    remaining: 712ms\n1546:   learn: 3323.0203677 total: 2.43s    remaining: 711ms\n1547:   learn: 3322.3440984 total: 2.43s    remaining: 709ms\n1548:   learn: 3321.0069035 total: 2.43s    remaining: 708ms\n1549:   learn: 3320.0109679 total: 2.43s    remaining: 706ms\n1550:   learn: 3319.6187768 total: 2.43s    remaining: 705ms\n1551:   learn: 3318.7561915 total: 2.44s    remaining: 703ms\n1552:   learn: 3317.9903066 total: 2.44s    remaining: 701ms\n1553:   learn: 3317.6085681 total: 2.44s    remaining: 700ms\n1554:   learn: 3317.3450224 total: 2.44s    remaining: 698ms\n1555:   learn: 3316.8069448 total: 2.44s    remaining: 697ms\n1556:   learn: 3316.8069448 total: 2.44s    remaining: 695ms\n1557:   learn: 3316.7241953 total: 2.44s    remaining: 693ms\n1558:   learn: 3316.6687696 total: 2.44s    remaining: 692ms\n1559:   learn: 3316.5287101 total: 2.45s    remaining: 690ms\n1560:   learn: 3315.8917801 total: 2.45s    remaining: 688ms\n1561:   learn: 3312.5867515 total: 2.45s    remaining: 687ms\n1562:   learn: 3312.1214123 total: 2.45s    remaining: 685ms\n1563:   learn: 3312.1214123 total: 2.45s    remaining: 683ms\n1564:   learn: 3312.0073329 total: 2.45s    remaining: 682ms\n1565:   learn: 3311.9389137 total: 2.45s    remaining: 680ms\n1566:   learn: 3310.6658293 total: 2.46s    remaining: 679ms\n1567:   learn: 3309.9400964 total: 2.46s    remaining: 677ms\n1568:   learn: 3308.8312949 total: 2.46s    remaining: 675ms\n1569:   learn: 3308.7929726 total: 2.46s    remaining: 674ms\n1570:   learn: 3308.3089963 total: 2.46s    remaining: 672ms\n1571:   learn: 3308.3089963 total: 2.46s    remaining: 670ms\n1572:   learn: 3308.1719911 total: 2.46s    remaining: 669ms\n1573:   learn: 3308.1719876 total: 2.46s    remaining: 667ms\n1574:   learn: 3307.5168749 total: 2.46s    remaining: 665ms\n1575:   learn: 3307.2858943 total: 2.47s    remaining: 664ms\n1576:   learn: 3307.1156302 total: 2.47s    remaining: 662ms\n1577:   learn: 3305.3260678 total: 2.47s    remaining: 661ms\n1578:   learn: 3304.6730614 total: 2.47s    remaining: 659ms\n1579:   learn: 3304.2796680 total: 2.47s    remaining: 658ms\n1580:   learn: 3303.7745689 total: 2.48s    remaining: 656ms\n1581:   learn: 3303.6877004 total: 2.48s    remaining: 654ms\n1582:   learn: 3302.7013801 total: 2.48s    remaining: 653ms\n1583:   learn: 3302.6610349 total: 2.48s    remaining: 651ms\n1584:   learn: 3302.4571914 total: 2.48s    remaining: 650ms\n1585:   learn: 3302.3956629 total: 2.48s    remaining: 648ms\n1586:   learn: 3302.3956629 total: 2.48s    remaining: 646ms\n1587:   learn: 3302.3749386 total: 2.48s    remaining: 645ms\n1588:   learn: 3302.3163183 total: 2.49s    remaining: 643ms\n1589:   learn: 3300.9251761 total: 2.49s    remaining: 642ms\n1590:   learn: 3300.7916655 total: 2.49s    remaining: 640ms\n1591:   learn: 3300.2545046 total: 2.49s    remaining: 639ms\n1592:   learn: 3300.0773203 total: 2.49s    remaining: 637ms\n1593:   learn: 3300.0773203 total: 2.49s    remaining: 635ms\n1594:   learn: 3299.7183538 total: 2.5s remaining: 634ms\n1595:   learn: 3299.5508090 total: 2.5s remaining: 632ms\n1596:   learn: 3299.4992761 total: 2.5s remaining: 631ms\n1597:   learn: 3299.0124633 total: 2.5s remaining: 629ms\n1598:   learn: 3298.1226298 total: 2.5s remaining: 628ms\n1599:   learn: 3298.1226298 total: 2.5s remaining: 626ms\n1600:   learn: 3298.1226298 total: 2.5s remaining: 624ms\n1601:   learn: 3297.9845713 total: 2.51s    remaining: 623ms\n1602:   learn: 3297.9165567 total: 2.51s    remaining: 621ms\n1603:   learn: 3297.8466098 total: 2.51s    remaining: 620ms\n1604:   learn: 3297.8466098 total: 2.51s    remaining: 618ms\n1605:   learn: 3297.6440647 total: 2.51s    remaining: 616ms\n1606:   learn: 3297.3682602 total: 2.51s    remaining: 615ms\n1607:   learn: 3296.5167491 total: 2.52s    remaining: 613ms\n1608:   learn: 3296.5167491 total: 2.52s    remaining: 612ms\n1609:   learn: 3295.8563649 total: 2.52s    remaining: 610ms\n1610:   learn: 3295.8494358 total: 2.52s    remaining: 608ms\n1611:   learn: 3294.8414772 total: 2.52s    remaining: 607ms\n1612:   learn: 3294.7052746 total: 2.52s    remaining: 606ms\n1613:   learn: 3292.9427939 total: 2.52s    remaining: 604ms\n1614:   learn: 3290.8777820 total: 2.53s    remaining: 603ms\n1615:   learn: 3290.2602050 total: 2.53s    remaining: 601ms\n1616:   learn: 3290.1507549 total: 2.53s    remaining: 600ms\n1617:   learn: 3289.8034961 total: 2.53s    remaining: 598ms\n1618:   learn: 3287.2757282 total: 2.54s    remaining: 597ms\n1619:   learn: 3286.3824528 total: 2.54s    remaining: 595ms\n1620:   learn: 3286.0591981 total: 2.54s    remaining: 593ms\n1621:   learn: 3285.7938179 total: 2.54s    remaining: 592ms\n1622:   learn: 3285.7938179 total: 2.54s    remaining: 590ms\n1623:   learn: 3285.7938179 total: 2.54s    remaining: 588ms\n1624:   learn: 3285.7921888 total: 2.54s    remaining: 587ms\n1625:   learn: 3285.7808692 total: 2.54s    remaining: 585ms\n1626:   learn: 3285.7808692 total: 2.54s    remaining: 583ms\n1627:   learn: 3285.6991139 total: 2.54s    remaining: 582ms\n1628:   learn: 3285.6384378 total: 2.55s    remaining: 580ms\n1629:   learn: 3285.6384378 total: 2.55s    remaining: 578ms\n1630:   learn: 3284.5288549 total: 2.55s    remaining: 577ms\n1631:   learn: 3284.1414311 total: 2.55s    remaining: 575ms\n1632:   learn: 3284.0838173 total: 2.55s    remaining: 574ms\n1633:   learn: 3284.0065353 total: 2.56s    remaining: 572ms\n1634:   learn: 3284.0065353 total: 2.56s    remaining: 571ms\n1635:   learn: 3283.8020325 total: 2.56s    remaining: 569ms\n1636:   learn: 3279.9206222 total: 2.56s    remaining: 568ms\n1637:   learn: 3279.9206222 total: 2.56s    remaining: 566ms\n1638:   learn: 3279.8859464 total: 2.56s    remaining: 564ms\n1639:   learn: 3279.8859464 total: 2.56s    remaining: 563ms\n1640:   learn: 3279.8832386 total: 2.56s    remaining: 561ms\n1641:   learn: 3279.0485989 total: 2.56s    remaining: 559ms\n1642:   learn: 3278.8821714 total: 2.57s    remaining: 558ms\n1643:   learn: 3278.8711028 total: 2.57s    remaining: 556ms\n1644:   learn: 3277.1513013 total: 2.57s    remaining: 555ms\n1645:   learn: 3275.8510292 total: 2.57s    remaining: 553ms\n1646:   learn: 3275.6650452 total: 2.57s    remaining: 552ms\n1647:   learn: 3275.6650452 total: 2.57s    remaining: 550ms\n1648:   learn: 3275.3498004 total: 2.58s    remaining: 548ms\n1649:   learn: 3274.9740567 total: 2.58s    remaining: 547ms\n1650:   learn: 3274.7822665 total: 2.58s    remaining: 545ms\n1651:   learn: 3274.6167592 total: 2.58s    remaining: 544ms\n1652:   learn: 3274.3646848 total: 2.58s    remaining: 542ms\n1653:   learn: 3274.1047843 total: 2.58s    remaining: 541ms\n1654:   learn: 3273.1818950 total: 2.59s    remaining: 539ms\n1655:   learn: 3273.1252249 total: 2.59s    remaining: 538ms\n1656:   learn: 3273.1252249 total: 2.59s    remaining: 536ms\n1657:   learn: 3271.8492413 total: 2.59s    remaining: 534ms\n1658:   learn: 3271.8492413 total: 2.59s    remaining: 533ms\n1659:   learn: 3271.6288756 total: 2.59s    remaining: 531ms\n1660:   learn: 3270.5058777 total: 2.6s remaining: 530ms\n1661:   learn: 3270.2210417 total: 2.6s remaining: 528ms\n1662:   learn: 3270.1062275 total: 2.6s remaining: 527ms\n1663:   learn: 3268.8241585 total: 2.6s remaining: 525ms\n1664:   learn: 3266.3782201 total: 2.6s remaining: 524ms\n1665:   learn: 3266.1759854 total: 2.6s remaining: 522ms\n1666:   learn: 3265.9703532 total: 2.61s    remaining: 521ms\n1667:   learn: 3265.8494761 total: 2.61s    remaining: 519ms\n1668:   learn: 3265.4209061 total: 2.61s    remaining: 518ms\n1669:   learn: 3265.4095181 total: 2.61s    remaining: 516ms\n1670:   learn: 3265.3124921 total: 2.61s    remaining: 514ms\n1671:   learn: 3265.1019024 total: 2.61s    remaining: 513ms\n1672:   learn: 3265.1019024 total: 2.61s    remaining: 511ms\n1673:   learn: 3264.8856079 total: 2.62s    remaining: 510ms\n1674:   learn: 3264.1545710 total: 2.62s    remaining: 508ms\n1675:   learn: 3263.4951654 total: 2.62s    remaining: 507ms\n1676:   learn: 3263.3284898 total: 2.62s    remaining: 505ms\n1677:   learn: 3263.0894233 total: 2.62s    remaining: 504ms\n1678:   learn: 3261.6791751 total: 2.63s    remaining: 502ms\n1679:   learn: 3261.6382914 total: 2.63s    remaining: 500ms\n1680:   learn: 3260.9623079 total: 2.63s    remaining: 499ms\n1681:   learn: 3260.5195101 total: 2.63s    remaining: 497ms\n1682:   learn: 3260.5195101 total: 2.63s    remaining: 496ms\n1683:   learn: 3260.4429661 total: 2.63s    remaining: 494ms\n1684:   learn: 3259.2850353 total: 2.63s    remaining: 493ms\n1685:   learn: 3259.1184729 total: 2.64s    remaining: 491ms\n1686:   learn: 3257.4318582 total: 2.64s    remaining: 490ms\n1687:   learn: 3256.3432607 total: 2.64s    remaining: 488ms\n1688:   learn: 3256.1738614 total: 2.64s    remaining: 487ms\n1689:   learn: 3256.0401169 total: 2.64s    remaining: 485ms\n1690:   learn: 3256.0141182 total: 2.64s    remaining: 483ms\n1691:   learn: 3255.6800659 total: 2.65s    remaining: 482ms\n1692:   learn: 3255.3824190 total: 2.65s    remaining: 480ms\n1693:   learn: 3255.3824190 total: 2.65s    remaining: 479ms\n1694:   learn: 3255.3198326 total: 2.65s    remaining: 477ms\n1695:   learn: 3255.0116992 total: 2.65s    remaining: 476ms\n1696:   learn: 3254.4885471 total: 2.65s    remaining: 474ms\n1697:   learn: 3253.9981370 total: 2.66s    remaining: 472ms\n1698:   learn: 3253.7226823 total: 2.66s    remaining: 471ms\n1699:   learn: 3252.0418807 total: 2.66s    remaining: 469ms\n1700:   learn: 3251.9126909 total: 2.66s    remaining: 468ms\n1701:   learn: 3251.7482284 total: 2.66s    remaining: 466ms\n1702:   learn: 3251.7482284 total: 2.66s    remaining: 465ms\n1703:   learn: 3251.4825436 total: 2.67s    remaining: 463ms\n1704:   learn: 3251.4825436 total: 2.67s    remaining: 461ms\n1705:   learn: 3251.0752854 total: 2.67s    remaining: 460ms\n1706:   learn: 3251.0323167 total: 2.67s    remaining: 458ms\n1707:   learn: 3250.9870171 total: 2.67s    remaining: 457ms\n1708:   learn: 3250.9870171 total: 2.67s    remaining: 455ms\n1709:   learn: 3250.8926521 total: 2.67s    remaining: 454ms\n1710:   learn: 3250.8926521 total: 2.67s    remaining: 452ms\n1711:   learn: 3250.6317391 total: 2.68s    remaining: 450ms\n1712:   learn: 3250.2868656 total: 2.68s    remaining: 449ms\n1713:   learn: 3247.8370753 total: 2.68s    remaining: 447ms\n1714:   learn: 3247.5119753 total: 2.68s    remaining: 446ms\n1715:   learn: 3247.4487605 total: 2.69s    remaining: 444ms\n1716:   learn: 3247.2618348 total: 2.69s    remaining: 443ms\n1717:   learn: 3246.1160185 total: 2.69s    remaining: 441ms\n1718:   learn: 3246.0611944 total: 2.69s    remaining: 440ms\n1719:   learn: 3245.9436964 total: 2.69s    remaining: 438ms\n1720:   learn: 3245.8172996 total: 2.69s    remaining: 437ms\n1721:   learn: 3245.5221004 total: 2.69s    remaining: 435ms\n1722:   learn: 3244.7470276 total: 2.7s remaining: 434ms\n1723:   learn: 3244.5450469 total: 2.7s remaining: 432ms\n1724:   learn: 3244.3835899 total: 2.7s remaining: 431ms\n1725:   learn: 3243.7042952 total: 2.7s remaining: 429ms\n1726:   learn: 3242.7265075 total: 2.7s remaining: 428ms\n1727:   learn: 3242.7265036 total: 2.71s    remaining: 426ms\n1728:   learn: 3242.6212553 total: 2.71s    remaining: 424ms\n1729:   learn: 3242.6212553 total: 2.71s    remaining: 423ms\n1730:   learn: 3242.6212553 total: 2.71s    remaining: 421ms\n1731:   learn: 3242.5020831 total: 2.71s    remaining: 419ms\n1732:   learn: 3242.2354098 total: 2.71s    remaining: 418ms\n1733:   learn: 3242.1319816 total: 2.71s    remaining: 416ms\n1734:   learn: 3239.9598138 total: 2.71s    remaining: 415ms\n1735:   learn: 3239.9598138 total: 2.71s    remaining: 413ms\n1736:   learn: 3239.9598138 total: 2.71s    remaining: 411ms\n1737:   learn: 3238.4933588 total: 2.72s    remaining: 410ms\n1738:   learn: 3238.4090395 total: 2.72s    remaining: 408ms\n1739:   learn: 3237.9460828 total: 2.72s    remaining: 407ms\n1740:   learn: 3237.9460828 total: 2.72s    remaining: 405ms\n1741:   learn: 3237.5741813 total: 2.72s    remaining: 403ms\n1742:   learn: 3237.5731105 total: 2.72s    remaining: 402ms\n1743:   learn: 3236.6579709 total: 2.73s    remaining: 400ms\n1744:   learn: 3236.6579709 total: 2.73s    remaining: 398ms\n1745:   learn: 3236.6004231 total: 2.73s    remaining: 397ms\n1746:   learn: 3236.5989813 total: 2.73s    remaining: 395ms\n1747:   learn: 3235.7497974 total: 2.73s    remaining: 394ms\n1748:   learn: 3235.6559333 total: 2.73s    remaining: 392ms\n1749:   learn: 3235.6559333 total: 2.73s    remaining: 390ms\n1750:   learn: 3235.6559333 total: 2.73s    remaining: 389ms\n1751:   learn: 3234.7204734 total: 2.73s    remaining: 387ms\n1752:   learn: 3234.0974067 total: 2.74s    remaining: 386ms\n1753:   learn: 3232.7146744 total: 2.74s    remaining: 384ms\n1754:   learn: 3232.0895286 total: 2.74s    remaining: 383ms\n1755:   learn: 3231.0815741 total: 2.74s    remaining: 381ms\n1756:   learn: 3230.3141273 total: 2.74s    remaining: 379ms\n1757:   learn: 3230.1224354 total: 2.75s    remaining: 378ms\n1758:   learn: 3229.9339115 total: 2.75s    remaining: 377ms\n1759:   learn: 3227.6837115 total: 2.75s    remaining: 375ms\n1760:   learn: 3227.1989900 total: 2.75s    remaining: 374ms\n1761:   learn: 3227.1208964 total: 2.75s    remaining: 372ms\n1762:   learn: 3227.1208964 total: 2.75s    remaining: 370ms\n1763:   learn: 3227.1208964 total: 2.75s    remaining: 369ms\n1764:   learn: 3227.1208964 total: 2.76s    remaining: 367ms\n1765:   learn: 3226.9229197 total: 2.76s    remaining: 366ms\n1766:   learn: 3226.8221035 total: 2.76s    remaining: 364ms\n1767:   learn: 3226.8221035 total: 2.76s    remaining: 362ms\n1768:   learn: 3226.8221035 total: 2.76s    remaining: 361ms\n1769:   learn: 3226.7462898 total: 2.77s    remaining: 359ms\n1770:   learn: 3226.6427809 total: 2.77s    remaining: 358ms\n1771:   learn: 3226.5801495 total: 2.77s    remaining: 356ms\n1772:   learn: 3225.3617666 total: 2.77s    remaining: 355ms\n1773:   learn: 3225.1428621 total: 2.77s    remaining: 354ms\n1774:   learn: 3224.9591976 total: 2.78s    remaining: 352ms\n1775:   learn: 3224.9591976 total: 2.78s    remaining: 350ms\n1776:   learn: 3224.6700894 total: 2.78s    remaining: 349ms\n1777:   learn: 3224.6466400 total: 2.78s    remaining: 347ms\n1778:   learn: 3224.6271374 total: 2.78s    remaining: 346ms\n1779:   learn: 3224.0563186 total: 2.79s    remaining: 344ms\n1780:   learn: 3222.5400069 total: 2.79s    remaining: 343ms\n1781:   learn: 3222.4027209 total: 2.79s    remaining: 341ms\n1782:   learn: 3222.4027209 total: 2.79s    remaining: 340ms\n1783:   learn: 3222.4027209 total: 2.79s    remaining: 338ms\n1784:   learn: 3222.2928774 total: 2.79s    remaining: 336ms\n1785:   learn: 3221.8128906 total: 2.79s    remaining: 335ms\n1786:   learn: 3221.7618866 total: 2.8s remaining: 333ms\n1787:   learn: 3221.2949108 total: 2.8s remaining: 332ms\n1788:   learn: 3221.2949108 total: 2.8s remaining: 330ms\n1789:   learn: 3221.2240167 total: 2.8s remaining: 329ms\n1790:   learn: 3221.0322063 total: 2.8s remaining: 327ms\n1791:   learn: 3221.0322063 total: 2.8s remaining: 325ms\n1792:   learn: 3220.9381865 total: 2.81s    remaining: 324ms\n1793:   learn: 3220.9381865 total: 2.81s    remaining: 322ms\n1794:   learn: 3220.9381865 total: 2.81s    remaining: 321ms\n1795:   learn: 3220.9377104 total: 2.81s    remaining: 319ms\n1796:   learn: 3220.7727369 total: 2.81s    remaining: 317ms\n1797:   learn: 3220.7356709 total: 2.81s    remaining: 316ms\n1798:   learn: 3220.7356709 total: 2.81s    remaining: 314ms\n1799:   learn: 3220.6184967 total: 2.81s    remaining: 313ms\n1800:   learn: 3220.6175269 total: 2.81s    remaining: 311ms\n1801:   learn: 3220.1059973 total: 2.81s    remaining: 309ms\n1802:   learn: 3220.1059973 total: 2.82s    remaining: 308ms\n1803:   learn: 3217.1888413 total: 2.82s    remaining: 306ms\n1804:   learn: 3217.1172900 total: 2.82s    remaining: 305ms\n1805:   learn: 3217.0688467 total: 2.82s    remaining: 303ms\n1806:   learn: 3216.7132980 total: 2.82s    remaining: 301ms\n1807:   learn: 3216.3568737 total: 2.82s    remaining: 300ms\n1808:   learn: 3216.3157387 total: 2.83s    remaining: 298ms\n1809:   learn: 3216.3157387 total: 2.83s    remaining: 297ms\n1810:   learn: 3216.0763913 total: 2.83s    remaining: 295ms\n1811:   learn: 3216.0763913 total: 2.83s    remaining: 294ms\n1812:   learn: 3216.0763913 total: 2.83s    remaining: 292ms\n1813:   learn: 3215.6931887 total: 2.83s    remaining: 290ms\n1814:   learn: 3215.6931887 total: 2.83s    remaining: 289ms\n1815:   learn: 3214.2313706 total: 2.83s    remaining: 287ms\n1816:   learn: 3213.7688487 total: 2.83s    remaining: 286ms\n1817:   learn: 3213.4543847 total: 2.84s    remaining: 284ms\n1818:   learn: 3212.0359410 total: 2.84s    remaining: 282ms\n1819:   learn: 3211.4011396 total: 2.84s    remaining: 281ms\n1820:   learn: 3211.2998259 total: 2.84s    remaining: 279ms\n1821:   learn: 3211.2998259 total: 2.84s    remaining: 278ms\n1822:   learn: 3211.1857236 total: 2.85s    remaining: 276ms\n1823:   learn: 3211.1852452 total: 2.85s    remaining: 275ms\n1824:   learn: 3211.0107423 total: 2.85s    remaining: 273ms\n1825:   learn: 3210.4542562 total: 2.85s    remaining: 272ms\n1826:   learn: 3210.1349881 total: 2.85s    remaining: 270ms\n1827:   learn: 3210.1349881 total: 2.85s    remaining: 268ms\n1828:   learn: 3210.1349881 total: 2.85s    remaining: 267ms\n1829:   learn: 3209.7725069 total: 2.85s    remaining: 265ms\n1830:   learn: 3208.8106518 total: 2.85s    remaining: 264ms\n1831:   learn: 3208.4329045 total: 2.86s    remaining: 262ms\n1832:   learn: 3208.3958028 total: 2.86s    remaining: 261ms\n1833:   learn: 3208.3958028 total: 2.86s    remaining: 259ms\n1834:   learn: 3208.0052413 total: 2.86s    remaining: 257ms\n1835:   learn: 3208.0052413 total: 2.86s    remaining: 256ms\n1836:   learn: 3208.0006715 total: 2.86s    remaining: 254ms\n1837:   learn: 3208.0006715 total: 2.86s    remaining: 252ms\n1838:   learn: 3207.6299710 total: 2.87s    remaining: 251ms\n1839:   learn: 3207.0709428 total: 2.87s    remaining: 249ms\n1840:   learn: 3206.2722778 total: 2.87s    remaining: 248ms\n1841:   learn: 3205.9844701 total: 2.87s    remaining: 246ms\n1842:   learn: 3205.9844701 total: 2.87s    remaining: 245ms\n1843:   learn: 3205.9844701 total: 2.87s    remaining: 243ms\n1844:   learn: 3205.9051384 total: 2.87s    remaining: 241ms\n1845:   learn: 3205.5338684 total: 2.88s    remaining: 240ms\n1846:   learn: 3205.3602770 total: 2.88s    remaining: 238ms\n1847:   learn: 3205.2578576 total: 2.88s    remaining: 237ms\n1848:   learn: 3203.8365263 total: 2.88s    remaining: 235ms\n1849:   learn: 3203.0163520 total: 2.88s    remaining: 234ms\n1850:   learn: 3202.9417611 total: 2.88s    remaining: 232ms\n1851:   learn: 3202.9417611 total: 2.88s    remaining: 231ms\n1852:   learn: 3201.1659404 total: 2.89s    remaining: 229ms\n1853:   learn: 3200.6824813 total: 2.89s    remaining: 227ms\n1854:   learn: 3200.4047298 total: 2.89s    remaining: 226ms\n1855:   learn: 3200.2798995 total: 2.89s    remaining: 224ms\n1856:   learn: 3200.0748374 total: 2.89s    remaining: 223ms\n1857:   learn: 3199.9563505 total: 2.9s remaining: 221ms\n1858:   learn: 3199.8089603 total: 2.9s remaining: 220ms\n1859:   learn: 3199.8089603 total: 2.9s remaining: 218ms\n1860:   learn: 3197.9298648 total: 2.9s remaining: 217ms\n1861:   learn: 3197.7025775 total: 2.9s remaining: 215ms\n1862:   learn: 3197.3721597 total: 2.9s remaining: 214ms\n1863:   learn: 3197.3140130 total: 2.9s remaining: 212ms\n1864:   learn: 3196.0015779 total: 2.91s    remaining: 210ms\n1865:   learn: 3195.3910311 total: 2.91s    remaining: 209ms\n1866:   learn: 3195.3308914 total: 2.91s    remaining: 207ms\n1867:   learn: 3195.2076011 total: 2.91s    remaining: 206ms\n1868:   learn: 3194.7759795 total: 2.91s    remaining: 204ms\n1869:   learn: 3192.4044117 total: 2.92s    remaining: 203ms\n1870:   learn: 3192.3708795 total: 2.92s    remaining: 201ms\n1871:   learn: 3192.3708795 total: 2.92s    remaining: 199ms\n1872:   learn: 3192.2939071 total: 2.92s    remaining: 198ms\n1873:   learn: 3192.2895993 total: 2.92s    remaining: 196ms\n1874:   learn: 3191.9587267 total: 2.92s    remaining: 195ms\n1875:   learn: 3191.9567183 total: 2.92s    remaining: 193ms\n1876:   learn: 3191.8971818 total: 2.92s    remaining: 192ms\n1877:   learn: 3191.1792680 total: 2.93s    remaining: 190ms\n1878:   learn: 3191.0838705 total: 2.93s    remaining: 189ms\n1879:   learn: 3190.8872837 total: 2.93s    remaining: 187ms\n1880:   learn: 3190.8872837 total: 2.93s    remaining: 185ms\n1881:   learn: 3190.3398846 total: 2.93s    remaining: 184ms\n1882:   learn: 3190.3178516 total: 2.93s    remaining: 182ms\n1883:   learn: 3190.3178516 total: 2.94s    remaining: 181ms\n1884:   learn: 3190.3178516 total: 2.94s    remaining: 179ms\n1885:   learn: 3190.1615305 total: 2.94s    remaining: 178ms\n1886:   learn: 3189.2103039 total: 2.94s    remaining: 176ms\n1887:   learn: 3188.4273779 total: 2.94s    remaining: 174ms\n1888:   learn: 3188.4238113 total: 2.94s    remaining: 173ms\n1889:   learn: 3188.2809772 total: 2.94s    remaining: 171ms\n1890:   learn: 3187.9026207 total: 2.94s    remaining: 170ms\n1891:   learn: 3187.7516608 total: 2.95s    remaining: 168ms\n1892:   learn: 3187.0001016 total: 2.95s    remaining: 167ms\n1893:   learn: 3186.8764795 total: 2.95s    remaining: 165ms\n1894:   learn: 3186.1777226 total: 2.95s    remaining: 164ms\n1895:   learn: 3185.9146383 total: 2.95s    remaining: 162ms\n1896:   learn: 3185.8595390 total: 2.96s    remaining: 160ms\n1897:   learn: 3185.6498040 total: 2.96s    remaining: 159ms\n1898:   learn: 3185.4838792 total: 2.96s    remaining: 157ms\n1899:   learn: 3185.1089346 total: 2.96s    remaining: 156ms\n1900:   learn: 3184.9103971 total: 2.96s    remaining: 154ms\n1901:   learn: 3184.9103971 total: 2.96s    remaining: 153ms\n1902:   learn: 3184.7112840 total: 2.96s    remaining: 151ms\n1903:   learn: 3184.4933206 total: 2.97s    remaining: 150ms\n1904:   learn: 3184.2238274 total: 2.97s    remaining: 148ms\n1905:   learn: 3183.9800255 total: 2.97s    remaining: 147ms\n1906:   learn: 3183.8295774 total: 2.97s    remaining: 145ms\n1907:   learn: 3182.8877936 total: 2.97s    remaining: 143ms\n1908:   learn: 3182.8877936 total: 2.97s    remaining: 142ms\n1909:   learn: 3182.8605341 total: 2.98s    remaining: 140ms\n1910:   learn: 3182.7561028 total: 2.98s    remaining: 139ms\n1911:   learn: 3182.6976531 total: 2.98s    remaining: 137ms\n1912:   learn: 3182.4886802 total: 2.98s    remaining: 136ms\n1913:   learn: 3182.4267129 total: 2.98s    remaining: 134ms\n1914:   learn: 3181.8080179 total: 2.98s    remaining: 133ms\n1915:   learn: 3181.8080179 total: 2.98s    remaining: 131ms\n1916:   learn: 3181.5897020 total: 2.99s    remaining: 129ms\n1917:   learn: 3181.3470077 total: 2.99s    remaining: 128ms\n1918:   learn: 3180.6396235 total: 2.99s    remaining: 126ms\n1919:   learn: 3180.4227108 total: 2.99s    remaining: 125ms\n1920:   learn: 3179.2487757 total: 3s   remaining: 123ms\n1921:   learn: 3178.8277430 total: 3s   remaining: 122ms\n1922:   learn: 3178.8277430 total: 3s   remaining: 120ms\n1923:   learn: 3178.8277430 total: 3s   remaining: 118ms\n1924:   learn: 3178.8277430 total: 3s   remaining: 117ms\n1925:   learn: 3178.6801247 total: 3s   remaining: 115ms\n1926:   learn: 3177.4687686 total: 3s   remaining: 114ms\n1927:   learn: 3176.8356898 total: 3s   remaining: 112ms\n1928:   learn: 3176.7893483 total: 3s   remaining: 111ms\n1929:   learn: 3176.6002622 total: 3.01s    remaining: 109ms\n1930:   learn: 3175.5122001 total: 3.01s    remaining: 108ms\n1931:   learn: 3175.5115561 total: 3.01s    remaining: 106ms\n1932:   learn: 3175.3155671 total: 3.01s    remaining: 104ms\n1933:   learn: 3175.0862539 total: 3.01s    remaining: 103ms\n1934:   learn: 3173.7085725 total: 3.02s    remaining: 101ms\n1935:   learn: 3173.6273355 total: 3.02s    remaining: 99.7ms\n1936:   learn: 3173.6273355 total: 3.02s    remaining: 98.1ms\n1937:   learn: 3172.2604329 total: 3.02s    remaining: 96.6ms\n1938:   learn: 3170.7569996 total: 3.02s    remaining: 95ms\n1939:   learn: 3170.7210008 total: 3.02s    remaining: 93.4ms\n1940:   learn: 3170.6571722 total: 3.02s    remaining: 91.9ms\n1941:   learn: 3170.4286382 total: 3.02s    remaining: 90.3ms\n1942:   learn: 3170.4038996 total: 3.02s    remaining: 88.7ms\n1943:   learn: 3167.9652011 total: 3.03s    remaining: 87.2ms\n1944:   learn: 3167.3331960 total: 3.03s    remaining: 85.6ms\n1945:   learn: 3167.0481107 total: 3.03s    remaining: 84.1ms\n1946:   learn: 3165.3433385 total: 3.03s    remaining: 82.5ms\n1947:   learn: 3163.1094795 total: 3.03s    remaining: 81ms\n1948:   learn: 3163.0667467 total: 3.04s    remaining: 79.4ms\n1949:   learn: 3163.0667467 total: 3.04s    remaining: 77.8ms\n1950:   learn: 3162.4798653 total: 3.04s    remaining: 76.3ms\n1951:   learn: 3161.3348713 total: 3.04s    remaining: 74.7ms\n1952:   learn: 3161.3117989 total: 3.04s    remaining: 73.2ms\n1953:   learn: 3160.8023680 total: 3.04s    remaining: 71.6ms\n1954:   learn: 3160.1140905 total: 3.04s    remaining: 70.1ms\n1955:   learn: 3158.9981598 total: 3.05s    remaining: 68.5ms\n1956:   learn: 3156.8978382 total: 3.05s    remaining: 67ms\n1957:   learn: 3156.7772601 total: 3.05s    remaining: 65.4ms\n1958:   learn: 3156.5825227 total: 3.05s    remaining: 63.9ms\n1959:   learn: 3156.1921972 total: 3.05s    remaining: 62.3ms\n1960:   learn: 3155.5738299 total: 3.06s    remaining: 60.8ms\n1961:   learn: 3154.9745268 total: 3.06s    remaining: 59.2ms\n1962:   learn: 3154.9745268 total: 3.06s    remaining: 57.7ms\n1963:   learn: 3154.9321185 total: 3.06s    remaining: 56.1ms\n1964:   learn: 3154.9193510 total: 3.06s    remaining: 54.5ms\n1965:   learn: 3153.3045270 total: 3.06s    remaining: 53ms\n1966:   learn: 3153.1427141 total: 3.06s    remaining: 51.4ms\n1967:   learn: 3152.9119379 total: 3.07s    remaining: 49.9ms\n1968:   learn: 3152.9117927 total: 3.07s    remaining: 48.3ms\n1969:   learn: 3152.9117927 total: 3.07s    remaining: 46.7ms\n1970:   learn: 3152.8367126 total: 3.07s    remaining: 45.2ms\n1971:   learn: 3152.8124488 total: 3.07s    remaining: 43.6ms\n1972:   learn: 3152.5892578 total: 3.07s    remaining: 42.1ms\n1973:   learn: 3152.4632281 total: 3.08s    remaining: 40.5ms\n1974:   learn: 3152.4632281 total: 3.08s    remaining: 38.9ms\n1975:   learn: 3152.4610389 total: 3.08s    remaining: 37.4ms\n1976:   learn: 3152.4070619 total: 3.08s    remaining: 35.8ms\n1977:   learn: 3151.5088242 total: 3.08s    remaining: 34.3ms\n1978:   learn: 3151.3192003 total: 3.08s    remaining: 32.7ms\n1979:   learn: 3151.0934751 total: 3.08s    remaining: 31.2ms\n1980:   learn: 3151.0934751 total: 3.09s    remaining: 29.6ms\n1981:   learn: 3150.9537433 total: 3.09s    remaining: 28ms\n1982:   learn: 3150.9537433 total: 3.09s    remaining: 26.5ms\n1983:   learn: 3150.8674274 total: 3.09s    remaining: 24.9ms\n1984:   learn: 3150.8674274 total: 3.09s    remaining: 23.4ms\n1985:   learn: 3149.2328055 total: 3.09s    remaining: 21.8ms\n1986:   learn: 3149.0072684 total: 3.09s    remaining: 20.2ms\n1987:   learn: 3148.3315876 total: 3.1s remaining: 18.7ms\n1988:   learn: 3147.7977344 total: 3.1s remaining: 17.1ms\n1989:   learn: 3147.6247163 total: 3.1s remaining: 15.6ms\n1990:   learn: 3146.5977276 total: 3.1s remaining: 14ms\n1991:   learn: 3146.1891474 total: 3.1s remaining: 12.5ms\n1992:   learn: 3146.1621749 total: 3.11s    remaining: 10.9ms\n1993:   learn: 3144.2741570 total: 3.11s    remaining: 9.36ms\n1994:   learn: 3144.2189615 total: 3.11s    remaining: 7.8ms\n1995:   learn: 3143.8690645 total: 3.11s    remaining: 6.24ms\n1996:   learn: 3143.3502296 total: 3.12s    remaining: 4.68ms\n1997:   learn: 3142.8227286 total: 3.12s    remaining: 3.12ms\n1998:   learn: 3142.8227286 total: 3.12s    remaining: 1.56ms\n1999:   learn: 3142.6323020 total: 3.12s    remaining: 0us\n\n\n\n\n\n\n\n\n\nBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=&lt;catboost.core.CatBoostRegressor object at 0x000001C05095EFD0&gt;,\n              n_iter=200, n_jobs=-1, random_state=1,\n              scoring='neg_root_mean_squared_error',\n              search_spaces={'colsample_bylevel': Real(low=0.1, high=1.0, prior='uniform', transform='normalize'),\n                             'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=2000, prior='uniform', transform='normalize'),\n                             'num_leaves': Integer(low=4, high=64, prior='uniform', transform='normalize'),\n                             'reg_lambda': Real(low=0, high=10000.0, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.1, high=1.0, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCVBayesSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n              estimator=&lt;catboost.core.CatBoostRegressor object at 0x000001C05095EFD0&gt;,\n              n_iter=200, n_jobs=-1, random_state=1,\n              scoring='neg_root_mean_squared_error',\n              search_spaces={'colsample_bylevel': Real(low=0.1, high=1.0, prior='uniform', transform='normalize'),\n                             'learning_rate': Real(low=0.0001, high=1.0, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=2, high=2000, prior='uniform', transform='normalize'),\n                             'num_leaves': Integer(low=4, high=64, prior='uniform', transform='normalize'),\n                             'reg_lambda': Real(low=0, high=10000.0, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.1, high=1.0, prior='uniform', transform='normalize')})estimator: CatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001C05095EFD0&gt;CatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001C05095EFD0&gt;\n\n\n\n\n13.2.4 Tuning Tips\nCheck the documentation for some tuning tips.\n\nIt is not recommended to use values greater than 64 for num_leaves, since it can significantly slow down the training process.\nThe maximum possible value of max_depth is 16.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "Lec10_Ensemble.html",
    "href": "Lec10_Ensemble.html",
    "title": "14  Ensemble modeling",
    "section": "",
    "text": "14.1 Ensembling regression models",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ensemble modeling</span>"
    ]
  },
  {
    "objectID": "Lec10_Ensemble.html#ensembling-regression-models",
    "href": "Lec10_Ensemble.html#ensembling-regression-models",
    "title": "14  Ensemble modeling",
    "section": "",
    "text": "14.1.1 Voting Regressor\nHere, we will combine the predictions of different models. The function VotingRegressor() averages the predictions of all the models.\nBelow are the individual models tuned in the previous chapters.\n\n# Tuned XGBoost model from Section 9.2.6\nmodel_xgb = xgb.XGBRegressor(random_state=1,max_depth=8,n_estimators=1000, subsample = 0.75, colsample_bytree = 1.0,\n                                         learning_rate = 0.01,reg_lambda=1, gamma = 100).fit(X, y)\nprint(\"RMSE for XGBoost = \", np.sqrt(mean_squared_error(model_xgb.predict(Xtest), ytest)))\n\n#Tuned AdaBoost model from Section 7.2.4\nmodel_ada = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10),n_estimators=50,learning_rate=1.0,\n                         random_state=1).fit(X, y)\nprint(\"RMSE for AdaBoost = \", np.sqrt(mean_squared_error(model_ada.predict(Xtest), ytest)))\n\n#Tuned Random forest model from Section 6.1.2\nmodel_rf = RandomForestRegressor(n_estimators=300, random_state=1,\n                        n_jobs=-1, max_features=2).fit(X, y)\nprint(\"RMSE for Random forest = \", np.sqrt(mean_squared_error(model_rf.predict(Xtest), ytest)))\n\n#Tuned gradient boosting model from Section 8.2.5\nmodel_gb = GradientBoostingRegressor(max_depth=8,n_estimators=100,learning_rate=0.1,\n                         random_state=1,loss='huber').fit(X, y)\nprint(\"RMSE for Gradient Boosting = \", np.sqrt(mean_squared_error(model_gb.predict(Xtest), ytest)))\n\nRMSE for XGBoost =  5497.553788113875\nRMSE for AdaBoost =  5693.165811600585\nRMSE for Random forest =  5642.45839697972\nRMSE for Gradient Boosting =  5405.787029062213\n\n\n\n#Voting ensemble: Averaging the predictions of all models\nen=VotingRegressor(estimators = [('xgb',model_xgb),('ada',model_ada),('rf',model_rf),('gb',model_gb)])\nen.fit(X,y)\nprint(\"Ensemble model RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n\nEnsemble model RMSE =  5361.7260763197\n\n\nRMSE of the ensembled model is less than that of each of the individual models.\n\n\n14.1.2 Stacking Regressor\nStacking is a more sophisticated method of ensembling models. The method is as follows:\n\nThe training data is split into K folds. Each of the K folds serves as a test data in one of the K iterations, and the rest of the folds serve as train data.\nEach model is used to make predictions on each of the K folds, after being trained on the remaining K-1 folds. In this manner, each model predicts the response on each train data point - when that train data point was not used to train the model.\nPredictions at each training data points are generated by each model in step 2 (the above step). These predictions are now used as predictors to train a meta-model (referred by the argument final_estimator), with the original response as the response. The meta-model (or final_estimator) learns to combine predictions of different models to make a better prediction.\n\n\n#Stacking using LinearRegression as the metamodel\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),('gb', model_gb)],\n                     final_estimator=LinearRegression(),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nen.fit(X,y)\nprint(\"Linear regression metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n\nLinear regression metamodel RMSE =  5311.789386389769\n\n\n\n#Co-efficients of the meta-model\nen.final_estimator_.coef_\n\narray([0.29641759, 0.25626987, 0.051808  , 0.41978153])\n\n\nNote the above coefficients of the meta-model. The model gives the highest weight to the gradient boosting model, and the lowest weight to the random forest model. Also, note that the coefficients need not sum to one.\n\n#Stacking using Lasso as the metamodel\nen = StackingRegressor(estimators = [('xgb', model_xgb),('ada', model_ada),('rf', model_rf),('gb', model_gb)],\n                     final_estimator=LassoCV(),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nen.fit(X,y)\nprint(\"Lasso metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n\nLasso metamodel RMSE =  5311.185592456483\n\n\n\n#Coefficients of the lasso metamodel\nen.final_estimator_.coef_\n\narray([0.17639973, 0.28186944, 0.1152561 , 0.45119952])\n\n\n\n#Stacking using MARS as the meta-model\nen = StackingRegressor(estimators = [('xgb',m1),('ada',m2),('rf',m3),('gb',m4)],\n                     final_estimator=Earth(max_degree=1),                                          \n                    cv = KFold(n_splits = 5, shuffle = True, random_state=1))\nen.fit(X,y)\nprint(\"MARS metamodel RMSE = \", np.sqrt(mean_squared_error(en.predict(Xtest),ytest)))\n\nEnsemble model RMSE =  5303.308982301974\n\n\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:813: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  pruning_passer.run()\nC:\\Users\\akl0407\\Anaconda3\\lib\\site-packages\\pyearth\\earth.py:1066: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  coef, resid = np.linalg.lstsq(B, weighted_y[:, i])[0:2]\n\n\n\nprint(en.final_estimator_.summary())\n\nEarth Model\n-------------------------------------\nBasis Function  Pruned  Coefficient  \n-------------------------------------\n(Intercept)     No      59644        \nh(x3-75435)     No      0.402779     \nh(75435-x3)     No      -0.406517    \nh(x1-74988)     No      0.822699     \nh(74988-x1)     No      -0.119104    \nh(x2-72702.8)   No      -0.449716    \nh(72702.8-x2)   No      -0.280938    \nx0              No      0.211986     \n-------------------------------------\nMSE: 25038308.7322, GCV: 25226136.6357, RSQ: 0.9070, GRSQ: 0.9063",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ensemble modeling</span>"
    ]
  },
  {
    "objectID": "Lec10_Ensemble.html#ensembling-classification-models",
    "href": "Lec10_Ensemble.html#ensembling-classification-models",
    "title": "14  Ensemble modeling",
    "section": "14.2 Ensembling classification models",
    "text": "14.2 Ensembling classification models\nWe’ll ensemble models for predicting accuracy of identifying people having a heart disease.\n\ndata = pd.read_csv('./Datasets/Heart.csv')\ndata.dropna(inplace = True)\n#Response variable\ny = pd.get_dummies(data['AHD'])['Yes']\n\n#Creating a dataframe for predictors with dummy variables replacing the categorical variables\nX = data.drop(columns = ['AHD','ChestPain','Thal'])\nX = pd.concat([X,pd.get_dummies(data['ChestPain']),pd.get_dummies(data['Thal'])],axis=1)\n\n#Creating train and test datasets\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size = 0.5,random_state=1)\n\nLet us tune the individual models first.\n\nAdaBoost\n\n# Tuning Adaboost for maximizing accuracy\nmodel = AdaBoostClassifier(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['base_estimator'] = [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), \n                          DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=4)]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',refit='accuracy')\n# execute the grid search\ngrid_result = grid_search.fit(Xtrain, ytrain)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\nBest: 0.871494 using {'base_estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 0.01, 'n_estimators': 200}\n\n\n\n\nGradient Boosting\n\n# Tuning gradient boosting for maximizing accuracy\nmodel = GradientBoostingClassifier(random_state=1)\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100,200,500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\ngrid['max_depth'] = [1,2,3,4,5]\ngrid['subsample'] = [0.5,1.0]\n# define the evaluation procedure\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# define the grid search procedure\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',refit='accuracy')\n# execute the grid search\ngrid_result = grid_search.fit(Xtrain, ytrain)\n# summarize the best score and configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\nBest: 0.871954 using {'learning_rate': 1.0, 'max_depth': 4, 'n_estimators': 100, 'subsample': 1.0}\n\n\n\n\nXGBoost\n\n# Tuning XGBoost for maximizing accuracy\nstart_time = time.time()\nparam_grid = {'n_estimators':[25, 100,250,500],\n                'max_depth': [4, 6 ,8],\n              'learning_rate': [0.01,0.1,0.2],\n               'gamma': [0, 1, 10, 100],\n               'reg_lambda':[0, 10, 100],\n               'subsample': [0.5, 0.75, 1.0]\n                'scale_pos_weight':[1.25,1.5,1.75]#Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances).\n             }\n\ncv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\noptimal_params = GridSearchCV(estimator=xgb.XGBClassifier(random_state=1),\n                             param_grid = param_grid,\n                             scoring = 'accuracy',\n                             verbose = 1,\n                             n_jobs=-1,\n                             cv = cv)\noptimal_params.fit(Xtrain,ytrain)\nprint(optimal_params.best_params_,optimal_params.best_score_)\nprint(\"Time taken = \", (time.time()-start_time)/60, \" minutes\")\n\nFitting 5 folds for each of 972 candidates, totalling 4860 fits\n{'gamma': 0, 'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 25, 'reg_lambda': 0, 'scale_pos_weight': 1.25} 0.872183908045977\nTime taken =  0.9524135629336039  minutes\n\n\n\n#Tuned Adaboost model\nmodel_ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=200, \n                               random_state=1,learning_rate=0.01).fit(Xtrain, ytrain)    \ntest_accuracy_ada = model_ada.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n    \n#Tuned Random forest model from Section 6.3\nmodel_rf = RandomForestClassifier(n_estimators=500, random_state=1,max_features=3,\n                        n_jobs=-1,oob_score=False).fit(Xtrain, ytrain)\ntest_accuracy_rf = model_rf.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n    \n#Tuned gradient boosting model\nmodel_gb = GradientBoostingClassifier(n_estimators=100, random_state=1,max_depth=4,learning_rate=1.0,\n                                     subsample = 1.0).fit(Xtrain, ytrain)\ntest_accuracy_gb = model_gb.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n\n#Tuned XGBoost model\nmodel_xgb = xgb.XGBClassifier(random_state=1,gamma=0,learning_rate = 0.2,max_depth=4,\n                              n_estimators = 25,reg_lambda = 0,scale_pos_weight=1.25).fit(Xtrain,ytrain)\ntest_accuracy_xgb = model_xgb.score(Xtest,ytest) #Returns the classification accuracy of the model on test data\n\nprint(\"Adaboost accuracy = \",test_accuracy_ada)\nprint(\"Random forest accuracy = \",test_accuracy_rf)\nprint(\"Gradient boost accuracy = \",test_accuracy_gb)\nprint(\"XGBoost model accuracy = \",test_accuracy_xgb)\n\nAdaboost accuracy =  0.7986577181208053\nRandom forest accuracy =  0.8120805369127517\nGradient boost accuracy =  0.7986577181208053\nXGBoost model accuracy =  0.7785234899328859\n\n\n\n\n14.2.1 Voting classifier - hard voting\nIn this type of ensembling, the predicted class is the one predicted by the majority of the classifiers.\n\nensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)])\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.825503355704698\n\n\nNote that the prediction accuracy of the ensemble is higher than the prediction accuracy of each of the individual models on unseen data.\n\n\n14.2.2 Voting classifier - soft voting\nIn this type of ensembling, the predicted class is the one based on the average predicted probabilities of all the classifiers. The threshold probability is 0.5.\n\nensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                 voting='soft')\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.7919463087248322\n\n\nNote that soft voting will be good only for well calibrated classifiers, i.e., all the classifiers must have probabilities at the same scale.\n\n\n14.2.3 Stacking classifier\nConceptually, the idea is similar to that of Stacking regressor.\n\n#Using Logistic regression as the meta model (final_estimator)\nensemble_model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                   final_estimator=LogisticRegression(random_state=1,max_iter=10000),n_jobs=-1,\n                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1))\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.7986577181208053\n\n\n\n#Coefficients of the logistic regression metamodel\nensemble_model.final_estimator_.coef_\n\narray([[0.81748051, 1.28663164, 1.64593342, 1.50947087]])\n\n\n\n#Using random forests as the meta model (final_estimator). Note that random forest will require tuning\nensemble_model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                   final_estimator=RandomForestClassifier(n_estimators=500, max_features=1,\n                                                                          random_state=1,oob_score=True),n_jobs=-1,\n                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1))\nensemble_model.fit(Xtrain,ytrain)\nensemble_model.score(Xtest, ytest)\n\n0.8322147651006712\n\n\nNote that a complex final_estimator such as random forest will require tuning. In the above case, the max_features argument of random forests has been tuned to obtain the maximum OOB score. The tuning is shown below.\n\n#Tuning the random forest parameters\nstart_time = time.time()\noob_score = {}\n\ni=0\nfor pr in range(1,5):\n    model = StackingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)],\n                                   final_estimator=RandomForestClassifier(n_estimators=500, max_features=pr,\n                                    random_state=1,oob_score=True),n_jobs=-1,\n                                   cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)).fit(Xtrain, ytrain)\n    oob_score[pr] = model.final_estimator_.oob_score_\n    \nend_time = time.time()\nprint(\"time taken = \", (end_time-start_time)/60, \" minutes\")\nprint(\"max accuracy = \", np.max(list(oob_score.values())))\nprint(\"Best value of max_features= \", np.argmax(list(oob_score.values()))+1)\n\ntime taken =  0.33713538646698  minutes\nmax accuracy =  0.8445945945945946\nBest value of max_features=  1\n\n\n\n#The final predictor (metamodel) - random forest obtains the maximum oob_score for max_features = 1\noob_score\n\n{1: 0.8445945945945946,\n 2: 0.831081081081081,\n 3: 0.8378378378378378,\n 4: 0.831081081081081}\n\n\n\n\n14.2.4 Tuning all models simultaneously\nIndividual model hyperparameters can be tuned simultaneously while ensembling them with a VotingClassifier(). However, this approach can be too expensive for even moderately-sized datasets.\n\n# Create the param grid with the names of the models as prefixes\n\nmodel_ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier())\nmodel_rf = RandomForestClassifier()\nmodel_gb = GradientBoostingClassifier()\nmodel_xgb = xgb.XGBClassifier()\n\nensemble_model = VotingClassifier(estimators=[('ada',model_ada),('rf',model_rf),('gb',model_gb),('xgb',model_xgb)])\n\nhp_grid = dict()\n\n# XGBoost\nhp_grid['xgb__n_estimators'] = [25, 100,250,50]\nhp_grid['xgb__max_depth'] = [4, 6 ,8]\nhp_grid['xgb__learning_rate'] = [0.01, 0.1, 1.0]\nhp_grid['xgb__gamma'] = [0, 1, 10, 100]\nhp_grid['xgb__reg_lambda'] = [0, 1, 10, 100]\nhp_grid['xgb__subsample'] = [0, 1, 10, 100]\nhp_grid['xgb__scale_pos_weight'] = [1.0, 1.25, 1.5]\nhp_grid['xgb__colsample_bytree'] = [0.5, 0.75, 1.0]\n\n# AdaBoost\nhp_grid['ada__n_estimators'] = [10, 50, 100,200,500]\nhp_grid['ada__base_estimator__max_depth'] = [1, 3, 5]\nhp_grid['ada__learning_rate'] = [0.01, 0.1, 0.2]\n\n# Random Forest\nhp_grid['rf__n_estimators'] = [100]\nhp_grid['rf__max_features'] = [3, 6, 9, 12, 15]\n\n# GradBoost\nhp_grid['gb__n_estimators'] = [10, 50, 100,200,500]\nhp_grid['gb__max_depth'] = [1, 3, 5]\nhp_grid['gb__learning_rate'] = [0.01, 0.1, 0.2, 1.0]\nhp_grid['gb__subsample'] = [0.5, 0.75, 1.0]\n\nstart_time = time.time()\ngrid = RandomizedSearchCV(ensemble_model, hp_grid, cv=5, scoring='accuracy', verbose = True,\n                         n_iter = 100, n_jobs=-1).fit(Xtrain, ytrain)\nprint(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")\n\n\ngrid.best_estimator_.score(Xtest, ytest)\n\n0.8120805369127517",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Ensemble modeling</span>"
    ]
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#instructions",
    "href": "Assignment1.html#instructions",
    "title": "Assignment 1",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Thursday, 11th April 2024 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (2 points). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#bias-variance-trade-off-for-regression-32-points",
    "href": "Assignment1.html#bias-variance-trade-off-for-regression-32-points",
    "title": "Assignment 1",
    "section": "1) Bias-Variance Trade-off for Regression (32 points)",
    "text": "1) Bias-Variance Trade-off for Regression (32 points)\nThe main goal of this question is to understand and visualize the bias-variance trade-off in a regression model by performing repetitive simulations.\nThe conceptual clarity about bias and variance will help with the main logic behind creating many models that will come up later in the course.\n\na)\nFirst, you need to implement the underlying function of the population you want to sample data from. Assume that the function is the Bukin function. Implement it as a user-defined function and run it with the test cases below to make sure it is implemented correctly. (3 points)\nNote: It would be more useful to have only one input to the function. You can treat the input as an array of two elements.\n\nprint(Bukin(np.array([1,2]))) # The output should be 141.177\nprint(Bukin(np.array([6,-4]))) # The output should be 208.966\nprint(Bukin(np.array([0,1]))) # The output should be 100.1\n\n\n\nb)\nUsing the following assumptions, sample a test dataset with 100 observations from the underlying function. Remember how the test dataset is supposed to be sampled for bias-variance calculations. No loops are allowed for this question - .apply should be very useful and actually simpler to use. (4 points)\nAssumptions:\n\nThe first predictor, \\(x_1\\), comes from a uniform distribution between -15 and -5. (\\(U[-15, -5]\\))\nThe second predictor, \\(x_2\\), comes from a uniform distribution between -3 and 3. (\\(U[-3, 3]\\))\nUse np.random.seed(100) for reproducibility.\n\n\n\nc)\nCreate an empty DataFrame with columns named degree, bias_sq and var. This will be useful to store the analysis results in this question. (1 point)\n\n\nd)\nSample 100 training datasets to calculate the bias and the variance of a Linear Regression model that predicts data coming from the underlying Bukin function. You need to repeat this process with polynomial transformations from degree 1 (which is the original predictors) to degree 7. For each degree, store the degree, bias-squared and variance values in the DataFrame. (15 points)\nNote:\n\nFor a linear regression model, bias refers to squared bias\nAssume that the noise in the population is a zero-mean Gaussian with a standard deviation of 10. (\\(N(0,10)\\))\nKeep the training data size the same as the test data size.\nYou need both the interactions and the higher-order transformations in your polynomial predictors.\nFor \\(i^{th}\\) training dataset, you can consider using np.random.seed(i) for reproducibility.\n\n\n\ne)\nUsing the results stored in the DataFrame, plot the (1) expected mean squared error, (2) expected squared bias, (3) expected variance, and (4) the expected sum of squared bias, variance and noise variance (i.e., summation of 2, 3, and noise variance), against the degree of the predictors in the model. (5 points)\nMake sure you add a legend to label the four lineplots. (1 point)\n\n\nf)\nWhat is the degree of the optimal model? (1 point) What are the squared bias, variance and mean squared error for that degree? (2 points)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#low-bias-low-variance-model-via-regularization-25-points",
    "href": "Assignment1.html#low-bias-low-variance-model-via-regularization-25-points",
    "title": "Assignment 1",
    "section": "2) Low-Bias-Low-Variance Model via Regularization (25 points)",
    "text": "2) Low-Bias-Low-Variance Model via Regularization (25 points)\nThe main goal of this question is to further reduce the total error by regularization - in other words, to implement the low-bias-low-variance model for the underlying function and the data coming from it.\n\na)\nFirst of all, explain why it is not guaranteed for the optimal model (with the optimal degree) in Question 1 to be the low-bias-low-variance model. (2 points) Why would regularization be necessary to achieve that model? (2 points)\n\n\nb)\nBefore repeating the process in Question 1, you should see from the figure in 1e and the results in 1f that there is no point in trying some degrees again with regularization. Find out these degrees and explain why you should not use them for this question, considering how regularization affects the bias and the variance of a model. (3 points)\n\n\nc)\nRepeat 1c and 1d with Ridge regularization. Exclude the degrees you found in 2b and also degree 7. Use Leave-One-Out (LOO) cross-validation (CV) to tune the model hyperparameter and use neg_root_mean_squared_error as the scoring metric. (7 points)\nConsider hyperparamter values in the range [1, 100].\n\n\nd)\nRepeat part 1e with Ridge regularization, using the results from 2c. (2 points)\n\n\ne)\nWhat is the degree of the optimal Ridge Regression model? (1 point) What are the bias-squared, variance and total error values for that degree? (1 point) How do they compare to the Linear Regression model results? (2 points)\n\n\nf)\nIs the regularization successful in reducing the total error of the regression model? (2 points) Explain the results in 2e in terms of how bias and variance change with regularization. (3 points)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#bias-variance-trade-off-for-classification-38-points",
    "href": "Assignment1.html#bias-variance-trade-off-for-classification-38-points",
    "title": "Assignment 1",
    "section": "3) Bias-Variance Trade-off for Classification (38 points)",
    "text": "3) Bias-Variance Trade-off for Classification (38 points)\nNow, it is time to understand and visualize the bias-variance trade-off in a classification model. As we covered in class, the error calculations for classification are different than regression, so it is necessary to understand the bias-variance analysis for classification as well.\nFirst of all, you need to visualize the underlying boundary between the classes in the population. Run the given code that implements the following:\n\n2000 test observations are sampled from a population with two predictors.\nEach predictor is uniformly distributed between -15 and 15. (\\(U[-15, 15]\\))\nThe underlying boundary between the classes is a circle with radius 10.\nThe noise in the population is a 30% chance that the observation is misclassified.\n\n\n# Number of observations\nn = 2000\n\nnp.random.seed(111)\n\n# Test predictors\nx1 = np.random.uniform(-15, 15, n)\nx2 = np.random.uniform(-15, 15, n)\nX_test = pd.DataFrame({'x1': x1, 'x2': x2})\n\n# Underlying boundary\nboundary = (x1**2) + (x2**2)\n\n# Test response (no noise!)\ny_test_wo_noise = (boundary &lt; 100).astype(int)\n\n# Test response with noise (for comparison)\nnoise_prob = 0.3\nnum_noisy_obs = int(noise_prob*n)\n\ny_test_w_noise = y_test_wo_noise.copy()\nnoise_indices = np.random.choice(range(len(y_test_w_noise)), num_noisy_obs, replace = False)\ny_test_w_noise[noise_indices] = 1 - y_test_wo_noise[noise_indices]\n\n\nsns.scatterplot(x = x1, y = x2, hue=y_test_wo_noise)\nplt.title('Sample without the noise')\nplt.show()\n\n\n\n\n\n\n\nsns.scatterplot(x = x1, y = x2, hue=y_test_w_noise)\nplt.title('Sample with the noise')\nplt.show()\n\n\n\n\n\n\n\na)\nCreate an empty DataFrame with columns named K, bias, var and noise. This will be useful to store the analysis results in this question. (1 point)\n\n\nb)\nSample 100 training datasets to calculate the bias and the variance of a K-Nearest Neighbors (KNN) Classifier that predicts data coming from the population with the circular underlying boundary. You need to repeat this process with a K value from 10 to 150, with a stepsize of 10. For each K, store the following values in the DataFrame:\n\nK,\nbias,\nvariance,\nexpected loss computed directly using the true response and predictions,\nexpected loss computed as (expected Bias) + (\\(c_2\\) expected variance) + (\\(c_1\\) expected noise)\n\n(20 points)\nNote:\n\nKeep the training data size the same as the test data size.\nThe given code should help you both with sampling the training data and adding noise to the training responses.\nFor \\(i^{th}\\) training dataset, you can consider using np.random.seed(i) for reproducibility.\nTo check the progress of the code while running, a simple but efficient method is to add a print(K) line in the loop.\n\n\n\nc)\nUsing the results stored in the DataFrame, plot the bias and the variance against the K value on one figure, and the expected loss (computed directly) & expected loss computed as (expected Bias) + (\\(c_2\\)expected variance) + (\\(c_1\\)expected noise) against the K value on a separate figure. (5 points) Make sure you add a legend to label the lineplots in the first figure. (1 point)\n\n\nd)\nWhat is the K of the optimal model? (1 point) What are the bias, variance and expected loss (computed either way) for that K? (2 points)\n\n\ne)\nIn part c, you should see the variance leveling off after a certain K value. Explain why this is the case, considering the effect of the K value on a KNN model. (2 points)\n\n\nf)\nLastly, visualize the decision boundary of a KNN Classifier with high-bias-low-variance (option 1) and low-bias-high-variance (option 2), using data from the same population.\n\nFor each option, pick a K value (1 and 90 would be good numbers.) You are expected to know which number belongs to which option.\nSample a training dataset. (Use np.random.seed(1).)\nUsing the training dataset, train a KNN model with the K value you picked.\nThe rest of the code is given below for you.\n\nNote that you need to produce two figures. (2x2 = 4 points) Put titles on the figures to describe which figure is which option. (2 points)\n\n# Develop and save the model as the 'model' object before using the code\nxx, yy = np.meshgrid(np.linspace(-15, 15, 100), np.linspace(-15, 15, 100))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nsns.scatterplot(x = x1, y = x2, hue=y_train, legend=False);\nplt.contour(xx, yy, Z, levels=[0.5], linewidths=2)\n\nplt.title('____-bias-____-variance Model')",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment 2 questions.html",
    "href": "Assignment 2 questions.html",
    "title": "Assignment 2 (Section 21)",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Assignment 2 (Section 21)</span>"
    ]
  },
  {
    "objectID": "Assignment 2 questions.html#instructions",
    "href": "Assignment 2 questions.html#instructions",
    "title": "Assignment 2 (Section 21)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Monday, 22nd April 2024 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (2 points). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Assignment 2 (Section 21)</span>"
    ]
  },
  {
    "objectID": "Assignment 2 questions.html#tuning-a-knn-classifier-with-sklearn-tools-40-points",
    "href": "Assignment 2 questions.html#tuning-a-knn-classifier-with-sklearn-tools-40-points",
    "title": "Assignment 2 (Section 21)",
    "section": "1) Tuning a KNN Classifier with Sklearn Tools (40 points)",
    "text": "1) Tuning a KNN Classifier with Sklearn Tools (40 points)\nIn this question, you will use classification_data.csv. Each row is a loan and the each column represents some financial information as follows:\n\nhi_int_prncp_pd: Indicates if a high percentage of the repayments went to interest rather than principal. This is the classification response.\nout_prncp_inv: Remaining outstanding principal for portion of total amount funded by investors\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\nint_rate: Interest Rate on the loan\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60.\nmort_acc: The number of mortgage accounts\napplication_type_Individual: 1 if the loan is an individual application or a joint application with two co-borrowers\ntot_cur_bal: Total current balance of all accounts\npub_rec: Number of derogatory public records\n\nAs indicated above, hi_int_prncp_pd is the response and all the remaining columns are predictors. You will tune and train a K-Nearest Neighbors (KNN) classifier throughout this question.\n\n1a)\nRead the dataset. Create the predictor and the response variables.\nCreate the training and the test data with the following specifications: - The split should be 75%-25%. - You need to ensure that the class ratio is preserved in the training and the test datasets. i.e. the data is stratified. - Use random_state=45.\nPrint the class ratios of the entire dataset, the training set and the test set to check if the ratio is kept the same.\n(1 point)\n\n\n1b)\nScale the datasets. The data is ready for modeling at this point.\nBefore creating and tuning a model, you need to create a sklearn cross-validation object to ensure the most accurate representation of the data among all the folds.\nUse the following specifications for your cross-validation settings: - Make sure the data is stratified in all the folds (Use StratifiedKFold()). - Use 5 folds. - Shuffle the data for more randomness. - Use random_state=14.\n(1 point)\nNote that you need to use these settings for the rest of this question (Q1) for consistency.\nCross-validate a KNN Classifier with the following specifications: - Use every odd K value between 1 and 50. (including 1) - Fix the weights at “uniform”, which is default. - Use the cv object you created in part 1(c). - Use accuracy as metric.\n(4 points)\nPrint the best average cross-validation accuracy and the K value that corresponds to it. (2 points)\n\n\n1c)\nUsing the optimal K value you found in part 1(b), find the threshold that maximizes the cross-validation accuracy with the following specifications:\n\nUse all the possible threshold values with a stepsize of 0.01.\nUse the cross-validation settings you created in part f.\nUse accuracy as metric, which is default.\n\n(4 points)\nPrint the best cross-validation accuracy (1 point) and the threshold value that corresponds to it. (1 points)\n\n\n1d)\nIs the method we used in parts 1(b) and 1(c) guaranteed to find the best K & threshold combination, i.e. tune the classifier to its best values? (1 point) Why or why not? (1 point)\n\n\n1e)\nUse the tuned classifier and threshold to find the test accuracy. (2 points) .\nHow does it compare to the cross-validation accuracy, i.e. is the model generalizing well? (1 point)\n\n\n1f)\nNow, you need to tune K and the threshold at the same time. Use the following specifications: - Use every odd K value between 1 and 50. (including 1) - Fix the weights at “uniform”. - Use all the possible threshold values with a stepsize of 0.01. - Use accuracy as metric.\n(5 points)\nPrint the best cross-validation accuracy, and the K and threshold values that correspond to it. (1 point)\n\n\n1g)\nHow does the best cross-validation accuracy in part 1(f) compare to parts 1(b) and 1(c)? (1 point) Did the K and threshold value change? (1 point) Explain why or why not. (2 points)\n\n\n1h)\nUse the tuned classifier and threshold from part 1(f) to find the test accuracy. (1 point)\n\n\n1i)\nCompare the methods you used in parts 1(b) & 1(c) with the method you used in part 1(f) in terms of computational power. How many K & threshold pairs did you try in both? (2 points) Combining your answer with the answer in part 1(i), explain the main trade-off while tuning a model. (2 points)\n\n\n1j)\nCross-validate a KNN classifier with the following specifications: - Use every odd K value between 1 and 50. (including 1) - Fix the weights at “uniform” - Use accuracy, precision and recall as three metrics at the same time.\nFind the K value that maximizes recall while having a precision above 75%. (3 points) Print the average cross-validation results of that K value. (1 point)\nWhich metric (among precision, recall, and accuracy) seems to be the least sensitive to the value of ‘K’. Why? (3 points)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Assignment 2 (Section 21)</span>"
    ]
  },
  {
    "objectID": "Assignment 2 questions.html#tuning-a-knn-regressor-with-sklearn-tools-55-points",
    "href": "Assignment 2 questions.html#tuning-a-knn-regressor-with-sklearn-tools-55-points",
    "title": "Assignment 2 (Section 21)",
    "section": "2) Tuning a KNN Regressor with Sklearn Tools (55 points)",
    "text": "2) Tuning a KNN Regressor with Sklearn Tools (55 points)\nIn this question, you will use bank_loan_train_data.csv to tune (the model hyperparameters) and train the model. Each row is a loan and the each column represents some financial information as follows:\n\nmoney_made_inv: Indicates the amount of money made by the bank on the loan. This is the regression response.\nout_prncp_inv: Remaining outstanding principal for portion of total amount funded by investors\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\nint_rate: Interest Rate on the loan\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60\nmort_acc: The number of mortgage accounts\napplication_type_Individual: 1 if the loan is an individual application or a joint application with two co-borrowers\ntot_cur_bal: Total current balance of all accounts\npub_rec: Number of derogatory public records\n\nAs indicated above, money_made_inv is the response and all the remaining columns are predictors. You will tune and train a K-Nearest Neighbors (KNN) regressor throughout this question.\n\n2a)\nFind the optimal hyperparameter values and the corresponging optimal cross-validated RMSE. The hyperparameters that you must consider are\n\nNumber of nearest neighbors,\nWeight of the neighbor, and\nthe power p of the Minkowski distance.\n\nFor the weights hyperparameter, in addition to uniform and distance, consider 3 custom weights as well. The custom weights to consider are weight inversely proportional to distance squared, weight inversely proportional to distance cube, and weight inversely proportional to distance raised to the power of 4. Mathematically, these weights can be written as:\nweight∝1weight \\propto 1,\nweight∝1distanceweight \\propto \\frac{1}{distance},\nweight∝1distance2weight \\propto \\frac{1}{distance^2}\nweight∝1distance3weight \\propto \\frac{1}{distance^3}\nweight∝1distance4weight \\propto \\frac{1}{distance^4}\nShow all the 3 search approaches - grid search, random search, and Bayes search. As this is a simple problem, all the 3 approaches should yield the same result.\nFor Bayes search, show the implementation of real-time monitoring of cross-validation error.\nNone of the cross-validation approaches should take more than a minute as this is a simlpe problem.\nHint:\nCreate three different user-defined functions. The functions should take one input, named distance and return 1/(1e-10+distance**n), where n is 2, 3, and 4, respectively. Note that the 1e-10 is to avoid computational overflow.\nName your functions, dist_power_n, where n is 2, 3, and 4, respectively. You can use these function names as the weights input to a KNN model.\n(15 points)\n\n\n2b)\nBased on the optimal model in 2(a), find the RMSE on test data (bank_loan_test_data.csv). It must be less than $1400.\nNote: You will achieve the test RMSE if you tuned the hyperparameters well in 2(a). If you did not, redo 2(a). You are not allowed to use test data for tuning the hyperparameter values.\n(2 points)\n\n\n2c)\nKNN performance may deteriorate significantly if irrelevant predictors are included. We’ll add variable selection as well in the cross-validation procedure along with tuning of the hyperparameters for those variables.\nUse a variable selection method to consider the best ‘r’ predictors, optimize the hyperparameters specified in 2(a), and compute the cross-validation error for those ‘r’ predictors. Note that ‘r’ will vary from 1 to 7, thus you will need to do 7 cross-validations - one for each ‘r’.\nReport the optimal value of ‘r’, the ‘r’ predictors, the optimal hyperparameter values, and the optimal cross-validated RMSE.\nYou are free to use any search method.\nHint: You may use Lasso to consider the best ‘r’ predictors as that is the only variable selection you have learned so far.\n(20 points)\n\n\n2d)\nFind the RMSE on test data based on the optimal model in 2(c). Your test RMSE must be less than $800.\nNote: You will achieve the test RMSE if you tuned the hyperparameters well in 2(c). If you did not, redo 2(c). You are not allowed to use test data for tuning the hyperparameter values.\n(2 points)\n\n\n2e)\nHow did you decide the range of hyperparameter values to consider in this question? Discuss for p and n_neighbors.\n(4 points)\n\n\n2f)\nIs it possible to futher improve the results if we also optimize the metric hyperparameter along with the hyperparameters specified in 2(a)? Why or why not?\n(4 points)\n\n\n2g)\nWhat is the benefit of using the RepeatedKFold() function over the KFold() function of the model_selection module of the sklearn library? Explain in terms of bias-variance of test error. Did you observe any benefit of using RepeatedKFold() over KFold() in Q2? Why or why not?\n(4 + 4 points)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Assignment 2 (Section 21)</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html",
    "href": "Assignment 3.html",
    "title": "Assignment 3 (Sections 21 & 22)",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Assignment 3 (Sections 21 & 22)</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#instructions",
    "href": "Assignment 3.html#instructions",
    "title": "Assignment 3 (Sections 21 & 22)",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Wednesday, 8th May 2024 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\n\nMust be an HTML file rendered using Quarto (2 pts). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file. If your issue doesn’t seem genuine, you will lose points.\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 pt)\nFinal answers of each question are written in Markdown cells (1 pt).\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text (1 pt)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Assignment 3 (Sections 21 & 22)</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#regression-problem---miami-housing",
    "href": "Assignment 3.html#regression-problem---miami-housing",
    "title": "Assignment 3 (Sections 21 & 22)",
    "section": "1) Regression Problem - Miami housing",
    "text": "1) Regression Problem - Miami housing\n\n1a) Data preparation\nRead the data miami-housing.csv. Check the description of the variables here. Split the data into 60% train and 40% test. Use random_state = 45. The response is SALE_PRC, and the rest of the columns are predictors, except PARCELNO. Print the shape of the predictors dataframe of the train data.\n(2 points)\n\n\n1b) Decision tree\nDevelop a decision tree model to predict SALE_PRC based on all the predictors. Use random_state = 45. Use the default hyperparameter values. What is the MAE (mean absolute error) on test data, and the cross-validated MAE?\n(3 points)\n\n\n1c) Tuning decision tree\nTune the hyperparameters of the decision tree model developed in the previous question, and compute the MAE on test data. You must tune the hyperparameters in the following manner:\nThe cross-validated MAE obtained must be less than $68,000. You must show the optimal values of the hyperparameters obtained, and the find the test MAE with the tuned model.\nHint:\n\nBayesSearchCV() may take less than a minute with max_depth and max_features\nYou are free to decide which hyperparameters to tune.\n\n(9 points)\n\n\n1d) Bagging decision trees\nBag decision trees, and compute the out-of-bag MAE. Use enough number of trees, such that the MAE stabilizes. Other than n_estimators, use default values of hyperparameters.\nThe out-of-bag cross-validated MAE must be less than $48,000.\n(4 points)\n\n\n1e) Bagging without bootstrapping\nBag decision trees without bootstrapping, i.e., put bootstrap = False while bagging the trees, and compute the cross-valdiated MAE. Why is the MAE obtained much higher than that in the previous question, but lower than that obtained in 1(b)?\n(1 point for code, 3 + 3 points for reasoning)\n\n\n1f) Bagging without bootstrapping samples, but bootstrapping features\nBag decision trees without bootstrapping samplse, but bootstrapping features, i.e., put bootstrap = False, and bootstrap_features = True while bagging the trees, and compute the cross-validated MAE. Why is the MAE obtained much lower than that in the previous question?\n(1 point for code, 3 points for reasoning)\n\n\n1g) Tuning bagged tree model\n\n1g)i) Approaches\nThere are two approaches for tuning a bagged tree model:\n\nOut of bag prediction\nKK-fold cross validation using GridSearchCV.\n\nWhat is the advantage of each approach over the other, i.e., what is the advantage of the out-of-bag approach over KK-fold cross validation, and what is the advantage of KK-fold cross validation over the out-of-bag approach?\n(3 + 3 points)\n\n\n1g)ii) Tuning the hyperparameters\nTune the hyperparameters of the bagged tree model developed in 1(d). You may use either of the approaches mentioned in the previous question. Show the optimal values of the hyperparameters obtained. Compute the MAE on test data with the tuned model. Your cross-validated MAE must be less than the cross-validate MAE ontained in the previous question.\nIt is up to you to pick the hyperparameters and their values in the grid.\nHint:\nGridSearchCV() may work better than BayesSearchCV() in this case. Why?\n(9 points)\n\n\n\n1h) Random forest\n\n1h)(i) Tuning random forest\nTune a random forest model to predict SALE_PRC, and compute the MAE on test data. The cross-validated MAE must be less than $46,000.\nIt is up to you to pick the hyperparameters and their values in the grid.\nHint: OOB approach will take less than a minute.\n(9 points)\n\n\n1h)(ii) Feature importance\nArrange and print the predictors in decreasing order of importance.\n(4 points)\n\n\n1h)(iii) Feature selection\nDrop the least important predictor, and find the cross-validated MAE of the tuned model again. You may need to adjust the max_features hyperparameter to account for the dropped predictor. Did the cross-validate MAE reduce?\n(4 points)\n\n\n1h)(iv) Random forest vs bagging: max_features\nNote that the max_features hyperparameter is there both in the RandomForestRegressor() function and the BaggingRegressor() function. Does it have the same meaning in both the functions? If not, then what is the difference?\nHint: Check scikit-learn documentation\n(1 + 3 points)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Assignment 3 (Sections 21 & 22)</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#classification---term-deposit",
    "href": "Assignment 3.html#classification---term-deposit",
    "title": "Assignment 3 (Sections 21 & 22)",
    "section": "2) Classification - Term deposit",
    "text": "2) Classification - Term deposit\nThe data for this question is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls, where bank clients were called to subscribe for a term deposit.\nThere is a train data - train.csv, which you will use to develop a model. There is a test data - test.csv, which you will use to test your model. Each dataset has the following attributes about the clients called in the marketing campaign:\n\nage: Age of the client\neducation: Education level of the client\nday: Day of the month the call is made\nmonth: Month of the call\ny: did the client subscribe to a term deposit?\nduration: Call duration, in seconds. This attribute highly affects the output target (e.g., if duration=0 then y=‘no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for inference purposes and should be discarded if the intention is to have a realistic predictive model.\n\n(Raw data source: Source. Do not use the raw data source for this assignment. It is just for reference.)\n\n2a) Data preparation\nConvert all the categorical predictors in the data to dummy variables. Note that month and education are categorical variables.\n(2 points)\n\n\n2b) Random forest\nDevelop and tune a random forest model to predict the probability of a client subscribing to a term deposit based on age, education, day and month. The model must have:\n\nMinimum overall classification accuracy of 75% among the classification accuracies on train.csv, and test.csv.\nMinimum recall of 60% among the recall on train.csv, and test.csv.\n\nPrint the accuracy and recall for both the datasets - train.csv, and test.csv.\nNote that:\n\nYou cannot use duration as a predictor. The predictor is not useful for prediction because its value is determined after the marketing call ends. However, after the call ends, we already know whether the client responded positively or negatively.\nYou are free to choose any value of threshold probability for classifying observations. However, you must use the same threshold on both the datasets.\nUse cross-validation on train data to optimize the model hyperparameters.\nUsing the optimal model hyperparameters obtained in (iii), develop the decision tree model. Plot the cross-validated accuracy and recall against decision threshold probability. Tune the decision threshold probability based on the plot, or the data underlying the plot to achieve the required trade-off between recall and accuracy.\nEvaluate the accuracy and recall of the developed model with the tuned decision threshold probability on both the datasets. Note that the test dataset must only be used to evaluate performance metrics, and not optimize any hyperparameters or decision threshold probability.\n\n(22 points - 8 points for tuning the hyperparameters, 5 points for making the plot, 5 points for tuning the decision threshold probability based on the plot, and 4 points for printing the accuracy & recall on both the datasets)\nHint:\n\nRestrict the search for max_depth to a maximum of 25, and max_leaf_nodes to a maximum of 45. Without this restriction, you may get a better recall for threshold probability = 0.5, but are likely to get a worse trade-off between recall and accuracy. Tune max_features, max_depth, and max_leaf_nodes with OOB cross-validation.\nUse oob_decision_function_ for OOB cross-validated probabilities.\n\nIt is up to you to pick the hyperparameters and their values in the grid.",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Assignment 3 (Sections 21 & 22)</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#predictor-transformations-in-trees",
    "href": "Assignment 3.html#predictor-transformations-in-trees",
    "title": "Assignment 3 (Sections 21 & 22)",
    "section": "3) Predictor transformations in trees",
    "text": "3) Predictor transformations in trees\nCan a non-linear monotonic transformation of predictors (such as log(), sqrt() etc.) be useful in improving the accuracy of decision tree models?\n(4 points for answer)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Assignment 3 (Sections 21 & 22)</span>"
    ]
  },
  {
    "objectID": "HomeworkAssignment4.html",
    "href": "HomeworkAssignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "HomeworkAssignment4.html#instructions",
    "href": "HomeworkAssignment4.html#instructions",
    "title": "Assignment 4",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThe assignment is worth 100 points, and is due on Friday, 24th May 2024 at 11:59 pm.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (1 point). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission. (1 point)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "HomeworkAssignment4.html#adaboost-vs-bagging-4-points",
    "href": "HomeworkAssignment4.html#adaboost-vs-bagging-4-points",
    "title": "Assignment 4",
    "section": "1) AdaBoost vs Bagging (4 points)",
    "text": "1) AdaBoost vs Bagging (4 points)\nWhich model among AdaBoost and Random Forest is more sensitive to outliers? (1 point) Explain your reasoning with the theory you learned on the training process of both models. (3 points)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "HomeworkAssignment4.html#regression-with-boosting-55-points",
    "href": "HomeworkAssignment4.html#regression-with-boosting-55-points",
    "title": "Assignment 4",
    "section": "2) Regression with Boosting (55 points)",
    "text": "2) Regression with Boosting (55 points)\nFor this question, you will use the miami_housing.csv file. You can find the description for the variables here.\nThe SALE_PRC variable is the regression response and the rest of the variables, except PARCELNO, are the predictors.\n\na)\nRead the dataset. Create the training and test sets with a 60%-40% split and random_state = 1. (1 point)\n\n\nb)\nTune an AdaBoost model to get below a cross-validation MAE of $48000. Keep all the random_states as 1. Getting below the given cutoff with a different random_state in ANY object will not receive any credit. (5 points for a search that makes sense + 5 points for the cutoff = 10 points)\nHints:\n\nRemember how you need to approach the tuning process with coarse and fine grids.\nRemember that you have different cross-validation settings available.\n\n\n\nc)\nFind the test MAE of the tuned AdaBoost model to see if it generalizes well. (1 point)\n\n\nd)\nUsing the tuned AdaBoost model, print the predictor names with their importances in decreasing order. You need to print a DataFrame with the predictor names in the first column and the importances in the second. (1 points)\nNote: Features importances can be taken with pretty much the same line of code for all the models in this assignment. It is asked only for AdaBoost and omitted for the remaining models to avoid repetition.\n\n\ne)\nMoving on to Gradient Boosting, in general, which is the most preferred loss function? (1 point) What are its advantages over other loss functions? (3 points)\n\n\nf)\nTune a Gradient Boosting model to get below a cross-validation MAE of $45000. Keep all the random_states as 1. Getting below the given cutoff with a different random_state in ANY object will not receive any credit. (5 points for a search that makes sense + 5 points for the cutoff = 10 points)\nHints:\n\nRemember how you need to approach the grid of Gradient Boosting.\nRemember that you have different cross-validation settings available.\n\n\n\ng)\nFind the test MAE of the tuned Gradient Boosting model to see if it generalizes well. (1 point)\n\n\nh)\nExplain how the tuned hyperparameters of AdaBoost and Gradient Boosting affect the bias and the variance of their model. Note that most hyperparameters are the same between the models, so give only one explanation for those. (You need to include four hyperparameters in total.) (1x4 = 4 points)\n\n\ni)\nMoving on to XGBoost:\n\nWhat are the additions that makes XGBoost superior to Gradient Boosting? You need to explain this in terms of runtime (1 point) with its reason (1 point) and the hyperparameters (1 point) with their effect of model behavior. (2 points).\nWhat is missing in XGBoost that is well-implemented in Gradient Boosting? (1 point)\n\n\n\nj)\nTune a XGBoost model to get below a cross-validation MAE of $43500. Keep all the random_states as 1. Getting below the given cutoff with a different random_state in ANY object will not receive any credit. (5 points for a search that makes sense + 5 points for the cutoff = 10 points)\nHints:\n\nRemember how you need to approach the grid of XGBoost.\nRemember that you have different cross-validation settings available.\n\n\n\nk)\nFind the test MAE of the tuned XGBoost model to see if it generalizes well. (1 point)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "HomeworkAssignment4.html#classification-with-boosting-42-points",
    "href": "HomeworkAssignment4.html#classification-with-boosting-42-points",
    "title": "Assignment 4",
    "section": "2) Classification with Boosting (42 points)",
    "text": "2) Classification with Boosting (42 points)\nFor this question, you will use the train.csv and test.csv files. Each observation is a marketing call from a banking institution. y variable represents if the client subscribed for a term deposit (1) or not (0) and it is the classification response.\nThe predictors are age, day, month, and education. (As mentioned last quarter, duration cannot be used as a predictor - no credit will be given to models that use it.)\n\na)\nPreprocess the data:\n\nRead the files.\nCreate the predictor and response variables.\nConvert the response to 1s and 0s.\nOne-hot-encode the categorical predictors (Do not use drop_first.)\n\n(1 point)\n\n\nb)\nMoving on to LightGBM and CatBoost, what are their advantages compared to Gradient Boosting and XGBoost? (2 points) How are these advantages implemented into the models? (2 points) Does any of them have any disadvantages? Describe if there is any. (1 point)\n\n\nc)\nFor all extensions of Gradient Boosting, (XGBoost/LightGBM/CatBoost) is there an additional input/hyperparameter you can use to handle a certain issue that is specific to classification? (1 point) If yes, describe what it stands for (1 point) and how its value should be handled most efficiently. (1 point)\n\n\nd)\nTune a LightGBM model to get above a cross-validation accuracy of 70% and a cross-validation recall of 65%. Keep all the random_states as 1. Getting above the given cutoffs with a different random_state in ANY object will not receive any credit. (7.5 points for a search that makes sense + 7.5 points for the cutoff = 15 points)\nHints:\n\nHandling the grid efficiently can be useful again.\nRemember that there are cross-validation settings that are specific to classification.\nRemember that for classification, you need to tune the threshold as well.\n\n\n\ne)\nFind the test accuracy and the test recall of the tuned LightGBM model and threshold to see if they generalize well. (2 points)\n\n\nf)\nTune a CatBoost model to get above a cross-validation accuracy of 70% and a cross-validation recall of 65%. Keep all the random_states as 1. Getting above the given cutoffs with a different random_state in ANY object will not receive any credit. (7.5 points for a search that makes sense + 7.5 points for the cutoff = 15 points)\nHints:\n\nHandling the grid efficiently can be useful again.\nRemember that there are cross-validation settings that are specific to classification.\nRemember that for classification, you need to tune the threshold as well. (Use a stepsize of 0.001)\n\n\n\ng)\nFind the test accuracy and the test recall of the tuned CatBoost model and threshold to see if they generalize well. (1 point)",
    "crumbs": [
      "Assignments",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "Stratified splitting.html",
    "href": "Stratified splitting.html",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "",
    "text": "A.1 Stratified splitting with respect to response\nQ: When splitting data into train and test for developing and assessing a classification model, it is recommended to stratify the split with respect to the response. Why?\nA: The main advantage of stratified splitting is that it can help ensure that the training and testing sets have similar distributions of the target variable, which can lead to more accurate and reliable model performance estimates.\nIn many real-world datasets, the target variable may be imbalanced, meaning that one class is more prevalent than the other(s). For example, in a medical dataset, the majority of patients may not have a particular disease, while only a small fraction may have the disease. If a random split is used to divide the dataset into training and testing sets, there is a risk that the testing set may not have enough samples from the minority class, which can lead to biased model performance estimates.\nStratified splitting addresses this issue by ensuring that both the training and testing sets have similar proportions of the target variable. This can lead to more accurate model performance estimates, especially for imbalanced datasets, by ensuring that the testing set contains enough samples from each class to make reliable predictions.\nAnother advantage of stratified splitting is that it can help ensure that the model is not overfitting to a particular class. If a random split is used and one class is overrepresented in the training set, the model may learn to predict that class well but perform poorly on the other class(es). Stratified splitting can help ensure that the model is exposed to a representative sample of all classes during training, which can improve its generalization performance on new, unseen data.\nIn summary, the advantages of stratified splitting are that it can lead to more accurate and reliable model performance estimates, especially for imbalanced datasets, and can help prevent overfitting to a particular class.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Stratified splitting (classification problem)</span>"
    ]
  },
  {
    "objectID": "Stratified splitting.html#stratified-splitting-with-respect-to-response-and-categorical-predictors",
    "href": "Stratified splitting.html#stratified-splitting-with-respect-to-response-and-categorical-predictors",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "A.2 Stratified splitting with respect to response and categorical predictors",
    "text": "A.2 Stratified splitting with respect to response and categorical predictors\nQ: Will it be better to stratify the split with respect to the response as well as categorical predictors, instead of only the response? In that case, the train and test datasets will be even more representative of the complete data.\nA: It is not recommended to stratify with respect to both the response and categorical predictors simultaneously, while splitting a dataset into train and test, because doing so may result in the test data being very similar to train data, thereby defeating the purpose of assessing the model on unseen data. This kind of a stratified splitting will tend to make the relationships between the response and predictors in train data also appear in test data, which will result in the performance on test data being very similar to that in train data. Thus, in this case, the ability of the model to generalize to new, unseen data won’t be assessed by test data.\nTherefore, it is generally recommended to only stratify the response variable when splitting the data for model training, and to use random sampling for the predictor variables. This helps to ensure that the model is able to capture the underlying relationships between the predictor variables and the response variable, while still being able to generalize well to new, unseen data.\nIn the extreme scenario, when there are no continuous predictors, and there are enough observations for stratification with respect to the response and the categorical predictors, the train and test datasets may turn out to be exactly the same. Example 1 below illustrates this scenario.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Stratified splitting (classification problem)</span>"
    ]
  },
  {
    "objectID": "Stratified splitting.html#example-1",
    "href": "Stratified splitting.html#example-1",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "A.3 Example 1",
    "text": "A.3 Example 1\nThe example below shows that the train and test data can be exactly the same if we stratify the split with respect to response and the categorical predictors.\n\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom itertools import product\nsns.set(font_scale=1.35)\n\nLet us simulate a dataset with 8 observations, two categorical predictors x1 and x2 and the the binary response y.\n\n#Setting a seed for reproducible results\nnp.random.seed(9)\n\n# 8 observations\nn = 8\n\n#Simulating the categorical predictors\nx1 = pd.Series(np.random.randint(0,2,n), name = 'x1')\nx2 = pd.Series(np.random.randint(0,2,n), name = 'x2')\n\n#Simulating the response\npr = (x1==1)*0.7+(x2==0)*0.3# + (x3*0.1&gt;0.1)*0.1\ny = pd.Series(1*(np.random.uniform(size = n) &lt; pr), name = 'y')\n\n#Defining the predictor object 'X'\nX = pd.concat([x1, x2], axis = 1)\n\n#Stratified splitting with respect to the response and predictors to create 50% train and test datasets\nX_train_stratified, X_test_stratified, y_train_stratified,\\\ny_test_stratified = train_test_split(X, y, test_size = 0.5, random_state = 45, stratify=data[['x1', 'x2', 'y']])\n\n#Train and test data resulting from the above stratified splitting\ndata_train = pd.concat([X_train_stratified, y_train_stratified], axis = 1)\ndata_test = pd.concat([X_test_stratified, y_test_stratified], axis = 1)\n\nLet us check the train and test datasets created with stratified splitting with respect to both the predictors and the response.\n\ndata_train\n\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n2\n0\n0\n1\n\n\n7\n0\n1\n0\n\n\n3\n1\n0\n1\n\n\n1\n0\n1\n0\n\n\n\n\n\n\n\n\n\ndata_test\n\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n4\n0\n1\n0\n\n\n6\n1\n0\n1\n\n\n0\n0\n1\n0\n\n\n5\n0\n0\n1\n\n\n\n\n\n\n\n\nNote that the train and test datasets are exactly the same! Stratified splitting tends to have the same proportion of observations corresponding to each strata in both the train and test datasets, where each strata is a unique combination of values of x1, x2, and y. This will tend to make the train and test datasets quite similar!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Stratified splitting (classification problem)</span>"
    ]
  },
  {
    "objectID": "Stratified splitting.html#example-2-simulation-results",
    "href": "Stratified splitting.html#example-2-simulation-results",
    "title": "Appendix A — Stratified splitting (classification problem)",
    "section": "A.4 Example 2: Simulation results",
    "text": "A.4 Example 2: Simulation results\nThe example below shows that train and test set performance will tend to be quite similar if we stratify the datasets with respect to the predictors and the response.\nWe’ll simulate a dataset consisting of 1000 observations, 2 categorical predictors x1 and x2, a continuous predictor x3, and a binary response y.\n\n#Setting a seed for reproducible results\nnp.random.seed(99)\n\n# 1000 Observations\nn = 1000\n\n#Simulating categorical predictors x1 and x2\nx1 = pd.Series(np.random.randint(0,2,n), name = 'x1')\nx2 = pd.Series(np.random.randint(0,2,n), name = 'x2')\n\n#Simulating continuous predictor x3\nx3 = pd.Series(np.random.normal(0,1,n), name = 'x3')\n\n#Simulating the response\npr = (x1==1)*0.7+(x2==0)*0.3 + (x3*0.1&gt;0.1)*0.1\ny = pd.Series(1*(np.random.uniform(size = n) &lt; pr), name = 'y')\n\n#Defining the predictor object 'X'\nX = pd.concat([x1, x2, x3], axis = 1)\n\nWe’ll comparing model performance metrics when the data is split into train and test by performing stratified splitting\n\nOnly with respect to the response\nWith respect to the response and categorical predictors\n\nWe’ll perform 1000 simulations, where the data is split using a different seed in each simulation.\n\n#Creating an empty dataframe to store simulation results of 1000 simulations\naccuracy_iter = pd.DataFrame(columns = {'train_y_stratified','test_y_stratified',\n                                        'train_y_CatPredictors_stratified','test_y_CatPredictors_stratified'})\n\n\n# Comparing model performance metrics when the data is split into train and test by performing stratified splitting\n# (1) only with respect to the response\n# (2) with respect to the response and categorical predictors\n\n# Stratified splitting is performed 1000 times and the results are compared\nfor i in np.arange(1,1000):\n \n    #--------Case 1-------------------#\n    # Stratified splitting with respect to response only to create train and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = i, stratify=y)\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Model accuracy on train and test data, with stratification only on response while splitting \n    # the complete data into train and test\n    accuracy_iter.loc[(i-1), 'train_y_stratified'] = model.score(X_train, y_train)\n    accuracy_iter.loc[(i-1), 'test_y_stratified'] = model.score(X_test, y_test)\n        \n    #--------Case 2-------------------#\n    # Stratified splitting with respect to response and categorical predictors to create train \n    # and test data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = i, \n                                                        stratify=pd.concat([x1, x2, y], axis = 1))\n    model.fit(X_train, y_train)\n\n    # Model accuracy on train and test data, with stratification on response and predictors while \n    # splitting the complete data into train and test\n    accuracy_iter.loc[(i-1), 'train_y_CatPredictors_stratified'] = model.score(X_train, y_train)\n    accuracy_iter.loc[(i-1), 'test_y_CatPredictors_stratified'] = model.score(X_test, y_test)\n    \n# Converting accuracy to numeric\naccuracy_iter = accuracy_iter.apply(lambda x:x.astype(float), axis = 1)\n\n\nDistribution of train and test accuracies\nThe table below shows the distribution of train and test accuracies when the data is split into train and test by performing stratified splitting:\n\nOnly with respect to the response (see train_y_stratified and test_y_stratified)\nWith respect to the response and categorical predictors (see train_y_CatPredictors_stratified and test_y_CatPredictors_stratified)\n\n\naccuracy_iter.describe()\n\n\n\n\n\n\n\n\n\ntrain_y_stratified\ntest_y_stratified\ntrain_y_CatPredictors_stratified\ntest_y_CatPredictors_stratified\n\n\n\n\ncount\n999.000000\n999.000000\n9.990000e+02\n9.990000e+02\n\n\nmean\n0.834962\n0.835150\n8.350000e-01\n8.350000e-01\n\n\nstd\n0.005833\n0.023333\n8.552999e-15\n8.552999e-15\n\n\nmin\n0.812500\n0.755000\n8.350000e-01\n8.350000e-01\n\n\n25%\n0.831250\n0.820000\n8.350000e-01\n8.350000e-01\n\n\n50%\n0.835000\n0.835000\n8.350000e-01\n8.350000e-01\n\n\n75%\n0.838750\n0.850000\n8.350000e-01\n8.350000e-01\n\n\nmax\n0.855000\n0.925000\n8.350000e-01\n8.350000e-01\n\n\n\n\n\n\n\n\nLet us visualize the distribution of these accuracies.\n\n\nA.4.1 Stratified splitting only with respect to the response\n\nsns.histplot(data=accuracy_iter, x=\"train_y_stratified\", color=\"red\", label=\"Train accuracy\", kde=True)\nsns.histplot(data=accuracy_iter, x=\"test_y_stratified\", color=\"skyblue\", label=\"Test accuracy\", kde=True);\nplt.legend()\nplt.xlabel('Accuracy')\n\nText(0.5, 0, 'Accuracy')\n\n\n\n\n\n\n\n\n\nNote the variability in train and test accuracies when the data is stratified only with respect to the response. The train accuracy varies between 81.2% and 85.5%, while the test accuracy varies between 75.5% and 92.5%.\n\n\nA.4.2 Stratified splitting with respect to the response and categorical predictors\n\nsns.histplot(data=accuracy_iter, x=\"train_y_CatPredictors_stratified\", color=\"red\", label=\"Train accuracy\", kde=True)\nsns.histplot(data=accuracy_iter, x=\"test_y_CatPredictors_stratified\", color=\"skyblue\", label=\"Test accuracy\", kde=True);\nplt.legend()\nplt.xlabel('Accuracy')\n\nText(0.5, 0, 'Accuracy')\n\n\n\n\n\n\n\n\n\nThe train and test accuracies are between 85% and 85.5% for all the simulations. As a results of stratifying the splitting with respect to both the response and the categorical predictors, the train and test datasets are almost the same because the datasets are engineered to be quite similar, thereby making the test dataset inappropriate for assessing accuracy on unseen data. Thus, it is recommended to stratify the splitting only with respect to the response.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Stratified splitting (classification problem)</span>"
    ]
  },
  {
    "objectID": "Parallel_processing_Bonus_Questions.html",
    "href": "Parallel_processing_Bonus_Questions.html",
    "title": "Appendix B — Parallel processing bonus Q",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, \\\ncross_validate, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, recall_score, mean_squared_error\nfrom scipy.stats import uniform\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nimport seaborn as sns\nfrom skopt.plots import plot_objective\nimport matplotlib.pyplot as plt\nimport warnings\nimport time as tm\n\n\n#Using the same datasets as used for linear regression in STAT303-2, \n#so that we can compare the non-linear models with linear regression\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\npredictors = ['mpg', 'engineSize', 'year', 'mileage']\nX_train = train[predictors]\ny_train = train['price']\nX_test = test[predictors]\ny_test = test['price']\n\n# Scale\nsc = StandardScaler()\n\nsc.fit(X_train)\nX_train_scaled = sc.transform(X_train)\nX_test_scaled = sc.transform(X_test)\n\n\nCase 1: No parallelization\n\ntime_taken_case1 = []\nfor i in range(50):\n    start_time = tm.time()\n    Ks = range(1, 20)\n    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n    cross_val_error = []\n    for k in Ks:\n        cross_val_error.append(-cross_val_score(KNeighborsRegressor(n_neighbors=k), \n                          X_train_scaled, y_train, cv = kfold,\n                           scoring=\"neg_root_mean_squared_error\").mean())\n    time_taken_case1.append(tm.time() - start_time)\n\n\n\nCase 2: Parallelization in cross_val_score()\n\ntime_taken_case2 = []\nfor i in range(50):\n    start_time = tm.time()\n    Ks = range(1, 20)\n    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n    cross_val_error = []\n    for k in Ks:\n        cross_val_error.append(-cross_val_score(KNeighborsRegressor(n_neighbors=k), \n                         X_train_scaled, y_train, cv = kfold, n_jobs = -1,\n                                  scoring=\"neg_root_mean_squared_error\").mean())\n    time_taken_case2.append(tm.time() - start_time)\n\n\n\nCase 3: Parallelization in KNeighborsRegressor()\n\ntime_taken_case3 = []\nfor i in range(50):\n    start_time = tm.time()\n    Ks = range(1, 20)\n    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n    cross_val_error = []\n    for k in Ks:\n        cross_val_error.append(-cross_val_score(KNeighborsRegressor(n_neighbors=k, \n                n_jobs= -1), X_train_scaled, y_train, cv = kfold, \n                                  scoring=\"neg_root_mean_squared_error\").mean())\n    time_taken_case3.append(tm.time() - start_time)\n\n\n\nCase 4: Nested parallelization: Both cross_val_score() and KNeighborsRegressor()\n\ntime_taken_case4 = []\nfor i in range(50):\n    start_time = tm.time()\n    Ks = range(1, 20)\n    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n    cross_val_error = []\n    for k in Ks:\n        cross_val_error.append(-cross_val_score(KNeighborsRegressor(n_neighbors=k, \n                n_jobs= -1), X_train_scaled, y_train, cv = kfold, n_jobs = -1,\n                scoring=\"neg_root_mean_squared_error\").mean())\n    time_taken_case4.append(tm.time() - start_time)\n\n\nsns.boxplot([time_taken_case1, time_taken_case2, time_taken_case3, time_taken_case4])\nplt.xticks([0, 1, 2, 3], ['Case 1', 'Case 2', 'Case 3', 'Case 4']);\nplt.ylabel('Time');\n\n\n\n\n\n\n\n\nQ1\nCase 1 is without parallelization. Why is Case 3 with parallelization of KNeighborsRegressor() taking more time than case 1?\n\n\nQ2\nIf nested parallelization is worse than parallelization, why is case 4 with nested parallelization taking less time than case 3 with parallelization of KNeighborsRegressor()?\n\n\nQ3\nIf nested parallelization is worse than no parallelization, why is case 4 with nested parallelization taking less time than case 1 with no parallelization?\n\n\nQ4\nIf nested parallelization is the best scenario, why is case 4 with nested parallelization taking more time than case 2 with with parallelization in cross_val_score()?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Parallel processing bonus Q</span>"
    ]
  },
  {
    "objectID": "Miscellaneous questions.html",
    "href": "Miscellaneous questions.html",
    "title": "Appendix C — Miscellaneous questions",
    "section": "",
    "text": "Q1\nWhy is boosting inappropriate for linear Regression, but appropriate for decision trees?\nThe question has been well answered in the post. The intuitive explanation is that the weighted average of a sequence of linear regression models will also be a single linear regression model. However, if the weighted average of the sequence of linear regression models results in a linear regression model (say boosted_linear_regression) that is different from the linear regression model that is obtained by fitting directly to the data (say regular_linear_regression), then the boosted_linear_regression model will have a higher bias than the regular_linear_regression model as the regular_linear_regression model minimizes the sum of squared errors (SSE). Thus, the boosted_linear_regression model should be the same as the regular_linear_regression model for the optimal hyperparameter values of the boosting algorithm. Thus, all the hard-work of tuning the boosting model will at best lead to the linear regression model that can be obtained by fitting a linear regression model directly to the train data!\nHowever, a sequence of shallow regression trees will not lead to the same regression tree that can be developed directly. A sequence of shallow trees will continuously reduce bias with relative less increase in variance. A single decision tree is likely to have a relatively high variance, and thus boosting with shallow trees may provide a better performance. Boosting aims to reduce bias by using low variance models, while a single decision tree has almost zero bias at the cost of having a high variance.\nThe second response in the post provides a mathematical explanation, which is more convincing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Miscellaneous questions</span>"
    ]
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix D — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Datasets, assignment and project files</span>"
    ]
  }
]