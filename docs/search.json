[
  {
    "objectID": "voting_stacking.html",
    "href": "voting_stacking.html",
    "title": "13¬† Advanced Ensemble Learning",
    "section": "",
    "text": "13.1 Why Ensemble Diverse Models\nEnsembling models can improve predictive performance by leveraging the diversity and collective wisdom of multiple models. Instead of relying on a single model, we train several individual models and combine their predictions to make a final decision.\nWe have already seen ensemble methods like bagging and boosting. These ensembles primarily reduce error by:\nIn this chapter, we‚Äôll go a step further and explore ensembles that combine different types of models. For example, we might ensemble a linear regression model, a random forest, and a gradient boosting model. The goal is to build stronger predictors by combining models that complement each other‚Äôs strengths and weaknesses.\nBias Reduction:\nVariance Reduction:",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Advanced Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "voting_stacking.html#why-ensemble-diverse-models",
    "href": "voting_stacking.html#why-ensemble-diverse-models",
    "title": "13¬† Advanced Ensemble Learning",
    "section": "",
    "text": "Different models often exhibit distinct biases. For example, a linear regression model might underfit complex patterns, while a Random Forest might overfit noisy data. Combining their predictions can mitigate these biases, leading to a more generalized model.\nExample: If a linear model overpredicts and a boosting model underpredicts for the same instance, averaging their predictions can cancel out the biases.\n\n\n\nAs seen with Random Forests, averaging predictions from multiple models reduces variance, especially when the models are uncorrelated (recall the variance reduction formula for bagging).\nKey Requirement: For effective variance reduction, the models should have low correlation in their predictions.\n\n\n13.1.1 Mathematical Justification\nWe can mathematically illustrate how ensembling improves prediction accuracy using the case of regression.\nLet the predictors be denoted by ( X ), and the response by ( Y ). Assume we have ( m ) individual models ( f_1, f_2, , f_m ). The ensemble predictor is the average:\n\\[\n\\hat{f}_{ensemble}(X) = \\frac{1}{m} \\sum_{i=1}^{m} f_i(X)\n\\]\nThe expected mean squared error (MSE) of the ensemble model is:\n\\[\nE(MSE_{Ensemble}) = E\\left[\\left( \\frac{1}{m} \\sum_{i = 1}^{m} f_i(X) - Y \\right)^2 \\right]\n\\]\nThis expands to:\n\\[\nE(MSE_{Ensemble}) = \\frac{1}{m^2} \\sum_{i = 1}^{m} E\\left[(f_i(X) - Y)^2 \\right] + \\frac{1}{m^2} \\sum_{i \\ne j} E\\left[(f_i(X) - Y)(f_j(X) - Y) \\right]\n\\]\n\\[\n= \\frac{1}{m} \\left( \\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i}) \\right) + \\frac{1}{m^2} \\sum_{i \\ne j} E\\left[(f_i(X) - Y)(f_j(X) - Y) \\right]\n\\]\nIf the individual models ( f_1, , f_m ) are unbiased, the cross terms become covariances:\n\\[\nE(MSE_{Ensemble}) = \\frac{1}{m} \\left( \\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i}) \\right) + \\frac{1}{m^2} \\sum_{i \\ne j} Cov(f_i(X), f_j(X))\n\\]\nIf the models are uncorrelated, the covariance terms vanish:\n\\[\nE(MSE_{Ensemble}) = \\frac{1}{m} \\left( \\frac{1}{m} \\sum_{i=1}^m E(MSE_{f_i}) \\right)\n\\]\n\nüîç Conclusion: When the individual models are both unbiased and uncorrelated, the expected MSE of the ensemble is strictly lower than the average MSE of the individual models. This provides a strong theoretical justification for ensembling diverse models to improve prediction accuracy.\nIn practice, the ensemble‚Äôs performance tends to improve unless a single model is significantly more accurate than the others. For example, if one model has near-zero MSE while others perform poorly, ensembling may actually hurt performance. Therefore, the benefit of ensembling depends not only on diversity but also on the relative quality of the individual models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Advanced Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "voting_stacking.html#combining-model-predictions-two-common-approaches",
    "href": "voting_stacking.html#combining-model-predictions-two-common-approaches",
    "title": "13¬† Advanced Ensemble Learning",
    "section": "13.2 Combining Model Predictions: Two Common Approaches",
    "text": "13.2 Combining Model Predictions: Two Common Approaches\nThere are two widely used methods for combining model predictions in ensemble learning:\n\nVoting: Combines the predictions of multiple models directly. In classification, this could be majority voting; in regression, it‚Äôs often simple averaging. Voting is intuitive and works well when the base models are reasonably strong and diverse.\n\nThe idea behind voting is similar to bagging in that it combines predictions from multiple models by averaging their predictions. However, unlike bagging which typically uses homogeneous models (e.g., multiple decision trees), voting allows for combining heterogeneous models‚Äîdifferent types of algorithms‚Äîto leverage their individual strengths.\n\nStacking: Trains a new model (called a meta-learner) to learn how to best combine the predictions of the base models. Stacking can capture more complex relationships among the models and often yields higher accuracy, especially when base models differ significantly in structure or behavior.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Advanced Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "voting_stacking.html#exploring-stacking-and-voting-in-regression",
    "href": "voting_stacking.html#exploring-stacking-and-voting-in-regression",
    "title": "13¬† Advanced Ensemble Learning",
    "section": "13.3 Exploring Stacking and Voting in Regression",
    "text": "13.3 Exploring Stacking and Voting in Regression\nBuilding on the previous chapters where we consistently used the car dataset for regression tasks, we will now explore the application of stacking and voting ensemble methods using the same dataset.\nLet‚Äôs begin by importing the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import cross_val_score,train_test_split, GridSearchCV, ParameterGrid, \\\nStratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import root_mean_squared_error, mean_squared_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, roc_auc_score, f1_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.ensemble import VotingRegressor, VotingClassifier, StackingRegressor, \\\nStackingClassifier, GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor, \\\nBaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.linear_model import LinearRegression,LogisticRegression, Ridge, ElasticNetCV\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools as it\nimport time as time\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nLoad the dataset\n\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\ncar.head()\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\nvw\nBeetle\n2014\nManual\n55457\nDiesel\n30\n65.3266\n1.6\n7490\n\n\n1\nvauxhall\nGTC\n2017\nManual\n15630\nPetrol\n145\n47.2049\n1.4\n10998\n\n\n2\nmerc\nG Class\n2012\nAutomatic\n43000\nDiesel\n570\n25.1172\n3.0\n44990\n\n\n3\naudi\nRS5\n2019\nAutomatic\n10\nPetrol\n145\n30.5593\n2.9\n51990\n\n\n4\nmerc\nX-CLASS\n2018\nAutomatic\n14000\nDiesel\n240\n35.7168\n2.3\n28990\n\n\n\n\n\n\n\nData preprocessing\n\nX = car.drop(columns=['price'])\ny = car['price']\n\n# extract the categorical columns and put them in a list\ncategorical_features = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nWhile some of the models we will ensemble‚Äîsuch as tree-based models‚Äîdo not require feature scaling or one-hot encoding, we‚Äôll apply a unified preprocessing pipeline for all models to ensure consistency and compatibility. This approach simplifies the workflow and avoids errors when combining models with different preprocessing requirements.\n\n# Create preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n    ])\n\nFor quick prototyping, we started with default model settings to evaluate whether ensembling improves performance. We also set a fixed random state to ensure reproducibility. Below is a list of the models you have learned so far.\n\n# Define models to evaluate\nregressor_models = {\n    'Baseline Linear Regression': LinearRegression(),\n    'Baseline KNN Regressor': KNeighborsRegressor(),\n    'Baseline Decision Tree': DecisionTreeRegressor(random_state=42),\n    'Baseline Random Forest': RandomForestRegressor( random_state=42),\n    'Baseline XGBoost': xgb.XGBRegressor( random_state=42),\n    'Baseline LightGBM': lgb.LGBMRegressor(random_state=42, verbose=0),\n    'Baseline CatBoost': cb.CatBoostRegressor(random_state=42, verbose=0)\n}\n\nWe will first build each model using its default settings to establish baseline performance before applying ensembling techniques.\n\n# store the results\nreg_results = {}\n\n# Loop through models\nfor name, model in regressor_models.items():\n    # Create a pipeline with preprocessing and the model\n    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('model', model)])\n    \n    # Fit the model\n    pipeline.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = pipeline.predict(X_test)\n    \n    # Calculate RMSE\n    rmse = root_mean_squared_error(y_test, y_pred)\n    \n    # Store the results\n    reg_results[name] = rmse\n\n# Convert results to DataFrame for better visualization\nreg_results_df = pd.DataFrame.from_dict(reg_results, orient='index', columns=['RMSE'])\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True)\nreg_results_df.reset_index(inplace=True)\nreg_results_df.columns = ['Model', 'RMSE']\n# Print the results\nreg_results_df\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nBaseline CatBoost\n3296.493137\n\n\n1\nBaseline XGBoost\n3397.155518\n\n\n2\nBaseline Random Forest\n3660.145970\n\n\n3\nBaseline LightGBM\n3729.955778\n\n\n4\nBaseline KNN Regressor\n4062.839680\n\n\n5\nBaseline Decision Tree\n5015.812547\n\n\n6\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nEvidently, CatBoost outperforms the other models using its default settings, achieving the lowest RMSE. This aligns with what you‚Äôve learned‚ÄîCatBoost typically requires less hyperparameter tuning compared to XGBoost and LightGBM.\n\n13.3.1 Voting Regressor\nIn this section, Next, we will build an ensemble model, starting with voting regressor, which assigns equal weight to each base model. In this approach, all predictions are treated equally when averaged to produce the final combined prediction.\nBelow is how you can ensemble the same models using VotingRegressor with the same preprocessor in a pipeline\n\n# Define base regressors (same as before)\nbase_regressor_list = [\n    ('lr', LinearRegression()),\n    ('knn', KNeighborsRegressor()),\n    ('dt', DecisionTreeRegressor(random_state=42)),\n    ('rf', RandomForestRegressor(random_state=42)),\n    ('xgb', xgb.XGBRegressor(random_state=42)),\n    ('lgb', lgb.LGBMRegressor(random_state=42, verbose=0)),\n    ('cat', cb.CatBoostRegressor(random_state=42, verbose=0))\n]\n\n# Create a VotingRegressor with equal weights\nvoting_regressor = VotingRegressor(estimators=base_regressor_list)\n\n# Create a pipeline with preprocessing and voting ensemble\nvoting_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('ensemble', voting_regressor)\n])\n\n# Fit the ensemble model (no need to fit individual models beforehand!)\nvoting_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('ensemble',\n                 VotingRegressor(estimators=[('lr', LinearRegression()),\n                                             ('knn', KNeighborsRegressor()),\n                                             ('dt',\n                                              DecisionTreeRegressor...\n                                                           max_cat_threshold=None,\n                                                           max_cat_to_onehot=None,\n                                                           max_delta_step=None,\n                                                           max_depth=None,\n                                                           max_leaves=None,\n                                                           min_child_weight=None,\n                                                           missing=nan,\n                                                           monotone_constraints=None,\n                                                           multi_strategy=None,\n                                                           n_estimators=None,\n                                                           n_jobs=None,\n                                                           num_parallel_tree=None, ...)),\n                                             ('lgb',\n                                              LGBMRegressor(random_state=42,\n                                                            verbose=0)),\n                                             ('cat',\n                                              &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)]))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('ensemble',\n                 VotingRegressor(estimators=[('lr', LinearRegression()),\n                                             ('knn', KNeighborsRegressor()),\n                                             ('dt',\n                                              DecisionTreeRegressor...\n                                                           max_cat_threshold=None,\n                                                           max_cat_to_onehot=None,\n                                                           max_delta_step=None,\n                                                           max_depth=None,\n                                                           max_leaves=None,\n                                                           min_child_weight=None,\n                                                           missing=nan,\n                                                           monotone_constraints=None,\n                                                           multi_strategy=None,\n                                                           n_estimators=None,\n                                                           n_jobs=None,\n                                                           num_parallel_tree=None, ...)),\n                                             ('lgb',\n                                              LGBMRegressor(random_state=42,\n                                                            verbose=0)),\n                                             ('cat',\n                                              &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)]))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['year', 'mileage', 'tax', 'mpg',\n                                  'engineSize']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) num['year', 'mileage', 'tax', 'mpg', 'engineSize'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') ensemble: VotingRegressor?Documentation for ensemble: VotingRegressorVotingRegressor(estimators=[('lr', LinearRegression()),\n                            ('knn', KNeighborsRegressor()),\n                            ('dt', DecisionTreeRegressor(random_state=42)),\n                            ('rf', RandomForestRegressor(random_state=42)),\n                            ('xgb',\n                             XGBRegressor(base_score=None, booster=None,\n                                          callbacks=None,\n                                          colsample_bylevel=None,\n                                          colsample_bynode=None,\n                                          colsample_bytree=None, device=None,\n                                          early_stopping_rounds=None,\n                                          enab...\n                                          max_cat_threshold=None,\n                                          max_cat_to_onehot=None,\n                                          max_delta_step=None, max_depth=None,\n                                          max_leaves=None,\n                                          min_child_weight=None, missing=nan,\n                                          monotone_constraints=None,\n                                          multi_strategy=None,\n                                          n_estimators=None, n_jobs=None,\n                                          num_parallel_tree=None, ...)),\n                            ('lgb', LGBMRegressor(random_state=42, verbose=0)),\n                            ('cat',\n                             &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)]) lrLinearRegression?Documentation for LinearRegressionLinearRegression() knnKNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor() dtDecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(random_state=42) rfRandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=42) xgbXGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...) lgbLGBMRegressorLGBMRegressor(random_state=42, verbose=0) catCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt; \n\n\nNote: You do not need to fit the models individually before including them in the VotingRegressor. Doing so would result in unnecessary computation and waste time, as VotingRegressor.fit() will handle training for all models internally.\n\n# Predict and evaluate\ny_pred_vote = voting_pipeline.predict(X_test)\nrmse_vote = root_mean_squared_error(y_test, y_pred_vote)\n\n# Add ensemble result to the results_df\nreg_results_df.loc[len(reg_results_df.index)] = ['Voting Regressor', rmse_vote]\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n\n# Show updated results\nreg_results_df\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nBaseline CatBoost\n3296.493137\n\n\n1\nVoting Regressor\n3302.202159\n\n\n2\nBaseline XGBoost\n3397.155518\n\n\n3\nBaseline Random Forest\n3660.145970\n\n\n4\nBaseline LightGBM\n3729.955778\n\n\n5\nBaseline KNN Regressor\n4062.839680\n\n\n6\nBaseline Decision Tree\n5015.812547\n\n\n7\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nIt‚Äôs not surprising that CatBoost outperformed the Voting Ensemble, as a well-optimized gradient boosting model can often outshine an ensemble‚Äîespecially when the ensemble‚Äôs constituent models lack diversity, are poorly tuned, or fail to leverage the dataset‚Äôs characteristics.\n\n13.3.1.1 Strategies to Improve Voting Ensemble Performance\nTo boost the effectiveness of a voting ensemble, consider the following enhancements:\n\nIncrease Model Diversity\nIncorporate a wider range of model types (e.g., SVMs, neural networks, or k-NN) alongside tree-based models. Diverse models are more likely to capture different patterns in the data and produce uncorrelated errors ‚Äî a key factor in ensemble success, as discussed earlier in this chapter.\nTune Base Models Individually\nOptimize each base model using hyperparameter tuning techniques such as Optuna. Well-tuned individual models provide stronger building blocks for the ensemble, improving the final averaged prediction.\nUse Weighted Voting\nInstead of assigning equal importance to each model, assign weights based on their individual performance (e.g., lower RMSE ‚Üí higher weight). This helps emphasize the contribution of stronger models like CatBoost or XGBoost.\n(Note: In a more advanced setup, stacking takes this further by learning the best combination strategy using a meta-model.)\n\n\n\n\n13.3.2 Stacking Regressor\nStacking is a more sophisticated ensembling technique that learns how to best combine multiple base models using a separate meta-model (also called the final_estimator).\nHere‚Äôs how the process works:\n\nCross-validated predictions for base models\nThe training data is split into K folds (typically using cross-validation). For each fold:\n\nThe base models are trained on the remaining K‚Äì1 folds.\nPredictions are made on the held-out fold.\n\nOut-of-fold predictions become new features\nThis process generates out-of-fold predictions for each training point from each base model (i.e., predictions made on data not seen during training). These predictions are used as features for the next stage.\nTraining the meta-model (final_estimator)\nThe meta-model is trained on these out-of-fold predictions as input features and the original target variable as the response. It learns how to combine the base model outputs to make a better overall prediction.\n\nPlease see the stacking implementation below\nTraining Set\n‚îÇ\n‚îú‚îÄ‚îÄ Cross-Validation Process (k folds)\n‚îÇ ‚îú‚îÄ‚îÄ Fold 1: Train C‚ÇÅ-C‚ÇÑ on folds 2-k ‚Üí Predict on Fold 1 ‚Üí P‚ÇÅ-P‚ÇÑ for Fold 1\n‚îÇ ‚îú‚îÄ‚îÄ Fold 2: Train C‚ÇÅ-C‚ÇÑ on folds 1,3-k ‚Üí Predict on Fold 2 ‚Üí P‚ÇÅ-P‚ÇÑ for Fold 2\n‚îÇ ‚îî‚îÄ‚îÄ ‚Ä¶ (repeat for all k folds)\n‚îÇ\n‚îú‚îÄ‚îÄ Aggregated Meta-Features: P‚ÇÅ-P‚ÇÑ for entire training set\n‚îÇ\n‚îî‚îÄ‚îÄ Meta-Classifier ‚Üí Final Prediction\n\nThe goal of stacking is to leverage the strengths of each individual model while minimizing their weaknesses, often resulting in improved accuracy over any single model.\n\n\n13.3.2.1 Metamodel: Linear regression\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n# Define a meta-model\nmeta_lr = LinearRegression()\n\n# Create the stacking regressor\nstacking_model = StackingRegressor(\n    estimators=base_regressor_list,\n    final_estimator=meta_lr,\n    cv=KFold(n_splits=5, shuffle=True, random_state=42), # ensures all base models use the same 5-fold CV\n    n_jobs=-1,  # Use all available cores\n    passthrough=False  # Set to True if you want to include original features in meta-model\n)\n\n# Wrap with pipeline (using your preprocessor)\nstacking_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('stacking', stacking_model)\n])\n\n# Fit the stacking model\nstacking_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n                                   estimators=[('lr', LinearRegression()),\n                                               ('knn...\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('lgb',\n                                                LGBMRegressor(random_state=42,\n                                                              verbose=0)),\n                                               ('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                                   final_estimator=LinearRegression(),\n                                   n_jobs=-1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n                                   estimators=[('lr', LinearRegression()),\n                                               ('knn...\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('lgb',\n                                                LGBMRegressor(random_state=42,\n                                                              verbose=0)),\n                                               ('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                                   final_estimator=LinearRegression(),\n                                   n_jobs=-1))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['year', 'mileage', 'tax', 'mpg',\n                                  'engineSize']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) num['year', 'mileage', 'tax', 'mpg', 'engineSize'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') stacking: StackingRegressor?Documentation for stacking: StackingRegressorStackingRegressor(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n                  estimators=[('lr', LinearRegression()),\n                              ('knn', KNeighborsRegressor()),\n                              ('dt', DecisionTreeRegressor(random_state=42)),\n                              ('rf', RandomForestRegressor(random_state=42)),\n                              ('xgb',\n                               XGBRegressor(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_byt...\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None, ...)),\n                              ('lgb',\n                               LGBMRegressor(random_state=42, verbose=0)),\n                              ('cat',\n                               &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                  final_estimator=LinearRegression(), n_jobs=-1) lrLinearRegression?Documentation for LinearRegressionLinearRegression() knnKNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor() dtDecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(random_state=42) rfRandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=42) xgbXGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...) lgbLGBMRegressorLGBMRegressor(random_state=42, verbose=0) catCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt; final_estimatorLinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# Predict and evaluate\ny_pred_stack = stacking_pipeline.predict(X_test)\nrmse_stack = root_mean_squared_error(y_test, y_pred_stack)\n\nprint(f\"Stacking Regressor RMSE: {rmse_stack:.2f}\")\n\nStacking Regressor RMSE: 3190.11\n\n\n\n# Append the Stacking Regressor result to results_df\nreg_results_df.loc[len(reg_results_df.index)] = ['Lr Stacking Regressor', rmse_stack]\n\n# Sort by RMSE in ascending order and reset index\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n\n# Display updated results\nreg_results_df\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nLr Stacking Regressor\n3190.112423\n\n\n1\nBaseline CatBoost\n3296.493137\n\n\n2\nVoting Regressor\n3302.202159\n\n\n3\nBaseline XGBoost\n3397.155518\n\n\n4\nBaseline Random Forest\n3660.145970\n\n\n5\nBaseline LightGBM\n3729.955778\n\n\n6\nBaseline KNN Regressor\n4062.839680\n\n\n7\nBaseline Decision Tree\n5015.812547\n\n\n8\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nFrom the results, the stacking Regressor not only outperforms the Voting Regressor, but also surpasses the best individual model‚ÄîCatBoost‚Äîwhen using default settings.\nWhile voting assigns equal weights to each base model, stacking uses a meta-model (in this case, linear regression) to learn an optimal weighted combination of predictions. This allows it to assign different importance to each base model based on their contributions to overall performance.\nNext, let‚Äôs examine the coefficients learned by the stacking model to understand how it weighted each base model‚Äôs prediction.\n\n# Access the trained meta-model inside the stacking pipeline\nmeta_model = stacking_pipeline.named_steps['stacking'].final_estimator_\n\n# Get model names in the same order as the coefficients\nmodel_names = [name for name, _ in base_regressor_list]\n\n# Extract coefficients\ncoefs = meta_model.coef_\n\n# Create a DataFrame to display model weights\nimport pandas as pd\ncoef_df = pd.DataFrame({'Base Model': model_names, 'Meta-Model Coefficient': coefs})\n\n# Sort by weight (optional)\ncoef_df = coef_df.sort_values(by='Meta-Model Coefficient', ascending=False).reset_index(drop=True)\n\n# Show the weights\ncoef_df\n\n\n\n\n\n\n\n\nBase Model\nMeta-Model Coefficient\n\n\n\n\n0\ncat\n0.763462\n\n\n1\nknn\n0.140259\n\n\n2\nxgb\n0.084874\n\n\n3\nlr\n0.043660\n\n\n4\ndt\n0.037379\n\n\n5\nrf\n0.005741\n\n\n6\nlgb\n-0.056054\n\n\n\n\n\n\n\nNote the above coefficients of the meta-model. The model gives the highest weight to the catboost model, and the rf model, and the lowest weight to the lgb model.\nAlso, note that the coefficients need not sum to one.\n\n\n13.3.2.2 Why did LightGBM get the lowest (even negative) coefficient?\n\nStacking is not based on model performance alone\n\nThe meta-model (LinearRegression) doesn‚Äôt assign weights based on RMSE directly.\nInstead, it learns how to combine the model predictions to best fit the training data (specifically, the out-of-fold predictions from each base model).\nSo, even if LightGBM performs decently on its own, its predictions may be redundant or highly correlated with stronger models (e.g., CatBoost or XGBoost).\n\nLightGBM and XGBoost are often similar\n\nBoth are gradient boosting methods ‚Äî if they make very similar predictions, the meta-model may favor just one of them (in this case, XGBoost slightly more).\nIncluding both may introduce multicollinearity, and the linear model tries to suppress redundancy by assigning a near-zero or negative weight.\n\nLinear regression allows negative weights\n\nUnlike voting (which only uses positive weights), a linear model may assign a negative coefficient if it slightly improves the overall fit.\nThis doesn‚Äôt mean the model is ‚Äúbad,‚Äù but rather that its prediction direction may not help much in the presence of other models.\n\n\nTo further improve the RMSE, we will refine the ensemble by removing weaker or highly correlated base models. Specifically, we will retain only the top four models, selected based on the magnitude of their coefficients in the linear regression meta-model.\n\n# Define top 4 base models\ntop4_regressor_list = [\n    ('cat', cb.CatBoostRegressor(random_state=42, verbose=0)),\n    ('rf', RandomForestRegressor(random_state=42)),\n    ('xgb', xgb.XGBRegressor(random_state=42)),\n    ('knn', KNeighborsRegressor())\n]\n\n# Meta-model\ntop4_meta_lr = LinearRegression()\n\n# Build the stacking ensemble\nstacking_top4 = StackingRegressor(\n    estimators=top4_regressor_list,\n    final_estimator=top4_meta_lr,\n    cv=5\n)\n\n# Create pipeline with preprocessing\nstacking_top4_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('stacking', stacking_top4)\n])\n\n# Fit the pipeline\nstacking_top4_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F68DB1D0&gt;),\n                                               ('rf',\n                                                Ra...\n                                                             interaction_constraints=None,\n                                                             learning_rate=None,\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('knn', KNeighborsRegressor())],\n                                   final_estimator=LinearRegression()))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F68DB1D0&gt;),\n                                               ('rf',\n                                                Ra...\n                                                             interaction_constraints=None,\n                                                             learning_rate=None,\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('knn', KNeighborsRegressor())],\n                                   final_estimator=LinearRegression()))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['year', 'mileage', 'tax', 'mpg',\n                                  'engineSize']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) num['year', 'mileage', 'tax', 'mpg', 'engineSize'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') stacking: StackingRegressor?Documentation for stacking: StackingRegressorStackingRegressor(cv=5,\n                  estimators=[('cat',\n                               &lt;catboost.core.CatBoostRegressor object at 0x000001F1F68DB1D0&gt;),\n                              ('rf', RandomForestRegressor(random_state=42)),\n                              ('xgb',\n                               XGBRegressor(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None, device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_m...\n                                            interaction_constraints=None,\n                                            learning_rate=None, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None, ...)),\n                              ('knn', KNeighborsRegressor())],\n                  final_estimator=LinearRegression()) catCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001F1F68DB1D0&gt; rfRandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=42) xgbXGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...) knnKNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor() final_estimatorLinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# Predict and evaluate\ny_pred_top4 = stacking_top4_pipeline.predict(X_test)\nrmse_top4 = root_mean_squared_error(y_test, y_pred_top4)\n\n# Add result to results_df\nreg_results_df.loc[len(reg_results_df.index)] = ['Lr Stacking Top 4 Regressor', rmse_top4]\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n\n# Show updated results\nreg_results_df\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nLr Stacking Top 4 Regressor\n3135.705167\n\n\n1\nLr Stacking Regressor\n3190.112423\n\n\n2\nBaseline CatBoost\n3296.493137\n\n\n3\nVoting Regressor\n3302.202159\n\n\n4\nBaseline XGBoost\n3397.155518\n\n\n5\nBaseline Random Forest\n3660.145970\n\n\n6\nBaseline LightGBM\n3729.955778\n\n\n7\nBaseline KNN Regressor\n4062.839680\n\n\n8\nBaseline Decision Tree\n5015.812547\n\n\n9\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nThe metamodel accuracy improves further, when strong models are ensembled.\n\n# Access the trained meta-model inside the stacking pipeline\nmeta_model_top4 = stacking_top4_pipeline.named_steps['stacking'].final_estimator_\n\n# Get the names of the base models\ntop_model_names = [name for name, _ in top4_regressor_list]\n\n# Extract coefficients\ntop4_coefs = meta_model_top4.coef_\n\n# Create a DataFrame to display the weights\ntop4_coef_df = pd.DataFrame({\n    'Base Model': top_model_names,\n    'Meta-Model Coefficient': top4_coefs\n}).sort_values(by='Meta-Model Coefficient', ascending=False).reset_index(drop=True)\n\n# Show the result\ntop4_coef_df\n\n\n\n\n\n\n\n\nBase Model\nMeta-Model Coefficient\n\n\n\n\n0\ncat\n0.602477\n\n\n1\nrf\n0.156181\n\n\n2\nknn\n0.131000\n\n\n3\nxgb\n0.122639\n\n\n\n\n\n\n\n\n\n13.3.2.3 Choosing the Meta-Model in Stacking\nIn stacking, the meta-model (also called the final_estimator) is responsible for learning how to combine the predictions of the base models. It takes the base models‚Äô predictions as input features and learns how to best map them to the target.\nWhile LinearRegression is a popular default choice due to its simplicity, speed, and interpretability, you are not limited to it. Any regression model can be used as the meta-model, depending on your goals:\n\nUse Ridge or Lasso if regularization is needed (e.g., to handle multicollinearity).\nUse a tree-based model (e.g., RandomForestRegressor, XGBRegressor) if you suspect nonlinear interactions between base model predictions.\nUse SVR, MLPRegressor, or KNeighborsRegressor for flexible, non-parametric alternatives (though often more sensitive to tuning and data scale).\n\nThe choice of meta-model can significantly affect the performance of your stacked ensemble.\n\n\n13.3.2.4 General Guidelines for Choosing a Meta-Model in Stacking\nWhen selecting a meta-model (final estimator) for a stacking ensemble, consider the following:\n\nIf your base models are highly correlated\nUse a regularized linear model like Ridge or Lasso. These models help reduce overfitting by shrinking or zeroing out redundant coefficients.\nIf you suspect nonlinear interactions between base model predictions\nUse a flexible, non-linear model like RandomForestRegressor, XGBRegressor, or MLPRegressor as the meta-model. These can better capture complex relationships among the base model outputs.\nIf interpretability is not a priority\nConsider using more powerful learners (e.g., tree ensembles or neural networks) for the meta-model to potentially boost performance, especially on complex datasets.\n\nThe meta-model should complement your base models and match the complexity of the prediction task.\nIn this car dataset, CatBoost, XGBoost, LightGBM, and Random Forest are all tree-based models, and they are likely generating similar predictions. This can lead to:\n\nMulticollinearity among the base models\n\nUnstable or suboptimal coefficients in the meta-model\n\n\n\n13.3.2.5 Metamodel: Ridge regression\nTo address this, we‚Äôll try using a regularized linear model like Ridge regression as the meta-model. Ridge can better handle redundant information by shrinking correlated coefficients, which helps reduce overfitting and improve generalization.\n\n# Replace meta-model\nstacking_ridge_model = StackingRegressor(\n    estimators=base_regressor_list,\n    final_estimator=Ridge(),\n    cv=5\n)\n\n# Create pipeline with preprocessing\nstacking_ridge_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('stacking', stacking_ridge_model)\n])\n# Fit the pipeline\nstacking_ridge_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('lr', LinearRegression()),\n                                               ('knn', KNeighborsRegressor()),\n                                               ('dt',\n                                                DecisionTreeRe...\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('lgb',\n                                                LGBMRegressor(random_state=42,\n                                                              verbose=0)),\n                                               ('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                                   final_estimator=Ridge()))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('lr', LinearRegression()),\n                                               ('knn', KNeighborsRegressor()),\n                                               ('dt',\n                                                DecisionTreeRe...\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                               ('lgb',\n                                                LGBMRegressor(random_state=42,\n                                                              verbose=0)),\n                                               ('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                                   final_estimator=Ridge()))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['year', 'mileage', 'tax', 'mpg',\n                                  'engineSize']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) num['year', 'mileage', 'tax', 'mpg', 'engineSize'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') stacking: StackingRegressor?Documentation for stacking: StackingRegressorStackingRegressor(cv=5,\n                  estimators=[('lr', LinearRegression()),\n                              ('knn', KNeighborsRegressor()),\n                              ('dt', DecisionTreeRegressor(random_state=42)),\n                              ('rf', RandomForestRegressor(random_state=42)),\n                              ('xgb',\n                               XGBRegressor(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None, device=None,\n                                            early_stopping_rounds=No...\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None, ...)),\n                              ('lgb',\n                               LGBMRegressor(random_state=42, verbose=0)),\n                              ('cat',\n                               &lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt;)],\n                  final_estimator=Ridge()) lrLinearRegression?Documentation for LinearRegressionLinearRegression() knnKNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor() dtDecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(random_state=42) rfRandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=42) xgbXGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=None,\n             n_jobs=None, num_parallel_tree=None, ...) lgbLGBMRegressorLGBMRegressor(random_state=42, verbose=0) catCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001F1F6715370&gt; final_estimatorRidge?Documentation for RidgeRidge() \n\n\n\n# Predict and evaluate\ny_pred_ridge = stacking_ridge_pipeline.predict(X_test)\nrmse_ridge = root_mean_squared_error(y_test, y_pred_ridge)\n# Add result to results_df\nreg_results_df.loc[len(reg_results_df.index)] = ['Stacking with Ridge', rmse_ridge]\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n# Show updated results\nreg_results_df\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nLr Stacking Top 4 Regressor\n3135.705167\n\n\n1\nStacking with Ridge\n3147.701164\n\n\n2\nLr Stacking Regressor\n3190.112423\n\n\n3\nBaseline CatBoost\n3296.493137\n\n\n4\nVoting Regressor\n3302.202159\n\n\n5\nBaseline XGBoost\n3397.155518\n\n\n6\nBaseline Random Forest\n3660.145970\n\n\n7\nBaseline LightGBM\n3729.955778\n\n\n8\nBaseline KNN Regressor\n4062.839680\n\n\n9\nBaseline Decision Tree\n5015.812547\n\n\n10\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nInterpretation of Results:\nFrom the table, we observe that the ‚ÄúLr Stacking Top 4 Regressor‚Äù achieved the lowest RMSE, indicating the best performance among all models evaluated. This model stacked the top 4 individually optimized regressors (CatBoost, XGBoost, Random Forest, and KNN) using a simple linear regression as the meta-model.\nInterestingly, the ‚ÄúStacking with Ridge‚Äù model‚Äîanother stacking model using Ridge Regression as the meta-model‚Äîperformed slightly worse than the simple linear stacking. This could be due to the Ridge model introducing regularization that slightly underweighted some strong base learners, leading to a minor decrease in overall performance.\nAdditional observations:\n\n‚ÄúVoting Regressor‚Äù performed better than most baseline models but was still outperformed by stacking approaches. This is expected, as voting ensembles assign equal weights to base learners, whereas stacking learns optimal weights through a meta-model.\nAll baseline tree-based models (CatBoost, XGBoost, Random Forest, LightGBM) performed reasonably well, with CatBoost being the best among them even without tuning.\nThe baseline linear regression performed the worst due to its limited capability to capture non-linear relationship in the data\n\n\n\n\n13.3.3 Should You Tune Base Models Before Stacking or Voting?\nTuning base models is not strictly required, but it‚Äôs strongly recommended for building effective ensembles ‚Äî especially when using stacking.\n\n13.3.3.1 Benefits of Tuning Base Models\n\nImproved accuracy: Better-tuned models contribute more meaningful predictions.\nReduced noise: Untuned or weak models can hurt ensemble performance, especially in voting, where all models contribute equally.\nStronger stacking: The meta-model in stacking learns how to combine predictions ‚Äî and works best when those predictions are strong and diverse.\n\n\n\n13.3.3.2 Summary\n\n\n\n\n\n\n\n\nEnsemble Method\nShould You Tune Base Models?\nWhy?\n\n\n\n\nVoting\nOptional but helpful\nWeak models can dilute strong ones.\n\n\nStacking\nStrongly recommended\nMeta-model depends on meaningful base predictions.\n\n\n\nFor quick prototyping, so far we have used default model settings to assess whether ensembling could improve performance. Now that we have identified the top 4 models, we will leverage their optimized versions for stacking. These models were fine-tuned in previous chapters using tools like Optuna, RandomizedSearchCV, or BayesSearchCV to efficiently search for the best hyperparameters.\n\n# tuned catboost model from section 11.3.6\ncatboost_tuned = cb.CatBoostRegressor(\n    iterations=1021,\n    learning_rate=0.0536,\n    depth=7,\n    l2_leaf_reg=2.56,\n    random_seed=42,\n    min_data_in_leaf=28,\n    bagging_temperature=0.034,\n    random_strength=2.2,\n    verbose=0\n)\n# tuned random forest model from section 7.4.4\nrf_tuned = RandomForestRegressor(\n    n_estimators=60,\n    max_depth=28,\n    max_features=0.58,\n    max_samples=1.0,\n    random_state=42\n)\n\n#tuned knn model from section 2.4.1\nknn_tuned = KNeighborsRegressor(\n    n_neighbors=8,\n    weights='distance',\n    metric='manhattan',\n    p=2\n)\n\n# tuned xgboost model from section  10.5.10\nxgb_tuned = xgb.XGBRegressor(\n    n_estimators=193,\n    learning_rate=0.15,\n    max_depth=7,\n    min_child_weight=1,\n    gamma=0,\n    subsample=0.78,\n    reg_lambda=9.33,\n    reg_alpha=5.0,\n    colsample_bytree=0.635,\n    random_state=42\n)\n\n\n# Define models to evaluate\ntuned_regressor_models = {\n    'Optmized KNN Regressor': knn_tuned,\n    'Optmized Random Forest': rf_tuned,\n    'Optmized XGBoost': xgb_tuned,\n    'Optmized CatBoost': catboost_tuned\n}\n\n\n# Initialize results dictionary\ntuned_top4_results = {}\n\n# Loop through tuned regressor models\nfor name, model in tuned_regressor_models.items():\n    # Create a pipeline with preprocessing and the model\n    tuned_pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n    \n    # Fit the model\n    tuned_pipeline.fit(X_train, y_train)\n    \n    # Make predictions\n    y_tuned_pred = tuned_pipeline.predict(X_test)\n    \n    # Calculate RMSE (correct target used)\n    tuned_rmse = root_mean_squared_error(y_test, y_tuned_pred)\n    \n    # Store the result with a consistent label\n    tuned_top4_results[name] = tuned_rmse\n\n# Append new results to reg_results_df\nfor name, rmse in tuned_top4_results.items():\n    reg_results_df.loc[len(reg_results_df)] = [name, rmse]\n\n# Sort and reset index\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n\n# Show updated results\nreg_results_df\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nOptmized XGBoost\n3105.835205\n\n\n1\nLr Stacking Top 4 Regressor\n3135.705167\n\n\n2\nStacking with Ridge\n3147.701164\n\n\n3\nOptmized CatBoost\n3175.161327\n\n\n4\nLr Stacking Regressor\n3190.112423\n\n\n5\nOptmized Random Forest\n3279.094977\n\n\n6\nBaseline CatBoost\n3296.493137\n\n\n7\nVoting Regressor\n3302.202159\n\n\n8\nBaseline XGBoost\n3397.155518\n\n\n9\nBaseline Random Forest\n3660.145970\n\n\n10\nOptmized KNN Regressor\n3680.370319\n\n\n11\nBaseline LightGBM\n3729.955778\n\n\n12\nBaseline KNN Regressor\n4062.839680\n\n\n13\nBaseline Decision Tree\n5015.812547\n\n\n14\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\n\n\n13.3.3.3 Metamodel: Linear Regression on Tuned Top 4 Models\n\n# define top 4 optimized base models\ntuned_top4_regressor_list = [\n    ('cat', catboost_tuned),\n    ('rf', rf_tuned),\n    ('knn', knn_tuned),\n    ('xgb', xgb_tuned)\n]\n\n\n# Build the stacking ensemble\nstacking_tuned_top4 = StackingRegressor(\n    estimators=tuned_top4_regressor_list,\n    final_estimator=LinearRegression(),\n    cv=5\n)\n# Create pipeline with preprocessing\nstacking_tuned_top4_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('stacking', stacking_tuned_top4)\n])\n\n# Fit the pipeline\nstacking_tuned_top4_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F65C5A60&gt;),\n                                               ('rf',\n                                                Ra...\n                                                             importance_type=None,\n                                                             interaction_constraints=None,\n                                                             learning_rate=0.15,\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=7,\n                                                             max_leaves=None,\n                                                             min_child_weight=1,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=193,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...))],\n                                   final_estimator=LinearRegression()))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['year', 'mileage', 'tax',\n                                                   'mpg', 'engineSize']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['brand', 'model',\n                                                   'transmission',\n                                                   'fuelType'])])),\n                ('stacking',\n                 StackingRegressor(cv=5,\n                                   estimators=[('cat',\n                                                &lt;catboost.core.CatBoostRegressor object at 0x000001F1F65C5A60&gt;),\n                                               ('rf',\n                                                Ra...\n                                                             importance_type=None,\n                                                             interaction_constraints=None,\n                                                             learning_rate=0.15,\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=7,\n                                                             max_leaves=None,\n                                                             min_child_weight=1,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=193,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...))],\n                                   final_estimator=LinearRegression()))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['year', 'mileage', 'tax', 'mpg',\n                                  'engineSize']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['brand', 'model', 'transmission',\n                                  'fuelType'])]) num['year', 'mileage', 'tax', 'mpg', 'engineSize'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['brand', 'model', 'transmission', 'fuelType'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') stacking: StackingRegressor?Documentation for stacking: StackingRegressorStackingRegressor(cv=5,\n                  estimators=[('cat',\n                               &lt;catboost.core.CatBoostRegressor object at 0x000001F1F65C5A60&gt;),\n                              ('rf',\n                               RandomForestRegressor(max_depth=28,\n                                                     max_features=0.58,\n                                                     max_samples=1.0,\n                                                     n_estimators=60,\n                                                     random_state=42)),\n                              ('knn',\n                               KNeighborsRegressor(metric='manhattan',\n                                                   n_neighbors=8,\n                                                   weights='distance')),\n                              ('xgb',\n                               XGBRegressor(base_score=None, booster=None,\n                                            callback...\n                                            grow_policy=None,\n                                            importance_type=None,\n                                            interaction_constraints=None,\n                                            learning_rate=0.15, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=7,\n                                            max_leaves=None, min_child_weight=1,\n                                            missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=193, n_jobs=None,\n                                            num_parallel_tree=None, ...))],\n                  final_estimator=LinearRegression()) catCatBoostRegressor&lt;catboost.core.CatBoostRegressor object at 0x000001F1F65C5A60&gt; rfRandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(max_depth=28, max_features=0.58, max_samples=1.0,\n                      n_estimators=60, random_state=42) knnKNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor(metric='manhattan', n_neighbors=8, weights='distance') xgbXGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.635, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=0, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=0.15, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=7,\n             max_leaves=None, min_child_weight=1, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=193,\n             n_jobs=None, num_parallel_tree=None, ...) final_estimatorLinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# Predict and evaluate\ny_pred_top4_tuned = stacking_tuned_top4_pipeline.predict(X_test)\nrmse_top4_tuned = root_mean_squared_error(y_test, y_pred_top4_tuned)\n\n# Add result to results_df\nreg_results_df.loc[len(reg_results_df.index)] = ['Stacking Optimized Top 4 Models', rmse_top4_tuned]\n\n# Sort and reset index\nreg_results_df = reg_results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n\n# Display results\nreg_results_df\n\n\n\n\n\n\n\n\nModel\nRMSE\n\n\n\n\n0\nStacking Optimized Top 4 Models\n2971.389848\n\n\n1\nOptmized XGBoost\n3105.835205\n\n\n2\nLr Stacking Top 4 Regressor\n3135.705167\n\n\n3\nStacking with Ridge\n3147.701164\n\n\n4\nOptmized CatBoost\n3175.161327\n\n\n5\nLr Stacking Regressor\n3190.112423\n\n\n6\nOptmized Random Forest\n3279.094977\n\n\n7\nBaseline CatBoost\n3296.493137\n\n\n8\nVoting Regressor\n3302.202159\n\n\n9\nBaseline XGBoost\n3397.155518\n\n\n10\nBaseline Random Forest\n3660.145970\n\n\n11\nOptmized KNN Regressor\n3680.370319\n\n\n12\nBaseline LightGBM\n3729.955778\n\n\n13\nBaseline KNN Regressor\n4062.839680\n\n\n14\nBaseline Decision Tree\n5015.812547\n\n\n15\nBaseline Linear Regression\n5801.435399\n\n\n\n\n\n\n\nKey Takeaway\n\nBy using the tuned regressors, the performance of our stacking model improved further, achieving the lowest RMSE overall.\nOptimized regularized boosting tree models‚Äîsuch as XGBoost and CatBoost‚Äîdemonstrated strong predictive performance even without stacking.\nThese optimized models outperformed several stacking models that used default base regressors.\nThis highlights the importance of proper hyperparameter tuning, especially for powerful individual learners.\n\nOverall, stacking with well-tuned base models and a simple meta-model (even without regularization) offers a significant boost in predictive accuracy, outperforming both individual models and simple averaging ensembles.\n\n\n\n13.3.4 Ensembling Models Based on Different Feature Sets\nEnsemble learning benefits significantly from model diversity‚Äîit helps reduce overfitting and improves the robustness of predictions. By combining models that learn from different perspectives, an ensemble has the potential to outperform any single model.\nSo far, we‚Äôve shown how to ensemble diverse models by varying algorithms. Another powerful approach is to introduce diversity through different feature subsets, allowing each model to focus on specific aspects of the data. This idea is similar to how methods like bagging and boosting inject diversity through data sampling or random feature selection (max_features, colsample_bytree, etc.).\nAn alternative way to introduce such diversity is to train strong base models on different subsets of features. These subsets can be selected using techniques like: - Polynomial features - Tree-based feature importance - Stepwise feature selection\nBy assigning tailored feature sets to different models, tuning each individually, and then combining them in a stacked ensemble, we can potentially achieve even lower RMSE.\nBuilding such a system involves: 1. Selecting a meaningful feature subset for each base model, 2. Tuning each model on its selected features, and 3. Ensembling them using a meta-learner.\nThis more advanced form of stacking is a powerful technique‚ÄîI‚Äôll leave its full exploration to you!",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Advanced Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "voting_stacking.html#exploring-stacking-and-voting-in-classification",
    "href": "voting_stacking.html#exploring-stacking-and-voting-in-classification",
    "title": "13¬† Advanced Ensemble Learning",
    "section": "13.4 Exploring Stacking and Voting in Classification",
    "text": "13.4 Exploring Stacking and Voting in Classification\n\n13.4.1 Voting Classifier\nThere are two main types of voting in classification ensembles:\nHard Voting:\n\nEach base model votes for a class label.\nThe final prediction is the majority class across models.\nSimple and interpretable.\n\nSoft Voting: - Each base model predicts class probabilities. - The probabilities are averaged, and the final prediction is the class with the highest average probability. - Usually performs better than hard voting (especially with well-calibrated models).\nThe figure below illustrates the difference between hard and soft voting.\n\n\n\n\n\nWe‚Äôll build an ensemble of models to predict whether a person has heart disease, focusing on improving classification accuracy.\n\nheart_data = pd.read_csv('./Datasets/Heart.csv')\nprint(heart_data.shape)\nheart_data.head()\n\n(303, 14)\n\n\n\n\n\n\n\n\n\nAge\nSex\nChestPain\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nThal\nAHD\n\n\n\n\n0\n63\n1\ntypical\n145\n233\n1\n2\n150\n0\n2.3\n3\n0.0\nfixed\nNo\n\n\n1\n67\n1\nasymptomatic\n160\n286\n0\n2\n108\n1\n1.5\n2\n3.0\nnormal\nYes\n\n\n2\n67\n1\nasymptomatic\n120\n229\n0\n2\n129\n1\n2.6\n2\n2.0\nreversable\nYes\n\n\n3\n37\n1\nnonanginal\n130\n250\n0\n0\n187\n0\n3.5\n3\n0.0\nnormal\nNo\n\n\n4\n41\n0\nnontypical\n130\n204\n0\n2\n172\n0\n1.4\n1\n0.0\nnormal\nNo\n\n\n\n\n\n\n\n\n# Define target and features\nclf_X = heart_data.drop(columns='AHD')\nclf_y = heart_data['AHD'].map({'Yes': 1, 'No': 0})  # Convert target to binary 0/1\n\n# Train-test split\nclf_X_train, clf_X_test, clf_y_train, clf_y_test = train_test_split(\n    clf_X, clf_y, stratify=clf_y, test_size=0.2, random_state=42\n)\n\n\n# Identify column types\nclf_numeric_cols = ['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak']\nclf_categorical_cols = ['Sex', 'ChestPain', 'Fbs', 'RestECG', 'ExAng', 'Slope', 'Ca', 'Thal']\n\n# Preprocessing pipelines\nclf_numeric_transformer = StandardScaler()\nclf_categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\nclf_preprocessor = ColumnTransformer([\n    ('num', clf_numeric_transformer, clf_numeric_cols),\n    ('cat', clf_categorical_transformer, clf_categorical_cols)\n])\n\n\n# Define all 8 base classifiers\nbase_clf_models = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n    'Decision Tree': DecisionTreeClassifier(random_state=42),\n    'KNN': KNeighborsClassifier(),\n    'Random Forest': RandomForestClassifier(random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n    'XGBoost': xgb.XGBClassifier(eval_metric='logloss', random_state=42),\n    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1),\n    'CatBoost': cb.CatBoostClassifier(random_state=42, verbose=0)\n}\n\n# Store results for both base and ensemble models\nclf_results = []\n\nfor name, model in base_clf_models.items():\n    # Create pipeline\n    base_clf_pipeline = Pipeline([\n        ('preprocessor', clf_preprocessor),\n        ('classifier', model)\n    ])\n    \n    # Train\n    base_clf_pipeline.fit(clf_X_train, clf_y_train)\n    \n    # Predict\n    base_y_pred = base_clf_pipeline.predict(clf_X_test)\n    base_y_proba = base_clf_pipeline.predict_proba(clf_X_test)[:, 1]\n    \n    # Compute metrics\n    base_acc = accuracy_score(clf_y_test, base_y_pred)\n    base_roc = roc_auc_score(clf_y_test, base_y_proba)\n    base_f1 = f1_score(clf_y_test, base_y_pred)\n    \n    clf_results.append({\n        'Model': name,\n        'Accuracy': base_acc,\n        'ROC AUC': base_roc,\n        'F1 Score': base_f1\n    })\n\n# Create DataFrame\nclf_results_df = pd.DataFrame(clf_results).sort_values(by='ROC AUC', ascending=False).reset_index(drop=True)\nclf_results_df\n\n\n\n\n\n\n\n\nModel\nAccuracy\nROC AUC\nF1 Score\n\n\n\n\n0\nLightGBM\n0.901639\n0.965368\n0.896552\n\n\n1\nLogistic Regression\n0.885246\n0.958874\n0.881356\n\n\n2\nKNN\n0.868852\n0.946970\n0.862069\n\n\n3\nCatBoost\n0.885246\n0.945887\n0.885246\n\n\n4\nGradient Boosting\n0.868852\n0.943723\n0.866667\n\n\n5\nRandom Forest\n0.868852\n0.937229\n0.866667\n\n\n6\nXGBoost\n0.836066\n0.920996\n0.838710\n\n\n7\nDecision Tree\n0.655738\n0.660173\n0.655738\n\n\n\n\n\n\n\nDefine base classifiers (as tuples) for ensemble models like VotingClassifier or StackingClassifier\n\n# Define all classifiers (as tuples for VotingClassifier)\nclf_base_learners = [\n    ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n    ('dt', DecisionTreeClassifier(random_state=42)),\n    ('knn', KNeighborsClassifier()),\n    ('rf', RandomForestClassifier(random_state=42)),\n    ('gb', GradientBoostingClassifier(random_state=42)),\n    ('xgb', xgb.XGBClassifier(eval_metric='logloss', random_state=42)),\n    ('lgb', lgb.LGBMClassifier(random_state=42)),\n    ('cat', cb.CatBoostClassifier(random_state=42, verbose=0))\n]\n\n\n13.4.1.1 Hard Voting Ensemble\n\n# Create a hard voting classifier (voting='hard')\nens_clf_hard_voting = VotingClassifier(\n    estimators=clf_base_learners,\n    voting='hard'  # Use predicted class labels for majority rule voting\n)\n\n# Pipeline with preprocessing\nens_clf_hard_voting_pipeline = Pipeline([\n    ('preprocessor', clf_preprocessor),\n    ('voting', ens_clf_hard_voting)\n])\n\n# Fit model\nens_clf_hard_voting_pipeline.fit(clf_X_train, clf_y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['Age', 'RestBP', 'Chol',\n                                                   'MaxHR', 'Oldpeak']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Sex', 'ChestPain', 'Fbs',\n                                                   'RestECG', 'ExAng', 'Slope',\n                                                   'Ca', 'Thal'])])),\n                ('voting',\n                 VotingClassifier(estimators=[('lr',\n                                               LogisticRegression(max_iter=1000,\n                                                                  random_state=42)),\n                                              ('dt',\n                                               Dec...\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                              ('lgb',\n                                               LGBMClassifier(random_state=42)),\n                                              ('cat',\n                                               &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)]))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['Age', 'RestBP', 'Chol',\n                                                   'MaxHR', 'Oldpeak']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Sex', 'ChestPain', 'Fbs',\n                                                   'RestECG', 'ExAng', 'Slope',\n                                                   'Ca', 'Thal'])])),\n                ('voting',\n                 VotingClassifier(estimators=[('lr',\n                                               LogisticRegression(max_iter=1000,\n                                                                  random_state=42)),\n                                              ('dt',\n                                               Dec...\n                                                             max_bin=None,\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                              ('lgb',\n                                               LGBMClassifier(random_state=42)),\n                                              ('cat',\n                                               &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)]))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['Sex', 'ChestPain', 'Fbs', 'RestECG', 'ExAng',\n                                  'Slope', 'Ca', 'Thal'])]) num['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['Sex', 'ChestPain', 'Fbs', 'RestECG', 'ExAng', 'Slope', 'Ca', 'Thal'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') voting: VotingClassifier?Documentation for voting: VotingClassifierVotingClassifier(estimators=[('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('dt', DecisionTreeClassifier(random_state=42)),\n                             ('knn', KNeighborsClassifier()),\n                             ('rf', RandomForestClassifier(random_state=42)),\n                             ('gb',\n                              GradientBoostingClassifier(random_state=42)),\n                             ('xgb',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,...\n                                            learning_rate=None, max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None, ...)),\n                             ('lgb', LGBMClassifier(random_state=42)),\n                             ('cat',\n                              &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)]) lrLogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) dtDecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(random_state=42) knnKNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier() rfRandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(random_state=42) gbGradientBoostingClassifier?Documentation for GradientBoostingClassifierGradientBoostingClassifier(random_state=42) xgbXGBClassifier?Documentation for XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, ...) lgbLGBMClassifierLGBMClassifier(random_state=42) catCatBoostClassifier&lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt; \n\n\n\n# Predict\nens_y_pred_hard = ens_clf_hard_voting_pipeline.predict(clf_X_test)\n\n# Compute metrics\nens_acc_hard = accuracy_score(clf_y_test, ens_y_pred_hard)\nens_roc_hard = roc_auc_score(clf_y_test, ens_y_pred_hard)\nens_f1_hard = f1_score(clf_y_test, ens_y_pred_hard)\n\n# Store results\nclf_results.append({\n    'Model': 'Hard Voting Classifier',\n    'Accuracy': ens_acc_hard,\n    'ROC AUC': ens_roc_hard,\n    'F1 Score': ens_f1_hard\n})\n\n# Create or update results DataFrame\nclf_results_df = pd.DataFrame(clf_results).sort_values(by='ROC AUC', ascending=False).reset_index(drop=True)\nclf_results_df\n\n\n\n\n\n\n\n\nModel\nAccuracy\nROC AUC\nF1 Score\n\n\n\n\n0\nLightGBM\n0.901639\n0.965368\n0.896552\n\n\n1\nLogistic Regression\n0.885246\n0.958874\n0.881356\n\n\n2\nKNN\n0.868852\n0.946970\n0.862069\n\n\n3\nCatBoost\n0.885246\n0.945887\n0.885246\n\n\n4\nGradient Boosting\n0.868852\n0.943723\n0.866667\n\n\n5\nRandom Forest\n0.868852\n0.937229\n0.866667\n\n\n6\nXGBoost\n0.836066\n0.920996\n0.838710\n\n\n7\nHard Voting Classifier\n0.901639\n0.906385\n0.900000\n\n\n8\nDecision Tree\n0.655738\n0.660173\n0.655738\n\n\n\n\n\n\n\n\n\n13.4.1.2 Soft Voting Ensemble\n\n## Soft Voting Ensemble\n\n# Define soft voting classifier\nens_clf_soft_voting = VotingClassifier(\n    estimators=clf_base_learners,\n    voting='soft'  # Use predicted probabilities\n)\n\n# Create pipeline with preprocessing\nens_clf_soft_voting_pipeline = Pipeline([\n    ('preprocessor', clf_preprocessor),\n    ('voting', ens_clf_soft_voting)\n])\n\n# Fit model\nens_clf_soft_voting_pipeline.fit(clf_X_train, clf_y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['Age', 'RestBP', 'Chol',\n                                                   'MaxHR', 'Oldpeak']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Sex', 'ChestPain', 'Fbs',\n                                                   'RestECG', 'ExAng', 'Slope',\n                                                   'Ca', 'Thal'])])),\n                ('voting',\n                 VotingClassifier(estimators=[('lr',\n                                               LogisticRegression(max_iter=1000,\n                                                                  random_state=42)),\n                                              ('dt',\n                                               Dec...\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                              ('lgb',\n                                               LGBMClassifier(random_state=42)),\n                                              ('cat',\n                                               &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)],\n                                  voting='soft'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['Age', 'RestBP', 'Chol',\n                                                   'MaxHR', 'Oldpeak']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Sex', 'ChestPain', 'Fbs',\n                                                   'RestECG', 'ExAng', 'Slope',\n                                                   'Ca', 'Thal'])])),\n                ('voting',\n                 VotingClassifier(estimators=[('lr',\n                                               LogisticRegression(max_iter=1000,\n                                                                  random_state=42)),\n                                              ('dt',\n                                               Dec...\n                                                             max_cat_threshold=None,\n                                                             max_cat_to_onehot=None,\n                                                             max_delta_step=None,\n                                                             max_depth=None,\n                                                             max_leaves=None,\n                                                             min_child_weight=None,\n                                                             missing=nan,\n                                                             monotone_constraints=None,\n                                                             multi_strategy=None,\n                                                             n_estimators=None,\n                                                             n_jobs=None,\n                                                             num_parallel_tree=None, ...)),\n                                              ('lgb',\n                                               LGBMClassifier(random_state=42)),\n                                              ('cat',\n                                               &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)],\n                                  voting='soft'))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['Sex', 'ChestPain', 'Fbs', 'RestECG', 'ExAng',\n                                  'Slope', 'Ca', 'Thal'])]) num['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak'] StandardScaler?Documentation for StandardScalerStandardScaler() cat['Sex', 'ChestPain', 'Fbs', 'RestECG', 'ExAng', 'Slope', 'Ca', 'Thal'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') voting: VotingClassifier?Documentation for voting: VotingClassifierVotingClassifier(estimators=[('lr',\n                              LogisticRegression(max_iter=1000,\n                                                 random_state=42)),\n                             ('dt', DecisionTreeClassifier(random_state=42)),\n                             ('knn', KNeighborsClassifier()),\n                             ('rf', RandomForestClassifier(random_state=42)),\n                             ('gb',\n                              GradientBoostingClassifier(random_state=42)),\n                             ('xgb',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,...\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=None,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=None, n_jobs=None,\n                                            num_parallel_tree=None, ...)),\n                             ('lgb', LGBMClassifier(random_state=42)),\n                             ('cat',\n                              &lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt;)],\n                 voting='soft') lrLogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=42) dtDecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(random_state=42) knnKNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier() rfRandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(random_state=42) gbGradientBoostingClassifier?Documentation for GradientBoostingClassifierGradientBoostingClassifier(random_state=42) xgbXGBClassifier?Documentation for XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, ...) lgbLGBMClassifierLGBMClassifier(random_state=42) catCatBoostClassifier&lt;catboost.core.CatBoostClassifier object at 0x000001F18B3AF380&gt; \n\n\n\n# Predict\nens_y_pred_soft = ens_clf_soft_voting_pipeline.predict(clf_X_test)\n\n# Compute metrics\nens_acc_soft = accuracy_score(clf_y_test, ens_y_pred_soft)\nens_roc_soft = roc_auc_score(clf_y_test, ens_y_pred_soft)\nens_f1_soft = f1_score(clf_y_test, ens_y_pred_soft)\n\n# Store results\nclf_results.append({\n    'Model': 'Soft Voting Classifier',\n    'Accuracy': ens_acc_soft,\n    'ROC AUC': ens_roc_soft,\n    'F1 Score': ens_f1_soft\n})\n\n# Update results DataFrame\nclf_results_df = pd.DataFrame(clf_results).sort_values(by='ROC AUC', ascending=False).reset_index(drop=True)\nclf_results_df\n\n\n\n\n\n\n\n\nModel\nAccuracy\nROC AUC\nF1 Score\n\n\n\n\n0\nLightGBM\n0.901639\n0.965368\n0.896552\n\n\n1\nLogistic Regression\n0.885246\n0.958874\n0.881356\n\n\n2\nKNN\n0.868852\n0.946970\n0.862069\n\n\n3\nCatBoost\n0.885246\n0.945887\n0.885246\n\n\n4\nGradient Boosting\n0.868852\n0.943723\n0.866667\n\n\n5\nRandom Forest\n0.868852\n0.937229\n0.866667\n\n\n6\nXGBoost\n0.836066\n0.920996\n0.838710\n\n\n7\nHard Voting Classifier\n0.901639\n0.906385\n0.900000\n\n\n8\nSoft Voting Classifier\n0.901639\n0.906385\n0.900000\n\n\n9\nDecision Tree\n0.655738\n0.660173\n0.655738\n\n\n\n\n\n\n\n\n\n\n13.4.2 Stacking Classifier\nConceptually, the idea is similar to that of Stacking regressor.\n\n# Create a meta-model\nens_meta_clf = LogisticRegression(max_iter=1000, random_state=42)\n\n# Define the stacking classifier\nens_clf_stacking = StackingClassifier(\n    estimators=clf_base_learners,\n    final_estimator=ens_meta_clf,\n    cv=5\n)\n\n# Create pipeline with preprocessing\nens_clf_stacking_pipeline = Pipeline([\n    ('preprocessor', clf_preprocessor),\n    ('stacking', ens_clf_stacking)\n])\n\n# Fit the pipeline\nens_clf_stacking_pipeline.fit(clf_X_train, clf_y_train)\n\n# Predict\nens_y_pred_stack = ens_clf_stacking_pipeline.predict(clf_X_test)\n\n# Compute metrics\nens_acc_stack = accuracy_score(clf_y_test, ens_y_pred_stack)\nens_roc_stack = roc_auc_score(clf_y_test, ens_y_pred_stack)\nens_f1_stack = f1_score(clf_y_test, ens_y_pred_stack)\n\n# Store results\nclf_results.append({\n    'Model': 'Stacking Classifier',\n    'Accuracy': ens_acc_stack,\n    'ROC AUC': ens_roc_stack,\n    'F1 Score': ens_f1_stack\n})\n\n# Update results DataFrame\nclf_results_df = pd.DataFrame(clf_results).sort_values(by='ROC AUC', ascending=False).reset_index(drop=True)\nclf_results_df\n\n\n\n\n\n\n\n\nModel\nAccuracy\nROC AUC\nF1 Score\n\n\n\n\n0\nLightGBM\n0.901639\n0.965368\n0.896552\n\n\n1\nLogistic Regression\n0.885246\n0.958874\n0.881356\n\n\n2\nKNN\n0.868852\n0.946970\n0.862069\n\n\n3\nCatBoost\n0.885246\n0.945887\n0.885246\n\n\n4\nGradient Boosting\n0.868852\n0.943723\n0.866667\n\n\n5\nRandom Forest\n0.868852\n0.937229\n0.866667\n\n\n6\nXGBoost\n0.836066\n0.920996\n0.838710\n\n\n7\nHard Voting Classifier\n0.901639\n0.906385\n0.900000\n\n\n8\nSoft Voting Classifier\n0.901639\n0.906385\n0.900000\n\n\n9\nStacking Classifier\n0.901639\n0.906385\n0.900000\n\n\n10\nDecision Tree\n0.655738\n0.660173\n0.655738\n\n\n\n\n\n\n\nLet‚Äôs print out the coefficients of the logistic regression meta-model\n\n# Access the trained meta-model inside the stacking pipeline\nens_meta_clf = ens_clf_stacking_pipeline.named_steps['stacking'].final_estimator_\n\n# Get model names in the same order as the coefficients\nmeta_model_names = [name for name, _ in clf_base_learners]\n\n# Extract coefficients\nmeta_model_coefs = ens_meta_clf.coef_[0]  # For binary classification\n\n# Create a DataFrame to display model weights\nmeta_coef_df = pd.DataFrame({\n    'Base Model': meta_model_names,\n    'Meta-Model Coefficient': meta_model_coefs\n})\n\n# Sort by coefficient value (optional)\nmeta_coef_df = meta_coef_df.sort_values(by='Meta-Model Coefficient', ascending=False).reset_index(drop=True)\n\n# Show the coefficients\nmeta_coef_df\n\n\n\n\n\n\n\n\nBase Model\nMeta-Model Coefficient\n\n\n\n\n0\nlr\n2.296773\n\n\n1\nrf\n1.251281\n\n\n2\ncat\n0.636938\n\n\n3\nknn\n0.471488\n\n\n4\ndt\n0.446707\n\n\n5\nxgb\n0.338406\n\n\n6\nlgb\n0.168090\n\n\n7\ngb\n-0.317501\n\n\n\n\n\n\n\n\n\n13.4.3 Interpretation of Ensemble Results\nInterestingly, the Hard Voting, Soft Voting, and Stacking Classifier all achieved identical performance across accuracy, ROC_AUC, and F1_score. This outcome can be explained by the following factors:\n\nHighly correlated base models: All three ensemble methods use the same set of strong classifiers (e.g., LightGBM, CatBoost, Logistic Regression), which already produce very similar predictions. As a result, the ensembles also produce very similar outputs.\nLimited benefit from ensembling: Since the individual base models perform well and are well-aligned, combining them using different ensemble strategies yields nearly identical predictions.\nMeta-model in stacking mimics soft voting: The stacking classifier uses a Logistic Regression meta-model, which may learn weights close to equal when base predictions are similar‚Äîeffectively behaving like soft voting.\nNo diversity in feature views or training subsets: All base models were trained on the same feature set and data. Adding diversity (e.g., by using different subsets of features or training data) could help stacking outperform voting-based ensembles.\n\nWhile ensemble methods are designed to improve performance by combining diverse models, in this case, the base learners were already strong and aligned, limiting the potential gain from ensembling.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Advanced Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "voting_stacking.html#summary-stacking-and-voting-ensembles",
    "href": "voting_stacking.html#summary-stacking-and-voting-ensembles",
    "title": "13¬† Advanced Ensemble Learning",
    "section": "13.5 Summary: Stacking and Voting Ensembles",
    "text": "13.5 Summary: Stacking and Voting Ensembles\nIn this chapter, we explored ensemble learning methods‚ÄîVoting and Stacking‚Äîand applied them to both regression and classification tasks. We incorporated all the models you‚Äôve learned throughout this sequence and compared their performance against individual base models using a variety of evaluation metrics, including RMSE, Accuracy, ROC AUC, and F1 Score.\nKey takeaways include:\n\nVoting combines predictions from multiple models either by majority rule (hard voting) or by averaging probabilities (soft voting). It‚Äôs simple, intuitive, and often improves robustness.\nStacking uses a meta-model to learn the best way to combine base model predictions. It has the potential to outperform voting when base models offer complementary strengths.\nIn practice, the performance gains from ensembling depend heavily on the diversity and strength of the base models. When base models are highly correlated or already strong, ensembling may yield limited additional benefit.\n\nThis chapter also highlighted the importance of:\n\nSelecting diverse and well-tuned base learners,\nUnderstanding the behavior of meta-models in stacking,\nEvaluating ensemble strategies based on the specific task and dataset.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Advanced Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "smarter_hyper_tuning.html",
    "href": "smarter_hyper_tuning.html",
    "title": "12¬† Smarter Hyperparameter Tuning",
    "section": "",
    "text": "12.1 Cross-Validation Basics\nTree-based models, like decision trees, random forests, and boosting algorithms (e.g., XGBoost, LightGBM, CatBoost), benefit significantly from hyperparameter optimization. Tools like GridSearchCV, RandomizedSearchCV, and cross_val_score in scikit-learn enable robust tuning via cross-validation, similar to linear or logistic regression models. Below, we explore these tools and smarter alternatives for complex models.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Smarter Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "smarter_hyper_tuning.html#cross-validation-basics",
    "href": "smarter_hyper_tuning.html#cross-validation-basics",
    "title": "12¬† Smarter Hyperparameter Tuning",
    "section": "",
    "text": "cross_val_score: Evaluates a model‚Äôs performance for a fixed set of hyperparameters using cross-validation. Requires manual loops to search over hyperparameter combinations, making it labor-intensive for extensive tuning.\nGridSearchCV and RandomizedSearchCV: Automate hyperparameter search with built-in cross-validation, internally handling loops to evaluate combinations efficiently.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Smarter Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "smarter_hyper_tuning.html#gridsearchcv-exhaustive-but-limited",
    "href": "smarter_hyper_tuning.html#gridsearchcv-exhaustive-but-limited",
    "title": "12¬† Smarter Hyperparameter Tuning",
    "section": "12.2 GridSearchCV: Exhaustive but Limited",
    "text": "12.2 GridSearchCV: Exhaustive but Limited\nGridSearchCV performs an exhaustive search over a predefined grid of hyperparameter values.\n\nPros:\n\nGuarantees finding the best combination within the specified grid.\nIdeal for small, well-defined search spaces (e.g., tuning max_depth=[3, 5, 7] and n_estimators=[100, 200]).\n\nCons:\n\nRequires discrete lists for each hyperparameter, unable to sample from continuous distributions.\nGrid size grows exponentially with more hyperparameters, making it computationally infeasible for complex models like boosting trees (e.g., XGBoost with 5+ hyperparameters).\nTime-consuming, especially for large datasets or slow models.\n\n\nUse Case: Best for simple models or when you have a small, targeted set of hyperparameter values.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Smarter Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "smarter_hyper_tuning.html#randomizedsearchcv-efficient-but-random",
    "href": "smarter_hyper_tuning.html#randomizedsearchcv-efficient-but-random",
    "title": "12¬† Smarter Hyperparameter Tuning",
    "section": "12.3 RandomizedSearchCV: Efficient but Random",
    "text": "12.3 RandomizedSearchCV: Efficient but Random\nRandomizedSearchCV samples a fixed number of hyperparameter combinations (n_iter) randomly from user-defined distributions.\n\nPros:\n\nMore efficient than GridSearchCV for large search spaces, as it evaluates fewer combinations.\nSupports continuous distributions (e.g., scipy.stats.loguniform for learning rate), offering greater flexibility.\nfrom scipy.stats import uniform\n'learning_rate': uniform(loc=0.01, scale=0.3),  # Range: [0.01, 0.31)\nFaster, as it avoids exhaustive enumeration.\n\nCons:\n\nRandom sampling may miss optimal regions, especially in high-dimensional spaces or with limited iterations.\nPerformance depends on the choice of n_iter and distribution quality.\n\n\nUse Case: Preferred for initial exploration or when computational resources are limited but the search space is large.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Smarter Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "smarter_hyper_tuning.html#challenges-with-boosting-models",
    "href": "smarter_hyper_tuning.html#challenges-with-boosting-models",
    "title": "12¬† Smarter Hyperparameter Tuning",
    "section": "12.4 Challenges with Boosting Models",
    "text": "12.4 Challenges with Boosting Models\nBoosting models (e.g., Gradient Boosting, XGBoost, LightGBM) are powerful but have many hyperparameters (e.g., learning rate, max depth, number of estimators, regularization terms). This complexity makes GridSearchCV impractical and RandomizedSearchCV suboptimal, as random sampling can be inefficient in high-dimensional spaces.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Smarter Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "smarter_hyper_tuning.html#smarter-alternatives-for-complex-models",
    "href": "smarter_hyper_tuning.html#smarter-alternatives-for-complex-models",
    "title": "12¬† Smarter Hyperparameter Tuning",
    "section": "12.5 Smarter Alternatives for Complex Models",
    "text": "12.5 Smarter Alternatives for Complex Models\nFor boosting models, neural networks, or other high-dimensional algorithms, intelligent optimization methods outperform traditional approaches by efficiently exploring the hyperparameter space. Here are the top alternatives:\n\n12.5.1 BayesSearchCV (Bayesian Optimization)\n\nHow it works: Uses a probabilistic surrogate model (e.g., Gaussian Process) to predict promising hyperparameter combinations based on past evaluations.\nAdvantages:\n\nConverges faster than random search by focusing on high-performing regions.\nIt works natively with scikit-learn pipelines, simplifying tuning for workflows with preprocessing\nGP (Gassian Process) - based optimization is effective for smooth, continuous hyperparameters (e.g., learning rate, regularization), converging faster in low-dimensional spaces (Tuning 2-5 continuous parameters).\nIdeal for produnction environment restricted to scikit-learn extensions\n\nLibrary: scikit-optimize or bayes_opt.\nUse Case: Ideal for small to medium search spaces with continuous parameters on small to medium-sized datasets.\n\nWe will now return to the car dataset and perform hyperparameter optimization on the XGBoost model using BayesSearchCV.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import root_mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\nfrom xgboost import XGBRegressor, XGBClassifier\nimport seaborn as sns\nimport plotly\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_convergence\nimport warnings\nfrom IPython import display\n\n\n12.5.1.1 Data Preprocessing\n\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\ncar.head()\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\nvw\nBeetle\n2014\nManual\n55457\nDiesel\n30\n65.3266\n1.6\n7490\n\n\n1\nvauxhall\nGTC\n2017\nManual\n15630\nPetrol\n145\n47.2049\n1.4\n10998\n\n\n2\nmerc\nG Class\n2012\nAutomatic\n43000\nDiesel\n570\n25.1172\n3.0\n44990\n\n\n3\naudi\nRS5\n2019\nAutomatic\n10\nPetrol\n145\n30.5593\n2.9\n51990\n\n\n4\nmerc\nX-CLASS\n2018\nAutomatic\n14000\nDiesel\n240\n35.7168\n2.3\n28990\n\n\n\n\n\n\n\n\nX = car.drop(columns=['price'])\ny = car['price']\n\n# extract the categorical columns and put them in a list\ncategorical_feature = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumerical_feature = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# convert the categorical columns to category type\nfor col in categorical_feature:\n    X[col] = X[col].astype('category')\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n12.5.1.2 Baseline Performance\n\n# build a baseline xgboost model\nxgb = XGBRegressor(objective='reg:squarederror', enable_categorical=True, random_state=42)\n# fit the model\nxgb.fit(X_train, y_train, )\n# make predictions\ny_pred = xgb.predict(X_test)\n\nprint ('Baseline Model: ')\n# calculate the RMSE\nrmse = root_mean_squared_error(y_test, y_pred)\nprint(f'RMSE: {rmse}')\n# calculate the R2 score\nr2 = r2_score(y_test, y_pred)\nprint(f'R2: {r2}')\n\nBaseline Model: \nRMSE: 3299.648193359375\nR2: 0.9628884792327881\n\n\n\n\n12.5.1.3 BayesSearchCV inital tuning\nLet‚Äôs use BayesSearchCV to boost its performance\n\n%%time\n# define the search space for Bayesian optimization\nsearch_space = {\n    'n_estimators': Integer(50, 500),\n    'max_depth': Integer(5, 30),\n    'learning_rate': Real(0.01, 0.3, prior='uniform'),\n    'subsample': Real(0.5, 1.0, prior='uniform'),\n    'colsample_bytree': Real(0.5, 1.0, prior='uniform'),\n    'gamma': Real(0, 5, prior='uniform'),\n}\n\n# define the model\nxgb = XGBRegressor(objective='reg:squarederror', enable_categorical=True, random_state=42)\n# define the Bayesian optimization search\nbayes_search = BayesSearchCV(\n    xgb,\n    search_space,\n    n_iter=50,\n    scoring='neg_root_mean_squared_error',\n    cv=3,\n    n_jobs=-1,\n    random_state=42,\n    verbose=0\n)\n# fit the model\nbayes_search.fit(X_train, y_train)\n\nCPU times: total: 46.6 s\nWall time: 3min 24s\n\n\nBayesSearchCV(cv=3,\n              estimator=XGBRegressor(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=True, eval_metric=None,\n                                     feature_types=None, feature_weights=None,\n                                     gamma=None, grow_policy=None,\n                                     importance_type=None,\n                                     interaction_constraints=None...\n                             'gamma': Real(low=0, high=5, prior='uniform', transform='normalize'),\n                             'learning_rate': Real(low=0.01, high=0.3, prior='uniform', transform='normalize'),\n                             'max_depth': Integer(low=5, high=30, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=50, high=500, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.5, high=1.0, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCViFittedBayesSearchCV(cv=3,\n              estimator=XGBRegressor(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=True, eval_metric=None,\n                                     feature_types=None, feature_weights=None,\n                                     gamma=None, grow_policy=None,\n                                     importance_type=None,\n                                     interaction_constraints=None...\n                             'gamma': Real(low=0, high=5, prior='uniform', transform='normalize'),\n                             'learning_rate': Real(low=0.01, high=0.3, prior='uniform', transform='normalize'),\n                             'max_depth': Integer(low=5, high=30, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=50, high=500, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.5, high=1.0, prior='uniform', transform='normalize')}) best_estimator_: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.7800962787418171, device=None,\n             early_stopping_rounds=None, enable_categorical=True,\n             eval_metric=None, feature_types=None, feature_weights=None,\n             gamma=5.0, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.03253670126492928,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=342, n_jobs=None,\n             num_parallel_tree=None, ...) XGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.7800962787418171, device=None,\n             early_stopping_rounds=None, enable_categorical=True,\n             eval_metric=None, feature_types=None, feature_weights=None,\n             gamma=5.0, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.03253670126492928,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=342, n_jobs=None,\n             num_parallel_tree=None, ...) \n\n\n\nprint('Bayesian Optimization Results: ')\n# get the best parameters\nbest_params_bayes = bayes_search.best_params_\nprint('Best Parameters: ')\nprint(best_params_bayes)\n# get the best score\nbest_score_bayes = bayes_search.best_score_\nprint('Best CV Score: ', best_score_bayes)\n# make predictions\ny_pred_bayes = bayes_search.predict(X_test)\n# calculate the RMSE\nrmse_bayes = root_mean_squared_error(y_test, y_pred_bayes)\nprint('Test RMSE: ', rmse_bayes)\n# calculate the R2 score\nr2_bayes = r2_score(y_test, y_pred_bayes)\nprint('Test R2: ', r2_bayes)\n\nBayesian Optimization Results: \nBest Parameters: \nOrderedDict({'colsample_bytree': 0.7800962787418171, 'gamma': 5.0, 'learning_rate': 0.03253670126492928, 'max_depth': 5, 'n_estimators': 342, 'subsample': 0.5})\nBest CV Score:  -3061.9464518229165\nTest RMSE:  3236.476318359375\nTest R2:  0.9642958641052246\n\n\n\n\n12.5.1.4 Visualizing BayesSearchCV Results\nVisualizing BayesSearchCV outcomes helps interpret the hyperparameter optimization process for tree-based models like XGBoost. Visualization can provide critical insights into convergence, parameter impact, and model behavior, helping you understand and refine model behavior.\nKey Benefits of Visualization:\n\nCheck Convergence:\nPlot best scores vs.¬†iterations to determine if the search has stabilized or requires more trials.\nIdentify Key Parameters:\nUse scatter plots to reveal which hyperparameters (e.g., learning_rate) significantly influence performance.\nExplore Interactions:\nUse heatmaps or contour plots to examine relationships between hyperparameters (e.g., learning_rate vs.¬†max_depth).\nDiagnose Issues:\nSpot poor search regions or insufficient iterations through patterns in the plots.\n\nPractical Tips\n\nPlots:\nUse convergence, scatter, heatmap, or parallel coordinate plots with libraries like Matplotlib or Seaborn.\nAccessing Results:\nExtract tuning history from the cv_results_ attribute of the BayesSearchCV object for plotting.\n\nLet‚Äôs plot the convergence first\n\nfrom skopt.plots import plot_convergence, plot_objective\n\n# plot the convergence\nbayes_res = bayes_search.optimizer_results_[0]\nplot_convergence(bayes_res)\nplt.show()\n\n\n\n\n\n\n\n\n\n# the raw objective values at each iteration\nfunc_vals = np.array(bayes_res.func_vals)\n\n# the best objective value found so far\nbest_val = func_vals.min()\n\n# All iterations (1-based) that hit that minimum\nbest_iters = np.where(func_vals == best_val)[0] + 1\n\nprint(f\"Best objective = {best_val:.4f}\")\nprint(f\"Reached at iteration(s): {best_iters.tolist()}\")\n\nBest objective = 3061.9465\nReached at iteration(s): [50]\n\n\n\nfunc_vals is an array of length n_iter, one entry per call.\nWe use argmin/min because by default BayesSearchCV minimizes the underlying surrogate objective (here negative MAE).\nIf you‚Äôre maximizing some metric, either negate func_vals or look at -best_val appropriately.\n\nLet‚Äôs plot the objective next\n\n# Plot and store the figure and axes\nfig, ax = plt.subplots(figsize=(14, 8))\nresult = plot_objective(bayes_search.optimizer_results_[0], ax=ax)\n\n# Find contour plots in all the axes\nfor i, ax in enumerate(fig.get_axes()):\n    # Look for collections in each axis\n    for collection in ax.collections:\n        if isinstance(collection, plt.matplotlib.collections.QuadMesh) or \\\n           isinstance(collection, plt.matplotlib.collections.PathCollection):\n            # Add colorbar for this collection\n            cbar = fig.colorbar(collection, ax=ax)\n            cbar.set_label('Objective Value')\n            break\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nLet‚Äôs plot evalutions next\n\nfrom skopt.plots import plot_evaluations\n\nplot_evaluations(bayes_search.optimizer_results_[0], bins=10)\nplt.show()\n\n\n\n\n\n\n\n\nLet‚Äôs visualize parameter distributions next\n\n# Get hyperparameter names from BayesSearchCV's search_spaces\nresults = bayes_search.cv_results_\nbest_params = bayes_search.best_params_\n\n# Convert results to DataFrame\nresults_df = pd.DataFrame(results)\n\n# Create plots for each parameter\nfig, axes = plt.subplots(1, len(best_params), figsize=(15, 4))\nparams = list(best_params.keys())\n\nfor i, param in enumerate(params):\n    param_name = f'param_{param}'\n    # Extract parameter values\n    param_values = results_df[param_name].values\n    \n    # Scatter plot: parameter value vs performance\n    axes[i].scatter(param_values, results_df['mean_test_score'])\n    axes[i].set_xlabel(param)\n    axes[i].set_ylabel('Mean Test Score')\n    axes[i].axvline(best_params[param], color='r', linestyle='--', label='Best value')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n12.5.1.5 Next Steps for Hyperparameter Tuning\nRefine Search Space\n\nBased on scatter plots, focus tuning on the most promising regions:\n\nlearning_rate: 0.01‚Äì0.1 (red stars clustered around 0.05)\nn_estimators: 50‚Äì200 (indicated by clustering and red stars)\nmax_depth: 5‚Äì15 (majority of samples fall in this range)\nsubsample: 0.6‚Äì0.9\ncolsample_bytree: 0.6‚Äì0.9\ngamma: 0‚Äì2 (most samples are below 2)\n\nNarrowing the hyperparameter space reduces computational cost and concentrates the search on high-performing regions.\n\nIncrease Iterations\n\nThe convergence plot suggests the search may have stabilized, but increasing the number of iterations (e.g., n_iter=75) within the refined space could help confirm the optimum.\n\n\n\n12.5.1.6 Finetuning with BayesSearchCV\n\n%%time\n# refine the search space for Bayesian optimization\nsearch_space = {\n    'n_estimators': Integer(50, 500),\n    'max_depth': Integer(5, 15),\n    'learning_rate': Real(0.01, 0.1, prior='uniform'),\n    'subsample': Real(0.6, 0.9, prior='uniform'),\n    'colsample_bytree': Real(0.6, 0.9, prior='uniform'),\n    'gamma': Real(0, 2, prior='uniform'),\n}\n\n# define the model\nxgb = XGBRegressor(objective='reg:squarederror', enable_categorical=True, random_state=42)\n# define the Bayesian optimization search\nbayes_search = BayesSearchCV(\n    xgb,\n    search_space,\n    n_iter=75,\n    scoring='neg_root_mean_squared_error',\n    cv=3,\n    n_jobs=-1,\n    random_state=42,\n    verbose=0\n)\n# fit the model\nbayes_search.fit(X_train, y_train)\n\nCPU times: total: 1min 37s\nWall time: 2min 59s\n\n\nBayesSearchCV(cv=3,\n              estimator=XGBRegressor(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=True, eval_metric=None,\n                                     feature_types=None, feature_weights=None,\n                                     gamma=None, grow_policy=None,\n                                     importance_type=None,\n                                     interaction_constraints=None...\n                             'gamma': Real(low=0, high=2, prior='uniform', transform='normalize'),\n                             'learning_rate': Real(low=0.01, high=0.1, prior='uniform', transform='normalize'),\n                             'max_depth': Integer(low=5, high=15, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=50, high=500, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.6, high=0.9, prior='uniform', transform='normalize')})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BayesSearchCViFittedBayesSearchCV(cv=3,\n              estimator=XGBRegressor(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=True, eval_metric=None,\n                                     feature_types=None, feature_weights=None,\n                                     gamma=None, grow_policy=None,\n                                     importance_type=None,\n                                     interaction_constraints=None...\n                             'gamma': Real(low=0, high=2, prior='uniform', transform='normalize'),\n                             'learning_rate': Real(low=0.01, high=0.1, prior='uniform', transform='normalize'),\n                             'max_depth': Integer(low=5, high=15, prior='uniform', transform='normalize'),\n                             'n_estimators': Integer(low=50, high=500, prior='uniform', transform='normalize'),\n                             'subsample': Real(low=0.6, high=0.9, prior='uniform', transform='normalize')}) best_estimator_: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.60000489767951, device=None,\n             early_stopping_rounds=None, enable_categorical=True,\n             eval_metric=None, feature_types=None, feature_weights=None,\n             gamma=1.3194686223941245, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.024138854425252557,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=6, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=409, n_jobs=None,\n             num_parallel_tree=None, ...) XGBRegressor?Documentation for XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.60000489767951, device=None,\n             early_stopping_rounds=None, enable_categorical=True,\n             eval_metric=None, feature_types=None, feature_weights=None,\n             gamma=1.3194686223941245, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.024138854425252557,\n             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=6, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=409, n_jobs=None,\n             num_parallel_tree=None, ...) \n\n\n\nprint('Bayesian Optimization Results: ')\n# get the best parameters\nbest_params_bayes = bayes_search.best_params_\nprint('Best Parameters: ')\nprint(best_params_bayes)\n# get the best score\nbest_score_bayes = bayes_search.best_score_\nprint('Best CV Score: ', best_score_bayes)\n# make predictions\ny_pred_bayes = bayes_search.predict(X_test)\n# calculate the RMSE\nrmse_bayes = root_mean_squared_error(y_test, y_pred_bayes)\nprint('Test RMSE: ', rmse_bayes)\n# calculate the R2 score\nr2_bayes = r2_score(y_test, y_pred_bayes)\nprint('Test R2: ', r2_bayes)\n\nBayesian Optimization Results: \nBest Parameters: \nOrderedDict({'colsample_bytree': 0.60000489767951, 'gamma': 1.3194686223941245, 'learning_rate': 0.024138854425252557, 'max_depth': 6, 'n_estimators': 409, 'subsample': 0.6717795261110148})\nBest CV Score:  -2997.3878580729165\nTest RMSE:  3131.67626953125\nTest R2:  0.9665706753730774\n\n\nAs can be seen, the test RMSE was further reduced after incorporating these steps.\n\n# plot objective function\nbayes_res = bayes_search.optimizer_results_[0]\nplot_convergence(bayes_res)\nplt.show()\n\n\n\n\n\n\n\n\nWith 75 iterations, the search appears sufficient, as the objective value stabilizes\nAlternatively, consider switching to Optuna, which offers advanced features like pruning, allowing it to stop unpromising trials early and focus computational effort on more promising regions of the search space.\nOptuna is often the preferred choice for tuning tree-based models such as XGBoost, LightGBM, and CatBoost, thanks to its intelligent Tree-structured Parzen Estimator (TPE) search algorithm and efficient pruning strategy.\n\n\n\n12.5.2 Optuna (Advanced TPE-Based Optimization)\nOptuna is a modern hyperparameter optimization framework that uses Tree-structured Parzen Estimator (TPE) and pruning to efficiently explore the hyperparameter space.\n\nHow it works: Models the hyperparameter space probabilistically and prioritizes promising combinations, stopping unpromising trials early (pruning).\nPros:\n\nHighly efficient due to pruning, saving compute time for expensive models like boosting trees.\nFlexible, supporting dynamic search spaces and easy integration with XGBoost, LightGBM, and CatBoost.\nOften finds better hyperparameters with fewer trials compared to RandomizedSearchCV.\n\nCons:\n\nRequires slightly more setup (e.g., defining an objective function) than scikit-learn tools.\n\nLibrary: optuna.\nUse Case: Best for most boosting model tasks, especially with large datasets or complex hyperparameter spaces.\n\n\n12.5.2.1 Hyperparameter tuning with Optuna\nWe will use the same search space for fair comparision\n\n%%time\n# use optuna\nimport optuna\nfrom sklearn.model_selection import cross_val_score\nfrom optuna import create_study\n\ndef objective(trial):\n    # Define the hyperparameters to tune\n    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n    max_depth = trial.suggest_int('max_depth', 5, 30)\n    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n    gamma = trial.suggest_float('gamma', 0, 5)\n\n    # Create the model\n    model = XGBRegressor(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        learning_rate=learning_rate,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        gamma=gamma,\n        objective='reg:squarederror',\n        enable_categorical=True,\n        random_state=42\n    )\n\n    # Perform cross-validation\n    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_root_mean_squared_error')\n    \n    return -scores.mean()\n\n# Create a study object\nstudy = create_study(direction='minimize')\n# Optimize the objective function\nstudy.optimize(objective, n_trials=50)\n\n[I 2025-05-18 14:07:55,357] A new study created in memory with name: no-name-f3ce2686-4303-4134-ba07-01df44899c17\n[I 2025-05-18 14:07:57,058] Trial 0 finished with value: 3331.6385904947915 and parameters: {'n_estimators': 110, 'max_depth': 13, 'learning_rate': 0.18884203283886744, 'subsample': 0.5854526666440897, 'colsample_bytree': 0.8937738801429804, 'gamma': 2.6946659405421385}. Best is trial 0 with value: 3331.6385904947915.\n[I 2025-05-18 14:08:07,356] Trial 1 finished with value: 3532.982177734375 and parameters: {'n_estimators': 405, 'max_depth': 24, 'learning_rate': 0.16910832412869004, 'subsample': 0.9737607498458992, 'colsample_bytree': 0.5976945917277403, 'gamma': 2.817167693022663}. Best is trial 0 with value: 3331.6385904947915.\n[I 2025-05-18 14:08:14,607] Trial 2 finished with value: 3277.841552734375 and parameters: {'n_estimators': 149, 'max_depth': 17, 'learning_rate': 0.04812097737053897, 'subsample': 0.9546726621457335, 'colsample_bytree': 0.8450595576069481, 'gamma': 2.3750014829760397}. Best is trial 2 with value: 3277.841552734375.\n[I 2025-05-18 14:08:31,440] Trial 3 finished with value: 3288.1484375 and parameters: {'n_estimators': 239, 'max_depth': 28, 'learning_rate': 0.07547917129486585, 'subsample': 0.8139622919192127, 'colsample_bytree': 0.614390583668776, 'gamma': 4.977457905957452}. Best is trial 2 with value: 3277.841552734375.\n[I 2025-05-18 14:08:33,724] Trial 4 finished with value: 3442.0733235677085 and parameters: {'n_estimators': 158, 'max_depth': 11, 'learning_rate': 0.04820580828068148, 'subsample': 0.9740363627954242, 'colsample_bytree': 0.5255147944629655, 'gamma': 3.2856067984172883}. Best is trial 2 with value: 3277.841552734375.\n[I 2025-05-18 14:08:46,455] Trial 5 finished with value: 3532.2062174479165 and parameters: {'n_estimators': 427, 'max_depth': 18, 'learning_rate': 0.26452891035565, 'subsample': 0.5071558754145372, 'colsample_bytree': 0.9173489957689693, 'gamma': 4.150003935321765}. Best is trial 2 with value: 3277.841552734375.\n[I 2025-05-18 14:09:01,942] Trial 6 finished with value: 3454.6267903645835 and parameters: {'n_estimators': 311, 'max_depth': 26, 'learning_rate': 0.1202287548017663, 'subsample': 0.8982432580618387, 'colsample_bytree': 0.561962457887061, 'gamma': 2.171604318548932}. Best is trial 2 with value: 3277.841552734375.\n[I 2025-05-18 14:09:03,372] Trial 7 finished with value: 3168.041748046875 and parameters: {'n_estimators': 259, 'max_depth': 6, 'learning_rate': 0.15479906550009093, 'subsample': 0.722784330360674, 'colsample_bytree': 0.9717808152882963, 'gamma': 0.16914348890876785}. Best is trial 7 with value: 3168.041748046875.\n[I 2025-05-18 14:09:13,799] Trial 8 finished with value: 3375.4777018229165 and parameters: {'n_estimators': 232, 'max_depth': 25, 'learning_rate': 0.24377703631670514, 'subsample': 0.7042201589884167, 'colsample_bytree': 0.7045843722953715, 'gamma': 3.672277794383117}. Best is trial 7 with value: 3168.041748046875.\n[I 2025-05-18 14:09:14,891] Trial 9 finished with value: 3246.5836588541665 and parameters: {'n_estimators': 249, 'max_depth': 5, 'learning_rate': 0.24904510550714254, 'subsample': 0.5364967258658128, 'colsample_bytree': 0.870248309348203, 'gamma': 0.8871442748221514}. Best is trial 7 with value: 3168.041748046875.\n[I 2025-05-18 14:09:16,406] Trial 10 finished with value: 3110.5743815104165 and parameters: {'n_estimators': 347, 'max_depth': 5, 'learning_rate': 0.12363016428276806, 'subsample': 0.6828890061277565, 'colsample_bytree': 0.9913288594327404, 'gamma': 0.028595940344607662}. Best is trial 10 with value: 3110.5743815104165.\n[I 2025-05-18 14:09:17,960] Trial 11 finished with value: 3080.7557779947915 and parameters: {'n_estimators': 344, 'max_depth': 5, 'learning_rate': 0.11966202039748526, 'subsample': 0.6966666409771092, 'colsample_bytree': 0.9907568324552126, 'gamma': 0.03193020721640565}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:09:22,688] Trial 12 finished with value: 3226.2923990885415 and parameters: {'n_estimators': 355, 'max_depth': 9, 'learning_rate': 0.11711488725389262, 'subsample': 0.631951758513077, 'colsample_bytree': 0.9983350666437509, 'gamma': 1.27168447617301}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:09:27,330] Trial 13 finished with value: 3197.4087727864585 and parameters: {'n_estimators': 485, 'max_depth': 8, 'learning_rate': 0.10490346463062429, 'subsample': 0.8054330102331452, 'colsample_bytree': 0.7392833809068201, 'gamma': 0.0009708831636260976}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:09:36,167] Trial 14 finished with value: 3265.5059407552085 and parameters: {'n_estimators': 350, 'max_depth': 14, 'learning_rate': 0.010974084771641884, 'subsample': 0.657436989440511, 'colsample_bytree': 0.9444260043103783, 'gamma': 1.1415074503215086}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:09:38,021] Trial 15 finished with value: 3211.10498046875 and parameters: {'n_estimators': 414, 'max_depth': 5, 'learning_rate': 0.20276262694735223, 'subsample': 0.7858874877363067, 'colsample_bytree': 0.8156010945025101, 'gamma': 0.6895297725422393}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:09:51,554] Trial 16 finished with value: 3234.952392578125 and parameters: {'n_estimators': 325, 'max_depth': 20, 'learning_rate': 0.14214931890640023, 'subsample': 0.6610397075583099, 'colsample_bytree': 0.7954031984128783, 'gamma': 1.5656159074060274}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:09:57,644] Trial 17 finished with value: 3401.1375325520835 and parameters: {'n_estimators': 496, 'max_depth': 9, 'learning_rate': 0.29514668447615683, 'subsample': 0.8632373925552281, 'colsample_bytree': 0.9436381134524987, 'gamma': 0.5407976360085849}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:10:05,745] Trial 18 finished with value: 3174.0157877604165 and parameters: {'n_estimators': 380, 'max_depth': 13, 'learning_rate': 0.09061151958846106, 'subsample': 0.5954891745420089, 'colsample_bytree': 0.710343809975879, 'gamma': 1.6534497188298685}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:10:08,286] Trial 19 finished with value: 3356.6737467447915 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.21357082994597948, 'subsample': 0.7407503163269148, 'colsample_bytree': 0.9833552054892948, 'gamma': 0.267820475624959}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:10:22,814] Trial 20 finished with value: 3189.736572265625 and parameters: {'n_estimators': 450, 'max_depth': 16, 'learning_rate': 0.04617199108711706, 'subsample': 0.6865570474083165, 'colsample_bytree': 0.7772785972725732, 'gamma': 1.5897266883914987}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:10:23,667] Trial 21 finished with value: 3140.2428385416665 and parameters: {'n_estimators': 184, 'max_depth': 5, 'learning_rate': 0.14599106624971991, 'subsample': 0.7301791371379666, 'colsample_bytree': 0.9701812221998805, 'gamma': 0.04016923066597301}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:10:24,836] Trial 22 finished with value: 3120.2205403645835 and parameters: {'n_estimators': 198, 'max_depth': 6, 'learning_rate': 0.13628764120275988, 'subsample': 0.7215881017734654, 'colsample_bytree': 0.9106993240806615, 'gamma': 0.7032606459017747}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:10:27,854] Trial 23 finished with value: 3221.032958984375 and parameters: {'n_estimators': 213, 'max_depth': 10, 'learning_rate': 0.12311335018092498, 'subsample': 0.7597429195477536, 'colsample_bytree': 0.9099463398246272, 'gamma': 0.6233266609610246}. Best is trial 11 with value: 3080.7557779947915.\n[I 2025-05-18 14:10:28,615] Trial 24 finished with value: 3057.6537272135415 and parameters: {'n_estimators': 81, 'max_depth': 7, 'learning_rate': 0.078715831835235, 'subsample': 0.5951589403340591, 'colsample_bytree': 0.8553935335757352, 'gamma': 0.9594533088744169}. Best is trial 24 with value: 3057.6537272135415.\n[I 2025-05-18 14:10:29,930] Trial 25 finished with value: 3171.862060546875 and parameters: {'n_estimators': 53, 'max_depth': 11, 'learning_rate': 0.08446318688885693, 'subsample': 0.605414568663358, 'colsample_bytree': 0.8600691427823842, 'gamma': 1.145614241815684}. Best is trial 24 with value: 3057.6537272135415.\n[I 2025-05-18 14:10:30,422] Trial 26 finished with value: 3118.3546549479165 and parameters: {'n_estimators': 64, 'max_depth': 7, 'learning_rate': 0.1763446382221982, 'subsample': 0.5599604709276149, 'colsample_bytree': 0.8324660495637575, 'gamma': 2.024470151715155}. Best is trial 24 with value: 3057.6537272135415.\n[I 2025-05-18 14:10:47,624] Trial 27 finished with value: 3235.0696614583335 and parameters: {'n_estimators': 351, 'max_depth': 21, 'learning_rate': 0.07197065204515055, 'subsample': 0.6467002339598596, 'colsample_bytree': 0.9487536672551595, 'gamma': 0.37977717359101093}. Best is trial 24 with value: 3057.6537272135415.\n[I 2025-05-18 14:10:49,614] Trial 28 finished with value: 3097.6119791666665 and parameters: {'n_estimators': 285, 'max_depth': 7, 'learning_rate': 0.09471444392323464, 'subsample': 0.6817657768958778, 'colsample_bytree': 0.8759056819860906, 'gamma': 0.9317315963616888}. Best is trial 24 with value: 3057.6537272135415.\n[I 2025-05-18 14:10:51,215] Trial 29 finished with value: 3725.931640625 and parameters: {'n_estimators': 97, 'max_depth': 12, 'learning_rate': 0.026960028516967147, 'subsample': 0.6139258759406401, 'colsample_bytree': 0.6628749649677554, 'gamma': 0.9646011976562785}. Best is trial 24 with value: 3057.6537272135415.\n[I 2025-05-18 14:10:59,064] Trial 30 finished with value: 3169.0587565104165 and parameters: {'n_estimators': 285, 'max_depth': 15, 'learning_rate': 0.09896665826089661, 'subsample': 0.5620050505228791, 'colsample_bytree': 0.8841366488845119, 'gamma': 1.736075725816671}. Best is trial 24 with value: 3057.6537272135415.\n[I 2025-05-18 14:11:01,242] Trial 31 finished with value: 3126.4650065104165 and parameters: {'n_estimators': 317, 'max_depth': 7, 'learning_rate': 0.0680504082792042, 'subsample': 0.681613021416451, 'colsample_bytree': 0.8925460900423244, 'gamma': 0.3739014472357273}. Best is trial 24 with value: 3057.6537272135415.\n[I 2025-05-18 14:11:03,903] Trial 32 finished with value: 3147.3534342447915 and parameters: {'n_estimators': 378, 'max_depth': 7, 'learning_rate': 0.1065619328138134, 'subsample': 0.6315716106994695, 'colsample_bytree': 0.9388945875286986, 'gamma': 2.6908234991554925}. Best is trial 24 with value: 3057.6537272135415.\n[I 2025-05-18 14:11:07,608] Trial 33 finished with value: 3277.1427408854165 and parameters: {'n_estimators': 274, 'max_depth': 10, 'learning_rate': 0.16713465741205957, 'subsample': 0.683331861347265, 'colsample_bytree': 0.9966072329286155, 'gamma': 1.3938586268372646}. Best is trial 24 with value: 3057.6537272135415.\n[I 2025-05-18 14:11:09,751] Trial 34 finished with value: 3032.910888671875 and parameters: {'n_estimators': 381, 'max_depth': 5, 'learning_rate': 0.06252869239380691, 'subsample': 0.7690084081861999, 'colsample_bytree': 0.8409337377788446, 'gamma': 0.9336069874026807}. Best is trial 34 with value: 3032.910888671875.\n[I 2025-05-18 14:11:13,047] Trial 35 finished with value: 3155.891357421875 and parameters: {'n_estimators': 390, 'max_depth': 8, 'learning_rate': 0.05772737980917477, 'subsample': 0.7727672665104645, 'colsample_bytree': 0.8382329436363554, 'gamma': 1.911793066576803}. Best is trial 34 with value: 3032.910888671875.\n[I 2025-05-18 14:11:16,438] Trial 36 finished with value: 3163.3846028645835 and parameters: {'n_estimators': 138, 'max_depth': 12, 'learning_rate': 0.03124181345416563, 'subsample': 0.8286701237393919, 'colsample_bytree': 0.7653391398221429, 'gamma': 0.8812646755072657}. Best is trial 34 with value: 3032.910888671875.\n[I 2025-05-18 14:11:41,945] Trial 37 finished with value: 3288.4474283854165 and parameters: {'n_estimators': 438, 'max_depth': 30, 'learning_rate': 0.07977446319685096, 'subsample': 0.9292180638154401, 'colsample_bytree': 0.8089212564431525, 'gamma': 3.040405215443998}. Best is trial 34 with value: 3032.910888671875.\n[I 2025-05-18 14:11:45,381] Trial 38 finished with value: 3190.393798828125 and parameters: {'n_estimators': 325, 'max_depth': 9, 'learning_rate': 0.0607949320144306, 'subsample': 0.8368624109449146, 'colsample_bytree': 0.8622021741277767, 'gamma': 2.3450412092952115}. Best is trial 34 with value: 3032.910888671875.\n[I 2025-05-18 14:11:47,866] Trial 39 finished with value: 3047.644775390625 and parameters: {'n_estimators': 455, 'max_depth': 6, 'learning_rate': 0.03783091966336392, 'subsample': 0.5685480281791342, 'colsample_bytree': 0.8410930211661991, 'gamma': 0.9928385438344886}. Best is trial 34 with value: 3032.910888671875.\n[I 2025-05-18 14:11:50,005] Trial 40 finished with value: 2999.4031575520835 and parameters: {'n_estimators': 401, 'max_depth': 6, 'learning_rate': 0.03991992047485554, 'subsample': 0.5117342764759609, 'colsample_bytree': 0.7318093708244227, 'gamma': 4.35126402777801}. Best is trial 40 with value: 2999.4031575520835.\n[I 2025-05-18 14:11:53,128] Trial 41 finished with value: 3000.5756022135415 and parameters: {'n_estimators': 468, 'max_depth': 6, 'learning_rate': 0.03638902988416309, 'subsample': 0.511431478578078, 'colsample_bytree': 0.7383564756305684, 'gamma': 4.796159164563539}. Best is trial 40 with value: 2999.4031575520835.\n[I 2025-05-18 14:11:55,478] Trial 42 finished with value: 2991.45703125 and parameters: {'n_estimators': 451, 'max_depth': 6, 'learning_rate': 0.03254778860582558, 'subsample': 0.5230436555378349, 'colsample_bytree': 0.7356860621113286, 'gamma': 4.747158547696816}. Best is trial 42 with value: 2991.45703125.\n[I 2025-05-18 14:11:57,923] Trial 43 finished with value: 2990.7191569010415 and parameters: {'n_estimators': 466, 'max_depth': 6, 'learning_rate': 0.03084135866822044, 'subsample': 0.502564756808311, 'colsample_bytree': 0.6563931619463742, 'gamma': 4.887027549415513}. Best is trial 43 with value: 2990.7191569010415.\n[I 2025-05-18 14:12:03,726] Trial 44 finished with value: 3033.5712076822915 and parameters: {'n_estimators': 471, 'max_depth': 10, 'learning_rate': 0.01468098928659789, 'subsample': 0.5008951225164894, 'colsample_bytree': 0.6756587019419354, 'gamma': 4.962494330375168}. Best is trial 43 with value: 2990.7191569010415.\n[I 2025-05-18 14:12:05,864] Trial 45 finished with value: 2999.9879557291665 and parameters: {'n_estimators': 417, 'max_depth': 6, 'learning_rate': 0.021606661037487623, 'subsample': 0.5288333074288567, 'colsample_bytree': 0.6186918608892495, 'gamma': 4.619632551925978}. Best is trial 43 with value: 2990.7191569010415.\n[I 2025-05-18 14:12:09,064] Trial 46 finished with value: 3025.92626953125 and parameters: {'n_estimators': 409, 'max_depth': 8, 'learning_rate': 0.023126790924218966, 'subsample': 0.5345419189292403, 'colsample_bytree': 0.6008910344234353, 'gamma': 4.6203027358091635}. Best is trial 43 with value: 2990.7191569010415.\n[I 2025-05-18 14:12:27,174] Trial 47 finished with value: 3161.40185546875 and parameters: {'n_estimators': 477, 'max_depth': 23, 'learning_rate': 0.04131664641254084, 'subsample': 0.5375997658247437, 'colsample_bytree': 0.6325464591765605, 'gamma': 4.045095333144472}. Best is trial 43 with value: 2990.7191569010415.\n[I 2025-05-18 14:12:29,501] Trial 48 finished with value: 3104.0904134114585 and parameters: {'n_estimators': 459, 'max_depth': 6, 'learning_rate': 0.05162428908533369, 'subsample': 0.5209638236554095, 'colsample_bytree': 0.5499729255374505, 'gamma': 4.424201509909468}. Best is trial 43 with value: 2990.7191569010415.\n[I 2025-05-18 14:12:34,440] Trial 49 finished with value: 3205.3310546875 and parameters: {'n_estimators': 424, 'max_depth': 9, 'learning_rate': 0.01062489230890902, 'subsample': 0.999521469464556, 'colsample_bytree': 0.732915260237857, 'gamma': 3.7623763401087156}. Best is trial 43 with value: 2990.7191569010415.\n\n\nCPU times: total: 25min 51s\nWall time: 4min 39s\n\n\n\nprint('Optuna Optimization Results: ')\n# get the best parameters\nbest_params_optuna = study.best_params\nprint('Best Parameters: ')\nprint(best_params_optuna)\n# get the best score\nbest_score_optuna = study.best_value\nprint('Best CV Score: ', best_score_optuna)\n\nOptuna Optimization Results: \nBest Parameters: \n{'n_estimators': 466, 'max_depth': 6, 'learning_rate': 0.03084135866822044, 'subsample': 0.502564756808311, 'colsample_bytree': 0.6563931619463742, 'gamma': 4.887027549415513}\nBest CV Score:  2990.7191569010415\n\n\n\n# retrain the model with the best parameters\nbest_model = XGBRegressor(\n    **best_params_optuna,\n    objective='reg:squarederror',\n    enable_categorical=True,\n    random_state=42\n)\n# fit the model\nbest_model.fit(X_train, y_train)\n# make predictions\ny_pred_optuna = best_model.predict(X_test)\n# calculate the RMSE\nrmse_optuna = root_mean_squared_error(y_test, y_pred_optuna)\nprint('Test RMSE: ', rmse_optuna)\n# calculate the R2 score\nr2_optuna = r2_score(y_test, y_pred_optuna)\nprint('Test R2: ', r2_optuna)\n\nTest RMSE:  3096.413818359375\nTest R2:  0.9673192501068115\n\n\n\n\n12.5.2.2 Visualize the Optuna tuning results\n\nimport optuna.visualization as vis\nimport plotly.io as pio\n\n# Generate figures\nfig1 = vis.plot_optimization_history(study)\n\n\n\n\n\n\n\nfig2 = vis.plot_param_importances(study)\n\n\n\n\n\n\n\nfig3 = vis.plot_slice(study)\n\n\n\n\n\n\n\n\n12.5.2.3 Enable Optuna parallel trials with n_jobs=-1\n\n%%time\n\ndef objective(trial):\n    # Define the hyperparameters to tune\n    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n    max_depth = trial.suggest_int('max_depth', 5, 30)\n    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n    gamma = trial.suggest_float('gamma', 0, 5)\n\n    # Create the model\n    model = XGBRegressor(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        learning_rate=learning_rate,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        gamma=gamma,\n        objective='reg:squarederror',\n        enable_categorical=True,\n        random_state=42\n    )\n\n    # Perform cross-validation\n    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_root_mean_squared_error')\n    \n    return -scores.mean()\n\nparallel_study = create_study(direction='minimize')\nparallel_study.optimize(objective, n_trials=50, n_jobs=-1)\n\n[I 2025-05-18 14:34:24,598] A new study created in memory with name: no-name-8c4110c7-8504-4b30-8239-4ca7a887a77e\n[I 2025-05-18 14:34:44,688] Trial 3 finished with value: 3386.8870442708335 and parameters: {'n_estimators': 186, 'max_depth': 7, 'learning_rate': 0.016917121150508863, 'subsample': 0.6205391387487309, 'colsample_bytree': 0.9226243614668972, 'gamma': 0.7198073118252574}. Best is trial 3 with value: 3386.8870442708335.\n[I 2025-05-18 14:34:51,587] Trial 15 finished with value: 3225.1217447916665 and parameters: {'n_estimators': 336, 'max_depth': 5, 'learning_rate': 0.22071165224743958, 'subsample': 0.9479603262439835, 'colsample_bytree': 0.633493959052336, 'gamma': 4.219844381569382}. Best is trial 15 with value: 3225.1217447916665.\n[I 2025-05-18 14:34:56,685] Trial 0 finished with value: 3780.4537760416665 and parameters: {'n_estimators': 75, 'max_depth': 28, 'learning_rate': 0.08734595383748141, 'subsample': 0.643816201392093, 'colsample_bytree': 0.5096005859401787, 'gamma': 2.346236402110034}. Best is trial 15 with value: 3225.1217447916665.\n[I 2025-05-18 14:35:00,827] Trial 1 finished with value: 3492.1652018229165 and parameters: {'n_estimators': 89, 'max_depth': 22, 'learning_rate': 0.26936321457491236, 'subsample': 0.9260739983960038, 'colsample_bytree': 0.7892145514084559, 'gamma': 0.03440365361497333}. Best is trial 15 with value: 3225.1217447916665.\n[I 2025-05-18 14:35:04,073] Trial 13 finished with value: 3158.33447265625 and parameters: {'n_estimators': 283, 'max_depth': 9, 'learning_rate': 0.0696919546852828, 'subsample': 0.5001580826879467, 'colsample_bytree': 0.8923178791715032, 'gamma': 4.714268285930873}. Best is trial 13 with value: 3158.33447265625.\n[I 2025-05-18 14:35:04,526] Trial 17 finished with value: 3402.6753743489585 and parameters: {'n_estimators': 61, 'max_depth': 13, 'learning_rate': 0.22097576760857726, 'subsample': 0.5157400674693156, 'colsample_bytree': 0.9153999202038743, 'gamma': 3.8810641950136353}. Best is trial 13 with value: 3158.33447265625.\n[I 2025-05-18 14:35:09,821] Trial 10 finished with value: 3173.780517578125 and parameters: {'n_estimators': 115, 'max_depth': 20, 'learning_rate': 0.05718427703256109, 'subsample': 0.7951084114071031, 'colsample_bytree': 0.6691682768529876, 'gamma': 0.9524397379855204}. Best is trial 13 with value: 3158.33447265625.\n[I 2025-05-18 14:35:12,310] Trial 20 finished with value: 3132.5121256510415 and parameters: {'n_estimators': 67, 'max_depth': 7, 'learning_rate': 0.18097723795904094, 'subsample': 0.6565256472443639, 'colsample_bytree': 0.8492487356738951, 'gamma': 3.094448539748332}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:35:27,118] Trial 5 finished with value: 3247.8447265625 and parameters: {'n_estimators': 185, 'max_depth': 18, 'learning_rate': 0.12450637517625557, 'subsample': 0.6530395458833994, 'colsample_bytree': 0.9123640605975669, 'gamma': 4.153197251161178}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:35:32,054] Trial 2 finished with value: 3298.5411783854165 and parameters: {'n_estimators': 198, 'max_depth': 19, 'learning_rate': 0.1625956401464969, 'subsample': 0.6454388243734961, 'colsample_bytree': 0.6773968413189418, 'gamma': 0.6809545266278161}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:35:38,519] Trial 19 finished with value: 3206.009765625 and parameters: {'n_estimators': 389, 'max_depth': 6, 'learning_rate': 0.16121536164972963, 'subsample': 0.5021218552699114, 'colsample_bytree': 0.5687043089885795, 'gamma': 1.9151520851034594}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:35:43,501] Trial 8 finished with value: 3190.1316731770835 and parameters: {'n_estimators': 185, 'max_depth': 24, 'learning_rate': 0.04861252706056661, 'subsample': 0.7514014541546887, 'colsample_bytree': 0.5947087728251452, 'gamma': 2.347498071325193}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:35:52,862] Trial 24 finished with value: 3393.3465169270835 and parameters: {'n_estimators': 268, 'max_depth': 6, 'learning_rate': 0.26658647968026516, 'subsample': 0.9687611646582986, 'colsample_bytree': 0.6473978497637801, 'gamma': 2.112593418725748}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:35:54,416] Trial 9 finished with value: 3281.0540364583335 and parameters: {'n_estimators': 267, 'max_depth': 18, 'learning_rate': 0.09626803123217077, 'subsample': 0.911374738293637, 'colsample_bytree': 0.8773073634628092, 'gamma': 3.481941940869614}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:35:57,296] Trial 4 finished with value: 3236.0377604166665 and parameters: {'n_estimators': 355, 'max_depth': 15, 'learning_rate': 0.10929163313929664, 'subsample': 0.7658160498873094, 'colsample_bytree': 0.7163396553124712, 'gamma': 2.652745236329293}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:35:58,600] Trial 16 finished with value: 3217.8741861979165 and parameters: {'n_estimators': 350, 'max_depth': 12, 'learning_rate': 0.02298579332922481, 'subsample': 0.9869225199063424, 'colsample_bytree': 0.7703334112529645, 'gamma': 0.9673314117244886}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:02,363] Trial 12 finished with value: 3801.4449869791665 and parameters: {'n_estimators': 323, 'max_depth': 18, 'learning_rate': 0.2970802052171264, 'subsample': 0.6557456264021807, 'colsample_bytree': 0.6426755505446563, 'gamma': 3.296566349997323}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:04,137] Trial 23 finished with value: 3145.9025065104165 and parameters: {'n_estimators': 149, 'max_depth': 20, 'learning_rate': 0.11627192901824128, 'subsample': 0.5531936628837026, 'colsample_bytree': 0.8862514403683301, 'gamma': 2.8135869506363886}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:04,702] Trial 21 finished with value: 3610.6133626302085 and parameters: {'n_estimators': 160, 'max_depth': 22, 'learning_rate': 0.2999769958923947, 'subsample': 0.5067125540912617, 'colsample_bytree': 0.8981157580104169, 'gamma': 4.806320208954713}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:06,371] Trial 14 finished with value: 3340.8546549479165 and parameters: {'n_estimators': 245, 'max_depth': 25, 'learning_rate': 0.17652467395345062, 'subsample': 0.611813588138444, 'colsample_bytree': 0.7342047981839466, 'gamma': 2.307485804181127}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:18,414] Trial 6 finished with value: 3459.354248046875 and parameters: {'n_estimators': 292, 'max_depth': 27, 'learning_rate': 0.14966590769182178, 'subsample': 0.8644768034280883, 'colsample_bytree': 0.585581150985163, 'gamma': 4.2857336056873905}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:19,888] Trial 18 finished with value: 3383.8411458333335 and parameters: {'n_estimators': 198, 'max_depth': 26, 'learning_rate': 0.21556692279966222, 'subsample': 0.5771557052612845, 'colsample_bytree': 0.9826376798012539, 'gamma': 4.5858180068589025}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:32,659] Trial 36 finished with value: 3416.326171875 and parameters: {'n_estimators': 137, 'max_depth': 9, 'learning_rate': 0.19558438716354623, 'subsample': 0.5673630855731072, 'colsample_bytree': 0.9859366413577002, 'gamma': 3.023613531245067}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:36,836] Trial 7 finished with value: 3498.0222981770835 and parameters: {'n_estimators': 410, 'max_depth': 23, 'learning_rate': 0.20412014730717506, 'subsample': 0.7787220663675116, 'colsample_bytree': 0.6104047744776355, 'gamma': 3.7782931152361905}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:45,340] Trial 22 finished with value: 3173.2679036458335 and parameters: {'n_estimators': 342, 'max_depth': 18, 'learning_rate': 0.0918444887804519, 'subsample': 0.6358151816674005, 'colsample_bytree': 0.8178206059616683, 'gamma': 2.3187025698274963}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:53,938] Trial 25 finished with value: 3260.6758626302085 and parameters: {'n_estimators': 474, 'max_depth': 13, 'learning_rate': 0.17976496530280672, 'subsample': 0.7974254896628823, 'colsample_bytree': 0.8171566934590944, 'gamma': 2.782580731869066}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:56,530] Trial 26 finished with value: 3207.96533203125 and parameters: {'n_estimators': 492, 'max_depth': 12, 'learning_rate': 0.11220378588280396, 'subsample': 0.7741909801743497, 'colsample_bytree': 0.7986588796025045, 'gamma': 3.1947401986428576}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:56,884] Trial 31 finished with value: 3403.3111165364585 and parameters: {'n_estimators': 482, 'max_depth': 10, 'learning_rate': 0.19587655584318253, 'subsample': 0.5695806262499338, 'colsample_bytree': 0.9939250598824909, 'gamma': 4.678107426427386}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:36:59,028] Trial 32 finished with value: 3394.078369140625 and parameters: {'n_estimators': 475, 'max_depth': 10, 'learning_rate': 0.18692130864394926, 'subsample': 0.5639172117854069, 'colsample_bytree': 0.9846996142831652, 'gamma': 4.703621516978366}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:00,622] Trial 27 finished with value: 3218.7230631510415 and parameters: {'n_estimators': 491, 'max_depth': 12, 'learning_rate': 0.11995416383753857, 'subsample': 0.5466551591729691, 'colsample_bytree': 0.8084353830772836, 'gamma': 4.925162229632911}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:05,596] Trial 30 finished with value: 3401.9541829427085 and parameters: {'n_estimators': 500, 'max_depth': 11, 'learning_rate': 0.2152643048780489, 'subsample': 0.5757354436323991, 'colsample_bytree': 0.811733236112631, 'gamma': 4.912356352479171}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:07,743] Trial 11 finished with value: 3266.311279296875 and parameters: {'n_estimators': 455, 'max_depth': 29, 'learning_rate': 0.13606170670586942, 'subsample': 0.6994896811489387, 'colsample_bytree': 0.7984754896325321, 'gamma': 4.09626197911706}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:07,953] Trial 28 finished with value: 3182.3749186197915 and parameters: {'n_estimators': 488, 'max_depth': 12, 'learning_rate': 0.12058008708744347, 'subsample': 0.558159655388937, 'colsample_bytree': 0.803756895143798, 'gamma': 3.3020110039626416}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:08,665] Trial 29 finished with value: 3314.0919596354165 and parameters: {'n_estimators': 488, 'max_depth': 12, 'learning_rate': 0.19983958431862286, 'subsample': 0.5547949555803708, 'colsample_bytree': 0.8342550435077614, 'gamma': 4.941482674825019}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:12,068] Trial 37 finished with value: 3134.2323404947915 and parameters: {'n_estimators': 483, 'max_depth': 9, 'learning_rate': 0.07480607389730205, 'subsample': 0.5676604688073226, 'colsample_bytree': 0.8150289462186665, 'gamma': 2.8866777653704037}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:21,475] Trial 44 finished with value: 3191.2845052083335 and parameters: {'n_estimators': 93, 'max_depth': 15, 'learning_rate': 0.06527491497109726, 'subsample': 0.711964328195734, 'colsample_bytree': 0.859090805004589, 'gamma': 1.639238662369149}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:22,158] Trial 45 finished with value: 3185.5326334635415 and parameters: {'n_estimators': 91, 'max_depth': 15, 'learning_rate': 0.06735780487715705, 'subsample': 0.7015638902916604, 'colsample_bytree': 0.8561282967129621, 'gamma': 1.7180282992248004}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:25,746] Trial 41 finished with value: 3219.3627115885415 and parameters: {'n_estimators': 236, 'max_depth': 10, 'learning_rate': 0.12794602478115893, 'subsample': 0.6954209090620064, 'colsample_bytree': 0.8670840861317897, 'gamma': 1.5273956878386659}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:27,160] Trial 49 finished with value: 3200.2061360677085 and parameters: {'n_estimators': 80, 'max_depth': 16, 'learning_rate': 0.07822655273812698, 'subsample': 0.7097404244580205, 'colsample_bytree': 0.855803024035887, 'gamma': 1.7360340110549568}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:28,636] Trial 47 finished with value: 3183.0796712239585 and parameters: {'n_estimators': 101, 'max_depth': 15, 'learning_rate': 0.07140184583521393, 'subsample': 0.710670554031622, 'colsample_bytree': 0.8547486119581857, 'gamma': 1.7173865194695117}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:29,253] Trial 38 finished with value: 3208.407958984375 and parameters: {'n_estimators': 465, 'max_depth': 10, 'learning_rate': 0.06658819756064548, 'subsample': 0.6995360789704701, 'colsample_bytree': 0.8341518409466513, 'gamma': 2.87278237478264}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:29,453] Trial 35 finished with value: 3316.11328125 and parameters: {'n_estimators': 443, 'max_depth': 15, 'learning_rate': 0.19403052858659814, 'subsample': 0.7068750385474248, 'colsample_bytree': 0.9925412520466177, 'gamma': 2.9755182321525635}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:31,653] Trial 48 finished with value: 3209.8841959635415 and parameters: {'n_estimators': 87, 'max_depth': 20, 'learning_rate': 0.07578210673049532, 'subsample': 0.7003441215271053, 'colsample_bytree': 0.8546647246781772, 'gamma': 1.7208102871319328}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:32,887] Trial 39 finished with value: 3200.1593424479165 and parameters: {'n_estimators': 492, 'max_depth': 10, 'learning_rate': 0.065333145587516, 'subsample': 0.6966584422268385, 'colsample_bytree': 0.8172215987900011, 'gamma': 1.6552485151252307}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:35,803] Trial 40 finished with value: 3249.5609537760415 and parameters: {'n_estimators': 475, 'max_depth': 10, 'learning_rate': 0.13465422545350136, 'subsample': 0.6978833200280267, 'colsample_bytree': 0.8370747967719179, 'gamma': 4.968542636826679}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:37,436] Trial 43 finished with value: 3238.1642252604165 and parameters: {'n_estimators': 234, 'max_depth': 15, 'learning_rate': 0.1275208269474538, 'subsample': 0.7058773643147542, 'colsample_bytree': 0.8551947162378626, 'gamma': 1.6458760605079936}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:40,436] Trial 46 finished with value: 3206.9140625 and parameters: {'n_estimators': 233, 'max_depth': 15, 'learning_rate': 0.06896196005667433, 'subsample': 0.6997340322682682, 'colsample_bytree': 0.8647504256146414, 'gamma': 1.4300308921007199}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:51,343] Trial 42 finished with value: 3237.631591796875 and parameters: {'n_estimators': 228, 'max_depth': 30, 'learning_rate': 0.13632890468699124, 'subsample': 0.7024617677120901, 'colsample_bytree': 0.8546145920857883, 'gamma': 1.7651829526887861}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:51,445] Trial 34 finished with value: 3386.8898111979165 and parameters: {'n_estimators': 492, 'max_depth': 30, 'learning_rate': 0.19771915593325928, 'subsample': 0.5711165410120198, 'colsample_bytree': 0.9985500871271142, 'gamma': 3.052090619977873}. Best is trial 20 with value: 3132.5121256510415.\n[I 2025-05-18 14:37:51,670] Trial 33 finished with value: 3355.239990234375 and parameters: {'n_estimators': 495, 'max_depth': 30, 'learning_rate': 0.194602408723096, 'subsample': 0.5698582471685457, 'colsample_bytree': 0.9947157446155714, 'gamma': 3.1410187252506825}. Best is trial 20 with value: 3132.5121256510415.\n\n\nCPU times: total: 26min 6s\nWall time: 3min 27s\n\n\nThe wall time was shortened from 4 minutes 39 seconds to 3 minutes 27 seconds, a reduction of 1 minute 12 seconds for this small dataset.\n\n# retrain the model with the best parameters\nbest_model = XGBRegressor(\n    **best_params_optuna,\n    objective='reg:squarederror',\n    enable_categorical=True,\n    random_state=42\n)\n# fit the model\nbest_model.fit(X_train, y_train)\n# make predictions\ny_pred_optuna = best_model.predict(X_test)\n# calculate the RMSE\nrmse_optuna = root_mean_squared_error(y_test, y_pred_optuna)\nprint('Test RMSE: ', rmse_optuna)\n# calculate the R2 score\nr2_optuna = r2_score(y_test, y_pred_optuna)\nprint('Test R2: ', r2_optuna)\n\nTest RMSE:  3096.413818359375\nTest R2:  0.9673192501068115\n\n\nThe result is the same\nOptuna supports multiple optimization strategies, with TPE as the default. You can customize its optimization behavior by selecting different samplers. The default settings are robust and adaptive, so for most practical use cases, tuning Optuna‚Äôs internal configuration is not necessary.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Smarter Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "smarter_hyper_tuning.html#automl",
    "href": "smarter_hyper_tuning.html#automl",
    "title": "12¬† Smarter Hyperparameter Tuning",
    "section": "12.6 AutoML",
    "text": "12.6 AutoML\nAutomated Machine Learning (AutoML) refers to the process of automating the end-to-end tasks of applying machine learning to real-world problems. This includes model selection, hyperparameter tuning, data preprocessing, and even ensemble building‚Äîreducing the need for manual intervention and expert knowledge.\nAutoML tools are especially useful when you want to: - Quickly benchmark multiple models, - Optimize hyperparameters efficiently, - Reduce the time and effort needed for model development.\n\n12.6.1 FLAML: A Lightweight AutoML Library\nFLAML (Fast and Lightweight AutoML) is an open-source AutoML library developed by Microsoft Research. It is designed for fast, cost-effective, and resource-efficient model selection and hyperparameter optimization.\nUnlike many other AutoML tools, FLAML:\n\nAvoids expensive Bayesian optimization,\nUses greedy search strategies with built-in heuristics,\nSupports regression, classification, and time series tasks,\nIs easy to integrate into existing machine learning pipelines.\n\nFLAML is particularly well-suited for scenarios where computational resources are limited or quick iteration is needed.\nJust like Optuna, FLAML needs to be installed separately, as it‚Äôs not part of the standard Python library. You can install it using the following command:\npip install flaml\n\npip install flaml\n\nCollecting flaml\n  Downloading FLAML-2.3.4-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: NumPy&gt;=1.17 in c:\\users\\lsi8012\\appdata\\local\\anaconda3\\lib\\site-packages (from flaml) (1.26.4)\nDownloading FLAML-2.3.4-py3-none-any.whl (314 kB)\n   ---------------------------------------- 0.0/314.2 kB ? eta -:--:--\n   - -------------------------------------- 10.2/314.2 kB ? eta -:--:--\n   -------------------- ------------------- 163.8/314.2 kB 2.5 MB/s eta 0:00:01\n   ---------------------------------------  307.2/314.2 kB 3.8 MB/s eta 0:00:01\n   ---------------------------------------- 314.2/314.2 kB 2.8 MB/s eta 0:00:00\nInstalling collected packages: flaml\nSuccessfully installed flaml-2.3.4\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n# setup flaml\nfrom flaml import AutoML\n\nsettings = {\n    \"time_budget\": 240,  # in seconds\n    \"metric\": 'rmse',\n    \"task\": 'regression',\n    \"log_file_name\": 'flaml.log',\n}\n\nautoml = AutoML()\nautoml.fit(X_train, y_train, **settings)\n# make predictions\ny_pred_flaml = automl.predict(X_test)\n# calculate the RMSE\nrmse_flaml = root_mean_squared_error(y_test, y_pred_flaml)\nprint('Test RMSE: ', rmse_flaml)\n# calculate the R2 score\nr2_flaml = r2_score(y_test, y_pred_flaml)\nprint('Test R2: ', r2_flaml)\n# get the best model\nbest_model_flaml = automl.model.estimator\n# get the best parameters\nbest_params_flaml = automl.best_config\nprint('Best Parameters: ')\nprint(best_params_flaml)\n# get the best score\nbest_score_flaml = automl.best_loss\nprint('Best CV Score: ', best_score_flaml)\n# get the best estimator\n\n[flaml.automl.logger: 05-18 15:00:23] {1728} INFO - task = regression\n[flaml.automl.logger: 05-18 15:00:23] {1739} INFO - Evaluation method: cv\n[flaml.automl.logger: 05-18 15:00:23] {1838} INFO - Minimizing error metric: rmse\n[flaml.automl.logger: 05-18 15:00:23] {1955} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost']\n[flaml.automl.logger: 05-18 15:00:23] {2258} INFO - iteration 0, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:23] {2393} INFO - Estimated sufficient time budget=1953s. Estimated necessary time budget=17s.\n[flaml.automl.logger: 05-18 15:00:23] {2442} INFO -  at 0.2s,   estimator lgbm's best error=12968.4586, best estimator lgbm's best error=12968.4586\n[flaml.automl.logger: 05-18 15:00:23] {2258} INFO - iteration 1, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:23] {2442} INFO -  at 0.4s,   estimator lgbm's best error=12968.4586, best estimator lgbm's best error=12968.4586\n[flaml.automl.logger: 05-18 15:00:23] {2258} INFO - iteration 2, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:24] {2442} INFO -  at 0.6s,   estimator lgbm's best error=9488.0927,  best estimator lgbm's best error=9488.0927\n[flaml.automl.logger: 05-18 15:00:24] {2258} INFO - iteration 3, current learner sgd\n[flaml.automl.logger: 05-18 15:00:25] {2442} INFO -  at 2.0s,   estimator sgd's best error=26664.1568,  best estimator lgbm's best error=9488.0927\n[flaml.automl.logger: 05-18 15:00:25] {2258} INFO - iteration 4, current learner xgboost\n[flaml.automl.logger: 05-18 15:00:25] {2442} INFO -  at 2.2s,   estimator xgboost's best error=13194.6180,  best estimator lgbm's best error=9488.0927\n[flaml.automl.logger: 05-18 15:00:25] {2258} INFO - iteration 5, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:26] {2442} INFO -  at 2.6s,   estimator lgbm's best error=5322.7602,  best estimator lgbm's best error=5322.7602\n[flaml.automl.logger: 05-18 15:00:26] {2258} INFO - iteration 6, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:26] {2442} INFO -  at 2.8s,   estimator lgbm's best error=5322.7602,  best estimator lgbm's best error=5322.7602\n[flaml.automl.logger: 05-18 15:00:26] {2258} INFO - iteration 7, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:26] {2442} INFO -  at 3.2s,   estimator lgbm's best error=4969.1631,  best estimator lgbm's best error=4969.1631\n[flaml.automl.logger: 05-18 15:00:26] {2258} INFO - iteration 8, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:27] {2442} INFO -  at 3.6s,   estimator lgbm's best error=4969.1631,  best estimator lgbm's best error=4969.1631\n[flaml.automl.logger: 05-18 15:00:27] {2258} INFO - iteration 9, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:27] {2442} INFO -  at 3.8s,   estimator lgbm's best error=4969.1631,  best estimator lgbm's best error=4969.1631\n[flaml.automl.logger: 05-18 15:00:27] {2258} INFO - iteration 10, current learner xgboost\n[flaml.automl.logger: 05-18 15:00:27] {2442} INFO -  at 4.0s,   estimator xgboost's best error=13194.6180,  best estimator lgbm's best error=4969.1631\n[flaml.automl.logger: 05-18 15:00:27] {2258} INFO - iteration 11, current learner extra_tree\n[flaml.automl.logger: 05-18 15:00:27] {2442} INFO -  at 4.4s,   estimator extra_tree's best error=11698.9242,   best estimator lgbm's best error=4969.1631\n[flaml.automl.logger: 05-18 15:00:27] {2258} INFO - iteration 12, current learner rf\n[flaml.automl.logger: 05-18 15:00:28] {2442} INFO -  at 4.8s,   estimator rf's best error=10181.3899,   best estimator lgbm's best error=4969.1631\n[flaml.automl.logger: 05-18 15:00:28] {2258} INFO - iteration 13, current learner rf\n[flaml.automl.logger: 05-18 15:00:28] {2442} INFO -  at 5.1s,   estimator rf's best error=7248.2128,    best estimator lgbm's best error=4969.1631\n[flaml.automl.logger: 05-18 15:00:28] {2258} INFO - iteration 14, current learner xgboost\n[flaml.automl.logger: 05-18 15:00:28] {2442} INFO -  at 5.3s,   estimator xgboost's best error=10198.0304,  best estimator lgbm's best error=4969.1631\n[flaml.automl.logger: 05-18 15:00:28] {2258} INFO - iteration 15, current learner rf\n[flaml.automl.logger: 05-18 15:00:29] {2442} INFO -  at 5.6s,   estimator rf's best error=7248.2128,    best estimator lgbm's best error=4969.1631\n[flaml.automl.logger: 05-18 15:00:29] {2258} INFO - iteration 16, current learner extra_tree\n[flaml.automl.logger: 05-18 15:00:29] {2442} INFO -  at 6.0s,   estimator extra_tree's best error=8654.0612,    best estimator lgbm's best error=4969.1631\n[flaml.automl.logger: 05-18 15:00:29] {2258} INFO - iteration 17, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:30] {2442} INFO -  at 7.0s,   estimator lgbm's best error=4307.6538,  best estimator lgbm's best error=4307.6538\n[flaml.automl.logger: 05-18 15:00:30] {2258} INFO - iteration 18, current learner sgd\n[flaml.automl.logger: 05-18 15:00:31] {2442} INFO -  at 8.3s,   estimator sgd's best error=23768.6052,  best estimator lgbm's best error=4307.6538\n[flaml.automl.logger: 05-18 15:00:31] {2258} INFO - iteration 19, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:32] {2442} INFO -  at 8.7s,   estimator lgbm's best error=4307.6538,  best estimator lgbm's best error=4307.6538\n[flaml.automl.logger: 05-18 15:00:32] {2258} INFO - iteration 20, current learner xgboost\n[flaml.automl.logger: 05-18 15:00:32] {2442} INFO -  at 8.9s,   estimator xgboost's best error=7947.7516,   best estimator lgbm's best error=4307.6538\n[flaml.automl.logger: 05-18 15:00:32] {2258} INFO - iteration 21, current learner xgboost\n[flaml.automl.logger: 05-18 15:00:32] {2442} INFO -  at 9.2s,   estimator xgboost's best error=7947.7516,   best estimator lgbm's best error=4307.6538\n[flaml.automl.logger: 05-18 15:00:32] {2258} INFO - iteration 22, current learner xgboost\n[flaml.automl.logger: 05-18 15:00:33] {2442} INFO -  at 9.6s,   estimator xgboost's best error=7947.7516,   best estimator lgbm's best error=4307.6538\n[flaml.automl.logger: 05-18 15:00:33] {2258} INFO - iteration 23, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:37] {2442} INFO -  at 14.0s,  estimator lgbm's best error=3327.8433,  best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:37] {2258} INFO - iteration 24, current learner rf\n[flaml.automl.logger: 05-18 15:00:38] {2442} INFO -  at 14.7s,  estimator rf's best error=5667.3382,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:38] {2258} INFO - iteration 25, current learner extra_tree\n[flaml.automl.logger: 05-18 15:00:38] {2442} INFO -  at 15.2s,  estimator extra_tree's best error=8654.0612,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:38] {2258} INFO - iteration 26, current learner sgd\n[flaml.automl.logger: 05-18 15:00:40] {2442} INFO -  at 17.2s,  estimator sgd's best error=19930.3228,  best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:40] {2258} INFO - iteration 27, current learner rf\n[flaml.automl.logger: 05-18 15:00:41] {2442} INFO -  at 17.7s,  estimator rf's best error=4587.0558,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:41] {2258} INFO - iteration 28, current learner xgboost\n[flaml.automl.logger: 05-18 15:00:41] {2442} INFO -  at 18.1s,  estimator xgboost's best error=6696.1709,   best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:41] {2258} INFO - iteration 29, current learner extra_tree\n[flaml.automl.logger: 05-18 15:00:42] {2442} INFO -  at 18.5s,  estimator extra_tree's best error=6623.2201,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:42] {2258} INFO - iteration 30, current learner extra_tree\n[flaml.automl.logger: 05-18 15:00:42] {2442} INFO -  at 19.0s,  estimator extra_tree's best error=5157.7198,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:42] {2258} INFO - iteration 31, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:47] {2442} INFO -  at 24.1s,  estimator lgbm's best error=3327.8433,  best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:47] {2258} INFO - iteration 32, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:50] {2442} INFO -  at 27.2s,  estimator lgbm's best error=3327.8433,  best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:50] {2258} INFO - iteration 33, current learner rf\n[flaml.automl.logger: 05-18 15:00:51] {2442} INFO -  at 28.0s,  estimator rf's best error=4587.0558,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:51] {2258} INFO - iteration 34, current learner extra_tree\n[flaml.automl.logger: 05-18 15:00:52] {2442} INFO -  at 28.5s,  estimator extra_tree's best error=5157.7198,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:52] {2258} INFO - iteration 35, current learner lgbm\n[flaml.automl.logger: 05-18 15:00:58] {2442} INFO -  at 35.1s,  estimator lgbm's best error=3327.8433,  best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:58] {2258} INFO - iteration 36, current learner xgboost\n[flaml.automl.logger: 05-18 15:00:59] {2442} INFO -  at 35.9s,  estimator xgboost's best error=5984.3581,   best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:00:59] {2258} INFO - iteration 37, current learner sgd\n[flaml.automl.logger: 05-18 15:01:00] {2442} INFO -  at 37.4s,  estimator sgd's best error=19930.3228,  best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:01:00] {2258} INFO - iteration 38, current learner xgboost\n[flaml.automl.logger: 05-18 15:01:01] {2442} INFO -  at 37.8s,  estimator xgboost's best error=5984.3581,   best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:01:01] {2258} INFO - iteration 39, current learner extra_tree\n[flaml.automl.logger: 05-18 15:01:01] {2442} INFO -  at 38.4s,  estimator extra_tree's best error=4253.3061,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:01:01] {2258} INFO - iteration 40, current learner rf\n[flaml.automl.logger: 05-18 15:01:02] {2442} INFO -  at 39.1s,  estimator rf's best error=4208.3611,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:01:02] {2258} INFO - iteration 41, current learner extra_tree\n[flaml.automl.logger: 05-18 15:01:03] {2442} INFO -  at 39.7s,  estimator extra_tree's best error=4253.3061,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:01:03] {2258} INFO - iteration 42, current learner catboost\n[flaml.automl.logger: 05-18 15:02:38] {2442} INFO -  at 135.1s, estimator catboost's best error=3340.2819,  best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:02:38] {2258} INFO - iteration 43, current learner extra_tree\n[flaml.automl.logger: 05-18 15:02:39] {2442} INFO -  at 135.6s, estimator extra_tree's best error=3851.9675,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:02:39] {2258} INFO - iteration 44, current learner lgbm\n[flaml.automl.logger: 05-18 15:02:43] {2442} INFO -  at 140.3s, estimator lgbm's best error=3327.8433,  best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:02:43] {2258} INFO - iteration 45, current learner extra_tree\n[flaml.automl.logger: 05-18 15:02:44] {2442} INFO -  at 141.0s, estimator extra_tree's best error=3851.9675,    best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:02:44] {2258} INFO - iteration 46, current learner catboost\n[flaml.automl.logger: 05-18 15:04:32] {2442} INFO -  at 248.8s, estimator catboost's best error=3340.2819,  best estimator lgbm's best error=3327.8433\n[flaml.automl.logger: 05-18 15:04:33] {2685} INFO - retrain lgbm for 0.9s\n[flaml.automl.logger: 05-18 15:04:33] {2688} INFO - retrained model: LGBMRegressor(colsample_bytree=0.6649148062238498,\n              learning_rate=0.17402065726724145, max_bin=255,\n              min_child_samples=3, n_estimators=93, n_jobs=-1, num_leaves=15,\n              reg_alpha=0.0009765625, reg_lambda=0.006761362450996489,\n              verbose=-1)\n[flaml.automl.logger: 05-18 15:04:33] {1985} INFO - fit succeeded\n[flaml.automl.logger: 05-18 15:04:33] {1986} INFO - Time taken to find the best model: 14.040556192398071\nTest RMSE:  3459.7530968991273\nTest R2:  0.9591996557531124\nBest Parameters: \n{'n_estimators': 93, 'num_leaves': 15, 'min_child_samples': 3, 'learning_rate': 0.17402065726724145, 'log_max_bin': 8, 'colsample_bytree': 0.6649148062238498, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.006761362450996489}\nBest CV Score:  3327.8432720196106\n\n\nFLAML is well-suited for quick AutoML tasks on small- to medium-sized datasets. Compared to tools like Optuna (which offers flexible search and pruning) and BayesSearchCV (which integrates tightly with scikit-learn), FLAML prioritizes speed, efficiency, and minimal configuration.\nIn contrast, there are also fully-managed cloud AutoML services, which handle the entire pipeline from data preprocessing to deployment. These services are convenient but come with usage costs.\nFully-Managed Cloud AutoML Services\n\n\n\nPlatform\nProduct Name\n\n\n\n\nGCP\nVertex AI AutoML\n\n\nAWS\nSageMaker Autopilot\n\n\n\n\n‚ö†Ô∏è Note: These cloud-based AutoML platforms are not free ‚Äî you pay for compute, storage, and usage time.\n\nCV-based hyperparameter tuning often leads to better model performance than generic cloud AutoML ‚Äî especially when you know what you‚Äôre doing.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Smarter Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "smarter_hyper_tuning.html#resources",
    "href": "smarter_hyper_tuning.html#resources",
    "title": "12¬† Smarter Hyperparameter Tuning",
    "section": "12.7 Resources",
    "text": "12.7 Resources\n\nscikit-optimize Official Website\nscikit-optimize GitHub Repository\nOptuna Paper\nOptuna GitHub Repository\nFLAML GitHub Repository\nFLAML Official Documentation\nAutoML Benchmark Paper",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Smarter Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html",
    "href": "LightGBM_CatBoost.html",
    "title": "11¬† LightGBM and CatBoost",
    "section": "",
    "text": "11.1 What They Share with XGBoost\nGradient boosting is one of the most powerful techniques for structured/tabular data, often serving as the go-to choice for tabular data tasks in machine learning competitions.\nIn the previous chapter, we explored XGBoost in detail‚Äîcovering its optimization objective, regularization techniques, split finding algorithms, and its role as a cornerstone in modern tabular modeling.\nWhile XGBoost is highly effective, other libraries have introduced innovations to address challenges like scalability and categorical feature handling. In this chapter, we focus on two advanced gradient boosting libraries and their key innovations:\nWhile LightGBM and CatBoost introduce unique innovations, they build on the same foundational principles as XGBoost:",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#what-they-share-with-xgboost",
    "href": "LightGBM_CatBoost.html#what-they-share-with-xgboost",
    "title": "11¬† LightGBM and CatBoost",
    "section": "",
    "text": "They use a similar objective function structure (loss plus regularization) to balance model fit and complexity.\nThey apply a second-order Taylor approximation for efficient optimization of the loss function.\nThey support histogram-based split-finding algorithms to speed up training, with LightGBM particularly optimized for this approach.\nThey support parallel tree building, significantly accelerating training compared to traditional gradient boosting.\nThey provide native support for categorical encoding, reducing the need for preprocessing (e.g., one-hot encoding), with XGBoost introducing this feature starting in version 1.5.0.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#lightgbm",
    "href": "LightGBM_CatBoost.html#lightgbm",
    "title": "11¬† LightGBM and CatBoost",
    "section": "11.2 LightGBM",
    "text": "11.2 LightGBM\n\n11.2.1 What is LightGBM?\nLightGBM (Light Gradient Boosting Machine) is a high-performance gradient boosting framework developed by Microsoft in 2017. Designed for speed and scalability, it is typically faster than XGBoost due to its optimized algorithms, with accuracy that is generally comparable but may require tuning. LightGBM excels in:\n\nHandling large-scale datasets with many rows and features\nAchieving high speed and memory efficiency through innovations like Gradient-based One-Side Sampling (GOSS), Exclusive Feature Bundling (EFB), and histogram-based splitting\n\nLike XGBoost and CatBoost, it supports native categorical encoding and parallel tree building, enhancing its efficiency for tabular data tasks. See the LightGBM paper for details on its algorithmic innovations and performance benchmarks.\n\n\n11.2.2 What Makes LightGBM Lighting Fast?\nLightGBM often outperforms XGBoost in training speed and memory efficiency, thanks to several key innovations:\n\n11.2.2.1 Leaf-Wise Tree Growth\n\nLightGBM splits the leaf with the largest potential loss reduction, unlike XGBoost‚Äôs level-wise approach.\nThis leads to lower loss per tree, making learning more efficient ‚Äî though it may overfit without proper regularization.\nMain controls:\n\nnum_leaves: primary control for tree complexity\nmax_depth: optional constraint to prevent overfitting\n\n\n\n\n11.2.2.2 GOSS (Gradient-based One-Side Sampling)\n\nGOSS improves speed by:\n\nRetaining all instances with large gradients (i.e., high error)\nRandomly sampling those with small gradients\n\nThis reduces the dataset size while maintaining accurate split decisions.\n\nIn gradient boosting, the tree is fit to the negative gradient of the loss:\n\\[\nr_m = -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f = f_{m-1}}\n\\]\nObservations with larger gradients have more influence on reducing the loss ‚Äî GOSS prioritizes those. This approach reduces the number of data points processed per iteration, speeding up training while preserving important information.\n\n\n11.2.2.3 Exclusive Feature Bundling (EFB)\n\nEFB reduces memory usage and accelerates training by bundling mutually exclusive features (i.e., features that rarely have non-zero values simultaneously) in high-dimensional sparse feature spaces.\nThis is particularly effective for datasets with many categorical variables or one-hot encoded features, avoiding the memory overhead of one-hot encoding and complementing LightGBM‚Äôs native categorical support.\n\nExample:\nThe table below shows two mutually exclusive features, feature1 and feature2, bundled into a single feature_bundle by assigning distinct value ranges (e.g., 1‚Äì4 for feature1, 5‚Äì6 for feature2):\n\n\n\nfeature1\nfeature2\nfeature_bundle\n\n\n\n\n0\n2\n6\n\n\n0\n1\n5\n\n\n0\n2\n6\n\n\n1\n0\n1\n\n\n2\n0\n2\n\n\n3\n0\n3\n\n\n4\n0\n4\n\n\n\n\nHyperparameter for EFB:\n\nenable_bundle: Enabled by default to activate automatic bundling.\nmax_conflict_rate: Controls the maximum conflict rate for bundling (default: 0.0, no conflicts allowed); adjust (e.g., 0.1) to allow minor overlaps.\n\n\nThis approach reduces the number of features processed per iteration, speeding up training while preserving important information.\nCombined with GOSS, EFB makes LightGBM especially well-suited for large-scale, sparse, tabular datasets, offering speed and scalability while maintaining comparable accuracy with proper tuning.\n\n\n\n11.2.3 Using LightGBM\nAlthough LightGBM is not part of Scikit-learn, it provides a Scikit-learn-compatible API through the lightgbm.sklearn module. This allows you to use LightGBM models seamlessly with Scikit-learn tools such as Pipeline, GridSearchCV, and cross_val_score.\nThe main classes are:\n\nLGBMRegressor: for regression tasks\n\nLGBMClassifier: for classification tasks\n\nTo install the package:\npip install lightgbm\n\nNote: LightGBM is a separate library, not part of Scikit-learn, but it provides a Scikit-learn-compatible API via LGBMClassifier and LGBMRegressor.\nThis makes it easy to integrate LightGBM models into Scikit-learn workflows such as Pipeline, GridSearchCV, and cross_val_score.\n\n\n11.2.3.1 Core LightGBM Hyperparameters\nCore Tree Structure:\n\nnum_leaves: Maximum number of leaves (terminal nodes) per tree.\nmin_data_in_leaf: Minimum number of data points required in a leaf.\nmax_depth: Maximum depth of a tree (used to control overfitting).\n\nLearning Control and Regularization:\n\nlearning_rate (Œ∑): Shrinks the contribution of each tree.\nn_estimators: Number of boosting rounds.\nlambda_l1 / lambda_l2: L1 and L2 regularization on leaf weights.\nmin_gain_to_split: Minimum loss reduction required to make a further split (structure regularization).\n\nData Handling:\n\nfeature_fraction: Fraction of features randomly sampled for each tree (a.k.a. colsample_bytree in XGBoost).\nbagging_fraction: Fraction of data randomly sampled for each iteration.\nbagging_freq: Frequency (in iterations) to perform bagging.\ncategorical_feature: Specifies which features are categorical (enables native handling).\n\nSpeed vs.¬†Accuracy Trade-offs:\n\nmax_bin: Number of bins used to bucket continuous features.\ndata_sample_strategy : bagging or goss\ntop_rate (goss only): Fraction of instances with the largest gradients to keep.\nother_rate (goss only): Fraction of small-gradient instances to randomly sample. -enable_bundle: set this to true to spped up the training for sparse datasets\n\nOptimization Control:\n\nboosting: Type of boosting algorithm (gbdt, dart, rf, etc.).\nearly_stopping_rounds: Stops training if the validation score doesn‚Äôt improve over a set number of rounds.\n\nImbalanced Data\n\nscale_pos_weight: Manually sets the weight for the positive class in binary classification.\nis_unbalance: Automatically adjusts class weights based on the training data distribution.\n\n\n‚ö†Ô∏è These two options are mutually exclusive ‚Äî use only one. If both are set, scale_pos_weight takes priority.\n\nFor full details and advanced options, see the LightGBM Parameters Guide.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import root_mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\nfrom xgboost import XGBRegressor, XGBClassifier\nimport lightgbm as lgb\nimport seaborn as sns\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_convergence\nimport warnings\n\nWe‚Äôll continue to use the same datasets that we have been using throughout the course.\n\n# Load the dataset\ncar = pd.read_csv('Datasets/car.csv')\ncar.head()\n\n\n\n\n\n\n\n\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\nvw\nBeetle\n2014\nManual\n55457\nDiesel\n30\n65.3266\n1.6\n7490\n\n\n1\nvauxhall\nGTC\n2017\nManual\n15630\nPetrol\n145\n47.2049\n1.4\n10998\n\n\n2\nmerc\nG Class\n2012\nAutomatic\n43000\nDiesel\n570\n25.1172\n3.0\n44990\n\n\n3\naudi\nRS5\n2019\nAutomatic\n10\nPetrol\n145\n30.5593\n2.9\n51990\n\n\n4\nmerc\nX-CLASS\n2018\nAutomatic\n14000\nDiesel\n240\n35.7168\n2.3\n28990\n\n\n\n\n\n\n\n\nX = car.drop(columns=['price'])\ny = car['price']\n\n# extract the categorical columns and put them in a list\ncategorical_feature = X.select_dtypes(include=['object']).columns.tolist()\n\n# extract the numerical columns and put them in a list\nnumerical_feature = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# convert the categorical columns to category type\nfor col in categorical_feature:\n    X[col] = X[col].astype('category')\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n11.2.3.2 Building a Baseline Model Using LightGBM‚Äôs Native Categorical Feature Support\nLightGBM provides built-in support for handling categorical features, eliminating the need for manual encoding (like one-hot or ordinal encoding). By directly passing categorical column names or indices to the model, LightGBM can internally apply efficient encoding and optimized split finding for categorical variables.\nIn this section, we‚Äôll use this native capability to quickly build a baseline model, taking advantage of LightGBM‚Äôs efficiency with structured data that includes categorical columns.\nThis baseline model serves as a starting point for comparison against more advanced tuning\n\n%%time\n# ===== 1. Baseline Model =====\nprint(\"\\n===== Baseline LightGBM Model =====\")\n# Initialize the LightGBM regressor\nmodel = lgb.LGBMRegressor(random_state=42)\n\n# Train the model with categorical features specified\nmodel.fit(\n    X_train, \n    y_train,\n    categorical_feature=categorical_feature\n)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Calculate evaluation metrics\nrmse = root_mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Output results\nprint(f\"Test RMSE: {rmse:.4f}\")\nprint(f\"Test R¬≤: {r2:.4f}\")\n\n\n===== Baseline LightGBM Model =====\nTest RMSE: 3680.8999\nTest R¬≤: 0.9538\nCPU times: total: 875 ms\nWall time: 82.6 ms\n\n\n\n\n11.2.3.3 Enabling GOSS and EFB in LightGBM\n\n11.2.3.3.1 GOSS is not enabled by default.\nThe default boosting type is gbdt (traditional Gradient Boosting Decision Tree), which uses all data instances for each iteration without sampling based on gradients.\nTo use GOSS, you must explicitly set the boosting_type parameter to goss in the model configuration. When you do this, LightGBM uses GOSS with default values for its specific hyperparameters:\n\ntop_rate: 0.2 (keeps 20% of instances with large gradients)\nother_rate: 0.1 (randomly samples 10% of instances with small gradients)\n\n\n\n11.2.3.3.2 EFB is enabled by default\nenable_bundle = True\nThis optimization reduces dimensionality by bundling mutually exclusive sparse features, such as those resulting from one-hot encoding.\n‚ö†Ô∏è Note: In our car dataset, the data size is small and there are only a few categorical features, so these optimizations may not have a noticeable impact. However, for large-scale datasets with many categorical features, enabling GOSS and EFB is highly recommended to improve training efficiency and reduce memory usage.\n\n%%time\n# ===== 2. LightGBM with GOSS Sampling =====\nprint(\"\\n===== LightGBM with GOSS Sampling =====\")\n\n# Initialize the LightGBM regressor with GOSS\nmodel_goss = lgb.LGBMRegressor(\n    boosting_type='goss',\n    random_state=42\n)\n\n# Train the model with categorical features specified\nmodel_goss.fit(\n    X_train,\n    y_train,\n    categorical_feature=categorical_feature\n)\n\n# Predict on the test set\ny_pred_goss = model_goss.predict(X_test)\n\n# Calculate evaluation metrics\nrmse_goss = root_mean_squared_error(y_test, y_pred_goss)\nr2_goss = r2_score(y_test, y_pred_goss)\n\n# Output results\nprint(f\"Test RMSE (GOSS): {rmse_goss:.4f}\")\nprint(f\"Test R¬≤ (GOSS): {r2_goss:.4f}\")\n\n\n===== LightGBM with GOSS Sampling =====\nTest RMSE (GOSS): 3510.7726\nTest R¬≤ (GOSS): 0.9580\nCPU times: total: 766 ms\nWall time: 79.6 ms\n\n\n\n\n\n11.2.3.4 Tuning top_rate and other_rate in GOSS\nEven with this small dataset, we observed a shorter execution time and a slight improvement in performance using GOSS.\nThe default settings are reasonable for many datasets. To leverage GOSS more effectively, optimize performance by tuning top_rate (e.g., 0.1, 0.2, 0.3, 0.4) and other_rate (e.g., 0.05, 0.1, 0.15, 0.2) using cross-validation, especially for large or noisy datasets.\n\n‚ö†Ô∏è Note: When using boosting_type='goss', LightGBM requires that\ntop_rate + other_rate ‚â§ 1.0\nThis constraint ensures that the combined sample used for training does not exceed the size of the full dataset.\n\n\n# tuning the top_rate and other_rate parameters\n# Initialize the LightGBM regressor with GOSS\nmodel_goss_tune = lgb.LGBMRegressor(\n    boosting_type='goss',\n    random_state=42\n)\n# Define the parameter grid for tuning\nparam_grid = {\n    'top_rate': Real(0.1, 0.6, prior='uniform'),\n    'other_rate': Real(0.1, 0.4, prior='uniform'),\n}\n# Initialize the BayesSearchCV object\nopt = BayesSearchCV(\n    model_goss_tune,\n    param_grid,\n    n_iter=10,\n    cv=3,\n    n_jobs=-1,\n    random_state=42\n)\n# Fit the model\nopt.fit(\n    X_train,\n    y_train,\n    categorical_feature=categorical_feature\n)\n# the best parameters\nprint(\"Best parameters found: \", opt.best_params_)\n\n# Predict on the test set\ny_pred_opt = opt.predict(X_test)\n\n# Calculate evaluation metrics\nrmse_opt = root_mean_squared_error(y_test, y_pred_opt)\nr2_opt = r2_score(y_test, y_pred_opt)\n# Output results\nprint(f\"Test RMSE (GOSS with tuning): {rmse_opt:.4f}\")\nprint(f\"Test R¬≤ (GOSS with tuning): {r2_opt:.4f}\")\n\nBest parameters found:  OrderedDict({'other_rate': 0.33986603248215197, 'top_rate': 0.31901459322046166})\nTest RMSE (GOSS with tuning): 3458.7664\nTest R¬≤ (GOSS with tuning): 0.9592\n\n\n\n\n11.2.3.5 Optimizing LightGBM with BayesSearchCV\nBayesSearchCV from scikit-optimize provides an efficient way to tune hyperparameters. Here‚Äôs how to set this up:\n\n%%time\n# ===== 2. Hyperparameter Tuning with Bayesian Optimization =====\n# Define the parameter space for Bayesian optimization\nparam_space = {\n    'num_leaves': Integer(20, 100),\n    'max_depth': Integer(5, 50),\n    'min_data_in_leaf': Integer(1, 100),\n    'learning_rate': Real(0.01, 0.5, prior='uniform'),\n    'n_estimators': Integer(50, 500),\n    'top_rate': Real(0.1, 0.6, prior='uniform'),\n    'other_rate': Real(0.1, 0.4, prior='uniform'),\n}\n# Create the Bayesian search object\nbayes_search = BayesSearchCV(\n    # using verbose=-1 to suppress warnings\n    # using n_jobs=-1 to use all available cores\n    # using random_state=42 for reproducibility\n    estimator=lgb.LGBMRegressor( categorical_feature=categorical_feature, random_state=42, boosting_type='goss', verbose=-1),\n    # Define the parameter space for Bayesian optimization\n    search_spaces=param_space,\n    n_iter=50,\n    scoring='neg_root_mean_squared_error',\n    cv=3,\n    n_jobs=-1,\n    random_state=42\n)\n# Fit the Bayesian search object to the training data\nbayes_search.fit(X_train, y_train)\n# Get the best parameters and score\nbest_params = bayes_search.best_params_\nbest_score = bayes_search.best_score_\nprint(f\"Best Parameters: {best_params}\")\nprint(f\"Best Score: {best_score}\")\n# Get the best model\nbest_model = bayes_search.best_estimator_\n# Make predictions on the test set\ny_pred_bayes = best_model.predict(X_test)\n# Calculate RMSE and R2 score for the best model\nrmse_bayes = root_mean_squared_error(y_test, y_pred_bayes)\nr2_bayes = r2_score(y_test, y_pred_bayes)\nprint(f\"RMSE (Bayesian Optimized): {rmse_bayes}\")\nprint(f\"R2 Score (Bayesian Optimized): {r2_bayes}\")\n\nBest Parameters: OrderedDict({'learning_rate': 0.31777940485083805, 'max_depth': 5, 'min_data_in_leaf': 47, 'n_estimators': 369, 'num_leaves': 20, 'other_rate': 0.4, 'top_rate': 0.6})\nBest Score: -3361.8218393725633\nRMSE (Bayesian Optimized): 3071.418344800289\nR2 Score (Bayesian Optimized): 0.9678447743461689\nCPU times: total: 49.4 s\nWall time: 1min 35s\n\n\nLightGBM achieved performance comparable to XGBoost. By leveraging GOSS (Gradient-based One-Side Sampling) for gradient sampling and EFB (Exclusive Feature Bundling) for feature reduction, it improved training speed slightly on large, sparse datasets. These optimizations, along with native categorical feature support, can also help reduce cross-validation tuning time by simplifying the feature space and accelerating learning.",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#catboost",
    "href": "LightGBM_CatBoost.html#catboost",
    "title": "11¬† LightGBM and CatBoost",
    "section": "11.3 CatBoost",
    "text": "11.3 CatBoost\n\n11.3.1 What is CatBoost?\nCatBoost (short for Categorical Boosting) is a high-performance gradient boosting framework developed by Yandex. While several modern boosting frameworks support native categorical features, CatBoost uses optimized encoding strategies‚Äîsuch as ordered target statistics‚Äîthat often lead to better performance with less risk of overfitting on categorical-heavy data.\nIn addition to its categorical handling, CatBoost includes features like ordered boosting and strong regularization, which help reduce overfitting. It typically requires less hyperparameter tuning than XGBoost or LightGBM, making it more user-friendly, especially on datasets with many categorical variables.\n\n\n11.3.2 What Makes CatBoost Unique?\nCatBoost introduces several key innovations that set it apart from other gradient boosting frameworks:\n\n11.3.2.1 Symmetric (Oblivious) Trees\nCatBoost builds symmetric (oblivious) decision trees, where the same feature and split threshold are used at each level of the tree across all nodes. This structure results in:\n\nRobust to noise\nImproved regularization\nFaster inference times\n\n\n\n11.3.2.2 Advanced Categorical Feature Handling\nCatBoost can natively process categorical features using an approach based on ordered target statistics, which:\n\nAvoids target leakage during training\nTypically outperforms traditional encodings like one-hot or label encoding\n\n\n\n11.3.2.3 Ordered Boosting (vs.¬†Standard Boosting)\nTraditional gradient boosting algorithms often suffer from prediction shift, a form of overfitting that occurs when the model uses the same data to compute residuals and to fit new trees.\nCatBoost addresses this with ordered boosting, a permutation-driven strategy that builds each tree on one subset of data and computes residuals on another (unseen) subset.\nRecall that gradient boosting fits trees on the gradient of the loss function:\n\\[\nr_m = -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f = f_{m-1}}\n\\]\nIn classic boosting, this gradient is calculated using the same training observations that were used to fit the model, which leads to target leakage.\nIn contrast, CatBoost:\n\nShuffles the data at each iteration\nComputes residuals for an observation only from prior observations in the permutation\nEnsures that each gradient estimate is based on unseen data\n\nThis significantly improves the model‚Äôs generalizability and reduces overfitting, especially on small or noisy datasets.\n\n\n11.3.2.4 Handling of Text and Embedding Features\nCatBoost can process text features directly by converting them into numerical representations (e.g., using bag-of-words or embeddings) within the model, reducing the need for external preprocessing. It also supports integration with pre-trained embeddings, which is useful for natural language processing (NLP) tasks.\nTogether, these innovations make CatBoost a strong candidate for modeling high-dimensional, categorical, and imbalanced tabular data, even with minimal feature engineering or hyperparameter tuning.\n\n\n11.3.2.5 Ease of Use and Defaults\nCatBoost‚Äôs default hyperparameters are well-tuned for a wide range of problems, reducing the need for extensive tuning. For example, its learning rate, depth, and regularization parameters often yield strong performance out of the box. In the paper, the authors also showed that CatBoost outperforms XGBoost and LightGBM without tuning, i.e., with default hyperparameter settings.\nRead the CatBoost paper for more details.\nHere is a good blog listing the key features of CatBoost.\n\n\n\n11.3.3 Using CatBoost\nCatBoost provides a scikit-learn-compatible API through CatBoostClassifier and CatBoostRegressor, which makes it easy to integrate into pipelines and use with tools like GridSearchCV, cross_val_score, and train_test_split.\n\n\n11.3.4 Installation\nTo install CatBoost, run:\npip install catboost\n\nüí° GPU users: CatBoost automatically detects and uses GPU if available. You can explicitly enable it with task_type='GPU'.\n\n\n\n11.3.5 CatBoost for Regression\nLet us check the performance of CatBoostRegressor() without tuning, i.e., with default hyperparameter settings on our car dataset\nThe parameter cat_features will be used to specify the indices of the categorical predictors for target encoding.\n\n# build a catboostregressor model\nfrom catboost import CatBoostRegressor\n# Initialize the CatBoost regressor\nmodel_cat = CatBoostRegressor(\n    cat_features=categorical_feature,\n    random_seed=42,\n    verbose=0\n)\n\n# Train the model\nmodel_cat.fit(X_train, y_train)\n# Predict on the test set\ny_pred_cat = model_cat.predict(X_test)\n# Calculate evaluation metrics\nrmse_cat = root_mean_squared_error(y_test, y_pred_cat)\nr2_cat = r2_score(y_test, y_pred_cat)\n# Output results\nprint(f\"Test RMSE (CatBoost): {rmse_cat:.4f}\")\nprint(f\"Test R¬≤ (CatBoost): {r2_cat:.4f}\")\n\nTest RMSE (CatBoost): 3307.2604\nTest R¬≤ (CatBoost): 0.9627\n\n\nEven with default hyperparameter settings, CatBoost has outperformed both XGBoost and LightGBM in terms of test RMSE and R-squared.\n\n\n11.3.6 Tuning CatBoostRegressor\nYou can tune the hyperparameters of CatBoostRegressor using Optuna or other tuning strategies, just as you would for XGBoost or LightGBM. However, CatBoost has a distinct set of hyperparameters, reflecting its unique design choices.\n\n11.3.6.1 ‚ùå Hyperparameters not used in CatBoost:\n\nreg_alpha: CatBoost does not support L1 regularization on leaf weights; it uses only L2 regularization (l2_leaf_reg).\ncolsample_bytree: CatBoost does not use this parameter; it uses rsm and handles feature selection differently.\n\nThese parameters are common in XGBoost and LightGBM but are not part of CatBoost‚Äôs configuration.\n\n\n11.3.6.2 ‚úÖ Unique Hyperparameters in CatBoost\nCatBoost introduces several hyperparameters related to categorical feature handling and ordered boosting:\n\none_hot_max_size: Threshold for switching between one-hot encoding and target encoding for categorical features.\nboosting_type='Ordered': Ordered boosting is enabled by default in CatBoost to reduce overfitting and prevent prediction shift.\nbootstrap_type='Bayesian': Default bootstrap method. Works well with ordered boosting.\nbagging_temperature: Works with bootstrap_type='Bayesian'.\nControls how sharply bootstrap weights are distributed:\n\nLow values (e.g., 0): more uniform sampling (close to deterministic).\nHigh values (e.g., 1, 5, 10): more aggressive sampling‚Äîsome rows are weighted more heavily.\n\nrandom_strength: Adds randomness to the split selection score, especially useful for regularizing categorical splits.\nrsm: Random selection rate for column sampling.\n\nThese CatBoost-specific hyperparameters are important when fine-tuning with Cross-Validation, particularly for datasets with many categorical features or at risk of overfitting.\n\nimport optuna\nfrom optuna import create_study\nfrom catboost import CatBoostRegressor, Pool\n\n# create a validation set for early stopping\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n\n#convert to Catboost pool\ntrain_pool = Pool(X_train, y_train, cat_features=categorical_feature)\nvalid_pool = Pool(X_valid, y_valid, cat_features=categorical_feature)\n\n# Define the objective function for Optuna\ndef objective(trial):\n    # Define the hyperparameters to tune\n    params = {\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 30),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n        'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n\n        # Fixed parameters\n        'iterations': 3000,  # Set to a high number, early stopping will determine the actual number\n        'verbose': False,\n        'random_seed': 42\n    }\n    \n    # Create and train the model with early stopping\n    model = CatBoostRegressor(**params)\n    \n    # Use early stopping to prevent overfitting\n    model.fit(\n        train_pool,\n        eval_set=valid_pool,\n        early_stopping_rounds=20,  # Stop if no improvement for 50 rounds\n        verbose=False,\n        n_jobs=-1\n    )\n    \n    # Evaluate on validation set\n    y_pred = model.predict(valid_pool)\n    val_rmse = root_mean_squared_error(y_valid, y_pred)\n    \n    # Return negative RMSE (for maximization)\n    return -val_rmse\n\n# Create and run the study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n\n[I 2025-05-16 06:53:24,620] A new study created in memory with name: no-name-2b89c789-86c1-45aa-9511-7e7990a5767a\n[I 2025-05-16 06:53:36,438] Trial 0 finished with value: -2885.7311361568736 and parameters: {'learning_rate': 0.12726746487613003, 'depth': 10, 'l2_leaf_reg': 0.00172009052841597, 'min_data_in_leaf': 13, 'bagging_temperature': 0.21730485409197553, 'random_strength': 0.0002251704542179119}. Best is trial 0 with value: -2885.7311361568736.\n[I 2025-05-16 06:54:07,518] Trial 1 finished with value: -2620.171198484414 and parameters: {'learning_rate': 0.05369228812155998, 'depth': 7, 'l2_leaf_reg': 2.5645654686725287e-07, 'min_data_in_leaf': 28, 'bagging_temperature': 0.03449070019194467, 'random_strength': 2.200162833623553}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:55:01,902] Trial 2 finished with value: -2696.8294539080816 and parameters: {'learning_rate': 0.02548811714993481, 'depth': 8, 'l2_leaf_reg': 0.0023811609906064252, 'min_data_in_leaf': 13, 'bagging_temperature': 0.9288401488970826, 'random_strength': 1.082500115732704}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:55:22,721] Trial 3 finished with value: -2724.528794214105 and parameters: {'learning_rate': 0.06451150035785631, 'depth': 9, 'l2_leaf_reg': 4.5131464056397556e-08, 'min_data_in_leaf': 30, 'bagging_temperature': 0.5960655866129079, 'random_strength': 9.28215317432208e-07}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:55:36,587] Trial 4 finished with value: -2878.820243617394 and parameters: {'learning_rate': 0.11527331094852836, 'depth': 4, 'l2_leaf_reg': 0.4475951626701213, 'min_data_in_leaf': 20, 'bagging_temperature': 0.7324000256379442, 'random_strength': 0.4338664457953787}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:55:49,775] Trial 5 finished with value: -2668.619984949151 and parameters: {'learning_rate': 0.1855528089325658, 'depth': 6, 'l2_leaf_reg': 1.8378952869939127e-05, 'min_data_in_leaf': 24, 'bagging_temperature': 0.5768414336451607, 'random_strength': 0.010522014874970544}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:55:57,119] Trial 6 finished with value: -2912.272947098299 and parameters: {'learning_rate': 0.2451855882147167, 'depth': 10, 'l2_leaf_reg': 1.7673885188960817e-06, 'min_data_in_leaf': 10, 'bagging_temperature': 0.4334470051053827, 'random_strength': 0.11406026772554442}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:56:13,622] Trial 7 finished with value: -2695.2648598625924 and parameters: {'learning_rate': 0.09755102190100745, 'depth': 10, 'l2_leaf_reg': 9.566125114415794e-08, 'min_data_in_leaf': 19, 'bagging_temperature': 0.9211592949414568, 'random_strength': 2.436347090461743e-08}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:56:26,903] Trial 8 finished with value: -2753.084945654914 and parameters: {'learning_rate': 0.18843128663283498, 'depth': 5, 'l2_leaf_reg': 5.835652831494221e-06, 'min_data_in_leaf': 6, 'bagging_temperature': 0.7987698449327993, 'random_strength': 0.06362728767715319}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:56:34,769] Trial 9 finished with value: -2921.4504143784643 and parameters: {'learning_rate': 0.22960449698497942, 'depth': 10, 'l2_leaf_reg': 1.1782309803914464e-08, 'min_data_in_leaf': 22, 'bagging_temperature': 0.7169193876429558, 'random_strength': 0.023405106337219345}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:56:45,103] Trial 10 finished with value: -2797.656433395867 and parameters: {'learning_rate': 0.29638954558703867, 'depth': 7, 'l2_leaf_reg': 7.121156648133439, 'min_data_in_leaf': 1, 'bagging_temperature': 0.028555585214749657, 'random_strength': 0.00013397670124843048}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:57:00,983] Trial 11 finished with value: -2987.804912052353 and parameters: {'learning_rate': 0.17465375823124038, 'depth': 6, 'l2_leaf_reg': 3.169840926062148e-05, 'min_data_in_leaf': 29, 'bagging_temperature': 0.40829527505593155, 'random_strength': 7.361593602808678}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:58:30,211] Trial 12 finished with value: -2656.269812196844 and parameters: {'learning_rate': 0.017073325011989847, 'depth': 7, 'l2_leaf_reg': 0.00018641156280634486, 'min_data_in_leaf': 25, 'bagging_temperature': 0.2504148253088835, 'random_strength': 0.0036050209677933906}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:59:27,791] Trial 13 finished with value: -2744.3027101379294 and parameters: {'learning_rate': 0.01817177055582332, 'depth': 7, 'l2_leaf_reg': 0.03706735720453296, 'min_data_in_leaf': 25, 'bagging_temperature': 0.002547331855073512, 'random_strength': 2.0356833250740667e-05}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 06:59:53,265] Trial 14 finished with value: -2697.6837955182573 and parameters: {'learning_rate': 0.06236194061749396, 'depth': 8, 'l2_leaf_reg': 5.295910383215519e-07, 'min_data_in_leaf': 27, 'bagging_temperature': 0.20340714193628523, 'random_strength': 0.0018522086502427496}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 07:00:21,452] Trial 15 finished with value: -2766.897166940931 and parameters: {'learning_rate': 0.061014851192040906, 'depth': 6, 'l2_leaf_reg': 0.00013014280192782276, 'min_data_in_leaf': 17, 'bagging_temperature': 0.21609021631792624, 'random_strength': 9.66330609620808}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 07:01:00,089] Trial 16 finished with value: -2731.9161533728666 and parameters: {'learning_rate': 0.027412623287634285, 'depth': 8, 'l2_leaf_reg': 0.0010697541231850665, 'min_data_in_leaf': 26, 'bagging_temperature': 0.1205465052440132, 'random_strength': 9.050362919853505e-06}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 07:01:13,872] Trial 17 finished with value: -2950.387762628911 and parameters: {'learning_rate': 0.06559997537771182, 'depth': 5, 'l2_leaf_reg': 0.03303703827909148, 'min_data_in_leaf': 22, 'bagging_temperature': 0.33823889215394143, 'random_strength': 0.004091235937143242}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 07:01:24,805] Trial 18 finished with value: -2799.2851339120025 and parameters: {'learning_rate': 0.08913764405155444, 'depth': 7, 'l2_leaf_reg': 4.0696494316792653e-07, 'min_data_in_leaf': 30, 'bagging_temperature': 0.3238078952128749, 'random_strength': 0.0012282635994702415}. Best is trial 1 with value: -2620.171198484414.\n[I 2025-05-16 07:01:36,339] Trial 19 finished with value: -2772.5541221320145 and parameters: {'learning_rate': 0.14059069364937926, 'depth': 9, 'l2_leaf_reg': 9.039692246883738e-05, 'min_data_in_leaf': 23, 'bagging_temperature': 0.07579428007376465, 'random_strength': 0.5896255350389338}. Best is trial 1 with value: -2620.171198484414.\n\n\n\n# Get best parameters and train final model with early stopping\nbest_params = study.best_params\nprint(\"Best parameters:\", best_params)\n\n# Get the best trial\nbest_trial = study.best_trial\nprint(\"Best trial:\", best_trial)\n\nBest parameters: {'learning_rate': 0.05369228812155998, 'depth': 7, 'l2_leaf_reg': 2.5645654686725287e-07, 'min_data_in_leaf': 28, 'bagging_temperature': 0.03449070019194467, 'random_strength': 2.200162833623553}\nBest trial: FrozenTrial(number=1, state=1, values=[-2620.171198484414], datetime_start=datetime.datetime(2025, 5, 16, 6, 53, 36, 439603), datetime_complete=datetime.datetime(2025, 5, 16, 6, 54, 7, 518257), params={'learning_rate': 0.05369228812155998, 'depth': 7, 'l2_leaf_reg': 2.5645654686725287e-07, 'min_data_in_leaf': 28, 'bagging_temperature': 0.03449070019194467, 'random_strength': 2.200162833623553}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.3, log=False, low=0.01, step=None), 'depth': IntDistribution(high=10, log=False, low=4, step=1), 'l2_leaf_reg': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'min_data_in_leaf': IntDistribution(high=30, log=False, low=1, step=1), 'bagging_temperature': FloatDistribution(high=1.0, log=False, low=0.0, step=None), 'random_strength': FloatDistribution(high=10.0, log=True, low=1e-08, step=None)}, trial_id=1, value=None)\n\n\n\n# Use column indices instead of names\ncat_feature_indices = [X_train.columns.get_loc(col) for col in categorical_feature]\n\n# Add iterations parameter back for final model\nbest_params['iterations'] = 3000  # High number, early stopping will be used\n\n# create a train+validation set for final model\ntrain_val_pool = Pool(\n    np.vstack((X_train, X_valid)),\n    np.concatenate((y_train, y_valid)),\n    cat_features=cat_feature_indices\n)\n\n# Create a test pool\ntest_pool = Pool(X_test, y_test, cat_features=categorical_feature)\n\n# Train final model on combined train+validation data\nfinal_model = CatBoostRegressor(**best_params)\nfinal_model.fit(\n    train_val_pool,\n    eval_set=test_pool,\n    early_stopping_rounds=50,\n    verbose=False\n)\n\n# Get actual number of trees used after early stopping\nactual_iterations = final_model.tree_count_\nprint(f\"Actual number of trees used: {actual_iterations}\")\n\n# Evaluate on test set\ny_pred_test = final_model.predict(X_test)\ntest_rmse = root_mean_squared_error(y_test, y_pred_test)\ntest_r2 = r2_score(y_test, y_pred_test)\nprint(f\"Test RMSE: {test_rmse:.4f}\")\nprint(f\"Test R¬≤: {test_r2:.4f}\")\n\nActual number of trees used: 1021\nTest RMSE: 3147.8477\nTest R¬≤: 0.9662\n\n\n\nfig1 = optuna.visualization.plot_optimization_history(study)\nfig1.show()\n    \nfig2 = optuna.visualization.plot_param_importances(study)\nfig2.show()\n    \n# Plot feature importance from the final model\n# Get feature importance values\nfeature_importance = final_model.get_feature_importance()\n\n# Get sorted indices\nsorted_idx = np.argsort(feature_importance)\n\n# Plot\nplt.figure(figsize=(5, 7))\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n\n# Use feature names if X is a DataFrame\nfeature_names = X.columns if hasattr(X, 'columns') else np.array(range(X.shape[1]))\nplt.yticks(range(len(sorted_idx)), feature_names[sorted_idx])\n\nplt.title('CatBoost Feature Importance')\nplt.tight_layout()\nplt.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\n\n\n\n\nCheck the documentation for hyperparameter tuning.\nWith proper hyperparameter tuning, CatBoost can achieve better performance than its default settings. However, even without tuning, CatBoost‚Äôs default configuration often outperforms the default settings of XGBoost and LightGBM, particularly on datasets with categorical features.\n\n\n\n11.3.7 When to Use CatBoost Over XGBoost\n\nWhen your dataset is ‚Äúcategorical-heavy**\nCatBoost tends to perform well out of the box with minimal hyperparameter tuning, making it more user-friendly for quick experimentation or deployment\n\nCatBoost‚Äôs GPU implementation is optimized for handling categorical data efficiently, and can outperform XGBoost on datasets dominated by categorical variables\n&gt; While both libraries support GPU acceleration, CatBoost‚Äôs architecture is particularly well-suited for categorical-heavy tasks",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost",
    "href": "LightGBM_CatBoost.html#handling-imbalanced-classification-xgboost-vs.-lightgbm-vs.-catboost",
    "title": "11¬† LightGBM and CatBoost",
    "section": "11.4 Handling Imbalanced Classification: XGBoost vs.¬†LightGBM vs.¬†CatBoost",
    "text": "11.4 Handling Imbalanced Classification: XGBoost vs.¬†LightGBM vs.¬†CatBoost\nImbalanced classification occurs when one class significantly outnumbers the other (e.g., fraud detection, disease diagnosis). Each boosting library offers tools to address this issue:\nXGBoost:\n\nParameter: scale_pos_weight\n\nFormula:\n\\[\n\\texttt{scale\\_pos\\_weight} = \\frac{\\text{Number of negative samples}}{\\text{Number of positive samples}}\n\\]\nIncreases the gradient of the positive class during training.\n\nAdditional Strategies:\n\nUse custom eval_metric (e.g., \"auc\", \"aucpr\", or \"logloss\")\nApply early stopping on validation AUC\n\n\nLightGBM:\n\nParameter: scale_pos_weight (same as in XGBoost)\nAlternative: is_unbalance = TRUE\n\nAutomatically adjusts class weights based on distribution\n\nOther Tips:\n\nUse metric = \"auc\" or \"binary_logloss\" for better guidance during training\nResampling techniques also compatible\n\n\nCatBoost:\n\nParameter: class_weights\n\nAccepts a numeric vector (e.g., class_weights = c(1, 5) for [negative, positive])\nDirectly modifies the loss function to emphasize minority class\n\nAdvantages:\n\nMore flexible than scale_pos_weight\nWorks well with default settings\n\nOther Tips:\n\nUse loss_function = \"Logloss\" and eval_metric = \"AUC\" for binary classification\n\n\nBelow is the summary table:\n\n\n\n\n\n\n\n\n\nLibrary\nImbalance Handling Parameter\nDefault Support\nRecommended Metric\n\n\n\n\nXGBoost\nscale_pos_weight\nNo\nauc, aucpr\n\n\nLightGBM\nscale_pos_weight, is_unbalance\nYes (with flag)\nauc, binary_logloss\n\n\nCatBoost\nclass_weights\nYes\nLogloss, AUC",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#summary-xgboost-vs.-lightgbm-vs.-catboost",
    "href": "LightGBM_CatBoost.html#summary-xgboost-vs.-lightgbm-vs.-catboost",
    "title": "11¬† LightGBM and CatBoost",
    "section": "11.5 Summary: XGBoost vs.¬†LightGBM vs.¬†CatBoost",
    "text": "11.5 Summary: XGBoost vs.¬†LightGBM vs.¬†CatBoost\nGradient boosting is a powerful ensemble technique, and XGBoost, LightGBM, and CatBoost are three of its most widely used implementations. Each has unique strengths and is well-suited to different use cases.\nXGBoost:\n\nStrengths: Robust, well-documented, strong performance on structured/tabular data\n\nSplit Finding: Level-wise tree growth\n\nRegularization: Explicit L1 and L2 regularization\n\nFlexibility: Highly customizable with many hyperparameters\n\nBest for: General-purpose tabular data, especially when you have time to tune parameters\n\nLightGBM:\n\nStrengths: Fast training, low memory usage, excellent scalability\n\nSplit Finding: Leaf-wise tree growth with depth control\n\nBinning: Uses histogram-based algorithm with max_bin to speed up training\n\nBest for: Large-scale datasets, high-dimensional features, and when training speed matters\n\nCatBoost:\n\nStrengths: Handles categorical features natively, works well with minimal tuning\n\nBoosting Innovation: Uses ordered boosting to prevent prediction shift\n\nCategorical Encoding: Uses target-based encoding internally\n\nBest for: Datasets with many categorical variables or limited time for tuning\n\nFinal Thoughts\nAll three libraries are powerful and battle-tested. Here‚Äôs a rough guideline:\n\nUse XGBoost if you want control, flexibility, and a well-documented standard\nUse LightGBM when training speed and large data scalability are your top priorities\nUse CatBoost when working with many categorical features or seeking strong baseline results with minimal tuning",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  },
  {
    "objectID": "LightGBM_CatBoost.html#references",
    "href": "LightGBM_CatBoost.html#references",
    "title": "11¬† LightGBM and CatBoost",
    "section": "11.6 References",
    "text": "11.6 References\n\nLightGBM Paper (Original NIPS 2017)\nLightGBM Official Website\nCatBoost Paper (arXiv)\nCatBoost Official Website",
    "crumbs": [
      "Tree based models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>LightGBM and CatBoost</span>"
    ]
  }
]